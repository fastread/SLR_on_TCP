"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Study on the cost/benefit/optimization of software safety test","Meng Li; Zhu Xu","Dept. of Comput. Sci. & Eng., Tongji Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Tongji Univ., Shanghai, China","2003 Test Symposium","","2003","","","510","","Although the safety-critical system has high demand on safety, the cost of software test therefore must be taken account of. In the test of railway computer interlocking software carried out, the safety test for a station software last several months, therefore, in order to reduce the test time, it is practical to choose functions from the function set to test through optimization. Software safety test is realized by running testing cases at the cost of labor and time. It is expected to detect dangerous function defects and reduce system loss to gain benefit. Optimization strategy is a best choice to consider testing cases.","1081-7735","0-7695-1951","10.1109/ATS.2003.1250881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250881","","Software safety;Software testing;Linear programming;Software economics;Costs;Optimization methods","safety-critical software;program testing;linear programming;software cost estimation;cost-benefit analysis;optimisation;railway safety","software safety test;safety-critical software;software test cost;station software;test optimization;dangerous function defects;railway interlocking software;linear programming","","","","","","","","","IEEE","IEEE Conferences"
"ADTEST: a test data generation suite for Ada software systems","M. J. Gallagher; V. Lakshmi Narasimhan","Sch. of Inf. Technol., Queensland Univ., Qld., Australia; NA","IEEE Transactions on Software Engineering","","1997","23","8","473","484","Presents the design of the software system ADTEST (ADa TESTing), for generating test data for programs developed in Ada83. The key feature of this system is that the problem of test data generation is treated entirely as a numerical optimization problem and, as a consequence, this method does not suffer from the difficulties commonly found in symbolic execution systems, such as those associated with input variable-dependent loops, array references and module calls. Instead, program instrumentation is used to solve a set of path constraints without explicitly knowing their form. The system supports not only the generation of integer and real data types, but also non-numerical discrete types such as characters and enumerated types. The system has been tested on large Ada programs (60,000 lines of code) and found to reduce the effort required to test programs as well as providing an increase in test coverage.","0098-5589;1939-3520;2326-3881","","10.1109/32.624304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624304","","Software testing;System testing;Software systems;Software design;Instruments;Optimization methods;Character generation;Inspection;Software tools;Production","program testing;Ada;software tools;optimisation;abstract data types","ADTEST;software test data generation suite;Ada83 software systems;numerical optimization;symbolic execution systems;program instrumentation;path constraints;integer data type;real data type;nonnumerical discrete types;characters;enumerated types;test coverage","","68","15","","","","","","IEEE","IEEE Journals & Magazines"
"Parametric test engineering optimization: methodology and software system","T. T. d'Ouville; F. Mendez; J. Bruines; L. Zangara; G. Durieu","Centre Commun CNET SGS-Thomson, Crolles, France; Centre Commun CNET SGS-Thomson, Crolles, France; Centre Commun CNET SGS-Thomson, Crolles, France; NA; NA","Proceedings of International Conference on Microelectronic Test Structures","","1996","","","185","189","To improve drastically the productivity and the quality of parametric testing, we have defined a new design methodology based on a global and concurrent approach. A CAD system has been developed allowing the automatic generation of all the parametric test tools and the secured transfer of all information about the test chip to the parametric tester. This CAD solution has been progressively implemented in the Centre Commun CNET SGS-Thomson to design the test chips for CMOS 0.5 /spl mu/m, CMOS 0.35 /spl mu/m and BiCMOS technologies. Global indicators confirm the productivity and quality improvement of the parametric test design process.","","0-7803-2783","10.1109/ICMTS.1996.535643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=535643","","Software testing;Optimization methods;Automatic testing;System testing;Productivity;Design automation;Design methodology;BiCMOS integrated circuits;CMOS technology;Process design","CMOS integrated circuits;BiCMOS integrated circuits;integrated circuit testing;circuit CAD;circuit optimisation;integrated circuit design;automatic testing;automatic test software;graphical user interfaces;design for testability","parametric test engineering optimization;software system;design methodology;CAD system;automatic test tool generation;CMOS technologies;BiCMOS technology;PILOT Station;0.35 micron;0.5 micron","","","1","","","","","","IEEE","IEEE Conferences"
"Optimal allocation of testing resources for modular software systems","Chin-Yu Huang; Jung-Hua Lo; Sy-Yen Kuo; M. R. Lyu","Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA","13th International Symposium on Software Reliability Engineering, 2002. Proceedings.","","2002","","","129","138","In this paper, based on software reliability growth models with generalized logistic testing-effort function, we study three optimal resource allocation problems in modular software systems during the testing phase: 1) minimization of the remaining faults when a fixed amount of testing-effort and a desired reliability objective are given; 2) minimization of the required amount of testing-effort when a specific number of remaining faults and a desired reliability objective are given; and 3) minimization of the cost when the number of remaining faults and a desired reliability objective are given. Several useful optimization algorithms based on the Lagrange multiplier method are proposed and numerical examples are illustrated. Our methodologies provide practical approaches to the optimization of testing-resource allocation with a reliability objective. In addition, we also introduce the testing-resource control problem and compare different resource allocation methods. Finally, we demonstrate how these analytical approaches can be employed in the integration testing. Using the proposed algorithms, project managers can allocate limited testing-resource easily and efficiently and thus achieve the highest reliability objective during software module and integration testing.","1071-9458","0-7695-1763","10.1109/ISSRE.2002.1173228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173228","","Resource management;Software testing;System testing;Software systems;Optimization methods;Software reliability;Logistics;Cost function;Lagrangian functions;Software algorithms","program testing;resource allocation;software reliability;minimisation","optimal testing resource allocation;modular software systems;software reliability growth models;generalized logistic testing;program testing;fault minimization;optimization algorithms;Lagrange multiplier method;testing-resource control problem;integration testing;software module testing","","7","","","","","","","IEEE","IEEE Conferences"
"System test case prioritization of new and regression test cases","H. Srikanth; L. Williams; J. Osborne","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","2005 International Symposium on Empirical Software Engineering, 2005.","","2005","","","10 pp.","","Test case prioritization techniques have been shown to be beneficial for improving regression-testing activities. With prioritization, the rate of fault detection is improved, thus allowing testers to detect faults earlier in the system-testing phase. Most of the prioritization techniques to date have been code coverage-based. These techniques may treat all faults equally. We build upon prior test case prioritization research with two main goals: (1) to improve user-perceived software quality in a cost effective way by considering potential defect severity and (2) to improve the rate of detection of severe faults during system-level testing of new code and regression testing of existing code. We present a value-driven approach to system-level test case prioritization called the prioritization of requirements for test (PORT). PORT prioritizes system test cases based upon four factors: requirements volatility, customer priority, implementation complexity, and fault proneness of the requirements. We conducted a PORT case study on four projects developed by students in advanced graduate software testing class. Our results show that PORT prioritization at the system level improves the rate of detection of severe faults. Additionally, customer priority was shown to be one of the most important prioritization factors contributing to the improved rate of fault detection.","","0-7803-9507","10.1109/ISESE.2005.1541815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541815","","System testing;Computer aided software engineering;Software testing;Fault detection;Software quality;Costs;Software engineering;Computer science;Statistical analysis;Phase detection","program testing;software quality;software fault tolerance;regression analysis","system-level test case prioritization;regression test case;fault detection;code coverage-based prioritization technique;software quality;software defect severity;value-driven approach;prioritization of requirements for test;PORT;requirements volatility;customer priority;implementation complexity;fault proneness;software testing","","46","32","","","","","","IEEE","IEEE Conferences"
"Tutorial: organizing, managing and optimizing software testing","H. Schaefer","Software Test Consulting, Valestrandsfossen, Norway","Proceedings of Joint 4th International Computer Science Conference and 4th Asia Pacific Software Engineering Conference","","1997","","","532","533","This tutorial introduces the philosophy of software testing as it is defined by the current literature. One goal is to give ideas and guidance for improving testing, i.e. to find more serious defects earlier and at a lower cost. The other goal is to improve control over the testing process.","","0-8186-8271","10.1109/APSEC.1997.640220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=640220","","Tutorial;Organizing;Software testing;Costs;Materials testing;Delay estimation;Frequency estimation;Feedback;Prototypes;Thumb","program testing;software management","software testing;philosophy;serious defects;cost;testing process control","","","","","","","","","IEEE","IEEE Conferences"
"What's so different about deep-submicron test?","C. Hunter","Somerset Design Centre, Motorola Inc., Austin, TX, USA","Proceedings of 1995 IEEE International Test Conference (ITC)","","1995","","","924","","Functional verification of products is attetmpting to prove the absence of logical defects in a design, while test is attempting to prove the absence of physical defects in the same design. Both have many common needs; both are critical to the design and development of our products. Validating the state of a device or product has become all but impossible, without direct access to the internal state of the device. Scan based diagnostic techniques are now being used at the board, system and software debug and development stages of product development cycles; these are the same requirements that device test has always had. The implication is to enable the use of on-chip test structures and features for more than just the manufacturing process. As these on-chip features become more prevalent, their standardization and proliferation will be the norm rather than the exception. As such, advancements in diagnostics and built-in self verifying (and testing) devices and products, can and will become the dominant strategy optimizing time to market of our products, ultimately allowing our customers to becomes more successful.","1089-3539","0-7803-2992","10.1109/TEST.1995.529933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529933","","Logic testing;Software debugging;Software systems;Product development;Software testing;System testing;Manufacturing processes;Standardization;Automatic testing;Time to market","integrated circuit testing;automatic testing;design for testability;logic testing;built-in self test;standardisation;production testing","deep-submicron test;functional verification;logical defects;physical defects;scan based diagnostic techniques;software debugging;product development;on-chip test structures;standardization;built-in self verifying;diagnostics;time to market","","","","","","","","","IEEE","IEEE Conferences"
"Test process optimization: closing the gap in the defect spectrum","N. Barrett; S. Martin; C. Dislis","Sector of Network Solution, Motorola Ireland Ltd., Cork, Ireland; NA; NA","International Test Conference 1999. Proceedings (IEEE Cat. No.99CH37034)","","1999","","","124","129","This paper describes our methodology of tuning the test process of the Motorola Operations and Maintenance Center product to systematically reduce field defects. The benefits include improved test cases, reduced defects and the availability of up to date field data for feature verification.","1089-3539","0-7803-5753","10.1109/TEST.1999.805621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=805621","","Software testing;GSM;Intelligent networks;System testing;Time to market;Feedback;Cellular networks;Land mobile radio cellular systems;Base stations;Graphical user interfaces","software fault tolerance;program testing;software maintenance","test process optimization;defect spectrum;Motorola product test process;reduced field defects;feature verification;defect coverage;GSM cellular products;systematic trend analysis;software testing tuning model;requirements-derived test;orthogonal defect classification;mapping process gaps","","4","9","","","","","","IEEE","IEEE Conferences"
"Scenario based integration testing for object-oriented software development","Youngchul Kim; C. R. Carlson","Dept. of Comput. Sci., Illinois Inst. of Technol., Chicago, IL, USA; NA","Proceedings Eighth Asian Test Symposium (ATS'99)","","1999","","","283","288","The adaptive use case methodology for software development proposed by Carlson and Hurlbutt [1997, 1998] forms the backdrop for this paper. Their methodology integrates the software design, development and testing processes through a series of design preserving, algorithmic transformations. This paper focuses on the software testing metrics used in the generation of object oriented test plans as part of that methodology. During the design phase, interaction diagrams are developed from which use case action matrices are then generated. A use case action matrix contains a collection of related scenarios each describing a specific variant of an executable sequence of use case actions. The proposed software testing metrics are employed to improve the productivity of the testing process through scenario prioritization.","1081-7735","0-7695-0315","10.1109/ATS.1999.810764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810764","","Software testing;Programming;Algorithm design and analysis;Software algorithms;Electrical capacitance tomography;Software systems;Control systems;Computer science;Scholarships;Software design","object-oriented methods;software engineering;program testing","scenario based integration testing;object-oriented software development;adaptive use case methodology;design preserving algorithmic transformations;testing metrics;object oriented test plans;interaction diagrams;use case action matrices;executable sequence;productivity;scenario prioritization","","5","10","","","","","","IEEE","IEEE Conferences"
"Test case prioritization: an empirical study","G. Rothermel; R. H. Untch; Chengyun Chu; M. J. Harrold","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA","Proceedings IEEE International Conference on Software Maintenance - 1999 (ICSM'99). 'Software Maintenance for Business Change' (Cat. No.99CB36360)","","1999","","","179","188","Test case prioritization techniques schedule test cases for execution in an order that attempts to maximize some objective function. A variety of objective functions are applicable; one such function involves rate of fault detection-a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during regression testing can provide faster feedback on a system under regression test and let debuggers begin their work earlier than might otherwise be possible. In this paper we describe several techniques for prioritizing test cases and report our empirical results measuring the effectiveness of these techniques for improving rate of fault detection. The results provide insights into the tradeoffs among various techniques for test case prioritization.","1063-6773","0-7695-0016","10.1109/ICSM.1999.792604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792604","","Computer aided software engineering;Fault detection;System testing;Software testing;Computer science;Job shop scheduling;Feedback;Software maintenance;Information science;Ear","program testing","test cases;test case prioritization;objective function;regression test;debuggers;fault detection","","109","21","","","","","","IEEE","IEEE Conferences"
"Automated software test data generation for complex programs","C. Michael; G. McGraw","Reliable Software Technol., Sterling, VA, USA; NA","Proceedings 13th IEEE International Conference on Automated Software Engineering (Cat. No.98EX239)","","1998","","","136","146","We report on GADGET, a new software test generation system that uses combinatorial optimization to obtain condition/decision coverage of C/C++ programs. The GADGET system is fully automatic and supports all C/C++ language constructs. This allows us to generate tests for programs more complex than those previously reported in the literature. We address a number of issues that are encountered when automatically generating tests for complex software systems. These issues have not been discussed in earlier work on test-data generation, which concentrates on small programs (most often single functions) written in restricted programming languages.","","0-8186-8750","10.1109/ASE.1998.732605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=732605","","Automatic testing;Software testing;System testing;Performance evaluation;Instruments;Simulated annealing;Software systems;Prototypes;Genetic algorithms","program testing;optimisation;C language;C++ language;object-oriented programming","software test data generation;complex programs;GADGET;software test generation system;combinatorial optimization;condition decision coverage;C programs;C++ programs","","12","14","","","","","","IEEE","IEEE Conferences"
"A test process optimization and cost modeling tool","T. J. Moore","Digital Equipment Corp., Maynard, MA, USA","Proceedings., International Test Conference","","1994","","","103","110","This paper will describe a test process modeling tool developed by Digital Equipment Corporation in Maynard Mass. to be used by design and manufacturing process engineering to determine the most economic test process without sacrificing test coverage or yield. The test cost modeling tool can be used to understand the behavior of the test process and determine how to reduce the impact of test upon the total module manufacturing cost. This paper discusses the test process modeling tool and how it can be used to eliminate test process steps and optimize the test process.","1089-3539","0-7803-2103","10.1109/TEST.1994.527941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=527941","","Cost function;Manufacturing processes;Software testing;Databases;Design engineering;Virtual manufacturing;Process design;Steady-state;Data engineering;Surface-mount technology","optimisation;production testing;costing;economics;digital simulation;electronic engineering computing","test process optimization;cost modeling tool;Digital Equipment Corporation;Maynard Mass;economic test process;test coverage;yield;total module manufacturing cost;test process modeling tool;test cost simulation model;defect reduction economics","","2","4","","","","","","IEEE","IEEE Conferences"
"Test prioritization using system models","B. Korel; L. H. Tahat; M. Harman","Dept. of Comput. Sci., Illinois Inst. of Technol., Chicago, IL, USA; NA; NA","21st IEEE International Conference on Software Maintenance (ICSM'05)","","2005","","","559","568","During regression testing, a modified system is retested using the existing test suite. Because the size of the test suite may be very large, testers are interested in detecting faults in the system as early as possible during the retesting process. Test prioritization tries to order test cases for execution so the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the code of the system. System modeling is a widely used technique to model state-based systems. In this paper, we present methods of test prioritization based on state-based models after changes to the model and the system. The model is executed for the test suite and information about model execution is used to prioritize tests. Execution of the model is inexpensive as compared to execution of the system; therefore the overhead associated with test prioritization is relatively small. In addition, we present an analytical framework for evaluation of test prioritization methods. This framework may reduce the cost of evaluation as compared to the existing evaluation framework that is based on experimentation (observation). We have performed an experimental study in which we compared different test prioritization methods. The results of the experimental study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.","1063-6773","0-7695-2368","10.1109/ICSM.2005.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510150","","System testing;Fault detection;Modeling;Software systems;Software maintenance;Software testing;Computer science;Technological innovation;Educational institutions;Costs","program testing;software fault tolerance","regression testing;system modeling;state-based model;test prioritization method;fault detection","","37","18","","","","","","IEEE","IEEE Conferences"
"Predicting the order of fault-prone modules in legacy software","T. M. Khoshgoftaar; E. B. Allen","Florida Atlantic Univ., Boca Raton, FL, USA; NA","Proceedings Ninth International Symposium on Software Reliability Engineering (Cat. No.98TB100257)","","1998","","","344","353","A goal of software quality modeling is to recommend modules for reliability enhancement early enough to prevent poor quality. Reliability improvement techniques include more rigorous design and code reviews and more extensive testing. This paper introduces the concept of module-order models for guiding software reliability enhancement and provides an empirical case study that shows how such models can be used. A module-order model predicts the rank-order of modules according to a quantitative quality factor. The case study examined a large legacy telecommunications system. We found that the amount of new and changed code due to the development of a release can be a better predictor of code churn due to subsequent bug fixes, compared to software product metrics alone. In such projects, process-related measures derived from configuration management data may be adequate for software quality modeling, without resorting to software product measurement tools and expertise.","1071-9458","0-8186-8991","10.1109/ISSRE.1998.730899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730899","","Software quality;Software measurement;Testing;Software reliability;Predictive models;Q factor;Software debugging;Quality management;Project management;Software tools","software quality;software reliability;software metrics;telecommunication computing;configuration management;program testing;subroutines","fault-prone module order prediction;legacy software;software quality modeling;software reliability enhancement;rigorous design reviews;rigorous code reviews;software testing;module-order models;case study;rank-order;quantitative quality factor;telecommunications system;changed code;software release;software development;code churn;bug fixes;software product metrics;process-related measures;configuration management data;software product measurement tools;expertise","","18","11","","","","","","IEEE","IEEE Conferences"
"Optimization of reliability allocation and testing schedule for software systems","M. R. Lyu; S. Rangarajan; A. P. A. van Moorsel","AT&T Bell Labs., Murray Hill, NJ, USA; NA; NA","Proceedings The Eighth International Symposium on Software Reliability Engineering","","1997","","","336","347","To ensure an overall reliability of an integrated software system, software components of the system have to meet certain reliability requirements, subject to some testing schedule and resource constraints. The system testing activity can be formulated as a combinatorial optimization problem with known cost, reliability, effort and other attributes of the system components. In this paper, we consider the software component reliability allocation problem for a system with multiple applications. The failure rate of components used to build the applications are related to the testing cost through various types of reliability growth curves. We achieve closed-form solutions to problems where there is one single application in the system. Analytical solutions are not readily available when there are multiple applications; however, numerical solutions can be obtained using a nonlinear programming tool. To ease the specification of the optimization problem, we develop a GUI front-end to existing mathematical software. We present a systematic outline of the problem formulation and solution, and apply this to an example of a telecommunication software system.","","0-8186-8120","10.1109/ISSRE.1997.630881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630881","","Software testing;System testing;Software systems;Application software;Redundancy;Cost function;Software reliability;Closed-form solution;Graphical user interfaces;Dynamic programming","subroutines;nonlinear programming;software reliability;program testing;integrated software;computer communications software;telecommunication computing;graphical user interfaces;mathematics computing;resource allocation;scheduling","software component reliability allocation problem;software testing schedule optimization;integrated software system;software component failure rate;resource constraints;combinatorial optimization problem;testing cost;reliability growth curves;closed-form solutions;multiple applications;numerical solutions;nonlinear programming;problem specification;GUI front-end;mathematical software;telecommunication software system","","7","21","","","","","","IEEE","IEEE Conferences"
"An automated framework for structural test-data generation","N. Tracey; J. Clark; K. Mander; J. McDermid","Dept. of Comput. Sci., York Univ., UK; NA; NA; NA","Proceedings 13th IEEE International Conference on Automated Software Engineering (Cat. No.98EX239)","","1998","","","285","288","Structural testing criteria are mandated in many software development standards and guidelines. The process of generating test data to achieve 100% coverage of a given structural coverage metric is labour-intensive and expensive. This paper presents an approach to automate the generation of such test data. The test-data generation is based on the application of a dynamic optimisation-based search for the required test data. The same approach can be generalised to solve other test-data generation problems. Three such applications are discussed-boundary value analysis, assertion/run-time exception testing, and component re-use testing. A prototype tool-set has been developed to facilitate the automatic generation of test data for these structural testing problems. The results of preliminary experiments using this technique and the prototype tool-set are presented and show the efficiency and effectiveness of this approach.","","0-8186-8750","10.1109/ASE.1998.732680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=732680","","Automatic testing;Software testing;Costs;Application software;Automation;Simulated annealing;Computer science;Programming;Software standards;Standards development","program testing;program control structures;computer aided software engineering;optimisation;search problems;boundary-value problems;exception handling;subroutines;software reusability","automated structural test-data generation;structural testing criteria;software development standards;structural coverage metric;dynamic optimisation-based search;boundary value analysis;assertion/run-time exception testing;component reuse testing;prototype tool-set;efficiency","","53","13","","","","","","IEEE","IEEE Conferences"
"Test-suite reduction and prioritization for modified condition/decision coverage","J. A. Jones; M. J. Harrold","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA","Proceedings IEEE International Conference on Software Maintenance. ICSM 2001","","2001","","","92","101","Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique, and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows, and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software, according to some criterion, as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. The paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of a case study of the test-suite reduction algorithm.","1063-6773","0-7695-1189","10.1109/ICSM.2001.972715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972715","","Software testing;Costs;Software algorithms;System testing;Software maintenance;Educational institutions;Safety;Electronic switching systems;FAA;Performance evaluation","program testing;program verification;software prototyping","test-suite reduction;modified condition/decision coverage;software testing;high-assurance software development;commercial airborne systems;Federal Aviation Administration;verification technique;safety faults;test cases;regression testing;test-suite size problem;test-suite prioritization algorithms;prioritization techniques;MC/DC adequate test suites;case study","","25","13","","","","","","IEEE","IEEE Conferences"
"Prioritizing test cases for regression testing","G. Rothermel; R. H. Untch; Chengyun Chu; M. J. Harrold","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","10","929","948","Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites.","0098-5589;1939-3520;2326-3881","","10.1109/32.962562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962562","","Computer aided software engineering;Software testing;Fault detection;Costs;Computer Society;System testing;Software maintenance;Processor scheduling;Feedback;Application software","program testing;program debugging","test case prioritization;regression testing;software testing;test case scheduling;software fault detection rate;software fault correction;code component coverage;experiments;cost-benefit analysis","","513","38","","","","","","IEEE","IEEE Journals & Magazines"
"Incorporating varying test costs and fault severities into test case prioritization","S. Elbaum; A. Malishevsky; G. Rothermel","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA","Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001","","2001","","","329","338","Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.","0270-5257","0-7695-1050","10.1109/ICSE.2001.919106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919106","","Costs;Computer aided software engineering;Fault detection;Software testing;Job shop scheduling;Computer science;System testing;Processor scheduling;Application software;Frequency","program testing;software metrics;software cost estimation","varying test costs;fault severities;test case prioritization;regression testing;performance goal;fault detection rate;testing process;APFD;fault severity;prioritized test cases;varying test case costs;fault costs;case study","","98","13","","","","","","IEEE","IEEE Conferences"
"Test factoring: focusing test suites for the task at hand","D. Saff; M. D. Ernst","Comput. Sci. & Artificial Intelligence Lab, MIT, USA; Comput. Sci. & Artificial Intelligence Lab, MIT, USA","Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.","","2005","","","656","","Frequent execution of a test suite during software maintenance can catch regression errors early, indicate whether progress is being made, and improve productivity. However, if the test suite takes a long time to produce feedback, the developer is slowed down, and the benefit of frequent testing is reduced. After a program is edited, ideally, only changed code would be tested. Any time spent executing previously tested, unchanged parts of the code is wasted. For a large test suite containing many small unit tests, test selection and prioritization can be effective. Test selection runs only those tests that are possibly affected by the most recent change, and test prioritization can run first the tests that are most likely to reveal a recently-introduced error.","0270-5257;1558-1225","1-59593-963","10.1109/ICSE.2005.1553636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553636","","Automatic testing;System testing;Software testing;Computer bugs;Costs;Instruments;Computer science;Artificial intelligence;Debugging;Algorithm design and analysis","software maintenance;program testing","test factoring;software maintenance;regression error;unit testing;test selection;test prioritization;mock object","","1","3","","","","","","IEEE","IEEE Conferences"
"A new software for test logic optimization in DFT","Zhe Zhang; Chen Hu; Rui Li; Youhua Shi; Longxing Shi","Nat. ASIC Syst. Eng. Center, Southeast Univ., Nanjing, China; NA; NA; NA; NA","ASICON 2001. 2001 4th International Conference on ASIC Proceedings (Cat. No.01TH8549)","","2001","","","654","657","This paper presents a new software named ASIC2000TA developed for design for test (DFT) aiming at optimizing test logic. This software consists of two modules: test analysis module and DFT module. Test analysis module can examine a circuit's testability, generate test vectors and perform fault simulation, in which some algorithms are described. DFT module automatically inserts test logic in gate-level netlist, including full scan and partial scan, in which a greedy search algorithm is discussed. Electronic design intermediate format (EDIF) acts as an interface between ASIC2000TA and Cadence. An experiment with ASIC2000TA is presented.","","0-7803-6677","10.1109/ICASIC.2001.982648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982648","","Logic testing;Software testing;Circuit testing;Design for testability;Design optimization;Logic design;Performance analysis;Algorithm design and analysis;Performance evaluation;Circuit faults","logic testing;circuit optimisation;application specific integrated circuits;fault simulation;circuit simulation;boundary scan testing;electronic data interchange;design for testability","test logic optimization;ASIC2000TA;test analysis module;test vectors;fault simulation;test logic;gate-level netlist;full scan;partial scan;greedy search algorithm;electronic design intermediate format;Cadence;DFT","","","5","","","","","","IEEE","IEEE Conferences"
"An ant colony optimization approach to test sequence generation for state based software testing","Huaizhong Li; C. P. Lam","Sch. of Comput. & Inf. Sci., Edith Cowan Univ., Churchlands, WA, Australia; Sch. of Comput. & Inf. Sci., Edith Cowan Univ., Churchlands, WA, Australia","Fifth International Conference on Quality Software (QSIC'05)","","2005","","","255","262","Properly generated test suites may not only locate the defects in software systems, but also help in reducing the high cost associated with software testing, ft is often desired that test sequences in a test suite can be automatically generated to achieve required test coverage. However, automatic test sequence generation remains a major problem in software testing. This paper proposes an ant colony optimization approach to automatic test sequence generation for state-based software testing. The proposed approach can directly use UML artifacts to automatically generate test sequences to achieve required test coverage.","1550-6002;2332-662X","0-7695-2472","10.1109/QSIC.2005.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579143","","Software testing;Ant colony optimization;Automatic testing;Artificial intelligence;System testing;Application software;Unified modeling language;Gold;Electrical capacitance tomography;Software systems","program testing;Unified Modeling Language;automatic programming;optimisation","ant colony optimization approach;state based software testing;software system;automatic test sequence generation;UML","","11","16","","","","","","IEEE","IEEE Conferences"
"Benchmarking","K. Ruparel","Apple Comput. Inc., Cupertino, CA, USA","Proceedings., International Test Conference","","1994","","","364","","The importance of determining the role and influence of the test tool when integrated into the ""existing"" design and test methodology of the user, and then formulating the benchmark process, is emphasised. This can affect shorter benchmark cycles. The focus of an evaluation process should be more of which tool offers the most productive way of using existing techniques to yield successful results"" versus ""which tool claims to offer the latest technology available"". An equally critical component of the benchmarking process is the proper selection of benchmark circuits. This issue is also applicable to research projects where there is more opportunity to use outdated and unrealistic circuits. The problems of testability can never be solved by throwing circuits at the test tools being evaluated. Testability can be achieved only through trade-offs. Furthermore, during the entire evaluation period, continual interaction with the vendor is critical to insure that benchmark experiments are performed with the right conditions and constraints on the tool. Overly optimistic benchmark parameters such as high-level models can easily generate impressive, but unrealistic results.","1089-3539","0-7803-2103","10.1109/TEST.1994.527974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=527974","","Benchmark testing;Circuit testing;Logic testing;Production;Software tools;Software testing;Computer errors;Packaging;Test pattern generators;Automatic test pattern generation","automatic testing;software performance evaluation;automatic test software;standards;production testing;automatic test equipment","benchmark process;benchmark cycles;evaluation process;benchmark circuits;testability;trade-offs","","","1","","","","","","IEEE","IEEE Conferences"
"Adaptive testing, oracle generation, and test case ranking for Web services","Wei-Tek Tsai; Yinong Chen; R. Paul; Hai Huang; Xinyu Zhou; Xiao Wei","Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; NA; NA; NA; NA","29th Annual International Computer Software and Applications Conference (COMPSAC'05)","","2005","1","","101","106 Vol. 2","Web services and service-oriented architecture are emerging technologies that are changing the way we develop and use computer software. Due to the standardization of Web services related description languages and protocols, as well as the open platforms, for the same Web service specification, many different implementations can be offered from different service providers. This paper presents an adaptive group testing technique that can test large number Web services simultaneously and effectively. Based on a case study, experiments are performed to validate the correctness and effectiveness of the technique.","0730-3157;0730-3157","0-7695-2413","10.1109/COMPSAC.2005.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510005","Web services;service-oriented architecture;group testing;web services ranking;test case ranking","Computer aided software engineering;Web services;Service oriented architecture;Simple object access protocol;Software testing;Application software;Automatic testing;Web and internet services;Publishing;Quality of service","program testing;Internet;formal specification;open systems;formal verification;standardisation;groupware","adaptive group testing;oracle generation;test case ranking;service-oriented architecture;computer software;standardization;description languages;protocols;open platforms;Web service specification","","20","29","","","","","","IEEE","IEEE Conferences"
"Code-coverage guided prioritized test generation","J. J. Li; H. Yee","Avaya Labs., Basking Ridge, NJ, USA; Avaya Labs., Basking Ridge, NJ, USA","Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.","","2004","2","","178","181 vol.2","With Internet applications spreading like wildfire, software testing is challenged with new topics related to the distributed nature of Web applications. We apply code based testing techniques to the testing of Web applications, specifically Java programs. Source code based automatic test generation is difficult because most previous methods use constraint satisfaction models as a solution, which is an NP complete problem [M. J. Gallagher et al. (1997)]. We present a method of guiding users through test case generations. Instead of automating the entire procedure, our method aims at generating a framework of test cases and providing instructions for users to instantiate the framework into actual executable test cases. An early experimental study of this method shows the effectiveness of this method.","0730-3157","0-7695-2209","10.1109/CMPSAC.2004.1342705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342705","","Application software;Automatic testing;Software testing;Java;System testing;Internet;Network servers;Protocols;Automatic generation control;Humans","program testing;Internet;Java;automatic testing;object-oriented methods;program compilers","code-coverage guided prioritized test generation;Internet applications;software testing;Java programs;source code based automatic test generation;constraint satisfaction models;NP complete problem;test case generations","","","7","","","","","","IEEE","IEEE Conferences"
"A history-based test prioritization technique for regression testing in resource constrained environments","Jung-Min Kim; A. Porter","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","Proceedings of the 24th International Conference on Software Engineering. ICSE 2002","","2002","","","119","129","Regression testing is an expensive and frequently executed maintenance process used to revalidate modified software. To improve it, regression test selection (RTS) techniques strive to lower costs without overly reducing effectiveness by carefully selecting a subset of the test suite. Under certain conditions, some can even guarantee that the selected test cases perform no worse than the original test suite. This ignores certain software development realities such as resource and time constraints that may prevent using RTS techniques as intended (e.g., regression testing must be done overnight, but RTS selection returns two days worth of tests). In practice, testers work around this by prioritizing the test cases and running only those that fit within existing constraints. Unfortunately this generally violates key RTS assumptions, voiding RTS technique guarantees and making regression testing performance unpredictable. Despite this, existing prioritization techniques are memoryless, implicitly assuming that local choices can ensure adequate long run performance. Instead, we propose a new technique that bases prioritization on historical execution data. We conducted an experiment to assess its effects on the long run performance of resource constrained regression testing. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.","","1-58113-472","10.1109/ICSE.2002.1007961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007961","","Software testing;Time factors;Educational institutions;Permission;System testing;Computer science;Software maintenance;Costs;Performance evaluation;Programming","program testing;software maintenance;program verification","history-based test prioritization;regression testing;resource constrained environments;software maintenance;modified software revalidation;costs;performance;experiment;software releases;program testing","","28","22","","","","","","IEEE","IEEE Conferences"
"Business constraints drive test decisions planning, partnerships and success","M. Campbell","NA","IEEE International Conference on Test, 2005.","","2005","","","2 pp.","1278","In the era of complex SOC semiconductors, the test matrices and complexity of system, manufacturing and pre-silicon design test analysis are continuously evolving, as are the demands of the customers for improved quality at reduced cost. Business constraints that drive test solutions can make or break products, if not companies, in today's market. Business constraints, namely cost and time to market, require that we strengthen and enlarge the current model for inter-company cooperation and development. This calls for improving existing systems modeling capabilities to achieve the reliability and predictability these systems models have the potential to offer. Significant advancements have been made; however, for full optimization of test cost and product quality, more is needed","1089-3539;2378-2250","0-7803-9038","10.1109/TEST.2005.1584106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1584106","","System testing;Costs;Business;Semiconductor device testing;Semiconductor device manufacture;Circuit testing;Electronic design automation and methodology;Design optimization;Time to market;Software testing","electron device testing;management science;planning;quality management;reliability;technology management;test facilities","business constraints;test decisions planning;business partnerships;SOC semiconductors;test matrices;design test analysis;inter-company cooperation;product quality","","","","","","","","","IEEE","IEEE Conferences"
"A defect prevention approach to architecture-based testing","N. S. Eickelmann; D. J. Richardson","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; NA","Proceedings Twenty-First Annual International Computer Software and Applications Conference (COMPSAC'97)","","1997","","","162","163","Research in software architectures often marginalizes the importance of testing in relation to architectural design decisions. It is the authors' belief that testing has a pivotal role in software architectural design, in particular, defect prevention and defect detection. How effectively testing fulfills these roles is dependent on when and how the test process is introduced into the life cycle. Defect prevention requires that software test planning begin with requirements. Effective test planning requires textual and visual tools to analyze the control and data structures to be tested. Architecture-based testing presents a unique testing challenge as data and control issues must be analyzed for complex interdependencies. They introduce a generalized program graph GPG to be used with architecture-based test criteria to support analyzing architectures for test optimization. They introduce what they believe to be the salient issues in achieving defect prevention and detection for architecture-based testing. They conclude with suggestions for future work.","0730-3157","0-8186-8105","10.1109/CMPSAC.1997.624781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624781","","Software testing;Life testing;Software architecture;Computer architecture;System testing;Data structures;Object oriented modeling;Computer science;Software design;Buildings","software engineering;program testing;data structures;program control structures;planning;optimisation;system monitoring","architecture-based testing;defect prevention;software architectures;architectural design decisions;software architectural design;defect detection;software test planning;textual tools;visual tools;control structures;data structures;complex interdependencies;generalized program graph;test optimization","","1","20","","","","","","IEEE","IEEE Conferences"
"A study of the effect of imperfect debugging on software development cost","Min Xie; Bo Yang","Dept. of Ind. & Syst. Eng., Nat. Univ. of Singapore, Singapore; NA","IEEE Transactions on Software Engineering","","2003","29","5","471","473","It is widely recognized that the debugging processes are usually imperfect. Software faults are not completely removed because of the difficulty in locating them or because new faults might be introduced. Hence, it is of great importance to investigate the effect of the imperfect debugging on software development cost, which, in turn, might affect the optimal software release time or operational budget. In this paper, a commonly used cost model is extended to the case of imperfect debugging. Based on this, the effect of imperfect debugging is studied. As the probability of perfect debugging, termed testing level here, is expensive to be increased, but manageable to a certain extent with additional resources, a model incorporating this situation is presented. Moreover, the problem of determining the optimal testing level is considered. This is useful when the decisions regarding the test team composition, testing strategy, etc., are to be made for more effective testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199075","","Programming;Cost function;Software debugging;Software testing;Resource management;Software reliability;Fault diagnosis;Software systems;Personnel","software engineering;program debugging;software reliability;software development management","debugging processes;imperfect debugging;testing optimization;software development cost;optimal software release time;software reliability;testing level","","54","10","","","","","","IEEE","IEEE Journals & Magazines"
"Using relative complexity to allocate resources in gray-box testing of object-oriented code","Jianqiang Zhuo; P. Oman; R. Pichai; S. Sahni","Software Eng. Test Lab., Idaho Univ., Moscow, ID, USA; NA; NA; NA","Proceedings Fourth International Software Metrics Symposium","","1997","","","74","81","Software testing costs would be reduced if managers and testing engineers could gauge which parts of a system were more complex and thus more likely to have faults. Once these areas are identified, testing resources and testing priority could be assigned accordingly. The paper defines a method that uses the relative complexity metric to allocate resources for gray box testing in an environment where object oriented code is used and historical data are not available. The proposed method can also be applied to black box and white box testing as well as software quality assessments such as maintainability and reliability. The work on an industrial C++ software subsystem presented here shows that the rank order of minor test areas of the subsystem by relative test complexity is significantly similar to the rank order obtained from the experts who designed, wrote and tested the code.","","0-8186-8093","10.1109/METRIC.1997.637167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637167","","Resource management;Object oriented modeling;Software testing;Software quality;System testing;Software systems;Software engineering;Costs;Engineering management;Maintenance","object-oriented programming;program testing;software metrics;resource allocation;C language;object-oriented languages","resource allocation;gray box testing;object oriented code;software testing costs;testing engineers;relative complexity metric;white box testing;software quality assessments;black box;maintainability;reliability;industrial C++ software subsystem;rank order;minor test areas;relative test complexity","","3","22","","","","","","IEEE","IEEE Conferences"
"Reliability optimization of redundant software with correlated failures","W. Gutjahr","Dept. of Stat., Wien Univ., Austria","Proceedings Ninth International Symposium on Software Reliability Engineering (Cat. No.98TB100257)","","1998","","","293","302","Several authors have addressed the problem of reliability optimization of modular software with redundancy. At present, these approaches seem to be limited by the fact that they use statistical independence assumptions for different program versions. In view of strong empirical evidence of positive failure correlations between program versions, it is desirable to generalize these reliability optimization techniques by taking such correlations explicitly into consideration. In the presented paper, this is done based on a multiversion software failure correlation model tracing back to D.E Eckhardt and L.D. Lee (1985). We show how taking failure correlation into account extends the corresponding optimization problems, and how they can be treated computationally. In particular, in the (practically relevant) case of components with relatively high reliability, the optimization problems reduce to knapsack type problems which can, for relevant problem sizes, be solved with rather low computational effort. Also for the general case, solution techniques are outlined.","1071-9458","0-8186-8991","10.1109/ISSRE.1998.730893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730893","","Costs;Statistics;Computer science;Software systems;Application software;Fault tolerant systems;Redundancy;Testing","software reliability;redundancy;configuration management;knapsack problems;optimisation","redundant software;correlated failures;reliability optimization;modular software;statistical independence assumptions;program versions;positive failure correlations;multiversion software failure correlation model;failure correlation;optimization problems;relatively high reliability;knapsack type problems","","2","14","","","","","","IEEE","IEEE Conferences"
"Needed resources for software module test, using the hyper-geometric software reliability growth model","Rong-Huei Hou; Sy-Yen Kuo; Yi-Ping Chang","Nat. Taiwan Univ., Taipei, Taiwan; NA; NA","IEEE Transactions on Reliability","","1996","45","4","541","549","Considerable testing resources are required during software module testing. This paper, based on the 'hyper-geometric distribution software reliability growth model' (HGDM) investigates two optimal resource allocation (OPT/RA) problems in software module testing: (1) minimization of the number of software faults (NSF) still undetected in the system after testing, given a fixed amount of testing resources; and (2) minimization of the total amount of testing resources required, given the NSF still undetected in the system after testing. Based on the concepts of average allocation and proportional allocation, two simple allocation methods are introduced. Experimental results show that the OPT/RA method can improve the quality and reliability of the software system much more than the simple allocation methods. Therefore, the OPT/RA method is very efficient for solving the 'testing resource allocation' problem.","0018-9529;1558-1721","","10.1109/24.556577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556577","","Software testing;Software reliability;System testing;Resource management;Optimized production technology;Software systems;Lagrangian functions;Software development management;Fault detection;Project management","software reliability;software quality;program testing;resource allocation;optimisation","hyper-geometric software reliability growth model;software module testing;testing resource requirements;optimal resource allocation;software fault number;average allocation;proportional allocation;software quality","","16","","","","","","","IEEE","IEEE Journals & Magazines"
"The cost of testing software","R. L. Vienneau","Kaman Sci. Corp., Utica, NY, USA","Annual Reliability and Maintainability Symposium. 1991 Proceedings","","1991","","","423","427","A simple quantitative cost model that allows a manager to determine the optimum amount of time needed to test a software system has been developed. At the minimum cost point, the cost of an infinitesimal increment of testing equals the marginal benefit to be gained from the increment. The model can also be used to estimate the cost of testing software if the release data is determined by some other criteria, such as reliability. Since the cost of testing software depends on the pattern of failures, the model is based on underlying software reliability models. The cost model is explicitly developed so as to apply to a wide class of reliability models. It is noted that special cases result from choosing specific models, for example, the Goel-Okumoto nonhomogeneous Poisson process model, in which all bugs have an equal impact on the failure rate, and the Musa-Okumoto logarithmic Poisson process model, in which bugs found earlier have a larger impact.<<ETX>>","","0-87942-661","10.1109/ARMS.1991.154473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=154473","","Software testing;Software reliability;Computer bugs;Context modeling;Software systems;Software debugging;Cost function;Software quality;Software development management;System testing","economics;failure analysis;optimisation;program testing;software reliability","software reliability;management;optimisation;testing;quantitative cost model;failures;Goel-Okumoto nonhomogeneous Poisson process;bugs;Musa-Okumoto logarithmic Poisson process","","","20","","","","","","IEEE","IEEE Conferences"
"Prioritized use cases as a vehicle for software inspections","T. Thelin; P. Runeson; C. Wohlin","Dept. of Commun. Syst., Lund Univ., Sweden; Dept. of Commun. Syst., Lund Univ., Sweden; NA","IEEE Software","","2003","20","4","30","33","The usage-based reading technique combines traditional inspection principles, use cases, and operational profile testing to create an efficient user-oriented software inspection reading technique. UBR can find faults more effectively and efficiently than the traditional checklist-based method.","0740-7459;1937-4194","","10.1109/MS.2003.1207451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207451","","Computer aided software engineering;Inspection;Computer industry;Software testing;Vehicle dynamics;Software quality;Software design;Performance analysis;Industrial relations;Software engineering","program testing;program verification;software engineering","software engineering;object-oriented design;software testing;inspection;verification;UBR;usage-based reading","","18","13","","","","","","IEEE","IEEE Journals & Magazines"
"An approach to the reliability optimization of software with redundancy","F. Belli; P. Jedrzejowicz","Dept. of Electr. & Electron. Eng., Paderborn Univ., Germany; NA","IEEE Transactions on Software Engineering","","1991","17","3","310","312","An approach to the optimization of software reliability is proposed. The emphasis is put on the software redundancy to achieve fault tolerance, i.e. the results of the optimization process are used to determine the optimal structure of the software to be developed. Two optimization models are formulated covering, respectively, modified recovery block scheme and multiversion programming approaches. Both cases are illustrated by simple examples. The models show that it is possible to formulate and solve some software related reliability optimization problems. They further show that the concept of redundancy to achieve fault tolerance (basic for the traditional theory of reliability) can be used in the field of software reliability optimization.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.75419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=75419","","Redundancy;Software reliability;Power system modeling;Reliability theory;Software systems;Predictive models;Software testing;Power system reliability;Application software;Mathematical model","fault tolerant computing;optimisation;redundancy;software reliability","reliability optimization;software;redundancy;software reliability;fault tolerance;modified recovery block scheme;multiversion programming","","30","11","","","","","","IEEE","IEEE Journals & Magazines"
"A high-resolution waveform analysis tool","P. M. Powers","Megatest Corp., San Jose, CA, USA","International Test Conference 1988 Proceeding@m_New Frontiers in Testing","","1988","","","547","550","A system is described for very-high-resolution measurement, statistical analysis, and graphical display of waveforms appearing on a VLSI device. Capabilities include superimposing graphs of waveforms resulting from differing test conditions or from different pins. Measurement is done by averaging the results of multiple trials. The measurement device has a resolution of 3 ps, accuracy of 100 ps, and is calibrated using the time-domain reflectometry technique. The system's graphical interface is user-friendly. The author describes the hardware used for the measurements, the software optimization techniques used to obtain sufficient throughput to allow interactive display of waveforms, and the range of applications of the package.<<ETX>>","1089-3539","0-8186-0870","10.1109/TEST.1988.207835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207835","","Displays;Statistical analysis;Very large scale integration;Testing;Pins;Time domain analysis;Reflectometry;Hardware;Software measurement;Throughput","automatic testing;computer aided analysis;integrated circuit testing;optimisation;software packages;statistical analysis;time-domain reflectometry;VLSI;waveform analysis","IC testing;waveform analysis tool;statistical analysis;graphical display;VLSI device;averaging;multiple trials;time-domain reflectometry;graphical interface;software optimization","","","","","","","","","IEEE","IEEE Conferences"
"Test-suite reduction and prioritization for modified condition/decision coverage","J. A. Jones; M. J. Harrold","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","2003","29","3","195","209","Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. This paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of empirical studies of these algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183927","","Software testing;Software algorithms;Costs;System testing;Software maintenance;Computer Society;Safety;FAA;Performance evaluation;Software performance","program testing;performance evaluation","critical software;software testing;commercial airborne systems;test suites;modified condition/decision coverage;test-suite prioritization","","144","15","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing test to reduce maintenance","M. Pighin; A. Marzona","Dipt. di Math. e Informatica, Udine Univ., Italy; Dipt. di Math. e Informatica, Udine Univ., Italy","21st IEEE International Conference on Software Maintenance (ICSM'05)","","2005","","","465","472","A software package evolves in time through various maintenance release steps whose effectiveness depends mainly on the number of faults left in the modules. Software testing is one of the most demanding and crucial phases to discover and reduce faults. In real environment, time available to test a software release is a given finite quantity. The purpose of this paper is to identify a criterion to estimate an efficient time repartition among software modules to enhance fault location in testing phase and to reduce corrective maintenance. The fundamental idea is to relate testing time to predicted risk level of the modules in the release under test. In our previous work we analyzed several kinds of risk prediction factors and their relationship with faults; moreover, we thoroughly investigated the behavior of faults on each module through releases to find significant fault proneness tendencies. Starting from these two lines of analysis, in this paper we propose a new approach to optimize the use of available testing time in a software release. We tuned and tested our hypotheses on a large industrial environment.","1063-6773","0-7695-2368","10.1109/ICSM.2005.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510141","","Software testing;Software maintenance;Software quality;Software measurement;Software packages;Fault diagnosis;Phase estimation;Fault location;Risk analysis;Production","program testing;software maintenance;software quality;software reliability","test optimization;software package;software testing;software release;fault location;corrective maintenance;industrial environment;software module time repartition;release under test;software quality;software reliability","","","19","","","","","","IEEE","IEEE Conferences"
"Standard test interface language (STIL) a new language for patterns and waveforms","T. Taylor; G. A. Maston","Credence Syst. Corp., Fremont, CA, USA; NA","Proceedings International Test Conference 1996. Test and Design Validity","","1996","","","565","570","This paper presents the major features and capabilities of the new standard test interface language (STIL). A synopsis of the language, typical applications of the language, work that has been done to date, and degree of acceptance by the industry are discussed.","1089-3539","0-7803-3541","10.1109/TEST.1996.557091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=557091","","Standards development;Circuit testing;Meetings;Integrated circuit testing;Circuit simulation;Digital integrated circuits;Hardware;Design optimization;Discrete event simulation;Electronic mail","automatic test software;high level languages;waveform analysis;measurement standards;timing;digital integrated circuits;integrated circuit testing;computational complexity","standard test interface language;acceptance;patterns;waveforms;industry;STIL;digital integrated circuits;data volume minimisation","","16","2","","","","","","IEEE","IEEE Conferences"
"Test case prioritization: a family of empirical studies","S. Elbaum; A. G. Malishevsky; G. Rothermel","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","2","159","182","To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies.","0098-5589;1939-3520;2326-3881","","10.1109/32.988497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988497","","Computer aided software engineering;Fault detection;Software testing;System testing;Costs;Software measurement;Radio access networks;Feedback;Debugging;Instruments","program testing","test case prioritization;regression testing;software testing;fault detection rate;fine granularity prioritization techniques;coarse granularity prioritization techniques;fault proneness measures","","364","35","","","","","","IEEE","IEEE Journals & Magazines"
"Getting results from search-based approaches to software engineering","M. Harman; J. Wegener","Brunel Univ., Middlesex, UK; NA","Proceedings. 26th International Conference on Software Engineering","","2004","","","728","729","Like other engineering disciplines, software engineering is typically concerned with near optimal solutions or those which fall within a specified applicable tolerance. More recently, search-based techniques have started to find application in software engineering problem domains. This area of search-based software engineering has its origins in work on search-based testing, which began in the mid 1990s. Already, search-based solutions have been applied to software engineering problems right through the development life cycle.","0270-5257","0-7695-2163","10.1109/ICSE.2004.1317508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317508","","Software engineering;Genetic algorithms;Application software;Evolutionary computation;Software testing;Software maintenance;Algorithm design and analysis;Robustness;Constraint optimization;Genetic engineering","search problems;software engineering;optimising compilers","search-based software engineering;optimization;search-based testing;software development life cycle","","5","26","","","","","","IEEE","IEEE Conferences"
"Developing engine test software in LabVIEW","P. Turley; M. Wright","CACI, San Antonio, TX, USA; NA","1997 IEEE Autotestcon Proceedings AUTOTESTCON '97. IEEE Systems Readiness Technology Conference. Systems Readiness Supporting Global Needs and Awareness in the 21st Century","","1997","","","575","579","CACI International Inc. is on contract with SAALC/LDAD, the Air Force engine tester program management office, to build an Engine Test/Trim Automated System II (ETTAS II) using Commercial Off The Shelf (COTS) hardware and software. This tester will ultimately replace the three aircraft engine test systems currently used by the Air Force, all of which are becoming increasingly difficult to maintain doe to hardware/software obsolescence problems. In keeping with the COTS requirement, we chose to develop our data acquisition and test program software in LabVIEW 4.0.1 for Windows NT/95. This paper discusses the advantages we have gained in using LabVIEW 4.0.1, a graphical programming language, rather than a conventional programming language as our software development environment. We detail how we were able to take advantage of LabVIEW's instrument control capabilities to optimize our VXI data acquisition process. We then discuss how LabVIEW can be used not only as an instrument control language, but also as a general purpose programming language. We discuss how we used LabVIEW for test program set (TPS) development and for rapidly prototyping user interfaces and program features for immediate operator/customer feedback. The paper also details how LabVIEW enabled os to readily establish a core of ""generic"" VIs (virtual instruments) for subsequent reuse in developing additional TPS for other aircraft engine types/variants.","","0-7803-4162","10.1109/AUTEST.1997.633678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633678","","Engines;Software testing;System testing;Computer languages;Instruments;Automatic testing;Hardware;Aircraft propulsion;Data acquisition;Contracts","aircraft testing;aerospace engines;automatic test software;programming environments;visual programming;data acquisition;software prototyping;aerospace computing;software reusability;military aircraft;graphical user interfaces","engine test software;LabVIEW;CACI International;Air Force;program management;ETTAS II;COTS hardware;COTS software;aircraft engine test;test program software;LabVIEW 4.0.1;Windows NT/95;graphical programming language;software development environment;rapidly prototyping;user interfaces;virtual instruments","","8","3","","","","","","IEEE","IEEE Conferences"
"Optimal allocation of test resources for software reliability growth modeling in software development","M. R. Lyu; S. Rangarajan; A. P. A. van Moorsel","Comput. Sci. & Eng. Dept., Chinese Univ. of Hong Kong, Shatin, China; NA; NA","IEEE Transactions on Reliability","","2002","51","2","183","192","A component-based software development approach has become a trend in integrating modern software systems. To ensure the overall reliability of an integrated software system, its software components have to meet certain reliability requirements, subject to some testing schedule and resource constraints. Efficiency improvement of the system-testing can be formulated as a combinatorial optimization problem with known cost, reliability, effort and other attributes of the system components. This paper considers ""software component testing resource allocation"" for a system with single or multiple applications, each with a pre-specified reliability requirement. The relation between failure rates of components and ""cost to decrease this rate"" is modeled by various types of reliability-growth curves. Closed-form solutions to the problem for systems with one single application are developed, and then ""how to solve the multiple application problem using nonlinear programming techniques"" are described. Also examined are the interactions between the system components, and inter-component failure dependencies are included in the modeling formula. In addition to regular systems, the technique is extended to address fault-tolerant systems. A procedure for a systematic approach to the testing resource allocation problem is developed, and its application in a case study of a telecommunications software system is described. This procedure is automated in a reliability allocation tool for an easy specification of the problem and an automatic application of the technique. This methodology gives the basic approach to optimization of testing schedules, subject to reliability constraints. This adds ""interesting new optimization opportunities in the software testing phase"" to the existing optimization literature that is concerned with structural optimization of the software architecture. Merging these two approaches improves the reliability planning accuracy in component-based software development.","0018-9529;1558-1721","","10.1109/TR.2002.1011524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011524","","Resource management;Software testing;Software reliability;Software systems;Application software;System testing;Programming;Cost function;Closed-form solution;Fault tolerant systems","telecommunication computing;software reliability;program testing;nonlinear programming;combinatorial mathematics;fault tolerant computing","component-based software development approach;software reliability growth modeling;test resources allocation optimisation;integrated software system reliability;combinatorial optimization problem;component failure rates;reliability-growth curves;nonlinear programming;inter-component failure dependencies;telecommunications software system;reliability allocation tool;testing schedules optimization;reliability planning accuracy","","46","28","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic test case optimization: a bacteriologic algorithm","B. Baudry; F. Fleurey; J. -. Jezequel; Y. Le Traon","Univ. de Rennes, France; Univ. de Rennes, France; Univ. de Rennes, France; Univ. de Rennes, France","IEEE Software","","2005","22","2","76","82","Improving test cases automatically is a nonlinear optimization problem. To solve this problem, we've developed a bacteriologic algorithm, adapted from genetic algorithms that can generate and optimize a set of test cases. A .NET component that parses C# source files illustrates our algorithm.","0740-7459;1937-4194","","10.1109/MS.2005.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407832","evolutionary computing;genetic algorithms;object-oriented programming;test design","Automatic testing;Computer aided software engineering;Software testing;Software algorithms;Software quality;Genetic mutations;Fault detection;Unified modeling language;System testing;Protection","program testing;genetic algorithms;grammars;automatic test pattern generation;network operating systems;object-oriented programming","automatic test case optimization;bacteriologic algorithm;genetic algorithm;.NET component;parser;program testing;object-oriented programming","","25","11","","","","","","IEEE","IEEE Journals & Magazines"
"Putting escape analysis to work for software testing","A. L. Souter; L. L. Pollock","Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA; NA","International Conference on Software Maintenance, 2002. Proceedings.","","2002","","","430","439","Developed primarily for optimization of functional and object-oriented software, escape analysis discerns information to determine whether the lifetime of data exceeds its static scope. We demonstrate how to apply escape analysis to software engineering tasks. In particular we present novel software testing and retesting techniques for object-oriented software which utilize escape analysis. We exploit a combined pointer and escape analysis that is able to identify how individual objects allocated in one region of a program interact with other regions of a program. The analysis framework increases flexibility and scalability as testing coverage can be targeted to a specific arbitrary region of a program, followed by integration testing that can be focused on particular sets of objects escaping the region. We demonstrate how regression testing can be performed utilizing this framework. We believe such a flexible framework becomes increasingly beneficial as applications become more component-oriented.","1063-6773","0-7695-1819","10.1109/ICSM.2002.1167800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167800","","Software testing;System testing;Information analysis;Software maintenance;Performance analysis;Software engineering;Performance evaluation;Application software;Software libraries;Computer science","program testing;object-oriented programming;software maintenance","escape analysis;software testing;optimization;functional software;object-oriented software;software engineering;software retesting;pointer analysis;Internet;object-based integration testing;software maintenance;scalability;integration testing;regression testing;component-oriented applications","","1","28","","","","","","IEEE","IEEE Conferences"
"Analysis tools for the evaluation of maintenance software","J. M. C. Barber","REME, Malvern, UK","AUTOTESTCON '88. Symposium Proceedings IEEE International Automatic Testing Conference, Futuretest.","","1988","","","37","40","The amalgamation of information from appropriate fields such as reliability, testability, logistic support, and software engineering gives rise to a constraint-based diagnostic logic model, which provides a valuable baseline for test software requirement definition. It is proposed that the resulting requirements be represented and assessed using techniques similar to and extending those used for the static analysis of source code. This will provide both test package evaluation and acceptance teams (TPEATs) and contractors with the ability to optimize, verify, and validate test software throughout the lifecycle, place less reliance on dynamic tests based on the application of simulated faults and results in cheaper, more effective BIT (built-in testing) and ATE (automated test equipment) software.<<ETX>>","","","10.1109/AUTEST.1988.9582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582","","Software maintenance;Software tools;Software testing;Automatic testing;Life testing;Logic testing;Embedded software;Logistics;Software engineering;Software packages","automatic test equipment;maintenance engineering;software tools","maintenance software;reliability;testability;logistic support;software engineering;constraint-based diagnostic logic model;test software;static analysis of source code;contractors;BIT;ATE","","1","","","","","","","IEEE","IEEE Conferences"
"A selective software testing method based on priorities assigned to functional modules","M. Hirayama; T. Yamamoto; J. Okayasu; O. Mizuno; T. Kikuno","R&D Center Syst. Eng. Lab, Toshiba Corp., Japan; NA; NA; NA; NA","Proceedings Second Asia-Pacific Conference on Quality Software","","2001","","","259","267","As software systems have been introduced to many advanced applications, the size of software systems increases so much. Simultaneously, the lifetime of software systems becomes very small and thus their development is required within a relatively short period. We propose a novel selective software testing method that aims to attain the requirement of short period development. The proposed method consists of 3 steps: assign priorities to functional modules (Step 1), derive a test specification (Step 2), and construct a test plan (Step 3) according to the priorities. In Step 1, for development of functional modules, we select both product and process properties to calculate priorities. Then, in Step 2, we generate detailed test items for each module according to its priority. Finally, in Step 3, we manage test resources including time and developer's skill to attain the requirement. As a result of experimental application, we can show the superiority of the proposed testing method compared to the conventional testing method.","","0-7695-1287","10.1109/APAQS.2001.990028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990028","","Software testing;System testing;Embedded software;Software systems;Fault detection;Application software;Programming;Research and development;Systems engineering and theory;Resource management","program testing;formal specification;resource allocation","selective software testing method;functional modules;priority assignment;software systems lifetime;short period development;test specification;test plan;process properties;product properties;test resources;design of testing specification","","2","11","","","","","","IEEE","IEEE Conferences"
"Empirical studies of test case prioritization in a JUnit testing environment","H. Do; G. Rothermel; A. Kinneer","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA","15th International Symposium on Software Reliability Engineering","","2004","","","113","124","Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites, in particular, Java and the JUnit testing framework are being used extensively in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites, but also reveal differences with respect to previous studies that can be related to the language and testing paradigm.","1071-9458","0-7695-2215","10.1109/ISSRE.2004.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383111","","System testing;Java;Software testing;Fault detection;Performance evaluation;Software systems;Computer science;Systems engineering and theory;Feedback;Software performance","program testing;software fault tolerance;Java","test case prioritization;JUnit testing environment;software fault detection;Java program;object-oriented system","","","33","","","","","","IEEE","IEEE Conferences"
"Prioritizing Software Requirements In An Industrial Setting","K. Ryan; J. Karlsson","University of Limerick; NA","Proceedings of the (19th) International Conference on Software Engineering","","1997","","","564","565","","0270-5257","0-89791-914","10.1145/253228.253453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610408","","Computer industry;Collaborative software;Costs;Permission;Software testing;System testing;Lead;Telephony;Software engineering;Prototypes","Requirements, prioritizing, release planning","","","6","2","","","","","","IEEE","IEEE Conferences"
"Issues in optimizing the test process-a Telecom case study","F. Frayman; M. Tegethoff; B. White","Hewlett Packard Labs., Vancouver, WA, USA; NA; NA","Proceedings International Test Conference 1996. Test and Design Validity","","1996","","","800","808","There is significant interest in the electronics industry to achieve the optimum test process. The HP have developed simulation methodologies that assist manufacturing engineers to optimize their test process. This paper illustrates this methodology in a generic cellular phone line.","1089-3539","0-7803-3541","10.1109/TEST.1996.557140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=557140","","Telecommunications;Computer aided software engineering;Cost function;Assembly;Manufacturing processes;Electronic equipment testing;Virtual manufacturing;Optimization methods;Laboratories;Cellular phones","cellular radio;telecommunication equipment testing;production testing;optimisation;sensitivity analysis;modelling;electronic equipment testing","test process;Telecom;electronics industry;optimum test process;simulation;manufacturing engineers;generic cellular phone line;Hewlett Packard","","1","6","","","","","","IEEE","IEEE Conferences"
"Methods to reduce test application time for accumulator-based self-test","A. P. Stroele; F. Mayer","Inst. of Comput. Design & Fault Tolerance, Karlsruhe Univ., Germany; NA","Proceedings. 15th IEEE VLSI Test Symposium (Cat. No.97TB100125)","","1997","","","48","53","Accumulators based on addition or subtraction can be used as test pattern generators. Some circuits, however, require long test lengths if the parameters of the accumulator are not properly adapted. This paper presents two different methods to minimize the test length without sacrificing fault coverage. The simulation-based reseeding method is suited to random pattern testable circuits and uses forward and reverse order simulation to skip ineffective patterns. The analytical method is appropriate for circuits with ""hard"" faults that are detected only by few test patterns. This method searches for an optimal input value of the accumulator and calculates the best seed analytically. The results show significant test length reductions. The proposed pattern generators can be implemented very efficiently in hardware using available blocks of a data path or in software using an embedded processor.","1093-0167","0-8186-7810","10.1109/VTEST.1997.599440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599440","","Automatic testing;Built-in self-test;Circuit testing;Registers;Circuit faults;Test pattern generators;Fault detection;Hardware;Circuit simulation;Embedded software","built-in self test;combinational circuits;fault diagnosis;logic testing;circuit optimisation;automatic test software;circuit analysis computing","accumulator-based self-test;test application time reduction;test pattern generators;test length minimization;fault coverage;simulation-based reseeding method;random pattern testable circuits;forward simulation;reverse order simulation;hard fault detection;optimal input value;test length reductions;data path blocks;embedded processor;BIST scheme;circuit optimization;ATALANTA fault simulation;combinatorial circuit testing","","18","21","","","","","","IEEE","IEEE Conferences"
"Variable search space for software testing","R. Sagarna; J. A. Lozano","Intelligent Syst. Group, Basque Country Univ., San Sebastian, Spain; Intelligent Syst. Group, Basque Country Univ., San Sebastian, Spain","International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003","","2003","1","","575","578 Vol.1","Testing is an essential phase in the software life cycle. One of the most important tasks testing involves is the automatic generation of the test inputs. The field of evolutionary testing aims at solving this task by means of combinatorial optimization search methods. An evolutionary testing based approach to the automatic generation of test cases is presented. The developed approach considers variable search regions in which appropriate test inputs are sought. The search is performed by means of an emerging, set of evolutionary algorithms called estimation of distribution algorithms. The experimental results obtained show this approach as a promising option for tackling this problem.","","0-7803-7702","10.1109/ICNNSP.2003.1279338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1279338","","Software testing;Automatic testing;Electronic design automation and methodology;Modems;Intelligent systems;Life testing;System testing;Optimization methods;Search methods;Evolutionary computation","evolutionary computation;search problems;program testing;optimisation;combinatorial mathematics","software testing;variable search space;evolutionary testing;combinatorial optimization search methods;estimation of distribution algorithms","","","13","","","","","","IEEE","IEEE Conferences"
"Natural optimization algorithms for optimal regression testing","P. Mansour; K. El-Fakih","Dept. of Comput. Sci., Lebanese American Univ., Beirut, Lebanon; NA","Proceedings Twenty-First Annual International Computer Software and Applications Conference (COMPSAC'97)","","1997","","","511","514","The optimal regression testing problem is that of determining the minimum number of test cases needed for revalidating modified software in the maintenance phase. The present two natural optimization algorithms, namely simulated annealing and genetic algorithms, for solving this problem. The algorithms are based on an integer programming problem formulation and the program's control-flow graph. The main advantage of these algorithms is that they do not suffer from exponential explosion for realistic program sizes. The experimental results show that they find optimal or near-optimal number of retests in a reasonable time.","0730-3157","0-8186-8105","10.1109/CMPSAC.1997.625060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=625060","","Linear programming;Computer science;Simulated annealing;Genetic algorithms;Software maintenance;Software testing;Costs;Flow graphs;Software algorithms;Computational modeling","genetic algorithms;simulated annealing;integer programming;program testing;software maintenance","natural optimization algorithms;optimal regression testing;minimum test cases;modified software revalidation;maintenance phase;simulated annealing algorithms;genetic algorithms;integer programming problem;program control flow graph;realistic program sizes;retests","","1","15","","","","","","IEEE","IEEE Conferences"
"An analysis of ATE computational architecture","A. R. Taylor","LTX/Trillium, San Jose, CA, USA","Proceedings. International Test Conference 1990","","1990","","","514","519","The author discusses the following aspects of test system software: the use of the latest computer technology; the use of the latest software technology, such as graphics and object-oriented programming; the role of the ATE (automatic test equipment) vendor in providing test solutions; the role of the tester user in designing device tests, analyzing test results, and developing production-worthy test solutions. The impact that changes in the state-of-the-art of computer and software design are having on ATE tester architectures is summarized. The importance of the need to be flexible and independent of these changes is shown as it relates to the system software engineer and, even more important, to the device engineer, who is the end user of the test system. The author discusses a two-tiered approach whereby an engineer who is extremely knowledgeable about the tester develops applications that optimize the use of the tester resources. Last, the enVision visual environment is shown; it is an environment ideally suited to the type of work that a device engineer needs to do.<<ETX>>","","0-8186-9064","10.1109/TEST.1990.114062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114062","","Computer architecture;Software testing;System testing;Automatic testing;System software;Computer graphics;Object oriented programming;Automatic test equipment;Software design;Systems engineering and theory","automatic test equipment;computer architecture;computer graphics;object-oriented programming;programming environments;software tools;user interfaces","test language;Vary Tool;Spec Tool;Pattern Tool;Waveform Tool;ATE computational architecture;graphics;object-oriented programming;automatic test equipment;software engineer;end user;enVision visual environment","","","","","","","","","IEEE","IEEE Conferences"
"A study of effective regression testing in practice","W. E. Wong; J. R. Horgan; S. London; H. Agrawal","Bellcore, Morristown, NJ, USA; NA; NA; NA","Proceedings The Eighth International Symposium on Software Reliability Engineering","","1997","","","264","274","The purpose of regression testing is to ensure that changes made to software, such as adding new features or modifying existing features, have not adversely affected features of the software that should not change. Regression testing is usually performed by running some, or all, of the test cases created to test modifications in previous versions of the software. Many techniques have been reported on how to select regression tests so that the number of test cases does not grow too large as the software evolves. Our proposed hybrid technique combines modification, minimization and prioritization-based selection using a list of source code changes and the execution traces from test cases run on previous versions. This technique seeks to identify a representative subset of all test cases that may result in different output behavior on the new software version. We report our experience with a tool called ATAC (Automatic Testing Analysis tool in C) which implements this technique.","","0-8186-8120","10.1109/ISSRE.1997.630875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630875","","Software testing;Performance evaluation;Software performance;Software debugging;Computer bugs;Time factors","statistical analysis;configuration management;software maintenance;program testing;software tools;minimisation","regression testing;software changes;software features;test cases;program modifications;previous software versions;hybrid technique;modification-based test selection;prioritization-based selection;source code changes;execution traces;representative subset;output behavior;ATAC;Automatic Testing Analysis tool in C;test set minimization;test set prioritization","","130","25","","","","","","IEEE","IEEE Conferences"
"Requirements driven software evolution","L. Tahvildari; K. Kontogiannis","Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada","Proceedings. 12th IEEE International Workshop on Program Comprehension, 2004.","","2004","","","258","259","Software evolution is an integral part of the software life cycle. Furthermore in the recent years the issue of keeping legacy systems operational in new platforms has become critical and one of the top priorities in IT departments worldwide. The research community and the industry have responded to these challenges by investigating and proposing techniques for analyzing, transforming, integrating, and porting software systems to new platforms, languages, and operating environments. However, measuring and ensuring that compliance of the migrant system with specific target requirements have not been formally and thoroughly addressed. We believe that issues such as the identification, measurement, and evaluation of specific re-engineering and transformation strategies and their impact on the quality of the migrant system pose major challenges in the software re-engineering community. Other related problems include the verification, validation, and testing of migrant systems, and the design of techniques for keeping various models (architecture, design, source code) during evolution, synchronized. In this working session, we plan to assess the state of the art in these areas, discuss on-going work, and identify further research issues.","1092-8138","0-7695-2149","10.1109/WPC.2004.1311070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311070","","Software quality;Computer architecture;System testing;Computer industry;Software systems;Software measurement;Software tools;Application software;Software design;Algorithm design and analysis","software prototyping;integrated software;systems re-engineering;formal verification;program testing;software architecture","requirements driven software evolution;software life cycle;legacy system;integrated software;operating environment;software re-engineering;formal verification;program testing;software architecture","","","13","","","","","","IEEE","IEEE Conferences"
"Optimizing automated testing for high throughput","J. C. Connell; L. Wheelwright","NA; NA","Proceedings, IEEE AUTOTESTCON","","2002","","","134","139","Throughput in automated testing is a critical issue, especially at the maintenance level. Some general rules and approaches, when used with an analysis of the total test requirement, can often substantially reduce test time. New programming environments and new (to test equipment) interface standards such as Ethernet, when available, may provide significant throughput improvement in a test system. Agilent's recent work with manufacturers in the wireless industry has helped validate a variety of approaches to optimize test throughput for transceiver manufacturing. We have found that very significant gains can be realized from only a modest investment. This paper describes successful practices and techniques for optimizing test throughput, and gives specific quantitative examples of the improvements that have been achieved.","1080-7725","0-7803-7441","10.1109/AUTEST.2002.1047883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047883","","Automatic testing;Temperature;Throughput;System testing;Frequency locked loops;Instruments;Low voltage;Lifting equipment;Manufacturing industries;Transceivers","automatic test equipment;optimisation;maintenance engineering;transceivers;telecommunication equipment testing;peripheral interfaces;local area networks;automatic test software","automated testing optimization;test throughput;maintenance level test;test time;programming environments;interface standards;Ethernet;wireless industry;transceiver manufacturing","","2","","","","","","","IEEE","IEEE Conferences"
"A detailed cost model for concurrent use with hardware/software co-design","D. Ragan; P. Sandborn; P. Stoaks","Elect. Syst. Cost Modeling Lab., Maryland Univ., College Park, MD, USA; Elect. Syst. Cost Modeling Lab., Maryland Univ., College Park, MD, USA; NA","Proceedings 2002 Design Automation Conference (IEEE Cat. No.02CH37324)","","2002","","","269","274","Hardware/software co-design methodologies generally focus on the prediction of system performance or co-verification of system functionality. This study extends this conventional focus through the development of a methodology and software tool that evaluates system (hardware and software) development, fabrication, and testing costs (dollar costs) concurrent with hardware/software partitioning in a co-design environment. Based on the determination of key metrics such as gate count and lines of software, a new tool called Ghost, evaluates software and hardware development, fabrication, packaging and testing costs. Ghost enables optimization of hardware/software partitioning as a function of specific combinations of hardware foundries and software development environments.","0738-100X","1-58113-461","10.1109/DAC.2002.1012634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012634","","Costs;Hardware;Software tools;Fabrication;Software testing;Software performance;System performance;Software systems;System testing;Software packages","hardware-software codesign;integrated circuit design;integrated circuit economics;integrated circuit modelling;integrated circuit packaging;integrated circuit testing;costing;circuit CAD;computer aided software engineering;project support environments;software cost estimation;software performance evaluation;program testing;software metrics","hardware/software co-design methodology;concurrent use cost model;system performance prediction;system functionality co-verification;concurrent system evaluation software tool;evaluation methodology;hardware/software development;system fabrication;system testing costs;dollar costs;hardware/software partitioning;co-design environment;key metrics;gate count;software lines;Ghost software evaluation tool;system packaging;partitioning optimization;hardware foundries;software development environments;cost modeling;cost-performance trade-off","","3","21","","","","","","IEEE","IEEE Conferences"
"In-parameter-order: a test generation strategy for pairwise testing","Yu Lei; K. C. Tai","Fujitsu Network Commun. Inc., Raleigh, NC, USA; NA","Proceedings Third IEEE International High-Assurance Systems Engineering Symposium (Cat. No.98EX231)","","1998","","","254","261","Pairwise testing (or 2-way testing) is a specification-based testing criterion, which requires that for each pair of input parameters of a system, every combination of valid values of these two parameters be covered by at least one test case. Empirical results show that pairwise testing is practical and effective for various types of software systems. We show that the problem of generating a minimum test set for pairwise testing is NP-complete. We propose a test generation strategy, called in-parameter-order (or IPO), for pairwise testing. For a system with two or more input parameters, the IPO strategy generates a pairwise test set for the first two parameters, extends the test set to generate a pairwise test set for the first three parameters, and continues to do so for each additional parameter. The IPO strategy allows the use of local optimization techniques for test generation and the reuse of existing tests when a system is extended with new parameters or new values of existing parameters. We present practical, IPO-based test generation algorithms. We describe the implementation of an IPO-based test generation tool and show some empirical results.","","0-8186-9221","10.1109/HASE.1998.731623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731623","","System testing;Computer science;Reactive power;Independent component analysis;Software testing;Software systems","program testing;optimisation;computational complexity","in-parameter-order;test generation strategy;pairwise testing;specification-based testing;software testing;NP-complete;IPO strategy;local optimization;test generation tool","","70","5","","","","","","IEEE","IEEE Conferences"
"System test cost modelling based on event rate analysis","D. Farren; A. P. Ambler","Digital Equipment Scotland Ltd., Ayr, UK; NA","Proceedings., International Test Conference","","1994","","","84","92","Unlike IC and board level test, system complexity generally limits the number of methods available to support cost-optimised system test strategy development. This paper describes a parameterised model of system behaviour during both production testing and initial field run-time. The model represents the occurrence rate of error and failure events under test and application workloads and the resulting parameters directly characterise system test effectiveness. These event rate models are fitted to actual data and incorporated into a cost function which calculates overall ""cost of test"" in relation to key variables. The approach is applicable to both hardware and software related events and promotes a customer view of system quality.","1089-3539","0-7803-2103","10.1109/TEST.1994.527939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=527939","","System testing;Software testing;Production systems;Hardware;Integrated circuit testing;Cost function;Application software;Design for testability;Runtime;Manufacturing","computer testing;production testing;DP industry;economics;automatic testing;failure analysis;optimisation;costing","system test cost modelling;event rate analysis;system complexity;cost-optimised system test strategy;parameterised model;production testing;field run-time;occurrence rate;failure events;application workloads;system test effectiveness;cost function;cost model","","9","41","","","","","","IEEE","IEEE Conferences"
"Genetic algorithms for dynamic test data generation","C. C. Michael; G. E. McGraw; M. A. Schatz; C. C. Walton","RST Res., Sterling, VA, USA; NA; NA; NA","Proceedings 12th IEEE International Conference Automated Software Engineering","","1997","","","307","308","In software testing, it is often desirable to find test inputs that exercise specific program features. To find these inputs by hand is extremely time-consuming, especially when the software is complex. Therefore, numerous attempts have been made to automate the process. Random test data generation consists of generating test inputs at random, in the hope that they will exercise the desired software features. Often, the desired inputs must satisfy complex constraints, and this makes a random approach seem unlikely to succeed. In contrast, combinatorial optimization techniques, such as those using genetic algorithms, are meant to solve difficult problems involving the simultaneous satisfaction of many constraints. In this paper, we discuss experiments with a test generation problem that is harder than the ones discussed in earlier literature-we use a larger program and more complex test adequacy criteria. We find a widening gap between a technique based on genetic algorithms and those based on random test generation.","","0-8186-7961","10.1109/ASE.1997.632858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632858","","Genetic algorithms;Software testing;Performance evaluation;Benchmark testing;Fuzzy logic;Automatic control;Constraint optimization;Automatic testing;Materials testing;Minimization methods","genetic algorithms;program testing","software testing;test data generation;program features;combinatorial optimization;test generation;test adequacy criteria;genetic algorithms;random test generation","","33","5","","","","","","IEEE","IEEE Conferences"
"Statistical analysis of time series data on the number of faults detected by software testing","S. Amasaki; T. Yoshitomi; O. Mizuno; T. Kikuno; Y. Takagi","Graduate Sch. of Inf. Sci. & Technol., Osaka Univ., Japan; Graduate Sch. of Inf. Sci. & Technol., Osaka Univ., Japan; Graduate Sch. of Inf. Sci. & Technol., Osaka Univ., Japan; Graduate Sch. of Inf. Sci. & Technol., Osaka Univ., Japan; Graduate Sch. of Inf. Sci. & Technol., Osaka Univ., Japan","Proceedings of the 11th Asian Test Symposium, 2002. (ATS '02).","","2002","","","272","277","According to a progress of the software process improvement, the time series data on the number of faults detected by the software testing are collected extensively. In this paper, we perform statistical analyses of relationships between the time series data and the field quality of software products. At first, we apply the rank correlation coefficient /spl tau/ to the time series data collected from actual software testing in a certain company, and classify these data into four types of trends: strict increasing, almost increasing, almost decreasing, and strict decreasing. We then investigate, for each type of trend, the field quality of software products developed by the corresponding software projects. As a result of statistical analyses, we showed that software projects having trend of almost or strict decreasing in the number of faults detected by the software testing could produce the software products with high quality.","1081-7735","0-7695-1825","10.1109/ATS.2002.1181723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181723","","Statistical analysis;Fault detection;Software testing;Software quality;Time series analysis;Software maintenance;Costs;Information science;Software performance;Performance evaluation","program testing;software process improvement;statistical analysis;time series;software maintenance;software quality","statistical analysis;time series data;software testing;field quality;rank correlation coefficient;strict increasing;almost increasing;almost decreasing;strict decreasing;software projects","","1","13","","","","","","IEEE","IEEE Conferences"
"Test time reduction through minimum execution of tester-hardware setting instructions","J. Hirase","NA","Proceedings 10th Asian Test Symposium","","2001","","","173","178","The introduction of low-priced test systems and the reduction of the test time are necessary in order to decrease the testing costs that are included in the cost of manufacturing VLSI. However, coupled with the miniaturization of the fabrication process, the test time tends to become considerably longer for multifunctional and complex VLSI with high integration. In this paper, we present a new method enabling the automatic reduction of the test time. This method consists of shortening the test time by installing virtual tester hardware on the tester CPU memory in order to delete duplicate tester hardware setting instructions. The efficiency of this method is proven by experiments showing that a test time reduction of 5/spl sim/25% could be obtained.","1081-7735","0-7695-1378","10.1109/ATS.2001.990277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990277","","Circuit testing;Very large scale integration;Hardware;System testing;Automatic testing;Time measurement;Voltage;Costs;Fabrication;Performance evaluation","VLSI;integrated circuit testing;automatic test equipment;automatic test pattern generation;automatic test software;production testing","test time reduction;tester-hardware setting instructions;minimum execution;low-priced test systems;testing costs;VLSI manufacturing cost;automatic reduction;virtual tester hardware;test time optimizer;ATPG;tester CPU memory;instructions duplication deletion","","2","3","","","","","","IEEE","IEEE Conferences"
"Membrane design for MCM and BGA substrate test","J. Cofield","Mentor Graphics Corp., San Jose, CA, USA","Proceedings. 1998 International Conference on Multichip Modules and High Density Packaging (Cat. No.98EX154)","","1998","","","70","73","Increasing IC complexity is placing new demands on multichip module (MCM) test engineers. Traditional ""bed of nails"" and ""flying probe"" test methods are not adequate to meet the demands of high pin count and density. Design tools need to keep pace with new test methods being employed to address this rising complexity. One approach to solving this problem is the use of a membrane test head. This paper discusses the design software requirements and CAD challenges that result from the membrane approach to substrate test. Some of these new challenges include data translation, test probe assignment and optimization, routing, and inner layer testing. The test methods and design requirements discussed are applicable to multichip, few-chip and ball grid array (BGA) packaging.","","0-7803-4850","10.1109/ICMCM.1998.670757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670757","","Biomembranes;Multichip modules;Integrated circuit testing;Software design;Design automation;Software testing;Probes;Routing;Design methodology;Electronics packaging","multichip modules;integrated circuit packaging;surface mount technology;network routing;optimisation;circuit CAD;software tools;printed circuit testing;printed circuit design;membranes","membrane design;BGA substrate test;MCM substrate test;IC complexity;multichip module test;bed of nails testing;flying probe testing;pin count;package density;design tools;test methods;membrane test head;design software;CAD;data translation;test probe assignment;test probe optimization;routing;inner layer testing;multichip packaging;few-chip packaging;ball grid array packaging","","","3","","","","","","IEEE","IEEE Conferences"
"Optimal testing resource allocation, and sensitivity analysis in software development","Chin-Yu Huang; M. R. Lyu","Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; NA","IEEE Transactions on Reliability","","2005","54","4","592","603","We consider two kinds of software testing-resource allocation problems. The first problem is to minimize the number of remaining faults given a fixed amount of testing-effort, and a reliability objective. The second problem is to minimize the amount of testing-effort given the number of remaining faults, and a reliability objective. We have proposed several strategies for module testing to help software project managers solve these problems, and make the best decisions. We provide several systematic solutions based on a nonhomogeneous Poisson process model, allowing systematic allocation of a specified amount of testing-resource expenditures for each software module under some constraints. We describe several numerical examples on the optimal testing-resource allocation problems to show applications & impacts of the proposed strategies during module testing. Experimental results indicate the advantages of the approaches we proposed in guiding software engineers & project managers toward best testing resource allocation in practice. Finally, an extensive sensitivity analysis is presented to investigate the effects of various principal parameters on the optimization problem of testing-resource allocation. The results can help us know which parameters have the most significant influence, and the changes of optimal testing-effort expenditures affected by the variations of fault detection rate & expected initial faults.","0018-9529;1558-1721","","10.1109/TR.2005.858099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546566","Non-homogeneous Poisson processes;sensitivity analysis;software reliability;testing resource allocation","Software testing;Resource management;Sensitivity analysis;Programming;System testing;Fault detection;Software reliability;Power system reliability;Project management;Tellurium","resource allocation;software fault tolerance;program testing;stochastic processes;project management;optimisation;sensitivity analysis;decision making","software testing-resource allocation;sensitivity analysis;software development;software reliability;software project manager;decision making;nonhomogeneous Poisson process;numerical analysis;software engineer;optimization;software fault","","40","40","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal resource allocation and sensitivity analysis for modular software testing","Chin-Yu Huang; Jung-Hua Lo; Jenn-Wei Lin; Chuan-Ching Sue; Chu-Ti Lin","Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; NA; NA; NA; NA","Fifth International Symposium on Multimedia Software Engineering, 2003. Proceedings.","","2003","","","231","238","Software testing takes place almost continuously throughout the software developmental life-cycle. Actually, module testing is the most detailed form of testing to be performed. The software programmer is generally responsible for testing the modules of the program, ensuring that each performs the function for which it was designed. Therefore, managers should know how to allocate the specified testing-resources among all the modules and develop quality software with high reliability. We present the optimal policies of testing-resource allocation for modular software systems. Our methodologies provide practical approaches to the optimization of testing-resource allocation with a reliability objective. Some theorems and numerical examples for the optimal testing-effort allocation policies are demonstrated. Besides, sensitivity analysis is also discussed in detail. Using the proposed strategies for module testing, project managers can make the best decisions.","","0-7695-2031","10.1109/MMSE.2003.1254446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254446","","Resource management;Sensitivity analysis;Software testing;Performance evaluation;Software performance;Programming profession;Quality management;Software development management;Software quality;System testing","program testing;software quality;software reliability;sensitivity analysis;resource allocation","modular software testing;module testing;software developmental life-cycle;software quality;software reliability;optimal resource allocation;sensitivity analysis","","3","23","","","","","","IEEE","IEEE Conferences"
"Putting your best tests forward","G. Rothermel; S. Elbaum","Oregon State Univ., Corvallis, OR, USA; NA","IEEE Software","","2003","20","5","74","77","Test case prioritization orders tests so that they help you meet your testing goals earlier during regression testing. Prioritization techniques can, for example, order tests to achieve coverage at the fastest rate possible, exercise features in order of expected frequency of use, or reveal faults as early as possible. We focus on the last goal, which we describe as ""increasing a test suite's rate of fault detection"" or the speed with which the test suite reveals faults. A faster fault detection rate during regression testing provides earlier feedback on a system under test, supporting earlier strategic decisions about release schedules and letting engineers begin debugging sooner. Also, if testing time is limited or unexpectedly reduced, prioritization increases the chance that testing resources will have been spent as cost effectively as possible in the available time.","0740-7459;1937-4194","","10.1109/MS.2003.1231157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231157","","Software testing;Costs;Fault detection;System testing;Feedback;History;Software quality;Irrigation;Frequency;Debugging","program testing;program debugging;software engineering","regression testing;software testing;test case prioritization;coverage;fault detection rate;release schedules;debugging","","12","10","","","","","","IEEE","IEEE Journals & Magazines"
"Proceedings Design, Automation and Test in Europe Conference and Exhibition","","","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","","","The following topics are dealt with: ambient intelligence - visions and achievements; energy-efficient memory systems; uncertainty; power-aware design and synthesis; test data compression; operating system abstraction and targeting; embedded software forum; analysis of jitter and noise for analogue systems; semiconductor device modelling and simulation; embedded system scheduling and analysis; DFT and BIST recent advances; analogue and RF modelling, simulation and optimisation; architectural level synthesis; scheduling in reconfigurable computing; delay testing and diagnosis; embedded operating systems for SoC; networks-on-chip; system level modelling; reconfigurable SoC; analogue and defect-oriented testing; energy aware software techniques; interconnect modelling and signal integrity; system level simulation; software optimisation; global approaches to layout synthesis; platform design and IP reuse methods; on-line testing and self-repair; safe automotive software development; mixed-signal design techniques; low power architectures; SoC testing; SAT based verification; highly integrated communication systems; chip estate zoning; asynchronous circuits; collaborative design and WWW-based tools; hardware/software codesign optimization; test pattern generation; low power software; application specific memory synthesis; CAD.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253577","","Intelligent networks;Uncertain systems;Data compression;Operating systems;Circuit noise;Jitter;Semiconductor device modeling;Processor scheduling;Design for testability;Self-testing;Circuit optimization;High-level synthesis;Reconfigurable architectures;Mixed analog-digital integrated circuits;Communication equipment;Integrated circuit design;Collaborative work;Integrated circuit testing;Asynchronous logic circuits;Internet;Semiconductor memories;Design automation","intelligent networks;low-power electronics;uncertain systems;data compression;operating systems (computers);embedded systems;circuit noise;jitter;semiconductor device models;processor scheduling;design for testability;built-in self test;circuit simulation;circuit optimisation;high level synthesis;reconfigurable architectures;system-on-chip;interconnections;industrial property;mixed analogue-digital integrated circuits;formal verification;telecommunication equipment;integrated circuit design;groupware;integrated circuit testing;asynchronous circuits;Internet;hardware-software codesign;automatic test pattern generation;semiconductor storage;CAD","ambient intelligence;energy-efficient memory systems;uncertainty;power-aware design;test data compression;operating systems;embedded software;analogue system noise;jitter;semiconductor device modelling;scheduling;DFT;BIST;RF modelling;analogue modelling;simulation;optimisation;high level synthesis;reconfigurable computing;delay testing;SoC;networks-on-chip;energy aware software;interconnect modelling;signal integrity;IP reuse;self-repair;on-line testing;automotive software;mixed-signal design;low power architectures;SAT based verification;integrated communication systems;chip estate zoning;asynchronous circuits;collaborative design;WWW-based tools;hardware/software codesign;test pattern generation;CAD;application specific memory synthesis","","","","","","","","","IEEE","IEEE Conferences"
"A hybrid BIST architecture and its optimization for SoC testing","G. Jervan; Z. Peng; R. Ubar; H. Kruus","Linkoping Univ., Sweden; Linkoping Univ., Sweden; NA; NA","Proceedings International Symposium on Quality Electronic Design","","2002","","","273","279","This paper presents a hybrid BIST architecture and methods for optimizing it to test system-on-chip in a cost effective way. The proposed self-test architecture can be implemented either only in software or by using some test related hardware. In our approach we combine pseudorandom test patterns with stored deterministic test patterns to perform core test with minimum time and memory, without losing test quality. We propose two algorithms to calculate the cost of the rest process. To speed up the optimization procedure, a Tabu search based method is employed for finding the global cost minimum. Experimental results have demonstrated the feasibility and efficiency of the approach and the significant decreases in overall test cost.","","0-7695-1561","10.1109/ISQED.2002.996750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996750","","Built-in self-test;Computer architecture;Optimization methods;Cost function;System testing;System-on-a-chip;Automatic testing;Software testing;Hardware;Performance evaluation","VLSI;application specific integrated circuits;integrated circuit testing;built-in self test;automatic test pattern generation;optimisation;search problems;logic testing;costing","hybrid BIST architecture;system-on-chip;self-test architecture;SoC testing;pseudorandom test patterns;stored deterministic test patterns;core test;ATPG based generation;fault table based generation;test quality;test process cost calculation;optimization procedure;Tabu search based method;global cost minimum","","14","15","","","","","","IEEE","IEEE Conferences"
"A controlled experiment assessing test case prioritization techniques via mutation faults","H. Do; G. Rothermel","Dept. of Comput. Sci. & Eng., Nebraska Univ., Omaha, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Omaha, NE, USA","21st IEEE International Conference on Software Maintenance (ICSM'05)","","2005","","","411","420","Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. We also compare our results to those collected earlier with respect to the relationship between hand-seeded faults and mutation faults, and the implications this has for researchers performing empirical studies of prioritization.","1063-6773","0-7695-2368","10.1109/ICSM.2005.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510136","","Computer aided software engineering;Genetic mutations;Software testing;Fault detection;Software maintenance;System testing;Performance evaluation;Computer science;Automatic testing;Automatic control","program testing;software maintenance","test case prioritization technique;mutation fault;regression testing;software maintenance;fault detection;hand-seeded faults","","26","28","","","","","","IEEE","IEEE Conferences"
"Quality Function Deployment (QFD) in testing","A. Rahman","IBM UK Labs. Ltd., Winchester, UK","IEE Colloquium on Automated Testing and Software Solutions","","1992","","","6/1","6/7","QFD is a quality-oriented process that can play an important role in the market-driven, total quality control environment. It can be deployed in almost all areas of product development, test and manufacturing processes. QFD is one way to ensure the reliability of the software products. The author is concerned with the role that software testing can play in increasing the reliability of software. He also examines how Quality Function Deployment (QFD) could be used to achieve this objective. QFD provides integration of the various functions by tying design and process activities together. It priorities product and manufacturing-process characteristics and highlights areas which would require further analysis.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=168144","","Automatic testing;Quality control;Software reliability","automatic testing;quality control;software reliability","total quality control;reliability;software products;software testing","","","","","","","","","IET","IET Conferences"
"Automatic test case optimization using a bacteriological adaptation model: application to .NET components","B. Baudry; F. Fleurey; J. -. Jezequel; Y. Le Traon","IRISA, Rennes, France; NA; NA; NA","Proceedings 17th IEEE International Conference on Automated Software Engineering,","","2002","","","253","256","In this paper, we present several complementary computational intelligence techniques that we explored in the field of .Net component testing. Mutation testing serves as the common backbone for applying classical and new artificial intelligence (AI) algorithms. With mutation tools, we know how to estimate the revealing power of test cases. With AI, we aim at automatically improving test case efficiency. We therefore looked first at genetic algorithms (GA) to solve the problem of test. The aim of the selection process is to generate test cases able to kill as many mutants as possible. We then propose a new AI algorithm that fits better to the test optimization problem, called bacteriological algorithm (BA): BAs behave better that GAs for this problem. However, between GAs and BAs, a family of intermediate algorithms exists: we explore the whole spectrum of these intermediate algorithms to determine whether an algorithm exists that would be more efficient than BAs.: the approaches are compared on a .Net system.","1938-4300","0-7695-1736","10.1109/ASE.2002.1115023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115023","","Automatic testing;Computer aided software engineering;Adaptation model;Genetic mutations;Artificial intelligence;System testing;Spine;Genetic algorithms;Computational intelligence;Robustness","network operating systems;genetic algorithms;automatic testing;object-oriented programming;program testing","bacteriological adaptation model;automatic test case optimization;.Net component testing;mutation testing;genetic algorithms;selection process;mutant killing;AI algorithm;bacteriological algorithm","","15","7","","","","","","IEEE","IEEE Conferences"
"Prioritize code for testing to improve code coverage of complex software","J. J. Li","Avaya Labs Res., Basking Ridge, NJ, USA","16th IEEE International Symposium on Software Reliability Engineering (ISSRE'05)","","2005","","","10 pp.","84","Code prioritization for testing promises to achieve the maximum testing coverage with the least cost. This paper presents an innovative method to provide hints on which part of code should be tested first to achieve best code coverage. This method claims two major contributions. First it takes into account a ""global view"" of the execution of a program being tested, by considering the impact of calling relationship among methods/functions of complex software. It then relaxes the ""guaranteed"" condition of traditional dominator analysis to be ""at least"" relationship among dominating nodes, which makes dominator calculation much simpler without losing its accuracy. It also then expands this modified dominator analysis to include global impact of code coverage, i.e. the coverage of the entire software other than just the current function. We implemented two versions of code prioritization methods, one based on original dominator analysis and the other on relaxed dominator analysis with global view. Our comparison study shows that the latter is consistently better in terms of identifying code for testing to increase code coverage","1071-9458;2332-6549","0-7695-2482","10.1109/ISSRE.2005.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544723","Testing;code coverage;dominator analysis;coverage priority","Software testing;Costs;Software reliability;Software design;Programming;History;Software engineering;Area measurement;Guidelines;Frequency","partial evaluation (compilers);program diagnostics;program testing","code coverage;software testing;code prioritization;program execution;dominating nodes;relaxed dominator analysis","","6","10","","","","","","IEEE","IEEE Conferences"
"Optimizing software reliability management process","A. Elentukh; R. Wang","Motorola, Mansfield, MA, USA; NA","[1992] Proceedings Third International Symposium on Software Reliability Engineering","","1992","","","34","40","Establishing a company-wide software reliability management (SRM) program is a well known challenge. Corporate support is imperative to sustain progress in this direction. It is a greater challenge to create a reliability management program that is integrated into the overall development/support process, optimizing both of these inherently parallel activities. This paper describes the authors' experience at Motorola Codex by outlining most of the activities of the SRM program. There are many reasons why the SRM program should be examined as a whole, rather than as separate components, such as testing or objective definition. The optimization of the complete program is unthinkable, for example, if people who select models do not clearly realize where the data for the models is coming from. This paper suggests a global view, which is the best for defining a logical order of SRM-related activities and fitting them into a maturity paradigm.<<ETX>>","","0-8186-2975","10.1109/ISSRE.1992.285860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285860","","Software reliability;Reluctance motors;Software testing;System testing;Failure analysis;Analytical models;Availability;Computer aided software engineering;Databases;Stability","DP management;optimisation;software reliability","process optimisation;company-wide programme;corporate support;logical order of activities;software reliability management;development/support process;parallel activities;Motorola Codex;global view;maturity paradigm","","1","9","","","","","","IEEE","IEEE Conferences"
"Techniques for testing component-based software","Ye Wu; Dai Pan; Mei-Hwa Chen","Dept. of Inf. & Software Eng., George Mason Univ., Fairfax, VA, USA; NA; NA","Proceedings Seventh IEEE International Conference on Engineering of Complex Computer Systems","","2001","","","222","232","Component-based software engineering is increasingly being adopted for software development. Although much work has been proposed for building component-based-systems, techniques for testing component-based systems have not been well developed. We present a test model that depicts a generic infrastructure of component-based systems and suggests key test elements. The test model is realized using a component interaction graph (CIG) in which the interactions and the dependence relationships among components are illustrated. By utilizing the CIG, we propose a family of test adequacy criteria which allow optimization of the balancing among budget, schedule, and quality requirements typically necessary in software development. The methodology proposed is efficient and effective, as demonstrated by promising results obtained from a case study.","","0-7695-1159","10.1109/ICECCS.2001.930181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=930181","","Software testing;System testing;Software systems;Software quality;Software reusability;Application software;Software engineering;Computer languages;Job shop scheduling;Programming","software reusability;program testing;software quality;graph theory","component-based software testing;software development;component interaction graph;dependence relationships;test adequacy criteria;budget;schedule;software quality;case study;software reuse","","19","41","","","","","","IEEE","IEEE Conferences"
"A quasi-renewal process for software reliability and testing costs","Hoang Pham; Hongzhou Wang","Dept. of Ind. Eng., Rutgers Univ., Piscataway, NJ, USA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","","2001","31","6","623","631","This paper models software reliability and testing costs using a new tool: a quasi-renewal process. It is assumed that the cost of fixing a fault during software testing phase, consists of both deterministic and incremental random parts, increases as the number of faults removed increases. Several software reliability and cost models by means of quasi-renewal processes are derived in which successive error-free times are independent and increasing by a fraction. The maximum likelihood estimates of parameters associated with these models are provided. Based on the valuable properties of quasi-renewal processes, the expected software testing and debugging cost, number of remaining faults in the software, and mean error-free time after testing are obtained. A class of related optimization problem is then contemplated and optimum testing policies incorporating both reliability and cost measures are investigated. Finally, numerical examples are presented through a set of real testing data to illustrate the models results.","1083-4427;1558-2426","","10.1109/3468.983418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983418","","Software reliability;Software testing;Cost function;Software debugging;Application software;Markov processes;Statistics;Software development management;Maximum likelihood estimation;Parameter estimation","software reliability;program testing;software cost estimation;software development management;maximum likelihood estimation;optimisation","availability;software release time;quasi renewal processes;software cost model;software reliability;software testing;maximum likelihood estimation;parameter estimation;optimization","","25","25","","","","","","IEEE","IEEE Journals & Magazines"
"Generating, selecting and prioritizing test cases from specifications with tool support","Y. T. Yu; S. P. Ng; E. Y. K. Chan","Dept. of Comput. Sci., City Univ. of Hong Kong, China; NA; NA","Third International Conference on Quality Software, 2003. Proceedings.","","2003","","","83","90","The classification-tree method provides a systematic way for software testers to derive test cases by considering important relevant aspects that are identified from the specification. The method has been used in many real-life applications and shown to be effective. This paper presents several enhancements to the method by annotating the classification tree with additional information to reduce manual effort in the generation, selection and prioritization of test cases. A tool for supporting this enhanced process is also described.","","0-7695-2015","10.1109/QSIC.2003.1319089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319089","","Computer aided software engineering;Software testing;Classification tree analysis;System testing;Computer science;Software systems;Automatic testing;Life testing;Tree graphs;Information technology","program testing;formal specification","test case generation;test case selection;test case prioritization;classification-tree method;test case derivation;classification tree annotation;black box testing;partition testing;software testing;specification-based testing","","8","17","","","","","","IEEE","IEEE Conferences"
"A Constraint-Based Solution for On-Line Testing of Processors Embedded in Real-Time Applications","M. Moraes; E. Cota; L. Carro; F. Wagner; M. Lubaszewski","PPGC - Instituto de lnformtica, UFRGS, Av. Bento Gonalves, 9500 - Bloco IV, 91501-970 Porto Alegre - Brazil. +55 51 3316-6161, msmoraes@inf.ufrgs.br; PPGC - Instituto de lnformtica, UFRGS, Av. Bento Gonalves, 9500 - Bloco IV, 91501-970 Porto Alegre - Brazil. +55 51 3316-6161, erika@inf.ufrgs.br; PPGC - Instituto de lnformtica, UFRGS, Av. Bento Gonalves, 9500 - Bloco IV, 91501-970 Porto Alegre - Brazil. +55 51 3316-6161; PPGEE - Depto. Energia Eltrica, UFRGS, Av. Osvaldo Aranha, 103, 90035-190 Porto Alegre - Brazil. +55 51 3316-3515, carro@eletro.ufrgs.br; PPGC - Instituto de lnformtica, UFRGS, Av. Bento Gonalves, 9500 - Bloco IV, 91501-970 Porto Alegre - Brazil. +55 51 3316-6161, flavio@inf.ufrgs.br; PPGC - Instituto de lnformtica, UFRGS, Av. Bento Gonalves, 9500 - Bloco IV, 91501-970 Porto Alegre - Brazil. +55 51 3316-6161; PPGEE - Depto. Energia Eltrica, UFRGS, Av. Osvaldo Aranha, 103, 90035-190 Porto Alegre - Brazil. +55 51 3316-3515, luba@eletro.ufrgs.br","2005 18th Symposium on Integrated Circuits and Systems Design","","2005","","","68","73","Software-based self-test has been proposed as a low-cost strategy for on-line periodic testing of embedded processors. In this paper, we show that structural test programs composed only by regular deterministic self-test routines may be unfeasible in a real-time embedded platform. Hence, we propose a method to consciously select a set of test routines from different test approaches to compose a test program for an embedded processor. The proposed method not only ensures the periodical execution of the test, but also considers the optimization of memory and real-time requirements of the application, which are important constraints in embedded systems. Experimental results for a Java processor running real-time tasks demonstrate the effectiveness of the proposed solution","","1-59593-174","10.1109/SBCCI.2005.4286834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286834","Performance;Design;Reliability;On-Line Testing;Software-Based Self-Test;Test Space Exploration;Embedded Processors;Real-Time Systems","Built-in self-test;Fault detection;System testing;Automatic testing;Software testing;Real time systems;Application software;Delay;Timing;Embedded system","built-in self test;embedded systems;integrated circuit testing;microprocessor chips","constraint-based solution;embedded processor online testing;real-time application;software-based self-test;test routine;test program;periodic testing;memory optimization;Java processor;test space exploration","","3","12","","","","","","IEEE","IEEE Conferences"
"Optimize defect detection techniques through empirical software engineering method","Hai Tao Sun","IBM Corp., Rochester, MN, USA","2005 IEEE International Conference on Electro Information Technology","","2005","","","6 pp.","6","This paper introduces twelve defect detection techniques and describes a non-controlled experiment related to defect detection techniques to address the uncertainty of how to test an embedded software and find defects effectively. In this non-controlled experiment, three common testing techniques were applied to a large scale embedded system. This study is intended to evaluate different defect detection techniques that are actually used by software engineers using empirical software engineering method. The objective of empirical software engineering is to improve the software development processes and quality. This could be done by evaluating, comparing and controlling defect detection methods. This study is also intended to find a best method to reduce defects and increase the defect detection rate in a large scale embedded system, since defect detection is considered as one of the most costly development process in software development cycle","2154-0357;2154-0373","0-7803-9232","10.1109/EIT.2005.1627056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1627056","Defect detection techniques;evaluation;empirical software engineering","Optimization methods;Software engineering;Programming;Software testing;Logic testing;Embedded software;Embedded system;Large-scale systems;Software tools;Costs","embedded systems;program testing;software engineering;software fault tolerance","defect detection;empirical software engineering;embedded software testing;software development processes;software development quality;software development cycle","","1","21","","","","","","IEEE","IEEE Conferences"
"Software reliability and redundancy optimization","D. -. Chi; H. -. Lin; W. Kuo","Iowa State Univ., Ames, IA, USA; NA; NA","Proceedings., Annual Reliability and Maintainability Symposium","","1989","","","41","45","A procedure for reliability-related quality programming is developed to fill existing gaps in software design and development so that a quality programming plan can be achieved. The authors investigate the tradeoff between system reliability improvement and resource consumption through the management phase. A software reliability-to-cost relation is developed from both a software reliability-related cost model and software redundancy models with common-cause failures. The software reliability optimization problem can be formulated into a mixed-integer programming problem and solved by a branch-and-bound technique.<<ETX>>","","","10.1109/ARMS.1989.49570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=49570","","Software reliability;Redundancy;Software debugging;Cost function;Software performance;Personnel;System testing;Hardware;Design optimization;Resource management","failure analysis;integer programming;optimisation;redundancy;software reliability","software development;software reliability;redundancy optimization;reliability-related quality programming;software design;quality programming plan;system reliability improvement;resource consumption;management phase;software reliability-to-cost relation;common-cause failures;mixed-integer programming;branch-and-bound technique","","8","10","","","","","","IEEE","IEEE Conferences"
"Constructing test suites for interaction testing","M. B. Cohen; P. B. Gibbons; W. B. Mugridge; C. J. Colbourn","Dept. of Comput. Sci., Univ. of Auckland, New Zealand; Dept. of Comput. Sci., Univ. of Auckland, New Zealand; Dept. of Comput. Sci., Univ. of Auckland, New Zealand; NA","25th International Conference on Software Engineering, 2003. Proceedings.","","2003","","","38","48","Software system faults are often caused by unexpected interactions among components. Yet the size of a test suite required to test all possible combinations of interactions can be prohibitive in even a moderately sized project. Instead, we may use pairwise or t-way testing to provide a guarantee that all pairs or t-way combinations of components are tested together This concept draws on methods used in statistical testing for manufacturing and has been extended to software system testing. A covering array, CA(N; t, k, v), is an N/spl times/k array on v symbols such that every N x t sub-array contains all ordered subsets from v symbols of size t at least once. The properties of these objects, however do not necessarily satisfy real software testing needs. Instead we examine a less studied object, the mixed level covering array and propose a new object, the variable strength covering array, which provides a more robust environment for software interaction testing. Initial results are presented suggesting that heuristic search techniques are more effective than some of the known greedy methods for finding smaller sized test suites. We present a discussion of an integrated approach for finding covering arrays and discuss how application of these techniques can be used to construct variable strength arrays.","0270-5257","0-7695-1877","10.1109/ICSE.2003.1201186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201186","","System testing;Software testing;Software systems;Local area networks;Linux;ISDN;Printers;Computer science;Statistical analysis;Manufacturing","program testing;software fault tolerance;optimisation","software system faults;t-way testing;statistical testing;software system testing;mixed level covering array;variable strength covering array;software interaction testing;heuristic search technique;greedy methods;interaction testing","","114","30","","","","","","IEEE","IEEE Conferences"
"Contributing to the bottom line: optimizing reliability cost schedule tradeoff and architecture scalability through test technology","D. Boyd; D. Cura; W. Ehreich; R. Gotberg; R. Hariharan; P. Reeser","AT&T Labs., USA; NA; NA; NA; NA; NA","Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000","","2000","","","136","147","A challenging problem in software testing is finding the optimal point at which costs justify the stop-test decision. We first present an economic model that can be used to evaluate the consequences of various stop-test decisions. We then discuss two approaches for assessing performance, automated load test generation in the context of empirical testing and performance modeling, and illustrate how these techniques can affect the stop-test decision. We then illustrate the application of these two techniques to evaluating the performance of Web servers that performs significant server-side processing through object-oriented (OO) computing. Implications of our work for Web server performance evaluation in general are discussed.","1071-9458","0-7695-0807","10.1109/ISSRE.2000.885867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885867","","Software testing;System testing;Cost function;Object oriented modeling;Fault detection;Automatic testing;Web server;Scalability;Context modeling;Application software","software reliability;software cost estimation;software architecture;program testing;software performance evaluation;file servers;Internet;distributed object management","reliability cost schedule tradeoff;architecture scalability;software testing;stop-test decision;economic model;automated load test generation;performance modeling;Web servers;server-side processing;object-oriented computing","","2","6","","","","","","IEEE","IEEE Conferences"
"A dynamic optimization strategy for evolutionary testing","Xiaoyuan Xie; Baowen Xu; Liang Shi; Changhai Nie; Yanxiang He","Dept. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Dept. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Dept. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Dept. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; NA","12th Asia-Pacific Software Engineering Conference (APSEC'05)","","2005","","","8 pp.","","Evolutionary testing (ET) is an efficient technique of automated test case generation. ET uses a kind of metaheuristic search technique, genetic algorithm (GA), to convert the task of test case generation into an optimal problem. The configuration strategies of GA have notable influences upon the performance of ET. In this paper, represent a dynamic self-adaptation strategy for evolutionary structural testing. It monitors evolution process dynamically, detects the symptom of prematurity by analyzing the population, and adjusts the mutation possibility to recover the diversity of the population. The empirical results show that the strategy can greatly improve the performance of the ET in many cases. Besides, some valuable advices are provided for the configuration strategies of ET by the empirical study.","1530-1362;1530-1362","0-7695-2465","10.1109/APSEC.2005.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607196","Software testing;evolutionary testing;structural testing;dynamic optimization","Automatic testing;Software testing;Evolution (biology);Computer science;Software engineering;Genetic algorithms;Genetic mutations;System testing;Delay;Laboratories","program testing;formal verification;genetic algorithms","dynamic optimization strategy;automated test case generation;metaheuristic search technique;genetic algorithm;evolutionary structural testing","","","16","","","","","","IEEE","IEEE Conferences"
"Modeling the effects of combining diverse software fault detection techniques","B. Littlewood; P. T. Popov; L. Strigini; N. Shryane","Centre for Software Reliability, City Univ., London, UK; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1157","1167","Considers what happens when several different fault-finding techniques are used together. The effectiveness of such multi-technique approaches depends upon a quite subtle interplay between their individual efficacies. The modeling tool we use to study this problem is closely related to earlier work on software design diversity which showed that it would be unreasonable even to expect software versions that were developed truly independently to fail independently of one another. The key idea was a ""difficulty function"" over the input space. Later work extended these ideas to introduce a notion of ""forced"" diversity. In this paper, we show that many of these results for design diversity have counterparts in diverse fault detection in a single software version. We define measures of fault-finding effectiveness and diversity, and show how these might be used to give guidance for the optimal application of different fault-finding procedures to a particular program. The effects on reliability of repeated applications of a particular fault-finding procedure are not statistically independent; such an incorrect assumption of independence will always give results that are too optimistic. For diverse fault-finding procedures, it is possible for effectiveness to be even greater than it would be under an assumption of statistical independence. Diversity of fault-finding procedures is a good thing and should be applied as widely as possible. The model is illustrated using some data from an experimental investigation into diverse fault-finding on a railway signalling application.","0098-5589;1939-3520;2326-3881","","10.1109/32.888629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888629","","Diversity reception;Fault detection;Software design;Application software;Redundancy;Hardware;Aerospace control;Battery powered vehicles;Software engineering;Particle measurements","system recovery;program diagnostics;software reliability;signalling;railways","software fault detection techniques;multi-technique approach;modeling tool;software design diversity;independently developed software versions;system failure;difficulty function;forced diversity;fault-finding effectiveness;repeated application reliability;diverse fault-finding procedures;statistical independence;railway signalling application;fault removal;software testing;software reliability growth","","20","15","","","","","","IEEE","IEEE Journals & Magazines"
"Automated experimental design for automatic test equipment software","L. G. Allred; P. B. Kelly; J. P. Harames","Software Eng. Syst. (TIS), Hill AFB, UT, USA; NA; NA","Conference Record. AUTOTESTCON '96","","1996","","","156","159","Achieving an effective stimulation pattern for an electronic circuit card is an elusive and labor intensive task. In contrast to the vectorless testing concept which assumes nothing about the circuit card, our implementation would contain a circuit topology and a designation of devices in the circuit, including standard designation input devices, output devices, voltage and current sources, etc. A library interface is supplied to specify (when required) the safe ranges of stimulation, signal sources and analog waveforms, probe paths and if desired, an operating range and quantization of a device. As opposed to passive testing, this technique allows the tester to jump into the operating range of the device. Stimulation pattern effectiveness is measured in terms of output entropy. While the initial stimulation pattern may be selected at random, directed search techniques allow the optimization of the stimulation by eliminating redundancy, and changing existing patterns toward a stimulation pattern of increased entropy. A digital application of this concept is being developed for the Electronic Work Bench (EWB), a new VXI system using components from Talon. While a fully automatic process is not always practical, this concept should make considerable headway toward reducing to human labor cost while improving the overall process.","1088-7725","0-7803-3379","10.1109/AUTEST.1996.547690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547690","","Design for experiments;Automatic test equipment;Circuit testing;Entropy;Electronic circuits;Circuit topology;Voltage;Libraries;Probes;Quantization","automatic test software;printed circuit testing;fault diagnosis;entropy;peripheral interfaces;software engineering;network topology;infrared imaging","Electronic Work Bench;automatic test equipment software;electronic circuit card;universal diagnostic capability;stimulation patterns;VXI;circuit topology;Talon;current sources;voltage sources;library interface;signal sources;analog waveforms;probe paths;labor cost;output entropy;directed search techniques;optimization;redundancy","","","12","","","","","","IEEE","IEEE Conferences"
"A software technique for optimizing measurement time and repeatability on instrument control buses","A. B. McCarthy","National Instruments, Austin, TX, USA","IEEE Autotestcon, 2005.","","2005","","","374","379","This paper and presentation examine major factors affecting measurement time and repeatability, especially the bus used to control the instrument. Test systems can generally be divided into two groups - those optimizing measurement time (such as in production) and those preferring to improve repeatability (such as automation or control unit testing). The paper demonstrates how to develop a performance benchmark for measurement time and repeatability using a high-precision timer and standard set of typical instrument tasks. Systematic benchmarking of individual changes in various hardware and software factors show that factors impact on measurement time and repeatability. Because the tests and methodologies are generally applicable, system designers can measure and control the factors required to optimize their system measurement time and repeatability.","1088-7725;1558-4550","0-7803-9101","10.1109/AUTEST.2005.1609161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609161","","Software measurement;Time measurement;Instruments;Automatic control;System testing;Automatic testing;Control systems;Benchmark testing;Production systems;Automation","computerised instrumentation;automatic test software;field buses;benchmark testing","software technique;system measurement time;system repeatability;instrument control buses;test systems;performance benchmark;high-precision timer;instrumentation buses;automatic test software","","2","","","","","","","IEEE","IEEE Conferences"
"The effect of imperfect error detection on reliability assessment via life testing","P. E. Ammann; S. S. Brilliant; J. C. Knight","Dept. of Inf. & Software Syst. Eng., George Mason Univ., Fairfax, VA, USA; NA; NA","IEEE Transactions on Software Engineering","","1994","20","2","142","148","Measurement of software reliability by life testing involves executing the software on large numbers of test cases and recording the results. The number of failures observed is used to bound the failure probability even if the number of failures observed is zero. Typical analyses assume that all failures that occur are observed, but, in practice, failures occur without being observed. In this paper, we examine the effect of imperfect error detection, i.e. the situation in which a failure of the software may not be observed. If a conventional analysis associated with life testing is used, the confidence in the bound on the failure probability is optimistic. Our results show that imperfect error detection does not necessarily limit the ability of life testing to bound the probability of failure to the very low values required in critical systems. However, we show that the confidence level associated with a bound on failure probability cannot necessarily be made as high as desired, unless very strong assumptions are made about the error detection mechanism. Such assumptions are unlikely to be met in practice, and so life testing is likely to be useful only for situations in which very high confidence levels are not required.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.265635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=265635","","Life testing;Software testing;Failure analysis;Life estimation;Software reliability;Hardware;Software measurement;Predictive models;NASA;Software systems","software reliability;error detection;life testing;probability","imperfect error detection;software reliability assessment;life testing;test cases;failure probability;unobserved failures;test oracles;critical systems;confidence level;software testing","","20","14","","","","","","IEEE","IEEE Journals & Magazines"
"Matlab extensions for the development, testing and verification of real-time DSP software","D. P. Magee","Texas Instruments, Dallas, TX, USA","Proceedings. 42nd Design Automation Conference, 2005.","","2005","","","603","606","The purpose of this paper is to present the required tools for the development, testing and verification of DSP software in Matlab. The paper motivates a DSP Simulator concept that can be combined with the MATLAB executable interface to develop, evaluate and test DSP software within a single environment. Programming guidelines and optimization results are also provided to demonstrate the effectiveness of the intrinsics software development approach.","0738-100X","1-59593-058","10.1145/1065579.1065736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510401","","MATLAB;Software testing;Digital signal processing;Software tools;Application software;Software algorithms;Software performance;Software libraries;Programming;Computer languages","programming environments;program testing;program verification;mathematics computing;signal processing","Matlab extensions;software testing;software verification;real-time DSP software simulator;programming guidelines;C intrinsics software development;optimisation results","","1","7","","","","","","IEEE","IEEE Conferences"
"Optimal placement of software monitors aiding systematic testing","C. V. Ramamoorthy; K. H. Kim; W. T. Chen","Computer Science Division, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Calif. 94720; Department of, Electrical Engineering, Systems and Computer Science Program, University of Southern California, Los Angeles, Calif; Computer Science Division, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Calif. 94720","IEEE Transactions on Software Engineering","","1975","SE-1","4","403","411","The usefulness of software monitors in testing large programs is discussed. Several types of testing strategies based on the use of monitors are surveyed. Since there is a computational overhead involved in employing monitors, attempts are made to minimize the number of monitors employed. This optimisation problem is formulated and analyzed in graph theoretic terms. Implementation aspects are considered through both discussion and examples.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1975.6312872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6312872","Optimal placement;overhead;program testing;software monitor;test-input;test-output;test-path;test-run","Monitoring;Testing;Software;Instruments;Computer science;Systematics;Algorithm design and analysis","program debugging","systematic testing;software monitors;overhead;optimisation problem;graph theoretic terms;optimal placement;test input and output;test path","","13","","","","","","","IEEE","IEEE Journals & Magazines"
"Constructing a stability index for software using probability ratio sequential testing","P. Franklin; X. Zhang","Reliability Dept., Lucent Technol., Holmdel, NJ, USA; Reliability Dept., Lucent Technol., Holmdel, NJ, USA","Annual Reliability and Maintainability Symposium, 2005. Proceedings.","","2005","","","606","609","In this paper, the results of software testing during development is considered. Progress toward completion may be measured in part by developing and tracking a stability index for the product under development. We propose that a stability index can be constructed based on probability ratio sequential testing. These indices can be manipulated to understand sensitivity to various customer profiles. A stability index constructed is useful for determining sensitivity to variable customer profiles, balancing the mix of categories of test cases, and optimizing total duration of stability testing.","0149-144X","0-7803-8824","10.1109/RAMS.2005.1408430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408430","","Stability;Software testing;Sequential analysis;System testing;Software reliability;Best practices;Risk management;Project management;Availability;Automatic testing","program testing;customer profiles;probability;software reliability;production engineering;product development","stability index;probability ratio sequential testing;software testing;product development;sensitivity;customer profiles;test cases;stability testing","","","3","","","","","","IEEE","IEEE Conferences"
"Reliability centered software testing","Z. Bluvband","ALD Ltd., Rishon-Lezion, Israel","Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No.02CH37318)","","2002","","","300","305","This paper introduces reliability centered software testing as a methodology for analysis, planning and control of the test factors affecting the reliability of software prior to the software release. These main factors are: resources and time-to-software-release. These two factors are interdependent, allowing you to determine whether to increase the applied resources and shorten TTSR or to reduce resources and extend TTSR. This paper argues that you can significantly improve the results of software quality testing by optimizing the test duration. This involves varying the rate of the above two factors (resources and TTSR) and using the appropriate failure intensity models. This paper, based on hands-on experience, presents a methodology for planning the trade-off between resources and TTSR by controlling testing phase duration. In the process, we attempt to define the most typical pitfalls in planning testing phase duration, i.e. TTSR, and offer some practical recommendations on how to avoid these pitfalls. Following this procedure, you will be able to select the ETSR level that is best suited to achieving the goal of releasing your software on time, while assuring its predetermined reliability level.","0149-144X","0-7803-7348","10.1109/RAMS.2002.981658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981658","","Software testing;Software reliability;Software quality;Process planning;Maintenance;Availability;Failure analysis;Labor resources;Humans;Calendars","software reliability;software quality;program testing","reliability centered software testing;software reliability;test planning;failure counting;software release;bootstrap;time-to-software-release;software quality testing;test duration optimisation;failure intensity models","","2","7","","","","","","IEEE","IEEE Conferences"
"Optimal test profile in the context of software cybernetics","Kai-Yuan Cai","Dept. of Autom. Control, Beijing Univ. of Aeronaut. & Astronaut., China","Proceedings Second Asia-Pacific Conference on Quality Software","","2001","","","157","166","Software cybernetics explores the interplay between software theory/engineering and control theory/engineering Following the idea of software cybernetics, the controlled Markov chains (CMC) approach to software testing treats software testing as a control problem. The software under test serves as a controlled object, and the (optimal) testing strategy determined by the theory of controlled Markov chains serves as a controller. The software under test and the corresponding (optimal) testing strategy constitute a closed-loop feedback system, and the software state transitions behave as a Markov chain. The paper analyzes the behavior of the corresponding optimal test profile determined by the CMC approach to software testing. It is shown that in some cases the optimal test profile is Markovian, whereas in some other cases the optimal test profile demonstrates a different scenario. The analyses presented in the paper deepen our understanding of the CMC approach to software testing and are related to software operational profile modeling.","","0-7695-1287","10.1109/APAQS.2001.990014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990014","","Software testing;Cybernetics;Control systems;System testing;Optimal control;Control theory;Design optimization;Motion control;Automatic control;State feedback","program testing;Markov processes;closed loop systems;optimisation;optimal control","optimal test profile;software cybernetics;software theory/engineering;control theory/engineering;controlled Markov chains approach;software testing;controlled object;optimal testing strategy;closed-loop feedback system;software state transitions;CMC approach;software operational profile modeling","","","9","","","","","","IEEE","IEEE Conferences"
"Checking inside the black box: regression testing by comparing value spectra","T. Xie; D. Notkin","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","","2005","31","10","869","883","Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Black-box program outputs have been used to characterize program behaviors and they are compared over program versions in traditional regression testing. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavioral differences between versions. In this paper, we present a new class of program spectra, value spectra, that enriches the existing program spectra family. We compare the value spectra of a program's old version and new version to detect internal behavioral deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavioral deviations. We also use path spectra (previously proposed program spectra) to approximate the program states in value spectra. We then similarly compare path spectra to detect behavioral deviations and locate deviation roots in the new version. We have conducted an experiment on eight C programs to evaluate our spectra-comparison approach. The results show that both value-spectra-comparison and path-spectra-comparison approaches can effectively expose program behavioral differences between program versions even when their program outputs are the same, and our value-spectra-comparison approach reports deviation roots with high accuracy for most programs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542068","Index Terms- Program spectra;regression testing;software testing;empirical studies;software maintenance.","Software testing;Software maintenance;Fault diagnosis;Propagation losses;Program processors;Optimizing compilers","program testing;software maintenance;formal verification;regression analysis","regression testing;program version;software maintenance;black-box program;program behavior;program spectra;deviation-propagation call tree;C program;value-spectra-comparison;path-spectra-comparison","","23","36","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of coverage-based and distribution-based techniques for filtering and prioritizing test cases","D. Leon; A. Podgurski","Electr. Eng. & Comput. Sci. Dept. Case, Western Reserve Univ., Cleveland, OH, USA; Electr. Eng. & Comput. Sci. Dept. Case, Western Reserve Univ., Cleveland, OH, USA","14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.","","2003","","","442","453","This paper presents an empirical comparison of four different techniques for filtering large test suites: test suite minimization, prioritization by additional coverage, cluster filtering with one-per-cluster sampling, and failure pursuit sampling. The first two techniques are based on selecting subsets that maximize code coverage as quickly as possible, while the latter two are based on analyzing the distribution of the tests' execution profiles. These techniques were compared with data sets obtained from three large subject programs: the GCC, Jikes, and javac compilers. The results indicate that distribution-based techniques can be as efficient or more efficient for revealing defects than coverage-based techniques, but that the two kinds of techniques are also complementary in the sense that they find different defects. Accordingly, some simple combinations of these techniques were evaluated for use in test case prioritization. The results indicate that these techniques can create more efficient prioritizations than those generated using prioritization by additional coverage.","1071-9458","0-7695-2007","10.1109/ISSRE.2003.1251065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251065","","Filtering;Computer aided software engineering;Costs;Automatic testing;Sampling methods;System testing;Software testing;Computer science;Java;Program processors","program testing;program compilers;Java;software engineering","coverage-based technique;distribution-based technique;test case prioritization;test case filtering;test suite minimization;cluster filtering;one-per-cluster sampling;failure pursuit sampling;subset selection;code coverage maximization;test execution profile distribution;GCC compiler;Jikes compiler;javac compiler;defect finding","","72","28","","","","","","IEEE","IEEE Conferences"
"Why Johnny can't test [software]","T. Yamaura","NA","IEEE Software","","1998","15","2","113","115","The question ""Can US programmers be great testers?"" is one side of a coin whose other side is, ""Can Japanese programmers be creative software designers?"" The author compares American and Japanese programmers because they represent the extremes in the range of programmers' behavior with regard to the ""value of software"". This is an important observation by a Japanese software engineer from a culture of rigorous practices who works with American software engineers from a markedly different culture. His pessimistic and fatalistic view is just a starting point for debate that could lead to optimistic and constructive propositions.","0740-7459;1937-4194","","10.1109/52.663795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=663795","","Software quality;Computer industry;Programming profession;Companies;Quality assurance;Delay;Cultural differences;Humans;Software testing;Technological innovation","program testing;socio-economic effects","software testing;American programmers;Japanese programmers;creative software design;software value;software engineers;culture","","3","2","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing and simplifying software metric models constructed using maximum likelihood methods","V. K. Y. Chan; W. E. Wong","Macao Polytech. Inst., Macau; NA","29th Annual International Computer Software and Applications Conference (COMPSAC'05)","","2005","1","","65","70 Vol. 2","A software metric model can be used to predict a target metric (e.g., the development work effort) for a future release of a software system based on the project's predictor metrics (e.g., the project team size). However, missing or incomplete data often appear in the data samples used to construct the model. So far, the least biased and thus the most recommended software metric models for dealing with the missing/incomplete data are those constructed by using the maximum likelihood methods. It is true that the inclusion of a particular predictor metric in the model construction is initially based on an intuitive or experience-based assumption that the predictor metric impacts significantly the target metric. Nevertheless, this assumption has to be verified. Previous research on metric models constructed by using the maximum likelihood methods simply took this verification for granted. This can result in probable inclusion of superfluous predictor metric(s) and/or unnecessary predictor metric complexity. In this paper, we propose a methodology to optimize and simplify such models based on the results of appropriate hypothesis tests. An experiment is also reported to demonstrate the use of our methodology in trimming redundant predictor metric(s) and/or unnecessary predictor metric complexity.","0730-3157;0730-3157","0-7695-2413","10.1109/COMPSAC.2005.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509998","software metrics;modeling;maximum likelihood method","Optimization methods;Software metrics;Predictive models;Software systems;Testing;Software measurement;Terminology;Statistical analysis;Least squares methods;Least squares approximation","software metrics;maximum likelihood estimation;software development management;optimisation","software metric model optimization;software metric model simplification;maximum likelihood method;development work effort;software system;project predictor metrics;project team size;predictor metric complexit","","3","16","","","","","","IEEE","IEEE Conferences"
"Agile development: evaluation and experience","W. F. Tichy","Karlsruhe Univ., Germany","Proceedings. 26th International Conference on Software Engineering","","2004","","","692","","Agile methods such as Extreme Programming, Crystal, Scrum, and others have attracted a lot of attention recently. Agile methods stress early and continuous delivery of software, welcome changing requirements, and value early feedback from customers. Agile methods seek to cut out inefficiency, bureaucracy, and anything that adds no value to a software product. Proponents of agile methods often see software specification and documentation as adding no value, which has led observers to conclude that agile development is nothing but unprincipled hacking, perhaps even an anarchic counter-reaction to bureaucratic, heavyweight software processes that demand ever more intermediate deliverables from developers. The purpose of this panel is to discuss under what circumstances agile methods work and don't work. Some of the key practices of agile methods are: scheduling according to feature priorities, incremental delivery of software, feedback from expert users, emphasis on face-to-face communication, pair development, minimalist design combined with refactoring, test-driven development, automated regression testing, daily integration, self-organizing teams, and periodic tuning of the methods. Working software is the primary measure of success. Find out what the latest practical experience with agile methods is and learn about the latest thinking in this area.","0270-5257","0-7695-2163","10.1109/ICSE.2004.1317492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317492","","Feedback;Automatic testing;Software testing;Stress;Documentation;Computer crime;Tuning;Software measurement;Software engineering","software engineering;program testing","agile development;Extreme Programming;Crystal;Scrum;software specification;program documentation;scheduling;software incremental delivery;face-to-face communication;test-driven development;automated regression testing","","2","","","","","","","IEEE","IEEE Conferences"
"Integrating DFT in the physical synthesis flow","L. Guiller; F. Neuveux; S. Duggirala; R. Chandramouli; R. Kapur","Synopsys, Inc, Mountain View, CA, USA; Synopsys, Inc, Mountain View, CA, USA; Synopsys, Inc, Mountain View, CA, USA; Synopsys, Inc, Mountain View, CA, USA; Synopsys, Inc, Mountain View, CA, USA","Proceedings. International Test Conference","","2002","","","788","795","The industry's adoption of powerful design methodologies, such as physical synthesis, formal verification, and static timing analysis are speeding the implementation and verification of multi-million gate ASICs and systems-on-chip (SoC). As the design community moves to the complete adoption of a physical synthesis flow, it is becoming evident that test synthesis must be aware of layout issues and well integrated within physical design tools. By bringing in key physical functions into the front-end of the DFT/physical synthesis flow, the designer is able to successfully meet all design and testability goals, with minimum impact on timing closure. In this paper, we present a DFT synthesis flow tightly integrated within physical synthesis to achieve physically optimized scan designs. This flow describes new test technology which uses physical information to achieve optimal scan chain partitioning, timing-driven scan ordering and DFT driven placement to dramatically reduce routing congestion, and achieve a rapid and predictable timing closure.","1089-3539","0-7803-7542","10.1109/TEST.2002.1041832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041832","","Timing;Design for testability;Routing;Logic design;Logic testing;Design methodology;Formal verification;System-on-a-chip;Design optimization;Bandwidth","circuit CAD;design for testability;system-on-chip;integrated circuit design;application specific integrated circuits;timing;software tools;circuit layout CAD;network routing;circuit optimisation","DFT integration;physical synthesis flow;design methodologies;formal verification;static timing analysis;design implementation;design verification;ASIC;system-on-chip;SoC;test synthesis;layout issues;physical design tool integration;DFT/physical synthesis flow front-end;design testability;timing closure;DFT synthesis flow;physically optimized scan designs;test technology;optimal scan chain partitioning;timing-driven scan ordering;DFT driven placement;routing congestion","","6","6","","","","","","IEEE","IEEE Conferences"
"Providing an empirical basis for optimizing the verification and testing phases of software development","L. C. Briand; V. R. Basili; C. J. Hetmanski","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","[1992] Proceedings Third International Symposium on Software Reliability Engineering","","1992","","","329","338","Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are limited and scheduling is tight. Therefore, one needs to be able to differentiate low/high fault density components so that testing/verification effort can be concentrated where needed. Such a strategy is ejected to detect more faults and thus improve the resulting reliability of the overall system. The authors present an alternative approach for constructing such models that is intended to fulfil specific software engineering needs, (i.e. dealing with partial/incomplete information and creating models that are easy to interpret). The approach to classification is to: measure the software system to be considered; and to build multivariate stochastic models for prediction. The authors present experimental results obtained by classifying FORTRAN components into two fault density classes: low and high. They also evaluate the accuracy of the model and the insights it provides into the software process.<<ETX>>","","0-8186-2975","10.1109/ISSRE.1992.285903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285903","","Software testing;System testing;Software systems;Stochastic processes;Fault diagnosis;Software engineering;NASA;Machine learning;Buildings;Data analysis","program verification;software metrics;software reliability","testing phases;software development;low/high fault density components;testing/verification effort;specific software engineering needs;partial/incomplete information;classification;multivariate stochastic models;FORTRAN components;fault density classes","","7","16","","","","","","IEEE","IEEE Conferences"
"Open systems architecture solutions for military avionics testing","S. Pizzica","Raytheon Co., El Segundo, CA, USA","IEEE Aerospace and Electronic Systems Magazine","","2001","16","8","4","9","Raytheon makes extensive use of open systems architecture methods in developing special test equipment (STE) for testing military avionics equipment. Such use has resulted in significant cost and schedule savings in the development of production test equipment for radar and infrared systems. With open systems architectures, a test system can be assembled using COTS products. This brings economies of scale to test equipment, which is normally built in very low quantities. Therefore, the potential cost savings due to COTS usage is proportionately greater in STE than in the higher volume avionics systems that are tested. A second major benefit of using COTS products is that test system development schedule cycle time is greatly reduced. This paper describes the application of Open Systems Architectures (OSA) to avionics testing. The following major architectures are surveyed: VME bus, VXI bus, IEEE GPIB, IEEE 1149.1 JTAG test bus, 1553 Military Bus, Fibre Channel, and COTS Test Applications Software. We describe how the benefits of OSA have been extended at Raytheon into achieving vertical test commonalities. The flexibility of OSA can be exploited to provide an overall optimum test solution, taking all levels of test into account. For example, test systems can be tailored with COTS products to provide integrated methods for avionics tests at the module, unit, and system levels. Test systems can be configured to maximize the reuse of COTS hardware over all test levels. Test software can also be programmed to optimize such reuse over levels of test. Additional test verticality synergies derived from such OSA usage are described, including: test false alarm avoidance; test cones of tolerance optimization; and efficient test of field returns.","0885-8985;1557-959X","","10.1109/62.942212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942212","","Open systems;Aerospace electronics;System testing;Software testing;Test equipment;Costs;Application software;Computer architecture;Military equipment;Production systems","military avionics;automatic test equipment;automatic test software;boundary scan testing;open systems;peripheral interfaces;system buses;military standards","military avionics testing;open systems architecture;special test equipment;cost and schedule savings;production test equipment;COTS products;system development schedule cycle time;VME bus;VXI bus;IEEE GPIB;1553 Military Bus;Fibre Channel;COTS Test Applications Software;IEEE 1149.1 JTAG test bus","","4","5","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling software quality: the Software Measurement Analysis and Reliability Toolkit","T. M. Khoshgoftaar; E. B. Allen; J. C. Busboom","Dept. of Comput. Sci. & Eng., Florida Atlantic Univ., Boca Raton, FL, USA; NA; NA","Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000","","2000","","","54","61","The paper presents the Software Measurement Analysis and Reliability Toolkit (SMART) which is a research tool for software quality modeling using case based reasoning (CBR) and other modeling techniques. Modern software systems must have high reliability. Software quality models are tools for guiding reliability enhancement activities to high risk modules for maximum effectiveness and efficiency. A software quality model predicts a quality factor, such as the number of faults in a module, early in the life cycle in time for effective action. Software product and process metrics can be the basis for such fault predictions. Moreover, classification models can identify fault prone modules. CBR is an attractive modeling method based on automated reasoning processes. However, to our knowledge, few CBR systems for software quality modeling have been developed. SMART addresses this area. There are currently three types of models supported by SMART: classification based on CBR, CBR classification extended with cluster analysis, and module-order models, which predict the rank-order of modules according to a quality factor. An empirical case study of a military command, control, and communications applied SMART at the end of coding. The models built by SMART had a level of accuracy that could be very useful to software developers.","1082-3409","0-7695-0909","10.1109/TAI.2000.889846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889846","","Software quality;Software measurement;Predictive models;Q factor;Fault diagnosis;Software reliability;Automatic testing;System testing;Software systems;Military communication","software quality;software metrics;software performance evaluation;software reliability;case-based reasoning;command and control systems;software tools","software quality modeling;Software Measurement Analysis and Reliability Toolkit;SMART;research tool;case based reasoning;software quality models;reliability enhancement activities;high risk modules;quality factor;life cycle;software product;process metrics;fault predictions;classification models;fault prone modules;automated reasoning processes;CBR systems;CBR classification;cluster analysis;module-order models;rank-order;military command control and communications;software developers","","4","19","","","","","","IEEE","IEEE Conferences"
"Quantifying the reliability of software: statistical testing based on a usage model","C. Trammell","Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA","Proceedings of Software Engineering Standards Symposium","","1995","","","208","218","When a population is too large for study, as is the case for all possible uses of a software system, a statistically correct sample must be drawn as a basis for inferences about the population. In statistical testing of software based on a Markov chain usage model, the rich body of analytical results available for Markov chains provides numerous insights that can be used in test planning. Further, the connection between Markov chains and operations research techniques permits a Markov usage model to be expressed as a system of constraints, with mathematical programming used to generate the optimal model for a particular objective function. Since a software usage model is based on the specification, all analyses may be performed early in the development cycle and used as a quantitative basis for management decisions. These techniques have been reduced to engineering practice and used in large projects by IBM, Ericsson, all branches of the US military, and others. In this paper, statistical experiments, Markov models, and optimization techniques are shown to provide a sound theoretical and practical basis far quantifying the reliability of software.","1082-3670","0-8186-7137","10.1109/SESS.1995.525966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525966","","Statistical analysis;Mathematical model;Software systems;Software testing;Operations research;Mathematical programming;Software performance;Performance analysis;Software development management;Acoustical engineering","software reliability;program testing;Markov processes;mathematical programming;formal specification;certification","software reliability;statistical testing;usage model;Markov chain usage model;operations research;mathematical programming;objective function;software usage model;specification;development cycle;management decisions","","7","16","","","","","","IEEE","IEEE Conferences"
"Modeling the cost-benefits tradeoffs for regression testing techniques","A. G. Malishevsky; G. Rothermel; S. Elbaum","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA","International Conference on Software Maintenance, 2002. Proceedings.","","2002","","","204","213","Regression testing is an expensive activity that can account for a large proportion of the software maintenance budget. Because engineers add tests into test suites as software evolves, over time, increased test suite size makes revalidation of the software more expensive. Regression test selection, test suite reduction, and test case prioritization techniques can help with this, by reducing the number of regression tests that must be run and by helping testers meet testing objectives more quickly. These techniques, however can be expensive to employ and may not reduce overall regression testing costs. Thus, practitioners and researchers could benefit from cost models that would help them assess the cost-benefits of techniques. Cost models have been proposed for this purpose, but some of these models omit important factors, and others cannot truly evaluate cost-effectiveness. In this paper, we present new cost-benefits models for regression test selection, test suite reduction, and test case prioritization, that capture previously omitted factors, and support cost-benefits analyses where they were not supported before. We present the results of an empirical study assessing these models.","1063-6773","0-7695-1819","10.1109/ICSM.2002.1167767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167767","","Software testing;Software maintenance;Fault detection;Computer science;Performance evaluation;Cost benefit analysis;Software performance;Maintenance engineering","cost-benefit analysis;software maintenance;program testing;software development management","cost-benefit tradeoff models;regression testing;software maintenance;regression test selection;regression test suite reduction;regression test case prioritization","","26","22","","","","","","IEEE","IEEE Conferences"
"Understanding and measuring the sources of variation in the prioritization of regression test suites","S. Elbaum; D. Gable; G. Rothermel","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA","Proceedings Seventh International Software Metrics Symposium","","2001","","","169","179","Test case prioritization techniques let testers order their test cases so that those with higher priority, according to some criterion, are executed earlier than those with lower priority. In previous work (1999, 2000), we examined a variety of prioritization techniques to determine their ability to improve the rate of fault detection of test suites. Our studies showed that the rate of fault detection of test suites could be significantly improved by using more powerful prioritization techniques. In addition, they indicated that rate of fault detection was closely associated with the target program. We also observed a large quantity of unexplained variance, indicating that other factors must be affecting prioritization effectiveness. These observations motivate the following research questions. (1) Are there factors other than the target program and the prioritization technique that consistently affect the rate of fault detection of test suites? (2) What metrics are most representative of each factor? (3) Can the consideration of additional factors lead to more efficient prioritization techniques? To address these questions, we performed a series of experiments exploring three factors: program structure, test suite composition and change characteristics. This paper reports the results and implications of those experiments.","1530-1435","0-7695-1043","10.1109/METRIC.2001.915525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915525","","Fault detection;Performance evaluation;Feedback;System testing;Debugging;Data analysis","program testing;software metrics;statistical analysis","regression test suites;test case prioritization techniques;variation sources;fault detection rate;software metrics;software testing;program structure;test suite composition;change characteristics","","8","20","","","","","","IEEE","IEEE Conferences"
"Elimination of crucial faults by a new selective testing method","M. Hirayama; T. Yamamoto; J. Okayasu; O. Mizuno; T. Kikuno","R&D Center, Toshiba Corp., Japan; R&D Center, Toshiba Corp., Japan; R&D Center, Toshiba Corp., Japan; NA; NA","Proceedings International Symposium on Empirical Software Engineering","","2002","","","183","191","Recent software systems contain a lot of functions to provide various services. According to this tendency, software testing becomes more difficult than before and cost of testing increases so much, since many test items are required. In this paper we propose and discuss such a new selective software testing method that is constructed from previous testing method by simplifying testing specification. We have presented, in the previous work, a selective testing method to perform highly efficient software testing. The selective testing method has introduced an idea of functional priority testing and generated test items according to their functional priorities. Important functions with high priorities are tested in detail, and functions with low priorities are tested less intensively. As a result, additional cost for generating testing instructions becomes relatively high. In this paper in order to reduce its cost, we change the way of giving information, with respect to priorities. The new method gives the priority only rather than generating testing instructions to each test item, which makes the testing method quite simple and results in cost reduction. Except for this change, the new method is essentially the same as the previous method. We applied this new method to actual development of software tool and evaluated its effectiveness. From the result of the application experiment, we confirmed that many crucial faults can be detected by using the proposed method.","","0-7695-1796","10.1109/ISESE.2002.1166937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166937","","Software testing;Costs;Performance evaluation;System testing;Application software;Software quality;Research and development;Systems engineering and theory;Information science;Fault detection","program testing;software fault tolerance","crucial faults elimination;selective testing method;software systems;software testing;testing specification;functional priority testing","","1","15","","","","","","IEEE","IEEE Conferences"
"Genes and bacteria for automatic test cases optimization in the .NET environment","B. Baudry; F. Fleurey; J. -. Jezequel; Y. Le Traon","IRISA, Rennes, France; NA; NA; NA","13th International Symposium on Software Reliability Engineering, 2002. Proceedings.","","2002","","","195","206","The level of confidence in a software component is often linked to the quality of its test cases. This quality can in turn be evaluated with mutation analysis: faulty components (mutants) are systematically generated to check the proportion of mutants detected (""killed"") by the test cases. But while the generation of basic test cases set is easy, improving its quality may require prohibitive effort. We focus on the issue of automating the test optimization. We looked at genetic algorithms to solve this problem and modeled it as follows: a test case can be considered as a predator while a mutant program is analogous to a prey. The aim of the selection process is to generate test cases able to kill as many mutants as possible. To overcome disappointing experimentation results on the studied .NET system, we propose a slight variation on this idea, no longer at the ""animal"" level (lions killing zebras) but at the bacteriological level. The bacteriological level indeed better reflects the test case optimization issue: it introduces a memorization function and suppresses the crossover operator. We describe this model and show how it behaves on the case study.","1071-9458","0-7695-1763","10.1109/ISSRE.2002.1173246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173246","","Microorganisms;Automatic testing;Computer aided software engineering;Software testing;Genetic mutations;Genetic algorithms;Software quality;Fault detection;System testing;Animals","program testing;object-oriented programming;software quality;genetic algorithms;predator-prey systems;object-oriented languages","automatic test case optimization;NET environment;software component;mutation analysis;software quality;genetic algorithms;predator prey systems;mutant program;bacteria;memorization function;crossover operator;genes;object oriented programming","","16","21","","","","","","IEEE","IEEE Conferences"
"Yet another optimisation article","M. Fowler","NA","IEEE Software","","2002","19","3","20","21","The author addresses the question: how do we achieve a fast program? For many programmers, performance is something you pay continuous attention to as you program. Every time you write a fragment of code, you consider the performance implications and code the program to maximize performance. Some performance work comes from architectural decisions, some from more tactical optimization activity. The author considers a specific set of steps for program optimization.","0740-7459;1937-4194","","10.1109/MS.2002.1003448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003448","","Optimizing compilers;Virtual machining;Java;Virtual manufacturing;Software design;Automatic testing;Computer bugs;Virtual prototyping;Optimization methods","optimisation;software performance evaluation;programming","program optimization steps;software performance;programming;software architecture;bottlenecks;profiler","","2","","","","","","","IEEE","IEEE Journals & Magazines"
"Fault models and test generation for hardware-software covalidation","I. G. Harris","California Univ., Irvine, CA, USA","IEEE Design & Test of Computers","","2003","20","4","40","47","Mixed hardware-software systems constitute a strong paradigm shift for system validation. The main barriers to overcome are finding the right fault models and optimizing the validation flow. This article presents a research summary of these issues.","0740-7475;1558-1918","","10.1109/MDT.2003.1214351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214351","","System testing;Hardware;Software testing;Refining;Fault detection;Automatic testing;Application software;Costs;Software design;Process design","hardware-software codesign;automatic test pattern generation;fault diagnosis;optimisation","mixed hardware-software systems;automatic test generation;optimisation;control-data flow fault models;textual fault models;gate-level fault models;application-specific fault models;hardware software covalidation","","18","50","","","","","","IEEE","IEEE Journals & Magazines"
"Architecting production test systems","C. Nair","Nat. Instrum. Corp., Austin, TX, USA","2000 IEEE Autotestcon Proceedings. IEEE Systems Readiness Technology Conference. Future Sustainment for Military Aerospace (Cat. No.00CH37057)","","2000","","","572","575","Choosing the right test platform is essential when developing production test systems. In this paper we evaluate basics of PXI, computer based instrumentation, switching and fixturing. Different configurations of automatic test systems are presented, and methods of optimizing timing and synchronization between different instruments are evaluated. Hybrid systems the seamless integration of multiple technologies including PXI, VXI and GPIB based instruments is discussed. Finally, test software solutions to improve your test are discussed.","1080-7725","0-7803-5868","10.1109/AUTEST.2000.885643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885643","","Production systems;System testing;Instruments;Costs;Automatic testing;Software testing;Optimized production technology;Timing;Manufacturing automation;Quality management","production testing;automatic test equipment;field buses;synchronisation;timing","production test systems;test platform selection;computer based instrumentation;switching;fixturing;automatic test systems;synchronization optimisation;timing optimisation;hybrid systems;PXI based instruments;VXI based instruments;GPIB based instruments;test software;CompactPCI architecture","","","","","","","","","IEEE","IEEE Conferences"
"Automatic test program generation using graphical tools for producing factory and CASS ATLAS test code","M. C. Scallan","Automated Test Int., Daytona Beach, FL, USA","AUTOTESTCON 93","","1993","","","27","32","The Consolidated Automated Support System (CASS), designed primarily for use on board Navy aircraft carriers, has been mandated by the Secretary of the Navy for use as the standard ATE for all Navy field and depot activities. To minimize the cost of fielding newly developed Test Program Sets (TPSs), the test programmer should use as much of the code developed for manufacturing and acceptance testing as possible. This paper describes a means of optimizing the automatic generation of test code for the factory using graphical virtual instrument panels, and translating the resultant instrument setup or command to syntactically correct CASS ATLAS test code.<<ETX>>","","0-7803-0646","10.1109/AUTEST.1993.396372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396372","","Automatic testing;Production facilities;System testing;Instruments;US Department of Defense;Design engineering;Manufacturing automation;Data engineering;Costs;Aircraft","automatic testing;software tools;aerospace computing;automatic test equipment;automatic programming;production testing;military computing","automatic test program generation;DOD;graphical tools;CASS ATLAS test code;Consolidated Automated Support System;Navy aircraft carriers;standard ATE;Navy field;Test Program Sets;acceptance testing;test code;graphical virtual instrument panels","","","","","","","","","IEEE","IEEE Conferences"
"An automated testing methodology based on self-checking software","T. Reinhart; C. Boettcher; H. Wasserman","Res. Lab., Wright Patterson AFB, OH, USA; NA; NA","Proceedings of the IEEE 1998 National Aerospace and Electronics Conference. NAECON 1998. Celebrating 50 Years (Cat. No.98CH36185)","","1998","","","205","212","New techniques are required to speed the software testing process and to enhance the ultimate reliability of mission-critical embedded information systems. Here we employ result-checking, a technique in which software assures the correctness of its own computations through efficient: run-time checks. Building on previous work at U.C. Berkeley and Raytheon Systems Company, the Air Force Research Laboratory is funding the Automated Generation of Avionics Software Tests (AGAST) program to demonstrate that result-checking can be the basis for a flexible new methodology of software testing. This methodology aims at the production of software which has an enhanced sensitivity to its own errors (complemented with appropriate instrumentation), and which therefore may be tested with extremely high efficiency and accuracy.","0547-3578","0-7803-4449","10.1109/NAECON.1998.710118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=710118","","Automatic testing;Software testing;Mission critical systems;Information systems;Military computing;Runtime;Laboratories;Aerospace electronics;System testing;Production","automatic test software;aerospace computing;program testing;optimisation","automated testing;self-checking software;reliability;embedded information systems;run-time checks;U.C. Berkeley;Avionics Software Tests;software testing;sensitivity;errors;efficiency;accuracy","","1","7","","","","","","IEEE","IEEE Conferences"
"Data dependence testing in practice","K. Psarris; K. Kyriakopoulos","Div. of Comput. Sci., Texas Univ., San Antonio, TX, USA; NA","1999 International Conference on Parallel Architectures and Compilation Techniques (Cat. No.PR00425)","","1999","","","264","273","Data dependence analysis is a fundamental step in an optimizing compiler. The results of the analysis enable the compiler to identify code fragments that can be executed in parallel. A number of data dependence tests have been proposed in the literature. In each test there are different tradeoffs between accuracy and efficiency. In this paper we present an experimental evaluation of several data dependence tests, including the Banerjee test, the I-Test and the Omega test. We compare these tests in terms of accuracy and efficiency. We run various experiments using the Perfect Club Benchmarks and the scientific libraries Eispack, Linpack and Lapack. Several observations and conclusions are derived from the experimental results, which are displayed and analyzed in this paper.","1089-795X","0-7695-0425","10.1109/PACT.1999.807571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=807571","","Testing;Data analysis;Program processors;Computer science;Read only memory;Parallel processing;Constraint optimization;Optimizing compilers;Load management;Data mining","optimising compilers;parallel programming;software libraries;program testing","data dependence testing;data dependence analysis;optimizing compiler;parallel execution;accuracy;efficiency;Banerjee test;I-Test;Omega test;Perfect Club Benchmarks;scientific libraries;Eispack;Linpack;Lapack","","9","21","","","","","","IEEE","IEEE Conferences"
"Optimizing test strategies during PCB design for boards with limited ICT access","A. Verma","Teradyne Inc., San Diego, CA, USA","27th Annual IEEE/SEMI International Electronics Manufacturing Technology Symposium","","2002","","","364","371","Engineers have used past experience or subjective preference as a means for assigning test strategies to new products without analyzing the benefits and weaknesses of various different test approaches in a quantitative manner. DFT (design for test) software tools that enable testability analysis during board design allow test engineers to work concurrently with designers. Case study results demonstrate that coverage predicted by DFT software is realistic when compared to actual fault coverage achieved in production. Using DFT software during PCB design to model the fault coverage of different test strategies and make ICT (in-circuit test) access tradeoffs can significantly reduce cost and improve quality. Defect capture rates more than doubled when using alternate test strategies and production line beat rates varied significantly depending on the test strategy chosen. When DFT software enables these decisions early in the product life cycle, both OEMs and EMS providers can win by driving cost reductions through the entire product life cycle from NPI (new product introduction) through manufacturing and warranty.","","0-7803-7301","10.1109/IEMT.2002.1032780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1032780","","Design optimization;Software testing;Design for testability;Software design;Production;Costs;Software tools;Design engineering;Software quality;Medical services","printed circuit design;printed circuit testing;design for testability;circuit optimisation;circuit CAD;fault diagnosis","design for test;new product introduction;PCB design;limited in-circuit test access;test strategy optimization;electronics manufacturing services;test approach quantitative analysis;DFT software tools;board design testability analysis;DFT software coverage prediction;fault coverage modeling;ICT access tradeoffs;cost reduction;quality improvement;defect capture rates;production line beat rates;CAD;product life cycle;OEM;EMS providers;NPI;warranty;PCB test","","1","","","","","","","IEEE","IEEE Conferences"
"Software implementation and statistical optimization of some electronic component's lifetime","K. C. Kouakou","LSTA, Paris VI Univ., France","Proceedings of European Design and Test Conference EDAC-ETC-EUROASIC","","1994","","","663","","Lifetime optimization of some industrial or biological items which can fail under internal or external pertubations are of great interest for researchers in many fields as electronic and computer engineering, medicine, mechanical engineering, railways and many others. Some useful goals of a software called EstiSurv which can be useful in the above fields, are described.<<ETX>>","","0-8186-5410","10.1109/EDTC.1994.326800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326800","","Biological information theory;Rail transportation;Stochastic processes;Network servers;Visualization;Iron;Biology computing;Humans;Automatic control;Medical control systems","electronic equipment testing;life testing;optimisation;statistical analysis;electronic engineering computing","statistical optimization;electronic component;lifetime;lifetime optimization;biological items;industrial items;electronics;computers;medicine;railway;software implementation;maintenance;EstiSurv","","","","","","","","","IEEE","IEEE Conferences"
"De-pipeline a software-pipelined loop","Bogong Su; Jian Wang; Erh-Wen Hu; J. Manzano","Dept. of Comput. Sci., William Paterson Univ. of New Jersey, Wayne, NJ, USA; NA; NA; NA","2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).","","2003","2","","II","237","Software pipelining is a loop optimization technique that has been widely implemented in modem optimizing compilers. In order to utilize fully the instruction level parallelism of the recent VLIW DSP processors, DSP programs have to be optimized by software pipelining. However, because of the transformation of the original sequential code, a software-pipelined loop is often difficult to understand, test, and debug. It is also very difficult to reuse and port a software-pipelined loop to other processors, especially when the original sequential code is unavailable. We propose a de-pipelining technique, which converts the optimized assembly code of a software-pipelined loop back to a semantically equivalent sequential counterpart. Preliminary experiments on 20 assembly programs verifies the validity of the proposed de-pipelining algorithm.","1520-6149","0-7803-7663","10.1109/ICASSP.2003.1202338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202338","","Digital signal processing;Pipeline processing;Assembly;Optimizing compilers;VLIW;Software testing;Sequential analysis;Application software;Artificial intelligence;Kernel","pipeline processing;program control structures;assembly language listings;optimisation;software portability;software reusability;optimising compilers;parallel programming;digital signal processing chips","de-pipelining algorithm;software pipelining;loop optimization;optimizing compilers;instruction level parallelism;VLIW DSP processors;sequential code;optimized assembly code;Texas Instruments C6X processor;StarCore SC 140 processor;TI C62 processor","","2","8","","","","","","IEEE","IEEE Conferences"
"A lifecycle approach to design validation is it necessary? Is it feasible?","S. Stoica","Adv. Vehicle Technol., Ford Motor Co., Dearborn, MI, USA","Proceedings International Test Conference 1998 (IEEE Cat. No.98CH36270)","","1998","","","784","792","Due to the ever increasing complexity of car electronics, the automotive industry is challenged by serious test problems. These problems no longer can be solved using ad-hoc test approaches, hence the industry is driven toward finding new strategies for test, strategies that can deal with the increased complexity land at the same time keep the product costs at a reasonable level. This paper is proposing a test strategy that can satisfy the conditions stated above. It is based on optimizing the tests performed on vehicle electronics from a lifecycle test perspective, i.e. by optimizing the tests performed at each integration level from an overall DV perspective. The paper also discusses the feasibility of such an approach from test coverage point of view and states the reasons for such a view. The paper also presents a test methodology that was successfully applied at Ford Motor Company and which can cover all the stages of product development. This methodology is currently in use at Ford for all stages of product validation.","1089-3539","0-7803-5093","10.1109/TEST.1998.743262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=743262","","Electronic equipment testing;Software testing;Life testing;System testing;Vehicle safety;Robustness;Automotive engineering;Performance evaluation;Software standards;Intelligent vehicles","automobile industry;automotive electronics;product development;concurrent engineering;production testing;life testing;quality management;Ford;cost-benefit analysis","design validation;life-cycle approach;automotive industry;test problems;complex car electronics;product costs;test strategy;Ford test methodology;product development;product validation;system approach;requirements specifications test;QA-type testing;white box test;black box test;test feasibility","","1","3","","","","","","IEEE","IEEE Conferences"
"Reducing test application time through interleaved scan","F. Corno; M. Sonza Reorda; G. Squillero","Dipt. di Automatica e Informatica, Politecnico di Torino, Italy; Dipt. di Automatica e Informatica, Politecnico di Torino, Italy; Dipt. di Automatica e Informatica, Politecnico di Torino, Italy","Proceedings. 15th Symposium on Integrated Circuits and Systems Design","","2002","","","89","94","This paper proposes a new method for reducing the test length for digital circuits by adopting an architecture derived from the popular scan approach. An evolutionary optimization algorithm is exploited to find the optimal solution. The proposed approach was tested on the ISCAS89 standard benchmarks and the experimental results show its effectiveness.","","0-7695-1807","10.1109/SBCCI.2002.1137642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137642","","Circuit testing;Circuit faults;Sequential analysis;Fault detection;Automatic test pattern generation;Automatic testing;Digital circuits;Test pattern generators;Benchmark testing;Hardware","circuit testing;logic testing;automatic test pattern generation;automatic test software;boundary scan testing;fault location;genetic algorithms;performance evaluation","automatic test pattern generator;ATPG;digital circuit interleaved scan test;test application time reduction;scan test architecture;test length reduction;evolutionary optimization algorithms;ISCAS89 benchmark testing;logic testing;genetic algorithms","","2","8","","","","","","IEEE","IEEE Conferences"
"A novel test time reduction algorithm for test architecture design for core-based system chips","S. K. Goel; E. J. Marinissen","Philips Res. Labs., Eindhoven, Netherlands; Philips Res. Labs., Eindhoven, Netherlands","Proceedings The Seventh IEEE European Test Workshop","","2002","","","7","12","This paper deals with the design of SoC test architectures which are efficient with respect to required ATE vector memory depth and test application time. We advocate the usage of a TestRail architecture, as this architecture, unlike others, allows not only for efficient core-internal testing, but also for efficient testing of the circuitry external to the cores. We present a novel heuristic algorithm that effectively optimizes the TestRail architecture for a given SoC by efficiently determining the number of TestRails and their widths, the assignment of cores to the TestRails, and the wrapper design per core. Experimental results for four benchmark SoCs show that, compared to previously published algorithms, we obtain comparable or better test times at negligible compute time.","1530-1877","0-7695-1715","10.1109/ETW.2002.1029633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029633","","System testing;Algorithm design and analysis;Circuit testing;Computer architecture;Benchmark testing;Integrated circuit testing;Heuristic algorithms;Pins;Laboratories;Digital integrated circuits","system-on-chip;integrated circuit testing;automatic test software;automatic test equipment","test time reduction algorithm;core-based system chips;SoC test architectures;ATE vector memory depth;test application time;TestRail architecture;core-internal testing;external circuitry testing;TR-ARCHITECT algorithm;heuristic algorithm;wrapper design per core","","4","17","","","","","","IEEE","IEEE Conferences"
"A scalable software-based self-test methodology for programmable processors","Li Chen; S. Ravi; A. Raghunathan; S. Dey","Dept. of ECE, California Univ., San Diego, CA, USA; NA; NA; NA","Proceedings 2003. Design Automation Conference (IEEE Cat. No.03CH37451)","","2003","","","548","553","Software-based self-test (SBST) is an emerging approach to address the challenges of high-quality, at-speed test for complex programmable processors and systems-on chips (SoCs) that contain them. While early work on SBST has proposed several promising ideas, many challenges remain in applying SBST to realistic embedded processors. We propose a systematic scalable methodology for SBST that automates several key steps. The proposed methodology consists of (i) identifying test program templates that are well suited for test delivery to each module within the processor, (ii) extracting input/output mapping functions that capture the controllability/observability constraints imposed by a test program template for a specific module-under-test, (iii) generating module-level tests by representing the input/output mapping functions as virtual constraint circuits, and (iv) automatic synthesis of a software self-test program from the module-level tests. We propose novel RTL simulation-based techniques for template ranking and selection, and techniques based on the theory of statistical regression for extraction of input/output mapping functions. An important advantage of the proposed techniques is their scalability, which is necessitated by the significant and growing complexity of embedded processors. To demonstrate the utility of the proposed methodology, we have applied it to a commercial state-of-the-art embedded processor (Xtensa form Tensilica Inc.). We believe this is the first practical demonstration of software-based self-test on a processor of such complexity. Experimental results demonstrate that software self-test programs generated using the proposed methodology are able to detect most (95.2%) of the functionally testable faults, and achieve significant simultaneous improvements in fault coverage and test length compared with conventional functional test.","","1-58113-688","10.1109/DAC.2003.1219068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1219068","","Built-in self-test;Automatic testing;Software testing;Circuit testing;Circuit faults;System testing;Controllability;Observability;Circuit synthesis;Circuit simulation","integrated circuit testing;logic testing;built-in self test;programmable circuits;microprocessor chips;fault tolerant computing","software-based self-test methodology;complex programmable processor;systems-on chip;embedded processor;scalable methodology;test program template;mapping function;controllability constraint;observability constraint;virtual constraint circuit;automatic synthesis;RTL simulation-based technique;template ranking;statistical regression;manufacturing test;at-speed test","","75","21","","","","","","IEEE","IEEE Conferences"
"Analyzing second-order effects between optimizations for system-level test-based model generation","T. Margaria; H. Raffelt; B. Steffen","Gottingen Univ., Germany; NA; NA","IEEE International Conference on Test, 2005.","","2005","","","7 pp.","467","Test-based model generation by classical automata learning is very expensive. It requires an impractically large number of queries to the system, each of which must be implemented as a system-level test case. Key towards the tractability of observation based model generation are powerful optimizations exploiting different kinds of expert knowledge in order to drastically reduce the number of required queries, and thus the testing effort. In this paper, we present a thorough experimental analysis of the second-order effects between such optimizations in order to maximize their combined impact","1089-3539;2378-2250","0-7803-9038","10.1109/TEST.2005.1584006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1584006","","System testing;Automatic testing;Learning automata;Application software;Power system modeling;Hardware;Software systems;Production systems;Power generation;Intellectual property","automatic test pattern generation;expert systems;learning automata;optimisation","second-order effects;system-level test;test-based model generation;classical automata learning;observation based model generation;expert knowledge;query reduction","","2","17","","","","","","IEEE","IEEE Conferences"
"An infrastructure to functionally test designs generated by compilers targeting FPGAs","R. Rodrigues; J. M. P. Cardoso","Fac. of Sci. & Technol., Univ. do Algarve, Faro, Portugal; Fac. of Sci. & Technol., Univ. do Algarve, Faro, Portugal","Design, Automation and Test in Europe","","2005","","","30","31 Vol. 1","The paper presents an infrastructure to test the functionality of the specific architectures output by a highlevel compiler targeting dynamically reconfigurable hardware. It results in a suitable scheme to verify the architectures generated by the compiler, each time new optimization techniques are included or changes in the compiler are performed. We believe this kind of infrastructure is important to verify, by functional simulation, further research techniques, as far as compilation to field-programmable gate array (FPGA) platforms is concerned.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395524","","Field programmable gate arrays;Automatic testing;Java;Computational modeling;XML;Hardware design languages;Runtime;Automation;Engines;Optimizing compilers","program compilers;Java;XML;logic simulation;circuit simulation;digital simulation;hardware-software codesign;logic testing;program testing;field programmable gate arrays","architecture functionality testing;highlevel compiler;dynamically reconfigurable hardware;architecture generation;optimization techniques;field-programmable gate array;FPGA;reconfigurable hardware;XML;Java;functional simulation;hardware/software cosimulation","","2","8","","","","","","IEEE","IEEE Conferences"
"Optimizing system design for rapid development, fast execution and re-use [test systems]","B. Wood","Agilent Technol., Loveland, CO, USA","Proceedings AUTOTESTCON 2003. IEEE Systems Readiness Technology Conference.","","2003","","","58","71","When developing a test system from scratch, the test engineer has many choices of instrumentation and software available. LAN- and USB-based rack and stack instruments are making strides versus their GPIB cousins. VXI and other cardcage-based platforms remain viable too. Test Executives and Microsoft's Visual Studio.NET development environment, along with ""helper"" toolkits, are making software development easier than ever. But there are design choices that should be made up front that can improve performance and make it easier to adapt to new applications as they arise. This paper explains instrumentation speed/performance tradeoffs, test development environments and architectural differences that the test engineer needs to know about in order to make the right decisions. To demonstrate the thinking process for system design, a system is designed from the ground up that can test an electronic throttle module (ETM) that responds to a brake input and a PWM signal from an accelerator and controls an electric motor which in turn operates a butterfly valve.","1080-7725","0-7803-7837","10.1109/AUTEST.2003.1243556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243556","","Design optimization;System testing;Software testing;Instruments;Pulse width modulation;Programming;Application software;Signal design;Electronic equipment testing;Signal processing","test equipment;local area networks;peripheral interfaces;software engineering","test system design optimization;test instrumentation;test software;LAN-based rack and stack instruments;USB-based instruments;GPIB;VXI;cardcage-based platforms;test development environment;software development toolkits;instrumentation speed/ performance tradeoffs;electronic throttle module;ETM;brake input;accelerator PWM signal;electric motor control;butterfly valve","","","2","","","","","","IEEE","IEEE Conferences"
"Low power embedded software optimization using symbolic algebra","A. Peymandoust; T. Simunic; G. De Micheli","Comput. Syst. Lab., Stanford Univ., CA, USA; NA; NA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","1052","1058","The market demand for portable multimedia applications has exploded in recent years. Unfortunately for such applications, current compilers and software optimization methods often require designers to do part of the optimization manually. Specifically, the high-level arithmetic optimizations and the use of complex instructions are left to the designers' ingenuity. In this paper, we present a tool flow, SymSoft, that automates the optimization of power-intensive algorithmic constructs using symbolic algebra techniques combined with energy profiling. SymSoft is used to optimize and tune the algorithmic level description of an MPEG Layer III (MP3) audio decoder for the SmartBadge (Maguire et al, 1998) portable embedded system. We show that our tool lowers the number of instructions and memory accesses and thus lowers the system power consumption. The optimized MP3 audio decoder software meets real-time constraints on the SmartBadge system with low energy consumption. Furthermore, the performance improves by a factor of 7.27 and the energy consumption decreases by a factor of 4.45 over the original executable specification.","1530-1591","0-7695-1471","10.1109/DATE.2002.998432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998432","","Embedded software;Algebra;Energy consumption;Application software;Design optimization;Digital audio players;Decoding;Optimization methods;Design methodology;Optimizing compilers","decoding;symbol manipulation;audio coding;software tools;multimedia systems;optimising compilers;embedded systems","low power embedded software optimization;symbolic algebra;portable multimedia applications;compilers;software optimization methods;high-level arithmetic optimizations;complex instructions;SymSoft tool flow;power-intensive algorithmic constructs;energy profiling;algorithmic level description;memory accesses;system power consumption;MPEG Layer III audio decoder;MP3 audio decoder;SmartBadge portable embedded system;instructions;real-time constraints;energy consumption;executable specification","","13","24","","","","","","IEEE","IEEE Conferences"
"Developing interpretable models with optimized set reduction for identifying high-risk software components","L. C. Briand; V. R. Brasili; C. J. Hetmanski","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1993","19","11","1028","1044","Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are tight. Therefore, one needs to low/high fault frequency components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. The authors present the optimized set reduction approach for constructing such models, which is intended to fulfill specific software engineering needs. The approach to classification is to measure the software system and build multivariate stochastic models for predicting high-risk system components. Experimental results obtained by classifying Ada components into two classes (is, or is not likely to generate faults during system and acceptance rest) are presented. The accuracy of the model and the insights it provides into the error-making process are evaluated.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.256851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=256851","","Software engineering;System testing;Software systems;Predictive models;Data analysis;Logistics;Classification tree analysis;Machine learning;Software testing;Frequency","program testing;program verification;software reliability","high-risk software components;testing effort;verification effort;optimized set reduction approach;multivariate stochastic model;classifying Ada components;error-making process","","87","29","","","","","","IEEE","IEEE Journals & Magazines"
"Failure modes and effects analysis for software reliability","Dong Nguyen","Sextant In-Flight Syst., Thomson-CSF, Irvine, CA, USA","Annual Reliability and Maintainability Symposium. 2001 Proceedings. International Symposium on Product Quality and Integrity (Cat. No.01CH37179)","","2001","","","219","222","This paper presents a systematic problem solving approach, which is based on the failure modes and effects analysis (FMEA), to system software reliability. This approach will practically: (a) ensure that all of conceivable failure modes and their effects on operational success of the software system have been considered; (b) list potential failures, and identify the magnitude of their effects; (c) develop criteria for test planning, design of the tests, and checkout systems (e.g., logging mechanism); (d) provide a basis for quantitative reliability and availability analysis; and (e) provide a basis for establishing corrective action priorities. This approach was created for software reliability analysis and testing in the multimedia digital distribution system (MDDS) at Thomson-CSF Sextant In-Flight Systems. First it was used to improve the software reliability for the ISDN Communication Control Unit (CCU) subsystem of the MDDS, and then globally applied to the software reliability analysis MDDS and improvement for the whole MDDS. It has been proven to be an effective and efficient approach to system software reliability.","0149-144X","0-7803-6615","10.1109/RAMS.2001.902470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902470","","Failure analysis;Software reliability;Software testing;System testing;System software;Problem-solving;Software systems;Availability;Multimedia systems;ISDN","software reliability;failure analysis;multimedia communication;digital communication","software reliability;failure modes and effects analysis;system software reliability;failure modes;operational success;software system;potential failures listing;test planning;design of the tests;checkout systems;logging mechanism;quantitative reliability analysis;quantitative availability analysis;corrective action priorities;software reliability analysis;multimedia digital distribution system;Thomson-CSF Sextant In-Flight Systems;ISDN Communication Control Unit","","3","2","","","","","","IEEE","IEEE Conferences"
"ATE self test","A. M. Greenspan","NA","IEEE Automatic Testing Conference.The Systems Readiness Technology Conference. Automatic Testing in the Next Decade and the 21st Century. Conference Record.","","1989","","","284","288","The ability of automatic test equipment (ATE) to introspectively assess its own well-being as well as assess the well-being of the UUTs (units under test) external to itself has long been understood to be a major advantage of offline ATE. The author argues that this inherent potential ATE system capability has not been used effectively. It has been treated as an afterthought and implemented by the most prosaic of methods. The results of this inadequate and inappropriate treatment of ATE self-test has been stagnation in improved MTBF of new ATE systems and regression in MTTR. The maintenance and training problems for new and modern ATE have been exacerbated rather than reduced. The author contends that this situation is a result of neglect and apathy on the part of ATE systems developers who have failed to be innovative or attentive to modern system techniques in the design of self-test for their ATE. The author proposes a five-phase ATE self-test approach that he hopes can resolve the above-mentioned problems. The phases are: pre-ATE planning; ATE planning; self-test implementation; self-test maturity and evaluation; and self-test feedback/archiving.<<ETX>>","","","10.1109/AUTEST.1989.81135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=81135","","Automatic testing;Software testing;System testing;Cost function;Footwear;Test equipment;Design for testability;Design optimization;Diversity reception;Trademarks","automatic test equipment;maintenance engineering","maintenance engineering;ATE self test;automatic test equipment;units under test;MTBF;MTTR;maintenance;training","","","","","","","","","IEEE","IEEE Conferences"
"Optimal allocation of testing-resource considering cost, reliability, and testing-effort","Chin-Yu Huang; Jung-Hua Lo; Sy-Yen Kuo; M. R. Lyu","Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; NA; NA; NA","10th IEEE Pacific Rim International Symposium on Dependable Computing, 2004. Proceedings.","","2004","","","103","112","We investigate an optimal resource allocation problem in modular software systems during testing phase. The main purpose is to minimize the cost of software development when the number of remaining faults and a desired reliability objective are given. An elaborated optimization algorithm based on the Lagrange multiplier method is proposed and numerical examples are illustrated. Besides, sensitivity analysis is also conducted. We analyze the sensitivity of parameters of proposed software reliability growth models and show the results in detail. In addition, we present the impact on the resource allocation problem if some parameters are either overestimated or underestimated. We can evaluate the optimal resource allocation problems for various conditions by examining the behavior of the parameters with the most significant influence. The experimental results greatly help us to identify the contributions of each selected parameter and its weight. The proposed algorithm and method can facilitate the allocation of limited testing-resource efficiently and thus the desired reliability objective during software module testing can be better achieved.","","0-7695-2076","10.1109/PRDC.2004.1276561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276561","","Cost function;Resource management;Software testing;Software systems;System testing;Programming;Optimization methods;Lagrangian functions;Sensitivity analysis;Software reliability","resource allocation;software reliability;software cost estimation;program testing","optimal resource allocation problem;software cost development;software reliability;Lagrange multiplier method;software module testing","","7","35","","","","","","IEEE","IEEE Conferences"
"A micro software reliability model for prediction and test apportionment","M. L. Shooman","Polytech. Univ., Farmingdale, NY, USA","Proceedings. 1991 International Symposium on Software Reliability Engineering","","1991","","","52","59","A discussion is given on a new micro model which allows reliability estimation to begin at the module test phase, continue during integration testing and carry over to field deployment. The model first decomposes the structure of the software into a set of execution paths. The failure rate of the software system is related to the frequency and time of path traversal, and the probability of encountering an error during traversal. A second stage of decomposition is necessary to relate the path reliability to the module reliabilities. In the second decomposition the failure probabilities are expressed by combinatorial expressions involving the probabilities of failure of the individual modules. Since the basic model decomposes the structure into execution paths the model can be used to apportion reliabilities and test efforts among the various execution paths. The optimum allocation is computed for a particular effort model and applied to a numerical example.<<ETX>>","","0-8186-2143","10.1109/ISSRE.1991.145354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145354","","Software reliability;Predictive models;Software testing;System testing;Frequency;Phase estimation;Software systems;Life testing","optimisation;program testing;software reliability","test apportionment;micro model;reliability estimation;module test phase;integration testing;field deployment;execution paths;path traversal;decomposition;path reliability;module reliabilities;failure probabilities;combinatorial expressions;optimum allocation;numerical example","","8","25","","","","","","IEEE","IEEE Conferences"
"Computer Aided Re-Engineering (CARE) for Automatic Test Equipment (ATE)","D. F. Tyler; S. Whiseant","Access Res. Corp., Reston, VA, USA; Access Res. Corp., Reston, VA, USA","Proceedings of AUTOTESTCON '94","","1994","","","623","635","This paper addresses a current and future requirement to maintain out moded and obsolete Automatic Test Equipment (ATE) and their associated Test Program Set (TPS) software. There are two major issues with regard to this requirement; first, the ATE hardware is no longer supportable and must be replaced with newer computers and instrumentation. Second, the TPS software must be migrated to the new ATE hardware. This paper presents currently available Computer Aided Re-Engineering (CARE) techniques that will greatly assist the user with this migration.<<ETX>>","","0-7803-1910","10.1109/AUTEST.1994.381559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381559","","Automatic test equipment;Costs;Hardware;Computer aided software engineering;Automatic testing;Software testing;Software maintenance;Instruments;Documentation;Quality management","automatic test equipment;automatic test software;economics;data analysis;optimisation;software engineering","ATE;test program set software;computer aided re-engineering;automatic test equipment;CARE;metric analysis;optimisation;external interface analysis","","2","8","","","","","","IEEE","IEEE Conferences"
"A comprehensive approach for modeling and testing analog and mixed-signal devices","T. M. Souders; G. N. Stenbakken","Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA","Proceedings. International Test Conference 1990","","1990","","","169","176","An approach to optimizing the testing of analog and mixed-signal devices is presented. Once an accurate model has been developed, simple algebraic operations on the model can be used to select an optimum set of test points that will minimize the test effort and maximize the test confidence; estimate the parameters of the model from measurements made at the selected test points; predict the response of the device at all candidate test points as a basis for accepting or rejecting units; calculate the accuracy of the parameter estimates and response predictions on the basis of the random measurement error; and test the validity of the model, online, so that changes in the manufacturing process can be constantly monitored and the model can be updated. The authors show how each of these procedures can be performed using simple calls to routines that are available in both public domain and commercial linear algebra software packages. The approach is quite general and has been experimentally applied to the measurement of the frequency response of an amplifier-attenuator network, to fault diagnosis of a bandpass filter using time-domain measurements, and to efficient linearity tests of A/D and D/A converters.<<ETX>>","","0-8186-9064","10.1109/TEST.1990.114015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114015","","Testing;Predictive models;Parameter estimation;Frequency measurement;Accuracy;Measurement errors;Manufacturing processes;Condition monitoring;Linear algebra;Software packages","analogue circuits;analogue-digital conversion;application specific integrated circuits;automatic testing;band-pass filters;circuit analysis computing;digital-analogue conversion;fault location;linear algebra;optimisation;production testing","A/D convertors;LINPACK;CLAM;MATLAB;linear error model;NIST;modeling;testing;mixed-signal devices;test confidence;selected test points;random measurement error;manufacturing process;linear algebra software packages;frequency response;amplifier-attenuator network;fault diagnosis;bandpass filter;time-domain measurements;linearity tests;D/A converters","","48","8","","","","","","IEEE","IEEE Conferences"
"Determining an optimal time interval for testing and debugging software","N. D. Singpurwalla","Sch. of Eng. & Appl. Sci., George Washington Univ., Washington, DC, USA","IEEE Transactions on Software Engineering","","1991","17","4","313","319","A decision-theoretic procedure for determining an optimal time interval for testing software prior to its release is proposed. The approach is based on the principles of decision-making under uncertainty and involves a maximization of expected utility. Two plausible forms for the utility function, one based on costs and the other involving the realized reliability of the software, are described. Using previous results on probabilistic models for software failure, the ensuing optimization problem (which can be addressed using numerical techniques) is outlined for the case of single-state testing. The sensitivity of the results to the various input parameters is discussed, and some directions for future research are outlined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=90431","","Software testing;Software debugging;Cost function;Bayesian methods;Certification;Computer bugs;Decision making;Uncertainty;Utility theory;Computer science","decision theory;program debugging;program testing;programming theory","software testing;software debugging;decision theory;software reliability;optimal time interval;decision-making;uncertainty;maximization;expected utility;utility function;costs;probabilistic models;software failure;optimization problem;single-state testing","","43","19","","","","","","IEEE","IEEE Journals & Magazines"
"Use object-oriented paradigm to design and implement an algorithm for object-oriented class-level testing","Yu Xia Sun; Huo Yan Chen","Dept. of Comput. Sci., Jinan Univ., Guangzhou, China; Dept. of Comput. Sci., Jinan Univ., Guangzhou, China","SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)","","2003","2","","1069","1074 vol.2","One of the means to enhance software reliability and quality is testing. The testing for object-oriented software is more complex and difficult than that for traditional programming. For object-oriented class-level and cluster-level testing, we proposed a significant methodology TACLLE which has been published in ACM Transactions on Software Engineering and Methodology (vol.7., no.3, p.250-95, 1998; vol.10, no.1, p.56-109, 2001). This methodology includes an important algorithm named DOE to determine whether two objects are observational equivalent or not. This paper presents the design and implementation for a new prototype of DOE using object-oriented paradigm. The new prototype is based on UML model. It is improved on the technique, and optimized in the system interface.","1062-922X","0-7803-7952","10.1109/ICSMC.2003.1244554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244554","","Algorithm design and analysis;Software testing;Clustering algorithms;US Department of Energy;Software prototyping;Prototypes;Software reliability;Object oriented programming;Software engineering;Unified modeling language","object-oriented programming;specification languages;software quality;software reliability;program testing","object-oriented paradigm;object-oriented class-level testing;software reliability;software quality;object-oriented software testing;TACLLE methodology;ACM transactions;software engineering;UML;system interface;Unified Modeling Language","","1","5","","","","","","IEEE","IEEE Conferences"
"How to choose semiconductor IP: embedded software","G. Martin","Cadence Design Syst., Berkeley, CA, USA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","16","","Embedded software intellectual property (IP) is becoming vital for today's complex system-on-chips. We first define the notion of hardware-dependent software, and then review the multidimensional criteria for choosing ESW IP, including retargetability and portability, flexibility, optimisation, validation and certification.","1530-1591","0-7695-1471","10.1109/DATE.2002.998243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998243","","Embedded software;Hardware;Application software;Software debugging;Software maintenance;Software reusability;Middleware;Software libraries;Digital signal processing;Intellectual property","industrial property;embedded systems;application specific integrated circuits;software portability;program verification;certification;software selection","semiconductor IP;embedded software;intellectual property;complex system-on-chips;hardware-dependent software;multidimensional criteria;retargetability;portability;flexibility;optimisation;validation;certification","","1","3","","","","","","IEEE","IEEE Conferences"
"Software optimization of the MPEG-audio decoder using a 32-bit MCU RISC processor","Keun-Sup Lee; Young Cheol Park; Dae Hee Youn","MCSP Lab., Yonsei Univ., Seoul, South Korea; NA; NA","IEEE Transactions on Consumer Electronics","","2002","48","3","671","676","This paper proposes optimization techniques to implement MPEG audio decoding algorithms in real-time using general-purpose 32-bit MCU RISC processor. Both MP3 and AAC LC profile decoding algorithms can be partitioned into two parts: control-intensive part and computation-intensive part. Optimization techniques in this study are focused on developing methodologies that are suitable for each part. We implemented MP3/AAC decoder using ARM-based RISC MPU and its test board. Both designed decoders with proposed optimizations could achieve decoding processes in real-time within an operating frequency of 35 MHz. ISO 13818-4 compliance test results confirmed that the proposed decoders ensured full compliance with the ISO 13818-3 audio decoder. These implementation results illustrate the efficiency of the proposed design methodology in both performance and cost.","0098-3063;1558-4127","","10.1109/TCE.2002.1037059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1037059","","Decoding;Reduced instruction set computing;Partitioning algorithms;Digital audio players;Testing;ISO standards;Optimization methods;Design optimization;Frequency;Design methodology","code standards;telecommunication standards;audio coding;decoding;digital signal processing chips;reduced instruction set computing;optimisation","software optimization;MPEG-audio decoder;MPEG audio decoding algorithms;general-purpose MCU RISC processor;AAC LC profile decoding algorithm;MP3 decoding algorithm;MP3/AAC decoder;ARM-based RISC MPU;test board;operating frequency;ISO 13818-4 compliance test results;ISO 13818-3 audio decoder;32 bit;35 MHz","","14","6","","","","","","IEEE","IEEE Journals & Magazines"
"ECSAMS - bridging the gap from EC flightline test to backshop and laboratory support","W. Williams","AAI Corp., Hunt Valley, MD, USA","IEEE Autotestcon, 2005.","","2005","","","9","15","The AN/USM-670 is the current standard in electronic combat (EC) system flight line level test - supporting a wide variety of EC systems across multiple platforms. In the lab, the test and maintenance of these same EC systems requires racks of stimulus and measurement instrumentation, and sophisticated operator knowledge of the stimulus and analysis requirements. Realizing that the USM-670 could be a useful (and cost-effective) asset in the lab environment, the question was posed: ""How could the USM-670 hardware (and more importantly, the software) is modified to fill the needs of laboratory testing?"" This paper will discuss the design evolution of the electronic combat stimulus and measurement system (ECSAMS), a set of rack mountable VME-based virtual instruments optimized for backshop / lab test and evaluation of EC systems including both the radar warning receivers and jamming countermeasures systems. Included will be a discussion of the hardware and software architecture, stimulus and measurement capabilities, and the user interface. To aide in the discussion - and to demonstrate ECSAMS capabilities and ease-of-use - several common EC test / measurement requirements will be presented, along with the ECSAMS solution. Examples will include: complex threat (stimulus) development; use of test scenarios; automated measurement of jamming techniques including range, velocity, amplitude modulation, and noise","1088-7725;1558-4550","0-7803-9101","10.1109/AUTEST.2005.1609091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609091","","Laboratories;System testing;Electronic equipment testing;Instruments;Hardware;Radar countermeasures;Jamming;Aerospace electronics;Software testing;Design optimization","automatic test software;electronic countermeasures;virtual instrumentation","ECSAMS;EC flightline test;backshop/lab test;AN/USM-670;electronic combat system;flight line level test;measurement instrumentation;sophisticated operator knowledge;electronic combat stimulus and measurement system;VME-based virtual instruments;radar warning receivers;jamming countermeasures systems;hardware and software architecture","","2","2","","","","","","IEEE","IEEE Conferences"
"Optimized synthesis of dedicated controllers with concurrent checking capabilities","R. Leveugle; G. Saucier","Inst. Nat. Polytech. de Grenoble/CSI, France; Inst. Nat. Polytech. de Grenoble/CSI, France","Proceedings. 'Meeting the Tests of Time'., International Test Conference","","1989","","","355","363","The authors present a novel synthesis method of dedicated controllers which aims at the detection of faults which cause errors in the state sequences. The state code flow is compacted through polynomial division. An implicit 'justifying signature' method is applied at the state code level and ensures identical signatures before each join node of the control flow graph. The signatures are then independent of the path followed previously in the graph, and the comparison with reference data is greatly facilitated. This property is obtained by a clever state assignment, nearly without area overhead. The controllers can therefore be checked by signature analysis, either by a built-in monitor or by an external checker. The software implementation of the synthesis tool is presented, and the hardware implementation of the concurrent checking is described.<<ETX>>","","","10.1109/TEST.1989.82319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=82319","","Flow graphs;Hardware;Flowcharts;Circuit testing;Fault detection;Error correction;Monitoring;Control systems;Fault tolerant systems;Built-in self-test","automatic test equipment;automatic testing;control system CAD;fault location;logic arrays;logic CAD;logic testing;optimisation;programmable controllers;state assignment","optimised synthesis;ATE;logic testing;dedicated controllers;concurrent checking;detection of faults;state code flow;polynomial division;justifying signature;control flow graph;state assignment;signature analysis;built-in monitor;external checker","","6","34","","","","","","IEEE","IEEE Conferences"
"Optimized CAD of power amplifiers, for maximum added power or minimum third order intermodulation, using an optimization software coupled to a single tone source and load-pull set-up","J. M. Nebus; J. P. Villotte; J. F. Vidalou; L. Hagerman; H. Jallageas; M. C. Albuquerque","Limoges Univ., France; Limoges Univ., France; Limoges Univ., France; Limoges Univ., France; Limoges Univ., France; NA","1988., IEEE MTT-S International Microwave Symposium Digest","","1988","","","1049","1052 vol.2","Using the narrow-bandwidth approximation, the authors show that added power and third-order intermodulation can be approximated with a symmetrical source and load-pull single-tone measurement setup coupled with optimization software. This measurement setup has a completely symmetrical topology allowing simultaneous active source and load-pull. The device under test is loaded by 50 Omega at the harmonic frequencies. A rigorous calibration procedure was implemented and validated at low power levels by comparison with an HP 8510 network analyzer. The data acquisition software is described, and large-signal experimental results are given for a typical transistor.<<ETX>>","","","10.1109/MWSYM.1988.22211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=22211","","Power amplifiers;Frequency;Design optimization;Artificial intelligence;Impedance;Power harmonic filters;Bandwidth;Ducts;Power transistors;Particle measurements","automatic testing;circuit CAD;electronic equipment testing;intermodulation measurement;power amplifiers","power amplifiers;maximum added power;minimum third order intermodulation;optimization software;single tone source;load-pull set-up;narrow-bandwidth approximation;symmetrical topology;calibration procedure;data acquisition software","","6","5","","","","","","IEEE","IEEE Conferences"
"IP for embedded robustness","M. Nicolaidis","IRoC Technol., France","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","240","241","The following topics are dealt with: formal verification of designs; cooling layout; power analysis; SAT; BDD; interconnects; low power design; advanced mixed signal testing; collaborative design; logic synthesis; SoC; symbolic techniques; EDA tools; platform based design; analogue simulation; asynchronous circuits; BIST; network on chip; modelling; embedded systems; reconfigurable architectures; test resource partitioning; deep submicron design; logic synthesis; buffering; automatic design; object oriented systems; real time systems; online testing; fault tolerance; design space evaluation; architectural level synthesis; memory testing; high level synthesis; coupling and switching noise; and power optimisation.","1530-1591","0-7695-1471","10.1109/DATE.2002.998277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998277","","Robustness;Circuit testing;Signal design;Logic testing;Automatic testing;System testing;Circuit synthesis;Network synthesis;Automatic logic units;Logic design","formal specification;integrated circuit layout;circuit CAD;fault diagnosis;integrated circuit testing;mixed analogue-digital integrated circuits;analogue integrated circuits;embedded systems;asynchronous circuits;high level synthesis;built-in self test;reconfigurable architectures;automatic testing;integrated circuit interconnections;integrated memory circuits;cooling;object-oriented methods;circuit optimisation","formal verification;cooling layout;power analysis;logic design;SAT;SoC;analogue circuit;asynchronous circuit;BIST;DFT;memory optimisation;low power architecture;layout design;embedded cores;deep submicron design;buffering;analogue design;mixed signal circuits;object oriented systems;interconnect modelling;online testing;fault tolerance;embedded software;high level synthesis;optimisation","","3","2","","","","","","IEEE","IEEE Conferences"
"Priority based data flow testing","R. Gupta; M. L. Soffa","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; Dept. of Comput. Sci., Pittsburgh Univ., PA, USA","Proceedings of International Conference on Software Maintenance","","1995","","","348","357","Software testing is an expensive component of software development and maintenance. For data flow testing, test cases must be found to test the def-use pairs in a program. Since some of the def-use pairs identified through static analysis may be infeasible, no amount of testing effort may result in exhaustive testing of a program. Therefore in practice a fixed amount of effort is spent in testing a program. We develop an approach for assigning priorities to def-use pairs, such that the def-use pairs with higher priorities can be expected to require less effort for test case generation and therefore testing. Thus, by using the priorities as a guide for ordering the def-use pairs for testing, we can maximize the number of def-use pairs tested using a fixed amount of testing effort. We apply the technique to regression testing during the software maintenance phase, in which case the priorities are assigned to capture not only the difficulty in test case generation but also the likelihood that an error introduced by a program change is uncovered by the test case.","1063-6773","0-8186-7141-60-8186-7677","10.1109/ICSM.1995.526556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526556","","Software testing;Computer science;Software maintenance;Data analysis","data flow analysis;program diagnostics;program testing;software maintenance;statistical analysis","priority based data flow testing;software testing;software development;software maintenance;def-use pairs;test case generation;regression testing;error;program change","","2","11","","","","","","IEEE","IEEE Conferences"
"Free MDD-based software optimization techniques for embedded systems","Changhee Kim; L. Lavagno; A. Sangiovanni-Vincentelli","Res. Inst. of Eng. & Tech., Hanyang Univ., Seoul, South Korea; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","14","18","Embedded systems make a heavy use of software to perform real-time embedded control tasks. Embedded software is characterized by a relatively long lifetime and by tight cost, performance and safety constraints. Several super-optimization techniques for embedded softwares based on multi-valued decision diagram (MDD) representations have been described in the literature, but they all share the same basic limitation. They are based on standard ordered MDD (OMDD) packages, and hence require a used order of evaluation for the MDD variables on every execution path. Free MDDs (FMDDs) lift this limitation, and hence open up more optimization opportunities. Finding the optimal variable ordering for FMDDs is a very difficult problem. Hence in this paper we describe a heuristic procedure that performs well in practice, and is based on FMDD cost estimation applied to recursive cofactoring. Experimental results show that our new variable ordering method obtains often. Smaller embedded software than previous (sifting-based) methods.","","0-7695-0537","10.1109/DATE.2000.840009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840009","","Embedded software;Embedded system;Software performance;Costs;Control system synthesis;Packaging;Hardware;Control systems;Data structures;Boolean functions","program testing;embedded systems;decision diagrams","free MDD-based software optimization techniques;embedded systems;multi-valued decision diagram;standard ordered MDD;execution path;heuristic procedure;recursive cofactoring;sifting-based methods","","1","20","","","","","","IEEE","IEEE Conferences"
"Optimal release policy for hyper-geometric distribution software-reliability growth model","Rong-Huei Bou; Sy-Yen Kuo; Yi-Ping Chang","Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA","IEEE Transactions on Reliability","","1996","45","4","646","651","The hyper-geometric distribution software-reliability growth model (HGDM) can estimate the number of initial faults in a software program. An important problem in software development is to determine when to stop testing and then release the software. This paper mainly investigates the optimal software release policies which minimize the mean total software cost and satisfy the software-reliability requirement based on the HGDM. The optimal software release times are determined and shown to be finite. A numerical example illustrates these optimal software release policies.","0018-9529;1558-1721","","10.1109/24.556588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556588","","Software testing;Cost function;Logistics;Constraint optimization;Solid modeling;Software systems;Product development;Lead;Programming;Software standards","software reliability;program testing","optimal release policy;hyper-geometric distribution software-reliability growth model;initial faults estimation;software program;software development;optimal software release policies;mean total software cost;optimal software release times","","6","","","","","","","IEEE","IEEE Journals & Magazines"
"A novel semiconductor test equipment concept: automatic test equipment with computational intelligence technique (ATE-CIT)","E. Liau; D. Schmitt-Landsiedel","MP Technol. & Innovation, Infineon Technol. AG, Munich, Germany; NA","Proceedings of the 21st IEEE Instrumentation and Measurement Technology Conference (IEEE Cat. No.04CH37510)","","2004","3","","2144","2149 Vol.3","Semiconductor automatic test equipment (ATE) analyses the responses from the semiconductor chip based on a set of pre-defined lest patterns and test conditions, and marks the chip as good or bad. This set of tests (patterns and conditions) is either manually developed by engineers or generated via circuit-simulation tools. The process of generating a set of worst case tests (patterns and conditions) is very time consuming, usually trial and error for different test combinations form a long iterative loop during the design (silicon) analysis phase. The major disadvantage is that ATE can not learn, manipulate and optimize by itself based on previous tests experiences. In this paper, we proposed a computational intelligence technique (CIT) with ATE concept, such that test responses can be described by fuzzy logic, learned by neural network, and tests can be optimized automatically by genetic algorithm. Our experimental results demonstrate an excellent efficiency using ATE-CIT during the design analysis phase.","1091-5281","0-7803-8248","10.1109/IMTC.2004.1351514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1351514","","Test equipment;Automatic test equipment;Computational intelligence;Circuit testing;Automatic testing;Pattern analysis;Test pattern generators;Logic testing;Semiconductor device testing;Silicon","automatic test equipment;automatic test pattern generation;integrated circuit testing;fuzzy logic;fuzzy neural nets;learning (artificial intelligence);inference mechanisms;software tools;electronic engineering computing;genetic algorithms","semiconductor test equipment concept;automatic test equipment;computational intelligence technique;worst case tests;fuzzy logic;neural network;genetic algorithm;design analysis phase;pattern generator;fuzzification;inference;learning rules;software-toolbox;embedded memory test chip","","3","16","","","","","","IEEE","IEEE Conferences"
"Optimized relay testing systems","M. Agrasar; J. R. Hernandez; F. Uriondo; J. Amantegui; J. M. Gallastegui; J. L. Martinez","Basque Country Univ., Spain; NA; NA; NA; NA; NA","IEEE Computer Applications in Power","","1997","10","4","55","59","With the appearance on the market of electronic test equipment that is controlled by software from personal computers, automation of classical test methods has been achieved and new methods have been developed. For engineers and technicians in charge of performing integrity and application tests, steady-state and dynamic tests can provide helpful information used in design, commissioning and preventive and corrective maintenance. These test methods result in savings of time and the cost involved in these tasks. Since 1990, the Spanish electric utility, Iberdrola, and the Electrical Department of the University of the Basque Country have been developing tools to: automate protective relay testing; analyze the location and characteristics of faults in the power network on the basis of oscillographic records; and simulate real or hypothetical fault conditions on a protective relay in order to monitor its behavior. Two automatic relay testing systems and experience of their use are introduced in this article. Particular testing protocols of different relays are incorporated and all useful features available to increase the automation grade are considered. Both systems minimize power system protection relay maintenance times and costs and improve the quality and reliability of relay management.","0895-0156;1558-4151","","10.1109/67.625376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=625376","","Relays;System testing;Power system relaying;Protective relaying;Automatic testing;Power system protection;Automation;Electronic equipment testing;Costs;Power system reliability","power system protection;power system relaying;relay protection;automatic test software;automatic test equipment;microcomputer applications;electrical faults;power engineering computing;maintenance engineering","power system protection relays;automated relay testing systems;electronic test equipment;personal computers;software;electric utility;fault location analysis;fault characterisation;maintenance","","4","4","","","","","","IEEE","IEEE Journals & Magazines"
"A unified SOC test approach based on test data compression and TAM design","V. Iyengar; A. Chandra","IBM Microeletronics, Essex Junction, VT, USA; NA","Proceedings 18th IEEE Symposium on Defect and Fault Tolerance in VLSI Systems","","2003","","","511","518","Test access mechanism (TAM) optimization and test data compression lead to a reduction in test data volume and testing time for SOCs. In this paper, we integrate for the first time both these approaches into a single test methodology. We show, how an integrated test architecture based on TAMs and test data decoders can be designed. The proposed approach offers considerable savings in test resource requirements. Two case studies using the integrated test architecture are presented. Experimental results on rest data volume reduction, savings in test application time and the low test pin overheads for a benchmark SOC demonstrate the effectiveness of this approach.","1550-5774","0-7695-2042","10.1109/DFTVS.2003.1250150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250150","","Test data compression;System-on-a-chip;System testing;Automatic testing;Costs;Logic testing;Pins;Software testing;Hardware;Job shop scheduling","logic design;logic testing;system-on-chip;data compression;integrated circuit testing","SOC test;test data compression;TAM design;TAM optimization;test access mechanism;test data volume reduction;testing time reduction;test data decoders;test resource requirements","","6","20","","","","","","IEEE","IEEE Conferences"
"Testing of the rapidly developed prototypes","M. S. Reston","Bell Commun. Res., Morristown, NJ, USA","[1990 Proceedings] The First International Workshop on Rapid System Prototying","","1990","","","139","143","A conflict occurs between rapid prototyping and the adequate testing of these prototypes. The author shows how much of the software testing process could be automated to produce faster and better quality prototypes. The application for which the automated testing tools were developed is a prototyped negotiation support system (NSS). The system's primary function is to assist in the negotiation of the networking services offered by Bellcore Client Companies (BCCs) to their customers. The Negotiation is preceded by an initialization process (called an NSS Maintenance), which creates a customer-specific Data Base (DB). The System for Performance Testing of NSS (SPT) was developed to automate NSS testing. SPT's major function is to simulate the NSS execution of several different classes of negotiation scenarios. The SPT system has been in use for almost six months, and the preliminary estimates show that testing productivity has significantly improved. In the remainder of this paper, the methodology used to build SPT as a general automated testing tool is described.<<ETX>>","","0-8186-2175","10.1109/IWRSP.1990.144047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=144047","","Prototypes;Software prototyping;Automatic testing;System testing;Vehicle crash testing;Performance evaluation;Humans;Databases;Optimization methods;Software performance","circuit CAD;testing","rapid prototyping;adequate testing;software testing process;automated testing tools;prototyped negotiation support system;Bellcore Client Companies;initialization process;NSS Maintenance;customer-specific Data Base;System for Performance Testing;testing productivity;general automated testing tool","","","2","","","","","","IEEE","IEEE Conferences"
"An adaptive diversity strategy for particle swarm optimization","Fang Wang; Naiqin Feng; Yuhui Qiu","Intelligent Software & Software Eng. Lab., Southwest Univ., Chongqing, China; Intelligent Software & Software Eng. Lab., Southwest Univ., Chongqing, China; Intelligent Software & Software Eng. Lab., Southwest Univ., Chongqing, China","2005 International Conference on Natural Language Processing and Knowledge Engineering","","2005","","","760","764","In this paper, we present a diversity strategy for particle swarm optimizer. The modified algorithm re-initializes part of particles with poorer fitness during the searching process. It is empirically tested and compared with other published methods on many famous benchmark functions. The experimental results illustrate that the proposed algorithm has the potential to achieve higher success ratio and better solution quality. It is very competitive for hard multimodal function optimization.","","0-7803-9361","10.1109/NLPKE.2005.1598838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598838","","Particle swarm optimization;Software engineering;Laboratories;Benchmark testing;Birds;Optimization methods;Genetic engineering;Genetic algorithms;Convergence;Stochastic processes","particle swarm optimisation;search problems","adaptive diversity strategy;particle swarm optimization","","2","14","","","","","","IEEE","IEEE Conferences"
"Putting the user in user-interface testing","J. Nielsen","SunSoft, Mountain View, CA, USA","IEEE Software","","1996","13","3","89","90","Discusses one of the most basic topics in user-interface design: how to run a user test. No matter how good we are at designing a user interface, we must always test it with real users to see whether it is as usable as we hope. The three main rules of user testing are: get real users, have them do real tasks, and shut up while they are trying. Tools, techniques and concepts to optimize user interfaces are described, and ethical issues are discussed.","0740-7459;1937-4194","","10.1109/52.493024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493024","","System testing;Materials testing;Recruitment;User interfaces;Software testing;Educational institutions;Unemployment;Frequency;Dentistry;Software quality","user interfaces;program testing;user centred design;task analysis;professional aspects","user interface testing;user interface design;real users;task testing;user interface optimization;noninterference policy;observation;ethical issues","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Factors determining test speed of automated EMI measurements","W. Schaefer","Syst. Div., Hewlett-Packard Co., Santa Rosa, CA, USA","Proceedings of International Symposium on Electromagnetic Compatibility","","1995","","","276","279","This paper describes the impact of different categories of factors on the overall test time of radiated EMI measurements. Throughput and efficiency, along with measurement accuracy and repeatability, are major concerns of EMI test facility operators today. Therefore various test methods, particularly in radiated EMI compliance measurements, are in use to reduce the test time. The difficulty of optimizing test throughput by minimizing time however lies in the variety of factors impacting the test time. Some of them are related to the equipment under test (EUT) itself, others related to the measurement system used and test procedure applied. For these reasons no one single approach of achieving utmost efficiency under all circumstances is available. Understanding the impact and the nature of time contributing factors allows an easier streamlining of each individual measurement, especially in case a dedicated application software is used.","","0-7803-2573-70-7803-3608","10.1109/ISEMC.1995.523562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=523562","","Automatic testing;Electromagnetic interference;Velocity measurement;Time measurement;Throughput;System testing;Test facilities;Particle measurements;Software measurement;Application software","automatic test software;electromagnetic interference","automated EMI measurements;test speed;radiated EMI measurements;throughput;efficiency;measurement accuracy;measurement repeatability;EMI test facility;test methods;radiated EMI compliance measurements;equipment under test;measurement system;test procedure;dedicated application software","","1","2","","","","","","IEEE","IEEE Conferences"
"Yield learning via functional test data","Young-Jun Kwon; D. M. H. Walker","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA","Proceedings of 1995 IEEE International Test Conference (ITC)","","1995","","","626","635","This paper presents a methodology to estimate the defect Pareto in an IC process through the use of production functional test data. This Pareto can then be used for yield improvement activities. We demonstrate the concept on several benchmark circuits. We show how limited IDDQ current testing can significantly improve the Pareto accuracy.","1089-3539","0-7803-2992","10.1109/TEST.1995.529891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529891","","Circuit testing;Circuit simulation;Production;Costs;Circuit faults;Optical losses;Condition monitoring;Predictive models;Performance evaluation;Computer science","circuit optimisation;integrated circuit yield;probability;production testing;integrated circuit testing;fault diagnosis;automatic test software;circuit analysis computing;VLSI;Monte Carlo methods;electric current measurement","yield learning;defect Pareto;IC process;production functional test data;yield improvement;benchmark circuits;limited IDDQ current testing;yield ramp;VLSI manufacture;local defects;fault simulation;VLASIC simulator;Monte Carlo simulator;ATPG","","13","46","","","","","","IEEE","IEEE Conferences"
"When zero picoseconds edge placement accuracy is not enough","J. Cheng","Teradyne Inc., Boston, MA, USA","Proceedings International Test Conference 2001 (Cat. No.01CH37260)","","2001","","","1134","1142","In the last ten years, test equipment suppliers have driven improvements in edge placement accuracy taking it from /spl plusmn/225ps to sub-100ps through a combination of architectural improvements and new calibration technology. However with the adoption of high speed source synchronous buses such as HyperTransport and RapidIO on high performance devices, it is no longer sufficient to just look at the tester EPA component in the overall timing budget. Although test system accuracy is still very important, error terms from the DUT must also be considered. The proposed methodology of device strobed comparators addresses both test system and device error terms.","1089-3539","0-7803-7169","10.1109/TEST.2001.966740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966740","","Clocks;Bandwidth;Switches;Transmitters;System testing;Microprocessors;Application software;Multimedia databases;Fabrics;Test equipment","system buses;comparators (circuits);synchronisation;calibration;automatic test equipment","test equipment;edge placement accuracy;calibration;high speed source synchronous buses;HyperTransport;RapidIO;timing budget;errors;device strobed comparators;sweeping window;burst time optimization;225 to 100 ps","","7","8","","","","","","IEEE","IEEE Conferences"
"An Algorithm to Allocate the Testing-Effort Expenditures Based on Sensitive Analysis Method for Software Module Systems","J. Lo","Department of Information Management, Lan-Yang Institute of Technology, I-Lan, Taiwan. losir@mail.fit.edu.tw","TENCON 2005 - 2005 IEEE Region 10 Conference","","2005","","","1","6","To develop a good reliable software system, a project manager must determine in advance how to effectively allocate these resources. Due to the dynamic properties of the system and the insufficiency of the failure data, the accurate parameter values are hard to determine. Therefore, the sensitivity analysis is often used in this stage to deal with this problem. In this paper, we study two optimal resource allocation problems in a module software during testing phase: (1) minimization of the remaining faults when a fixed amount of testing-effort is given, and (2) minimization of the required amount of testing-effort when a specific reliability requirement is given. Several useful optimization algorithms based on Lagrange multiplier method are proposed. Furthermore, we present the sensitivity analysis on these allocation problems in order to determine which of the parameters affects the system most. Finally, a numerical example is evaluated to validate and show the effectiveness of the proposed approach.","2159-3442;2159-3450","0-7803-9312-00-7803-9311","10.1109/TENCON.2005.301151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4085019","","Software algorithms;Software testing;System testing;Algorithm design and analysis;Software systems;Resource management;Sensitivity analysis;Project management;Software development management;Optimization methods","optimisation;resource allocation;sensitivity analysis;software reliability","testing-effort expenditures;software module systems;reliable software system;sensitivity analysis;resource allocation;optimization;Lagrange multiplier","","1","24","","","","","","IEEE","IEEE Conferences"
"Increase software trustability with self-testable classes in Java","D. Deveaux; P. Frison; J. -. Jezequel","Lab. Valoria, South Britany Univ., Vannes, France; NA; NA","Proceedings 2001 Australian Software Engineering Conference","","2001","","","3","11","The rise of component-based software development poses the problem of components trustability. To know whether a given component can be used within a certain context, there must be a way of telling what the component is supposed to do (its contract) without entering into the details of the how. Based on this idea, we have proposed a general Design-for-Trustability methodology (""DfT""), starting from simple self-testing of individual classes to optimized integration testing, This paper describes the ""DfT"" methodology, the self-testable class model that supports it and the associated tools for Java development.","1530-0803","0-7695-1254","10.1109/ASWEC.2001.948491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948491","","Built-in self-test;Java;Documentation;Contracts;Automatic testing;Computer industry;Libraries;Design methodology;Design optimization;Software tools","Java;software process improvement;program testing;software reliability","component-based software development;components trustability;design-for-trustability methodology;self-testing;optimized integration testing;self testable class model Java development;software component;software development process;design by contract;XML","","9","24","","","","","","IEEE","IEEE Conferences"
"A transformation system for interactive reformulation of design optimization strategies","T. Euman; J. Keane; T. Murata; M. Schwabacher","Dept. of Comput. Sci., Rutgers Univ., Piscataway, NJ, USA; Dept. of Comput. Sci., Rutgers Univ., Piscataway, NJ, USA; Dept. of Comput. Sci., Rutgers Univ., Piscataway, NJ, USA; Dept. of Comput. Sci., Rutgers Univ., Piscataway, NJ, USA","Proceedings 1995 10th Knowledge-Based Software Engineering Conference","","1995","","","44","51","Numerical design optimization algorithms are highly sensitive to the particular formulation of the optimization problems they are given. The formulation of the search space, the objective function and the constraints will generally have a large impact on the duration of the optimization process as well as the quality of the resulting design. Furthermore, the best formulation will vary from one application domain to another, and from one problem to another within a given application domain. Unfortunately, a design engineer may not know the best formulation in advance of attempting to set up and run a design optimization process. In order to attack this problem, we have developed a software environment that supports interactive formulation, testing and reformulation of design optimization strategies. Our system represents optimization strategies in terms of second-order dataflow graphs. Reformulations of strategies are implemented as transformations between dataflow graphs. The system permits the user to interactively generate and search a space of design optimization strategies, and experimentally evaluate their performance on test problems, in order to find a strategy that is suitable for his application domain. The system has been implemented in a domain independent fashion, and is being tested in the domain of racing yacht design.","1068-3062","0-8186-7204","10.1109/KBSE.1995.490118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=490118","","Design optimization;Application software;Design engineering;Constraint optimization;Boats;Software testing;System testing;Computer science;Pathology","optimisation;programming environments;program testing;data flow graphs;interactive programming;CAD","interactive reformulation;design optimization strategies;numerical design optimization algorithms;transformation system;search space;objective function;constraints;application domain;software environment;interactive formulation;interactive testing;second-order dataflow graphs;racing yacht design","","","9","","","","","","IEEE","IEEE Conferences"
"Software architectural transformations: a new approach to low energy embedded software","T. K. Tan; A. Raghunathan; N. K. Jha","Dept. of Electr. Eng., Princeton Univ., NJ, USA; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","1046","1051","Previous work on software optimization for low energy has focussed on instruction-level optimizations and compiler techniques. We argue, and demonstrate, that significant energy savings could be ""left on the table"" if energy is not considered during the design of the software architecture. As a first step towards addressing this gap, we propose a systematic framework for software architectural transformations to reduce energy consumption. We consider software architectural transformations in the context of the multi-process software style driven by an operating system (OS), which is very commonly employed in energy-sensitive embedded systems. Our methodology for applying software architectural transformations consists of: (i) constructing a software architecture graph representation, (ii) deriving initial energy and performance statistics using a detailed energy simulation framework, (iii) constructing sequences of atomic software architectural transformations, guided by energy change estimates derived from high-level energy macro-models, that result in maximal energy reduction, and (iv) generation of program source code to reflect the optimized software architecture. We employ a wide suite of software architectural transformations whose effects span the application-OS boundary, including how the program functionality is structured into architectural components (e.g., application processes, signal handlers, and device drivers), and connectors between them (inter- component synchronization and communication mechanisms). We present experimental results on several multi-process embedded software programs, in the context of an embedded system that features the Intel StrongARM processor and the embedded Linux OS. The presented results clearly underscore the potential of the proposed methodology (up to 66.1% reduction in energy is obtained). In a broader sense, our work demonstrates the impact of considering energy during the earlier stages of the software design process.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253742","","Embedded software;Software architecture;Optimizing compilers;Software design;Embedded system;Software performance;Software systems;Energy consumption;Operating systems;Statistics","embedded systems;software architecture;operating systems (computers);minimisation","software architectural transformations;low energy embedded software;energy consumption reduction;multi-process software style;operating system;energy-sensitive embedded systems;software architecture graph representation;performance statistics;energy simulation framework;high-level energy macro-models;program source code generation;multi-process embedded software programs;Intel StrongARM processor;embedded Linux OS;software design process","","13","18","","","","","","IEEE","IEEE Conferences"
"Optimizing testing efficiency with error-prone path identification and genetic algorithms","J. R. Birt; R. Sitte","Sch. of Inf. Technol., Griffith Univ., Gold Coast, Qld., Australia; Sch. of Inf. Technol., Griffith Univ., Gold Coast, Qld., Australia","2004 Australian Software Engineering Conference. Proceedings.","","2004","","","106","115","We present a method for optimizing software testing efficiency by identifying the most error prone path clusters in a program. We do this by developing variable length genetic algorithms that optimize and select the software path clusters which are weighted with sources of error indexes. Although various methods have been applied to detecting and reducing errors in a whole system, there is little research into partitioning a system into smaller error prone domains for testing. Exhaustive software testing is rarely possible because it becomes intractable for even medium sized software. Typically only parts of a program can be tested, but these parts are not necessarily the most error prone. Therefore, we are developing a more selective approach to testing by focusing on those parts that are most likely to contain faults, so that the most error prone paths can be tested first. By identifying the most error prone paths, the testing efficiency can be increased.","","0-7695-2089","10.1109/ASWEC.2004.1290463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290463","","Genetic algorithms;Software testing;Software reliability;Gold;Computer errors;Australia;Flow graphs;Error correction codes;Information technology;Postal services","program testing;genetic algorithms;software reliability","software testing efficiency;error prone path identification;genetic algorithm;software reliability;optimization;software path cluster;error indexes","","5","15","","","","","","IEEE","IEEE Conferences"
"Test optimization of bus-structured SoCs using embedded processor","M. H. Tehranipour; M. Nourani; S. M. Fakhraie; C. A. Papachristou","Dept. of Electron. Eng., Texas Univ., Richardson, TX, USA; Dept. of Electron. Eng., Texas Univ., Richardson, TX, USA; NA; NA","The 2002 45th Midwest Symposium on Circuits and Systems, 2002. MWSCAS-2002.","","2002","1","","I","168","Embedded processors are now widely used in system-on-chips. This paper presents an optimization technique for testing a bus-structured system using an embedded processor.We present a systematic approach for test access mechanism that allows processor to access all cores with minimum overhead. We show an ILP formulation to minimize the test schedule. The method requires negligible overhead but provides great flexibility in terms of access mechanism and future reuse.","","0-7803-7523","10.1109/MWSCAS.2002.1187183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1187183","","Circuit testing;System testing;Logic testing;Built-in self-test;Software testing;Processor scheduling;Test pattern generators;Digital signal processing chips;Protection;Intellectual property","system-on-chip;embedded systems;microprocessor chips;integrated circuit testing;integer programming;linear programming","test optimization;embedded processor;system-on-chip;test access mechanism;bus-structured system;integer linear programming","","","13","","","","","","IEEE","IEEE Conferences"
"Tool support for testing and documenting framework-based software","W. Strunk; C. Lilienthal","Micrologica AG, Bargteheide, Germany; NA","Proceedings of Technology of Object-Oriented Languages and Systems - TOOLS 30 (Cat. No.PR00278)","","1999","","","237","246","Complex object oriented applications are these days built on the basis of frameworks. While it is clear that a framework and the applications built using the framework conform to some design, we experienced a mismatch between the ""idealized"" software architecture presented in the documentation and the architectural structures actually existing in the source code. This mismatch belongs either to the failures of the developer in implementing the proposed architecture or to the lack of tool support. By providing tool support for dynamic diagrams, applications can be animated for debug and optimization purposes and actual design documentation can be extracted from the implemented model. Framework inspection using a true object oriented approach enables the software engineer to access the source level using an object oriented map of the observed application instead of switching to a procedural way of working.","","0-7695-0278","10.1109/TOOLS.1999.787552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=787552","","Software testing;Software tools;Application software;Documentation;Software architecture;Computer architecture;Animation;Design optimization;Object oriented modeling;Inspection","program testing;object-oriented programming;software architecture;system documentation;program visualisation","tool support;framework based software testing;complex object oriented applications;idealized software architecture;architectural structures;source code;dynamic diagrams;optimization;actual design documentation;framework inspection;object oriented approach;software engineer;source level access;object oriented map;procedural working","","1","20","","","","","","IEEE","IEEE Conferences"
"Derivation of an integrated operational profile and use case model","P. Runeson; B. Regnell","Dept. of Commun. Syst., Lund Univ., Sweden; NA","Proceedings Ninth International Symposium on Software Reliability Engineering (Cat. No.98TB100257)","","1998","","","70","79","Requirements engineering and software reliability engineering both involve model building related to the usage of the intended system; requirements models and test case models respectively are built. Use case modelling for requirements engineering and operational profile testing for software reliability engineering are techniques which are evolving into software engineering practice. Approaches towards integration of the use case model and the operational profile model are proposed. By integrating the derivation of the models, effort may be saved in both development and maintenance of software artifacts. Two integration approaches are presented, transformation and extension. It is concluded that the use case model structure can be transformed into an operational profile model adding the profile information. As a next step, the use case model can be extended to include the information necessary for the operational profile. Through both approaches, modelling and maintenance effort as well as risks for inconsistencies can be reduced. A positive spin-off effect is that quantitative information on usage frequencies is available in the requirements, enabling planning and prioritizing based on that information.","1071-9458","0-8186-8991","10.1109/ISSRE.1998.730843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730843","","Computer aided software engineering;System testing;Reliability engineering;Software testing;Software reliability;Costs;Software engineering;Programming;Software quality;Probability","formal specification;software reliability;software maintenance;software maintenance","integrated operational profile/use case model;requirements engineering;software reliability engineering;requirements models;test case models;software artifact development;software artifact maintenance;transformation;extension;quantitative information;usage frequencies;planning;prioritization","","8","16","","","","","","IEEE","IEEE Conferences"
"Optimal budget spending for software testing under the condition of nonlinear constraint","H. Yongming; W. Xianglin; Y. Chaoyuan","Institute of System Engineering, Huazhong University of Science &#x0026; Technology, Wuhan 430074. P. R. China; Institute of System Engineering, Huazhong University of Science &#x0026; Technology, Wuhan 430074. P. R. China; Institute of System Engineering, Huazhong University of Science &#x0026; Technology, Wuhan 430074. P. R. China","Journal of Systems Engineering and Electronics","","2003","14","3","92","97","Software testing is a very important phase of the software development process. It is a very difficult job for a software manager to allocate optimally the financial budget to a software project during testing. In this paper the problem of optimal allocation of the software testing cost is studied. There exist several models focused on the development of software costs measuring the number of software errors remaining in the software during testing. The purpose of this paper is to use these models to formulate the optimization problems of resource allocation: Minimization of the total number of software errors remaining in the system. On the assumption that a software project consists of some independent modules, the presented approach extends previous work by defining new goal functions and extending the primary assumption and precondition.","1004-4132","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6077533","Software development project;Nonlinear programming;Software testing;Budget allocation;Optimal control","Resource management;Software reliability;Software systems;Software testing","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Design for diagnostics views and experiences","V. R. Rao","Intel Corp., Santa Clara, CA, USA","Proceedings International Test Conference 1998 (IEEE Cat. No.98CH36270)","","1998","","","1137","","Design For Diagnostics (DFD) is critical for rapid silicon debug of new product and ramping to high volume manufacturing. I feel two aspects of DFD are important. The first enables software fault localization tools that quickly localize to the failing node (this includes ad hoc DFT methods, scan etc.). The second type of DFD enables success with physical localization tools. This includes designed in physical features to enable rapid bug verification with focused ion beam milling, and probe points for internal signal probing. Both types of DFD are needed and complement each other in the different phases of silicon debug and root cause analysis.","1089-3539","0-7803-5093","10.1109/TEST.1998.743328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=743328","","Design for disassembly;Silicon;Probes;Delay;Ion beams;Milling;Performance analysis;Testing;Design optimization;Inverters","fault diagnosis;integrated circuit testing;design for testability;focused ion beam technology;logic testing","design for diagnostics;Intel experiences;rapid silicon debug;ramping;high volume manufacturing;software fault localization tools;failing node;ad hoc DFT methods;scan methods;physical localization tools;designed in physical features;rapid bug verification;focused ion beam milling;probe points;internal signal probing;root cause analysis;bonus cells;uncommitted logic elements;speed path fix;bonus inverter gates;die internal probing","","","","","","","","","IEEE","IEEE Conferences"
"Evaluation of system BIST using computational performance measures","D. L. Landis; D. C. Muha","Dept. of Electr. Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Electr. Eng., Pennsylvania State Univ., University Park, PA, USA","International Test Conference 1988 Proceeding@m_New Frontiers in Testing","","1988","","","531","536","The impact of built-in self-test (BIST) techniques and system maintenance strategies on the performance of a VLSI processor system is examined. The specific BIST technique used was shown to have a significant influence upon instantaneous and cumulative system reward. It was shown that the additional overhead of the distributed and BILBO approaches is justified for this model when area utilization and cumulative area utilization are considered. For the assumed design parameters, the results presented allow a VLSI system designer to choose an optimal configuration based on system requirements and individual component parameters. The optimal performance will also depend on system and component parameters such as processor failure rates and fault coverage. These results are relevant to the design, evaluation, and optimization of highly reliable, high-performance digital processing systems.<<ETX>>","1089-3539","0-8186-0870","10.1109/TEST.1988.207833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207833","","Built-in self-test;Circuit testing;Computer applications;Fault tolerant systems;Semiconductor device measurement;Application software;Circuit faults;Fault detection;Very large scale integration;Area measurement","automatic testing;circuit CAD;integrated circuit testing;maintenance engineering;microprocessor chips;optimisation;VLSI","IC testing;BIST;built-in self-test;maintenance;VLSI processor system;BILBO;optimal configuration;failure rates;fault coverage;optimization;digital processing systems","","1","21","","","","","","IEEE","IEEE Conferences"
"Managing the maintenance of ported, outsourced, and legacy software via orthogonal defect classification","K. Bassin; P. Santhanam","Center for Software Eng., IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA","Proceedings IEEE International Conference on Software Maintenance. ICSM 2001","","2001","","","726","734","From the perspective of maintenance, software systems that include COTS software, legacy, ported or outsourced code pose a major challenge. The dynamics of enhancing or adapting a product to address evolving customer usage and the inadequate documentation of these changes over a period of time (and several generations) are just two of the factors which may have a debilitating effect on the maintenance effort. While many approaches and solutions have been offered to address the underlying problems, few offer methods which directly affect a team's ability to quickly identify and prioritize actions targeting the product which is already in front of them. The paper describes a method to analyze the information contained in the form of defect data and arrive at technical actions to address explicit product and process weaknesses which can be feasibly addressed in the current effort. The defects are classified using Orthogonal Defect Classification (ODC) and actual case studies are used to illustrate the key points.","1063-6773","0-7695-1189","10.1109/ICSM.2001.972791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972791","","Software maintenance;Costs;Electrical capacitance tomography;Engineering management;Software engineering;Inspection;Software testing;Stability;Measurement standards;Standards development","software maintenance;outsourcing;program testing;software development management","legacy software maintenance;orthogonal defect classification;software systems;COTS software;ported code;outsourced code;evolving customer usage;documentation;maintenance effort;defect data;technical actions;explicit product;Orthogonal Defect Classification;case studies;software evaluation;ODC","","4","9","","","","","","IEEE","IEEE Conferences"
"Maximizing interval reliability in operational software system with rejuvenation","H. Suzuki; T. Dohi; N. Kaio; K. S. Trivedi","Dept. of Inf. Eng., Hiroshima Univ., Japan; Dept. of Inf. Eng., Hiroshima Univ., Japan; NA; NA","14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.","","2003","","","479","490","Software aging often affects the performance of a software system and eventually causes it to fail. A novel approach to handle transient software failures is called software rejuvenation which can be regarded as a preventive and proactive solution that is particularly useful for counteracting the phenomenon of software aging. In this paper, we consider the optimal software rejuvenation policy maximizing the interval reliability in the general semi-Markov framework. We derive analytically the optimal software rejuvenation timing which maximizes the limiting interval reliability or the interval reliability with exponentially distributed operation times. Further, we examine numerically the transient behavior of the interval reliability at an arbitrary operation time. Our results under the interval reliability criteria are extentions of some earlier work, since the interval reliability can be specialized to the pointwise availability and the common reliability function.","1071-9458","0-7695-2007","10.1109/ISSRE.2003.1251068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251068","","Software systems;Aging;Application software;Reliability engineering;Software performance;Software testing;Availability;Software reliability;Humans;Software safety","software maintenance;software performance evaluation;software reliability;Markov processes;optimisation","interval reliability maximization;software reliability;operational software system;software aging;software system performance;software failure;semiMarkov framework;optimal software rejuvenation timing;transient behavior;pointwise availability","","8","25","","","","","","IEEE","IEEE Conferences"
"Support for modular parsing in software reengineering","I. Peake; E. Salzman","Centre for Software Maintenance, Univ. of Queensland, Qld., Australia; NA","Proceedings Eighth IEEE International Workshop on Software Technology and Engineering Practice incorporating Computer Aided Software Engineering","","1997","","","58","66","As reengineering increasingly contributes to software engineering, so can software engineering principles contribute to cost-effective reengineering tool development. The cost of modelling languages motivates support for modular parsers which can, like program modules, be assembled cheaply from smaller, tested components. We describe a scheme which achieves this by extending the expressive and flexible combinator parsing scheme using object-oriented constructs (class inheritance and dynamic method dispatch). Related schemes either do not fully support code sharing or sacrifice flexibility. The scheme has been implemented in a prototype reengineering environment and successfully tested on grammars such as Modula-2. The generation time for extensions is linear in the size of the extension. The run-time performance is potentially as bad as for general parsing algorithms, but can be linear (10 times slower than LALR for Modula-2) after optimization.","","0-8186-7840","10.1109/STEP.1997.615464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=615464","","Software engineering;Testing;Object oriented modeling;Software maintenance;Information technology;Australia;Costs;Assembly;Prototypes;Production","systems re-engineering;computer aided software engineering;software tools;grammars;object-oriented programming;inheritance;software performance evaluation;Modula","modular parsing;software reengineering;software engineering principles;cost-effective reengineering tool development;modelling languages;combinator parsing scheme;object-oriented constructs;class inheritance;dynamic method dispatch;code sharing;flexibility;expressiveness;prototype reengineering environment;grammars;Modula-2;extension generation time;optimization;run-time performance","","2","27","","","","","","IEEE","IEEE Conferences"
"Testing, optimization, and games","M. Yannakakis","Dept. of Comput. Sci., Columbia Univ., New York, NY, USA","Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science, 2004.","","2004","","","78","88","We discuss algorithmic problems arising in the testing of reactive systems, i.e. systems that interact with their environment. The goal is to design test sequences so that we can deduce desired information about the given system under test, such as whether it conforms to a given specification model, or whether it satisfies given requirement properties. Test generation can be approached from different points of view - as an optimization problem of minimizing cost and maximizing the effectiveness of the tests; as a game between tester and system under test; or as a learning problem. We touch on some of these aspects and related algorithmic questions.","1043-6871","0-7695-2192","10.1109/LICS.2004.1319602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319602","","System testing;Automata;Logic testing;Computer science;Cost function;Automatic testing;Software testing;Software systems;Hardware","program testing;optimisation;game theory;program verification;formal specification;conformance testing","algorithmic problems;reactive system testing;test sequences;test generation;optimization problem","","7","33","","","","","","IEEE","IEEE Conferences"
"Genetic programming-based decision trees for software quality classification","T. M. Khoshgoftaar; N. Seliya; Yi Liu","Dept. of Comput. Sci. & Eng., Florida Atlantic Univ., Boca Raton, FL, USA; NA; NA","Proceedings. 15th IEEE International Conference on Tools with Artificial Intelligence","","2003","","","374","383","The knowledge of the likely problematic areas of a software system is very useful for improving its overall quality. Based on such information, a more focused software testing and inspection plan can be devised. Decision trees are attractive for a software quality classification problem which predicts the quality of program modules in terms of risk-based classes. They provide a comprehensible classification model which can be directly interpreted by observing the tree-structure. A simultaneous optimization of the classification accuracy and the size of the decision tree is a difficult problem, and very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (gp) based decision tree modeling technique for the software quality classification problem. Genetic programming is ideally suited for problems that require optimization of multiple criteria. The proposed technique is based on multi-objective optimization using strongly typed GP. In the context of an industrial high-assurance software system, two fitness functions are used for the optimization problem: one for minimizing the average weighted cost of misclassification, and one for controlling the size of the decision tree. The classification performances of the GP-based decision trees are compared with those based on standard GP, i.e., S-expression tree. It is shown that the GP-based decision tree technique yielded better classification models. As compared to other decision tree-based methods, such as C4.5, GP-based decision trees are more flexible and can allow optimization of performance objectives other than accuracy. Moreover, it provides a practical solution for building models in the presence of conflicting objectives, which is commonly observed in software development practice.","1082-3409","0-7695-2038","10.1109/TAI.2003.1250214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250214","","Decision trees;Software quality;Classification tree analysis;Genetic programming;Software systems;Software testing;Inspection;Computer industry;Electrical equipment industry;Industrial control","software quality;genetic algorithms;decision trees;program testing;software metrics","GP-based decision trees;software quality classification;software system;software testing;software inspection;program module;risk-based classes;classification model;tree-structure;simultaneous optimization;automated genetic programming;multiple criteria;S-expression tree;C4.5 decision tree;software development;software metrics;multiobjective optimization;misclassification cost","","12","23","","","","","","IEEE","IEEE Conferences"
"Software reliability measurement and modeling for multiple releases of commercial software","J. Tian; P. Lu","IBM, North York, Ont., Canada; NA","Proceedings of 1993 IEEE International Symposium on Software Reliability Engineering","","1993","","","253","260","This paper summarizes our experience and findings in measuring and modeling software reliability for a large IBM software product. Four consecutive releases of this product were studied, with various reliability models fitted for individual releases. The models are evaluated by how good they can fit the observed data and how predictive they are. Model sensitivity and performance analysis are performed for across release comparisons, resulting in the models being ranked according to their robustness and accuracy in predicting failures across different data sets. Various ways of combining the multiple release information for generalization and extrapolation to successor releases are also explored.","","0-8186-4010","10.1109/ISSRE.1993.624295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624295","","Software reliability;Software measurement;Predictive models;System testing;Application software;Laboratories;Reliability engineering;Software testing;Milling machines;Electronic mail","software reliability","model sensitivity;reliability measurement;commercial software;software reliability;IBM software product;reliability models;performance analysis;data sets","","2","8","","","","","","IEEE","IEEE Conferences"
"Towards optimization of the coverage testing of interactive systems","F. Belli; C. J. Budnik","Dept. of Comput. Sci., Electr. Eng. & Math., Paderborn Univ., Germany; Dept. of Comput. Sci., Electr. Eng. & Math., Paderborn Univ., Germany","Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.","","2004","2","","18","19 vol.2","This work introduces a model-based approach for minimization of test costs for interactive systems. Results known from state-based conformance testing and graph theory are used and extended to construct algorithms for test case generation and selection to cover the behavioral model of the system under test (SUT). The test case selection on the basis of the established model is ruled by an adequacy criterion, which provides a measure of how effective a given set of test cases is in terms of its potential to reveal faults. The approach is specification-oriented; i.e., the underlying model represents the system behavior interacting with the user's actions, that are viewed here as events.","0730-3157","0-7695-2209","10.1109/CMPSAC.2004.1342657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342657","","System testing;Interactive systems;Computer science;Mathematics;Human computer interaction;Safety;Costs;Graph theory","interactive systems;conformance testing;graph theory;formal specification;program testing","system coverage testing;interactive system;model-based approach;test cost minimisation;state-based conformance testing;system under test;test case selection","","1","5","","","","","","IEEE","IEEE Conferences"
"A modified particle swarm optimizer with roulette selection operator","Fang Wang; Yuhui Qiu","Intelligent Software & Software Eng. Lab., Southwest Univ., Chongqing, China; Intelligent Software & Software Eng. Lab., Southwest Univ., Chongqing, China","2005 International Conference on Natural Language Processing and Knowledge Engineering","","2005","","","765","768","In this paper, a novel particle swarm optimizer combined with the roulette selection operator is proposed, which provides a mechanism to restrain the predominating of super particles in early stage and can effectively avoid the premature problem. We conduct variety experiments to test the proposed algorithm and compare it with other published methods on several test functions taken from the literature. The computational results demonstrate that this revised algorithm is promising to achieve faster convergence and better solutions, especially for multimodal function optimization.","","0-7803-9361","10.1109/NLPKE.2005.1598839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598839","","Particle swarm optimization;Testing;Birds;Optimization methods;Competitive intelligence;Software engineering;Laboratories;Functional programming;Computational intelligence;Computational modeling","particle swarm optimisation","modified particle swarm optimizer;roulette selection operator;multimodal function optimization","","5","15","","","","","","IEEE","IEEE Conferences"
"Setting maintenance quality objectives and prioritizing maintenance work by using quality metrics","N. F. Schneidewind","Naval Postgraduate Sch., Monterey, CA, USA","Proceedings. Conference on Software Maintenance 1991","","1991","","","240","249","Metrics that are collected and validated during development can be used during maintenance to control quality and prioritize maintenance work. Validity criteria are defined mathematically. The approach is based on validating selected metrics against related quality factors during development and using the validated metrics during maintenance to: establish initial quality objectives and quality control criteria and prioritize software components (e.g., module) and allocate resources to maintain them. The author illustrates both a case of passing a validation test (discriminative power) and failing a validation test (tracking).<<ETX>>","","0-8186-2325","10.1109/ICSM.1991.160337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=160337","","Software quality;Resource management;Software maintenance;Q factor;Quality control;Testing;Gain measurement;Software measurement;Power measurement;Financial management","quality control;resource allocation;software maintenance;software metrics;software reliability","maintenance quality objectives;quality metrics;maintenance work;selected metrics;related quality factors;validated metrics;initial quality objectives;quality control criteria;software components;validation test;discriminative power;tracking","","1","14","","","","","","IEEE","IEEE Conferences"
"Selecting Software Test Data Using Data Flow Information","S. Rapps; E. J. Weyuker","Courant Institute of Mathematical Sciences, Department of Computer Science, New York University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","4","367","375","This paper defines a family of program test data selection criteria derived from data flow analysis techniques similar to those used in compiler optimization. It is argued that currently used path selection criteria, which examine only the control flow of a program, are inadequate quate. Our procedure associates with each point in a program at which a variable is defined, those points at which the value is used. Several test data selection criteria, differing in the type and number of these associations, are defined and compared.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702019","Data flow;program testing;test data selection","Software testing;Error correction;Data analysis;Information analysis;Program processors;Optimizing compilers;Computer science;System testing;Intelligent systems","","Data flow;program testing;test data selection","","459","13","","","","","","IEEE","IEEE Journals & Magazines"
"A temporal approach for testing distributed systems","A. Khoumsi","Sherbrooke Univ., Que., Canada","IEEE Transactions on Software Engineering","","2002","28","11","1085","1103","This paper deals with testing distributed software systems. In the past, two important problems have been determined for executing tests using a distributed test architecture: controllability and observability problems. A coordinated test method has subsequently been proposed to solve these two problems. In the present article: 1) we show that controllability and observability are indeed resolved if and only if the test system respects timing constraints, even when the system under test is non-real-time; 2) we determine these timing constraints; 3) we determine other timing constraints which optimize the duration of test execution; 4) we show that the communication medium used by the test system does not necessarily have to be FIFO; and 5) we show that the centralized test method can be considered just as a particular case of the proposed coordinated test method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049406","","System testing;Timing;Controllability;Observability;Software testing;Constraint optimization;Fault detection;Software systems;Computer architecture;Error correction","program testing;timing;distributed programming","distributed software system testing;distributed test architecture;controllability;observability;temporal approach;coordinated test method;timing constraints;communication medium;centralized test method","","30","10","","","","","","IEEE","IEEE Journals & Magazines"
"Parameter optimization tool for enhancing on-chip network performance","J. Riihimaki; E. Salminen; K. Kuusilinna; T. Hamalainen","Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland; Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland; Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland; Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland","2002 IEEE International Symposium on Circuits and Systems. Proceedings (Cat. No.02CH37353)","","2002","4","","IV","IV","In this paper, we present a tool to be used in the optimization of interconnection parameters in order to achieve optimal performance and implementation with minimal costs. The optimization tool uses an iterative algorithm to optimize the interconnection parameters, such as data width, priorities, and the time an agent can reserve the interconnection, to fulfill the given constraints. In the used test case, the required area decreased 50% while 85% of the original bandwidth was obtained. This was due to an improved arbitration process.","","0-7803-7448","10.1109/ISCAS.2002.1010388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010388","","Network-on-a-chip;System-on-a-chip;Optimization methods;Constraint optimization;Testing;Cost function;Iterative algorithms;Bandwidth;Digital systems;Complex networks","circuit optimisation;software tools;circuit CAD;iterative methods;integrated circuit design;integrated circuit interconnections;integrated circuit metallisation","parameter optimization;on-chip network performance enhancement;interconnection parameters;optimal performance;optimal implementation;minimal costs;iterative algorithm;data width;interconnection priorities;agent interconnection reservation time;interconnection constraints;interconnection test;interconnection bandwidth;arbitration process","","2","8","","","","","","IEEE","IEEE Conferences"
"Failure analysis for full-scan circuits","K. De; A. Gunda","LSI Logic Corp., Milpitas, CA, USA; LSI Logic Corp., Milpitas, CA, USA","Proceedings of 1995 IEEE International Test Conference (ITC)","","1995","","","636","645","We present a complete system for failure analysis of full-scan circuits. A novel scheme has been proposed to handle multiple faults up to a certain extent by ranking the faults according to the likelihood of being present in the defective part. The user can interactively recompute the suspect fault list by changing some parameters. If the suspect fault list is large, we generate new test patterns to distinguish the faults in the suspect list. User can iterate over a defective part several times until the suspect fault list is reasonably small. Then each suspect site is probed using E-beam. This tool is integrated into design environment of LSI Logic Corporation and produced good results when applied on a few industry circuits.","1089-3539","0-7803-2992","10.1109/TEST.1995.529892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529892","","Failure analysis;Circuit faults;Fault diagnosis;Dictionaries;Circuit testing;System testing;Large scale integration;Logic circuits;Sequential circuits;Information analysis","fault diagnosis;logic testing;fault location;automatic testing;boundary scan testing;design for testability;electron beam testing;integrated circuit testing;automatic test software","full-scan circuits;failure analysis;multiple faults;suspect fault list;test pattern generation;electron beam probed;integrated into design environment;fault diagnosis;stuck-at fault model;fault localisation;fault merit scale;algorithm;SCAN-FA tool","","28","21","","","","","","IEEE","IEEE Conferences"
"Zero-one integer programming model in path selection problem of structural testing","J. -. Lin; C. -. Chung","Inst. of Comput. Sci. & Inf. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Inst. of Comput. Sci. & Inf. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan","[1989] Proceedings of the Thirteenth Annual International Computer Software & Applications Conference","","1989","","","618","627","A major issue in structural program testing is how to select a minimal set of test paths to meet certain test requirements. The zero-one integer programming model, a generalized optimal path selection method for node (or statement) testing and branch testing criteria, is extended in such a way that it can be used for DD-path testing, TER/sub n/ measurement, and all types of local coverage test criteria. With slight modification, it can also be applied to all types of data-flow-oriented test criteria. The model can be used for program testing based on any coverage criterion of the structural testing approach. If a mixture of multiple test criteria is needed, the model is still workable. The model can be applied to program testing with various objective functions and can be extended to multiple goal objective function problems. Since the objective functions are independent from the constraints of test criteria, it is possible to have various combinations of optimization criteria and coverage requirements according to the specified test strategy. Characteristics of the zero-one integer programming model are discussed.<<ETX>>","","0-8186-1964","10.1109/CMPSAC.1989.65156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65156","","Linear programming;Software testing;Software quality;Computer science;Reliability engineering;Constraint optimization;Mathematical model","integer programming;program testing;software engineering","node testing;statement testing;path selection problem;structural program testing;minimal set;test paths;test requirements;zero-one integer programming model;optimal path selection;branch testing;DD-path testing;TER/sub n/ measurement;local coverage test criteria;data-flow-oriented test criteria;coverage criterion;multiple test criteria;multiple goal objective function problems;optimization criteria;coverage requirements","","3","30","","","","","","IEEE","IEEE Conferences"
"Fully automatic test program generation for microprocessor cores","F. Corno; G. Cumani; M. Sonza Reorda; G. Squillero","Dipt. di Automatica e Informatica, Politecnico di Torino, Italy; Dipt. di Automatica e Informatica, Politecnico di Torino, Italy; Dipt. di Automatica e Informatica, Politecnico di Torino, Italy; Dipt. di Automatica e Informatica, Politecnico di Torino, Italy","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","1006","1011","Microprocessor cores are a major challenge in the test arena: not only is their complexity always increasing, but also their specific characteristics intensify all difficulties. A microprocessor embedded inside a SoC is even harder to test since its input might be harder to control and its behavior may be harder to observe. Functional testing is an effective solution which consists in forcing the microprocessor to execute a suitable test program. This paper presents a new approach to automatic test program generation exploiting an evolutionary paradigm. It overcomes the main limitations of previous methodologies and provides significantly better results. Human intervention is limited to the enumeration of all assembly instructions. Also internal parameters of the optimizer are auto-adapted Experimental results show the effectiveness of the approach.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253736","","Automatic testing;Microprocessors;System-on-a-chip;Circuit faults;Assembly;Logic;Design for testability;Performance evaluation;Built-in self-test;Circuit testing","system-on-chip;microprocessor chips;automatic test software;software performance evaluation;integrated circuit testing;logic testing","automatic test program generation;microprocessor cores;SoC embedded cores;functional testing;evolutionary paradigm;software-based methodology","","42","14","","","","","","IEEE","IEEE Conferences"
"When to stop testing software? A fuzzy interval approach","L. Gemoets; V. Kreinovich; H. Melendez","Dept. of Decision & Inf. Eng., Texas Univ., El Paso, TX, USA; NA; NA","NAFIPS/IFIS/NASA '94. Proceedings of the First International Joint Conference of The North American Fuzzy Information Processing Society Biannual Conference. The Industrial Fuzzy Control and Intellige","","1994","","","182","186","the experience of many programmers shows that it is actually impossible to extract all the faults from system-type software, that is commonly involved in resource contention, or from the programs with a sophisticated user interface. Also, it is necessary to estimate the time interval during which the remaining faults will not influence the program. This problem is vital for estimating the cost of the software project. At present about three dozen statistical models (called software reliability models) are used to get such estimates. However, these statistical models lack convincing theoretical-explanation. They are semi-heuristic, and often look like curve-fitting. In this paper, we consider a fuzzy approach to program testing. We formulate the problem of choosing the best fuzzy software reliability model as a mathematical optimization problem, and solve this problem.<<ETX>>","","0-7803-2125","10.1109/IJCF.1994.375102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=375102","","Software testing;Software reliability;Computer science;Information systems;Computer errors;Error correction;Programming profession;Software systems;User interfaces;Debugging","program testing;software reliability;program debugging;fuzzy logic;optimisation","software testing;fuzzy interval approach;fuzzy software reliability models;statistical models;optimization","","3","7","","","","","","IEEE","IEEE Conferences"
"Software failure rate and reliability incorporating repair policies","S. S. Gokhale","Dept. of Comput. Sci. & Eng., Connecticut Univ., Storrs, CT, USA","10th International Symposium on Software Metrics, 2004. Proceedings.","","2004","","","394","404","Reliability of a software application, its failure rate and the residual number of faults in an application are the three most important metrics that provide a quantitative assessment of the failure characteristics of an application. Typically, one of many stochastic models known as software reliability growth models (SRGMs) is used to describe the failure behavior of an application in its testing phase, and obtain an estimate of the above metrics. In order to ensure analytical tractability, SRGMs are based on an assumption of instantaneous repair and thus the estimates of the metrics obtained using SRGMs tend to be optimistic. In practice, fault repair activity consumes a nonnegligible amount of time and resources. Also, repair may be conducted according to many policies which are reflective of the schedule and budget constraints of a project. A few research efforts that have sought to incorporate repair into SRGMs are restrictive, since they consider only one of the several SRGMs, model the repair process using a constant rate, and provide an estimate of only the residual number of faults. These techniques do not address the issue of estimating application failure rate and reliability in the presence of repair. In this paper we present a generic framework which relies on the rate-based simulation technique in order to provide the capability to incorporate various repair policies into the finite failure nonhomogeneous Poisson process (NHPP) class of software reliability growth models. We also present a technique to compute the failure rate and the reliability of an application in the presence of repair. The potential of the framework to obtain quantitative estimates of the above three metrics taking into consideration different repair policies is illustrated using several scenarios.","1530-1435","0-7695-2129","10.1109/METRIC.2004.1357924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357924","","Application software;Software reliability;Stochastic processes;Phase estimation;Computer science;Reliability engineering;Software testing;Computational modeling;Information technology;Environmental economics","software fault tolerance;software metrics;software maintenance;program testing;probability","software failure rate;repair policies;software reliability growth models;rate-based simulation;finite failure nonhomogeneous Poisson process;software metrics","","7","25","","","","","","IEEE","IEEE Conferences"
"Maintaining software with a security perspective","K. Jiwnani; M. Zelkowitz","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","International Conference on Software Maintenance, 2002. Proceedings.","","2002","","","194","203","Testing for software security is a lengthy, complex and costly process. Currently, security testing is done using penetration analysis and formal verification of security kernels. These methods are not complete and are difficult to use. Hence it is essential to focus testing effort in areas that have a greater number of security vulnerabilities to develop secure software as well as meet budget and time constraints. We propose a testing strategy based on a classification of vulnerabilities to develop secure and stable systems. This taxonomy will enable a system testing and maintenance group to understand the distribution of security vulnerabilities and prioritize their testing effort according to the impact the vulnerabilities have on the system. This is based on Landwehr's (1994) classification scheme for security flaws and we evaluated it using a database of 1360 operating system vulnerabilities. This analysis indicates vulnerabilities tend to be focused in relatively few areas and associated with a small number of software engineering issues.","1063-6773","0-7695-1819","10.1109/ICSM.2002.1167766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167766","","Software maintenance;System testing;Software testing;Data security;Formal verification;Kernel;Time factors;Taxonomy;Databases;Operating systems","security of data;program testing;software maintenance;operating systems (computers)","software security testing;budget constraints;time constraints;vulnerability classification;stable systems;software maintenance;security flaw classification scheme;operating system vulnerabilities;software engineering","","17","25","","","","","","IEEE","IEEE Conferences"
"Optimal design for software reliability and development cost","D. -. Chi; W. Kuo","Dept. of Ind. & Manuf. Syst. Eng., Iowa State Univ., Ames, IA, USA; Dept. of Ind. & Manuf. Syst. Eng., Iowa State Univ., Ames, IA, USA","IEEE Journal on Selected Areas in Communications","","1990","8","2","276","282","A process for reliability-related quality programming is developed to fill existing gaps in software design and development so that a quality programming plan can be achieved. The tradeoffs among system reliability improvement, resource consumption, and other relevant constraints through the management phase are investigated. A software reliability-to-cost relation is developed both from a software reliability-related cost model and from software redundancy models with common-cause failures. A generic N-component redundancy model is also developed. The software reliability optimization problems can be formulated into a mixed-integer programming problem.<<ETX>>","0733-8716;1558-0008","","10.1109/49.46882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46882","","Software design;Software reliability;Cost function;Redundancy;Hardware;Debugging;Testing;Software performance;Delay;Personnel","economics;integer programming;optimisation;quality control;software reliability","software development;optimal design;software reliability;development cost;reliability-related quality programming;software design;system reliability;resource consumption;management;cost model;software redundancy models;failures;optimization;mixed-integer programming problem","","13","10","","","","","","IEEE","IEEE Journals & Magazines"
"Development of an ATLAS test language to automatic test markup language translator","J. Gomes","Navair Lakehurst, Florissant, MO, USA","Proceedings AUTOTESTCON 2004.","","2004","","","191","195","Development of an IEEE standard ATLAS syntax based translator for legacy ATLAS code into automatic test markup language, an XML schema, (ATML). Utilizing a design focused on deterministic finite automata generation tools for LALR languages coding efforts are minimized while optimizing repeatability of design and reducing development and maintenance costs. Jflex and CUP are utilized as the Java code generators of the lexical and syntactical parsing scanners. The formal syntax and sub-field definitions presented in the IEEE Standard ATLAS test language are directly utilized in writing the source code input for the parser and lexical analyzer generation tools. The resulting translator program is open source written in Java utilizing deterministic finite automata structures for the lexical and syntax parsing functions on a PC platform. A minimum level of completion representing a subset of the ATLAS language has been developed to show proof of concept.","1088-7725","0-7803-8449","10.1109/AUTEST.2004.1436827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436827","","Automatic testing;Markup languages;Code standards;Automata;Java;Standards development;XML;Design optimization;Cost function;Writing","automatic test pattern generation;automatic test equipment;automatic test software;IEEE standards;program compilers;Java;XML;deterministic automata;finite automata","ATLAS test language;automatic test markup language translator;IEEE standard;XML schema;deterministic finite automata;Java code generator;lexical parsing;syntactical parsing","","","15","","","","","","IEEE","IEEE Conferences"
"Software test data generation using program instrumentation","M. J. Gallagher; V. L. Narasimhan","The PAUSE Res. Lab., Queensland Univ., Brisbane, Qld., Australia; The PAUSE Res. Lab., Queensland Univ., Brisbane, Qld., Australia","Proceedings 1st International Conference on Algorithms and Architectures for Parallel Processing","","1995","2","","575","584 vol.2","This paper presents the design of the software system, ADTEST, for generating test data for programs developed in Ada. The key feature of this system is that the problem of test data generation is treated entirely as a dynamic numerical optimisation problem and, as a, consequence, this method does not suffer from difficulties commonly found in symbolic execution systems, such as those associated with input-variable-dependent loops, array references, and module calls. Instead, program instrumentation is used to solve a set of path constraints without explicitly knowing their form. The system supports not only the generation of integer and real data types, but also non-numeric data types such as characters and enumerated types. The system has been tested on large Ada programs (>60000 lines of code) and found to reduce the effort required to test programs as well as provide an increase in test coverage.<<ETX>>","","0-7803-2018","10.1109/ICAPP.1995.472243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472243","","Software testing;Instruments;System testing;Character generation;Laboratories;Australia;Software design;Software systems;Design optimization;Software tools","program testing;Ada","software test data generation;program instrumentation;ADTEST;Ada;dynamic numerical optimisation problem;input-variable-dependent loops;array references;module calls;path constraints","","2","10","","","","","","IEEE","IEEE Conferences"
"Optimized timed hardware software cosimulation without roll-back","Wonyong Sung; Soonhoi Ha","Dept. of Comput. Eng., Seoul Nat. Univ., South Korea; NA","Proceedings Design, Automation and Test in Europe","","1998","","","945","946","An optimized hardware software cosimulation method based on the backplane approach is presented in this paper. To enhance the performance of cosimulation, efforts are focused on reducing control packets between simulators as well as concurrent execution of simulators without roll-back.","","0-8186-8359","10.1109/DATE.1998.655981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655981","","Hardware;Backplanes;Discrete event simulation;Clocks;Optimization methods;Acceleration;Timing;Variable structure systems;Utility programs;Libraries","software engineering;circuit analysis computing;digital simulation;timing","optimized timed hardware software cosimulation;backplane approach;concurrent execution","","3","2","","","","","","IEEE","IEEE Conferences"
"Testing for millennium risk management","M. Feord","Compuware, Netherlands","IEEE Software","","1997","14","3","126","127","The Year 2000 conversion is a challenge to the economics of both testing and maintenance, but as a whole we are not responding with balanced priorities. Those looking for Year 2000 solutions typically allocate their first energies and budgets to acquiring automated analysis and conversion deals and services. This disturbing tendency ignores two facts that surface in virtually every analysis of the Year 2000 challenge: fifty percent or more of the effort will be in testing; and despite consuming a wealth of resources each year, current testing practices cannot satisfy the demands of current maintenance unrelated to the Y2K conversion. For the moment, most organizations continue to delay action on the Y2K Test problem while wading through the dozens of available analysis and conversion solutions. As a result, many Y2K projects have started on master plans that will need major revision once the true needs and benefits of testing automation become apparent. A growing number of those projects have already corrected course, revising strategy and reallocating budgets once they appreciated the nature of the Y2K testing challenge. Embarrassment will probably be the least of many worries for those that ignore the challenge much longer.","0740-7459;1937-4194","","10.1109/52.589256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589256","","Risk management;Automatic testing;System testing;Costs;Production facilities;Software testing;Manufacturing automation;Productivity;Costing;Performance evaluation","program testing;software maintenance;risk management;software development management","millennium risk management testing;Year 2000 conversion;software maintenance;Year 2000 solutions;testing practices;Y2K conversion;Y2K Test problem;conversion solutions;Y2K projects;master plans;budgets;Y2K testing challenge","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Summary of dynamically discovering likely program invariants","M. D. Ernst","Lab. for Comput. Sci., MIT, Cambridge, MA, USA","Proceedings IEEE International Conference on Software Maintenance. ICSM 2001","","2001","","","540","544","The dissertation dynamically discovering likely program invariants introduces dynamic detection of program invariants, presents techniques for detecting such invariants from traces, assesses the techniques' efficacy, and points the way for future research. Invariants are valuable in many aspects of program development, including design, coding, verification, testing, optimization, and maintenance. They also enhance programmers' understanding of data structures, algorithms, and program operation. Unfortunately, explicit invariants are usually absent from programs, depriving programmers and automated tools of their benefits. The dissertation shows how invariants can be dynamically detected from program traces that capture variable values at program points of interest. The user runs the target program over a test suite to create the traces, and an invariant detector determines which properties and relationships hold over both explicit variables and other expressions. Properties that hold over the traces and also satisfy other tests, such as being statistically justified, not being over unrelated variables, and not being implied by other reported invariants, are reported as likely invariants. Like other dynamic techniques such as testing, the quality of the output depends in part on the comprehensiveness of the test suite. If the test suite is inadequate, then the output indicates how, permitting its improvement. Dynamic analysis complements static techniques, which can be made sound but for which certain program constructs remain beyond the state of the art. Experiments demonstrate a number of positive qualities of dynamic invariant detection and of a prototype implementation, Daikon. Invariant detection is accurate-it rediscovers formal specifications-and useful-it assists programmers in programming tasks. It runs quickly and produces output of modest size. Test suites found in practice tend to be adequate for dynamic invariant detection.","1063-6773","0-7695-1189","10.1109/ICSM.2001.972767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972767","","Testing;Instruments;Programming profession;Detectors;Data structures;Formal specifications;Laboratories;Computer science;Design optimization;Prototypes","software maintenance;program verification;program testing;data structures","dynamically discovering likely program invariants;program invariants;software coding;software verification;software testing;software maintenance;data structures;Daikon","","6","9","","","","","","IEEE","IEEE Conferences"
"Energy awareness through software optimisation as a performance estimate case study of the MC68HC908GP32 microcontroller","J. Oliver; O. Mocanu; C. Ferrer","Univ. Autonoma de Barcelona, Bellaterra, Spain; Univ. Autonoma de Barcelona, Bellaterra, Spain; Univ. Autonoma de Barcelona, Bellaterra, Spain","Proceedings. 4th International Workshop on Microprocessor Test and Verification - Common Challenges and Solutions","","2003","","","111","116","We treat the topic of the energy consumption for a typical controller involved in smart sensor applications. Since in practice a common situation involves using off-the-shelf processors, in our case a Motorola HC908 family microcontroller, we concentrate on techniques leading to an optimisation at software level for low power requirements. The aim we envisage is to lower the energy consumption by means of due instruction selection and reordering, cycle and branch optimisation, and memory use such that without changing the original task a program performs, the energy consumed while executing it should decrease.","","0-7695-2045","10.1109/MTV.2003.1250271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250271","","Software performance;Computer aided software engineering;Microcontrollers;Intelligent sensors;Energy consumption;Application software;Embedded software;Sensor phenomena and characterization;Sensor systems;Hardware","microcontrollers;low-power electronics;circuit optimisation;intelligent sensors;power consumption;optimisation","energy consumption;smart sensor application;off-the-shelf processor;Motorola HC908;branch optimisation;software optimisation;performance estimation;low power requirements;MC68HC908GP32 microcontroller;energy awareness","","4","11","","","","","","IEEE","IEEE Conferences"
"Optimal software release time incorporating fault correction","S. S. Gokhale","Dept. of Comput. Sci. & Eng., Connecticut Univ., Storrs, CT, USA","28th Annual NASA Goddard Software Engineering Workshop, 2003. Proceedings.","","2003","","","175","184","The ""stopping rule"" problem which involves determining an optimal release time for a software application at which costs justify the stop test decision has been addressed by several researchers. However, most of these research efforts assume instantaneous fault correction, an assumption that underlies many software reliability growth models, and hence provide optimistic predictions of both the cost at release and the release time. In this paper, we present an economic cost model which takes into consideration explicit fault correction in order to provide realistic predictions of release time and release cost. We also present a methodology to compute the failure rate of the software in the presence of fault correction, which is necessary in order to apply the cost model. We illustrate the utility of the cost model to provide realistic predictions of release time and cost with a case study.","","0-7695-2064","10.1109/SEW.2003.1270741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270741","","Cost function;Software reliability;Software testing;Predictive models;Application software;Software debugging;Computer science;Economic forecasting;Programming;Delay","software reliability;program testing","optimal software release time;fault correction;stopping rule problem;stop test decision;software reliability;economic cost model","","6","33","","","","","","IEEE","IEEE Conferences"
"IVI instrument driver guided tour","P. Franklin; J. Ryland","Keithley Instrum. Inc., Cleveland, OH, USA; Keithley Instrum. Inc., Cleveland, OH, USA","Proceedings AUTOTESTCON 2004.","","2004","","","167","173","Test engineers are constantly challenged to reduce the costs and time associated with developing new test systems, while they strive to improve throughput, test coverage, and quality. Test software design and development often represents a significant portion of overall test system development time and cost. Therefore, any component, tool, or technique that minimizes the test system development effort required has value in meeting the challenge. Many instrument manufacturers and T&M software vendors now provide IVI instrument drivers designed to help test engineers incorporate their equipment and software into test systems. IVI drivers are the latest industry effort to provide an improved interface between test software and test instruments. Building on SCPI and VXIPnP drivers, IVI drivers offer features and functions with real potential to address test system development challenges. However, IVI drivers aren't appropriate for every situation; realizing the full potential of the improvements incorporated in them requires a good understanding of driver architecture, features, and functions. This paper provides a guided tour of IVI instrument drivers, designed to help test engineers understand how to obtain the maximum benefits from using them. First, the different styles of IVI drivers are described. The basic architecture of drivers is discussed, as well as guidelines for installing, configuring, and using them. Techniques for optimizing performance, evaluating tradeoffs, and troubleshooting are described. The tour concludes with a discussion of more advanced topics, including simulation, multithreading, and extending and modifying driver functionality.","1088-7725","0-7803-8449","10.1109/AUTEST.2004.1436819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436819","","Instruments;System testing;Software testing;Costs;Design engineering;Throughput;Software design;Manufacturing;Software systems;Computer industry","automatic test software;automatic test equipment;multi-threading;distributed object management;object-oriented programming;software architecture;formal specification;virtual instrumentation","test software design;test software development;IVI instrument driver;test instrument;troubleshooting;simulation;multithreading;IVI architecture specification;distributed object management;virtual instrumentation","","1","","","","","","","IEEE","IEEE Conferences"
"An automatic and optimized test generation technique applying to TCP/IP protocol","C. Besse; A. Cavalli; D. Lee","Inst. Nat. des Telecommun., Evry, France; NA; NA","14th IEEE International Conference on Automated Software Engineering","","1999","","","73","80","In this paper an automatic and optimized technique for test generation for communication protocol control and data portion is described, the goal is to minimize the number of tests with a guaranteed coverage. The test generation algorithm is applied to the client layer part of the TCP/IP protocol. The protocol used for the experiment is TCP-Reno, which is specified in the SDL language and is one of the commonly referenced implementations. For such a sophisticated protocol, the algorithm efficiently constructs 22 tests that cover all the required portions of the protocol.","","0-7695-0415","10.1109/ASE.1999.802094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=802094","","Automatic testing;TCPIP;Protocols;Circuit testing;System testing;Circuit faults;Automata;Communication system control;Specification languages;Switching circuits","transport protocols;program testing;automatic test software;formal specification","automatic test generation technique;optimized test generation technique;TCP/IP protocol;communication protocol control;guaranteed coverage;client layer part;TCP-Reno;SDL language;algorithm;communication protocol data portion","","5","16","","","","","","IEEE","IEEE Conferences"
"Testing nondeterminate systems","T. Menzies; B. Cukic; H. Singh; J. Powell","NASA/WVU Software Res. lab., Fairmont, VA, USA; NA; NA; NA","Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000","","2000","","","222","231","The behavior of nondeterminate systems can be hard to predict, since similar inputs at different times can generate different outputs. In other words, the behavior seen during the testing process may not be seen at runtime. Due to the uncertainties associated with nondeterminism, the standard view is that we should avoid such nondeterminate systems, especially for systems requiring high reliability. While this is a valid guideline, at least in two application areas such nondeterminacy is unavoidable. Early life-cycle requirements and AI software are becoming widely used, yet both are imprecise and may exhibit nondeterminate behaviour if explored rigorously by a test device. Based on a literature review and some theoretical studies, we argue that many stable properties exist within the space of all possible nondeterminate behaviors. However, we also show that seemingly trivial changes to a nondeterministic system can turn an easily testable system into an impossibly hard system to test. Finally, we stress that this analysis does not imply a correlation between stable zones of nondeterminate testability and the ultimate maintainability of nondeterminate systems. That is, while we are optimistic about testing nondeterminate systems, we remain cautious about the maintenance of such systems.","1071-9458","0-7695-0807","10.1109/ISSRE.2000.885874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885874","","System testing;Artificial intelligence;Uncertainty;Software reliability;Statistics;Runtime;Guidelines;Stress;Maintenance;Modems","program testing;artificial intelligence;software maintenance;stability;software reliability","nondeterminate systems testing;uncertainties;reliability;life-cycle requirements;AI software;literature review;stable properties;nondeterminate behaviors;nondeterminate testability;stable zones;system maintainability;software maintenance","","6","22","","","","","","IEEE","IEEE Conferences"
"Improvement of a configuration management system","F. Titze","CAD-UL AG, Ulm, Germany","Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium","","2000","","","618","625","The company CAD-UL AG develops software tools for embedded systems. Single tools such as compilers, linkers and debuggers are offered as well as complete development tool chains for the software development process. In contrast to application software for personal computers, embedded systems require very specialized software of highly optimized and exhaustively tested code. Since the previously existing configuration management was not efficient in comparison to the state-of-the-art in software engineering, an improvement was implemented by the introduction of a modern configuration management (CM) system (Cederqvist, 1998). In the paper, CAD-UL intends to show the results and the experiences of the European Systems and Software Initiative Process Improvement Experiment (ESSI-PIE) ICMS with the new configuration management system.","0270-5257","1-58113-206","10.1145/337180.337488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870455","","Embedded system;Engineering management;Software tools;Programming;Application software;Microcomputers;Embedded software;Software testing;System testing;Software engineering","configuration management;software tools;embedded systems;research initiatives;software development management","configuration management system;CAD-UL AG;software tools;embedded systems;program compilers;program linkers;program debuggers;software development;European Systems and Software Initiative;Process Improvement Experiment","","1","7","","","","","","IEEE","IEEE Conferences"
"Optimized reasoning-based diagnosis for non-random, board-level, production defects","C. O'Farrill; M. Moakil-Chbany; B. Eklow","Jabil Circuit, Inc., USA; NA; NA","IEEE International Conference on Test, 2005.","","2005","","","7 pp.","179","The ""back-end"" costs associated with debug of functional test failures can be one of the highest cost adders in the manufacturing process. As boards become more dense and more complex, debug of functional failures will become more and more difficult. Test strategies try to detect and diagnose failures early on in the test process (component and structural tests), but inevitably some defects are not detected until functional testing is done on the board. Finding these defects usually requires an ""expert"", with engineering level skills in both hardware and software. Depending on the complexity of the product, it could take several months (even years) to develop this level of expertise. During the initial product ramp, this expertise is usually most needed and often unavailable. Debug time is usually very long and scrap rates are generally high. This paper will provide an overview of reasoning-based diagnosis techniques and how they can significantly decrease debug time, especially during new product introduction. Because these engines are ""model-based"", there is no guarantee how they will perform in real life. In almost all cases, the reasoning engine will have to be modified based on instances where the reasoning engine could not correctly identify the failing component. Making these adjustments to the reasoning is a very complex and sometimes risky endeavor. While the new model may correctly identify the previously missed failure, the reasoning may have been altered to a point where several other diagnoses have now been unknowingly compromised. This paper will propose enhancements to the reasoning engine that will allow a simpler approach to adapting to diagnostic escapes without risking compromises to the original diagnostic engine","1089-3539;2378-2250","0-7803-9038","10.1109/TEST.2005.1583974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1583974","","Engines;Inference mechanisms;Cost function;Circuit testing;System testing;Hardware;Production systems;Adders;Manufacturing processes;Bayesian methods","automatic test pattern generation;case-based reasoning;fault simulation;printed circuit testing","reasoning diagnosis;production defects;backend costs;functional test failures debugging;manufacturing process;failure detection;failure diagnosis;test process;component tests;structural tests;functional testing;debug time;reasoning engine;failing component","","22","7","","","","","","IEEE","IEEE Conferences"
"An empirical investigation of simulated annealing applied to property-oriented testing","O. Abdellatif-Kaddour; P. Thevenod-Fosse; H. Waeselynck","CNRS, Toulouse, France; CNRS, Toulouse, France; CNRS, Toulouse, France","ACS/IEEE International Conference on Computer Systems and Applications, 2003. Book of Abstracts.","","2003","","","92","","Summary form only given. Property-oriented testing uses the specification of a property to drive the testing process. Our aim is to validate a program with respect to a target property, that is to exercise the program and observe whether the property's violated or not. We define a test strategy for safety properties in cyclic control systems. It consists of the stepwise construction of test scenarios. Each step explores possible continuations of the dangerous scenarios found at the previous step, using black-box sampling techniques. Here, emphasis is put on using a heuristic search technique - namely, simulated annealing - an automatic way to ample over the input pace. Empirical investigation is conducted on a steam boiler case study. The target property is the ""non explosion"" of the boiler in presence of faults in the physical devices. The experimental results lead us to propose a revised version of the basic simulated annealing algorithm, whose efficiency is promising.","","0-7803-7983","10.1109/AICCSA.2003.1227524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227524","","Simulated annealing;Software testing;System testing;Safety;Automatic control;Control systems;Sampling methods;Boilers;Explosions;Software algorithms","simulated annealing;formal specification;program testing;boilers;program verification;control systems;sampling methods;heuristic programming","simulated annealing empirical investigation;property-oriented testing;property specification;cyclic control system;black-box sampling technique;heuristic search technique;steam boiler case study;software testing;high-level safety property;test data generation;optimization technique;target property","","","","","","","","","IEEE","IEEE Conferences"
"SALT-an integrated environment to automate generation of function tests for APIs","A. Paradkar","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000","","2000","","","304","316","Automation of test design during the function testing phase is essential both to reduce the substantial cost of testing and to improve the delivered software reliability. We argue for a model based approach specifically designed from a test perspective to automate test design. We describe features of the Specification and Abstraction Language for Testing (SALT) environment which embodies this perspective. SALT allows testers to capture relationships among partitions of input and output variables for a function under test. The tester can also specify (potential) updates to context which result from the function invocation. This context enables the generation of sequences of function invocations with expected outputs. These test specifications along with a fault model, allow generation of an optimized set of test variations. We describe an example to illustrate SALT usage and report results of our pilot study using SALT.","1071-9458","0-7695-0807","10.1109/ISSRE.2000.885881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885881","","Automatic testing;Software testing;Design automation;Ambient intelligence;Cost function;Life testing;Redundancy;Character generation;Data analysis;Target tracking","application program interfaces;formal specification;program testing;specification languages;automatic programming","SALT;integrated environment;automated function test generation;APIs;test design;function testing phase;software reliability;model based approach;test perspective;automated test design;Specification and Abstraction Language for Testing;function invocation;expected outputs;test specifications;fault model;optimized test variations;application program interfaces","","3","14","","","","","","IEEE","IEEE Conferences"
"Supervision of real-time software systems using optimistic path prediction and rollbacks","D. A. Simser; R. E. Seviora","Bell Canada Software Reliability Lab., Waterloo Univ., Ont., Canada; NA","Proceedings of ISSRE '96: 7th International Symposium on Software Reliability Engineering","","1996","","","340","349","Real time supervision is a technique for automatically detecting and reporting failures in the external behaviour of real time software systems. Failure detection is achieved by monitoring the target system's external inputs and outputs, in a 'black box' manner and comparing its behaviour with the formally specified behaviour of the system. The paper presents the Optimistic Path Prediction and Rollbacks (OPPR) approach to real time supervision. In this technique, the supervisor predicts a single likely behaviour of the target system and, if the observed behaviour does not match the prediction, rolls back and creates a new prediction of the legal behaviour. A failure is detected when the supervisor has explored all valid behaviours without matching the observed behaviour. The paper opens by introducing the field of real time supervision and examining existing techniques. The core of the paper presents the basic algorithm of the OPPR method, with an example to illustrate its operation. The paper closes by describing an evaluation system, summarizing the experimental results and examining the performance of the OPPR scheme.","1071-9658","0-8186-7707","10.1109/ISSRE.1996.558892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558892","","Real time systems;Software systems;Software reliability;Software testing;Costs;Reliability engineering;Software engineering;Instruments;Automatic testing;Laboratories","software performance evaluation","real time software system supervision;optimistic path prediction and rollbacks;real time supervision;failure detection;failure reporting;external behaviour;formally specified behaviour;single likely behaviour;legal behaviour;OPPR method","","3","16","","","","","","IEEE","IEEE Conferences"
"A software process scheduling simulator","F. Padberg","Fak. fur Inf., Karlsruhe Univ., Germany","25th International Conference on Software Engineering, 2003. Proceedings.","","2003","","","816","817","To cut development cost and meet tight deadlines in short staffed software projects, managers must optimize the project schedule. Scheduling a software project is extremely difficult, though, because the time needed to complete a software development activity is hard to estimate. Often, the completion of a task is delayed because of unanticipated rework caused by feedback between activities in the process.","0270-5257","0-7695-1877","10.1109/ICSE.2003.1201301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201301","","Project management;Feedback;Cost function;Software development management;Programming;Delay;Testing;Software engineering;Software measurement;Software design","software development management;project management;discrete event simulation;scheduling","software process scheduling simulator;software project;process simulation;discrete-time simulator;probabilistic scheduling model","","2","9","","","","","","IEEE","IEEE Conferences"
"Software optimization of the MPEG-audio decoder using a 32-bit MCU RISC processor","Keun-Sup Lee; Young Cheol Park; Dae Hee Youn","Dept. of Electr. & Electron. Eng., Yonsei Univ., Seoul, South Korea; Dept. of Electr. & Electron. Eng., Yonsei Univ., Seoul, South Korea; Dept. of Electr. & Electron. Eng., Yonsei Univ., Seoul, South Korea","2002 Digest of Technical Papers. International Conference on Consumer Electronics (IEEE Cat. No.02CH37300)","","2002","","","330","331","This paper proposes optimization techniques to implement MPEG audio decoding algorithms in real-time using a general-purpose 32-bit MCU RISC processor. We implemented an MP3/AAC decoder using an ARM-based RISC MPU and its test board. Both the designed decoders with the proposed optimizations could achieve decoding processes in real-time within an operating frequency of 35 MHz.","","0-7803-7300","10.1109/ICCE.2002.1014052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1014052","","Decoding;Reduced instruction set computing;Digital audio players;Partitioning algorithms;Design optimization;Testing;Frequency;Digital signal processing;Clocks;Filter bank","optimisation;audio coding;data compression;code standards;decoding;real-time systems;reduced instruction set computing;microprocessor chips","software optimization;MPEG audio decoder;audio decoding;real time algorithm;general-purpose RISC processor;MCU RISC processor;microcontroller unit;advanced audio coding;MP3/AAC decoder;ARM-based MPU;test board;32 bit;35 MHz","","","5","","","","","","IEEE","IEEE Conferences"
"Smart debugging software architectural design in SDL","W. E. Wong; T. Sugeta; Yu Qi; J. C. Maldonado","Dept. of Comput. Sci., Texas Univ., Richardson, TX, USA; Dept. of Comput. Sci., Texas Univ., Richardson, TX, USA; Dept. of Comput. Sci., Texas Univ., Richardson, TX, USA; NA","Proceedings 27th Annual International Computer Software and Applications Conference. COMPAC 2003","","2003","","","41","47","Statistical data show that it is much less expensive to correct software bugs at the early design stage rather than the late stage of the development process when the final system has already been implemented and integrated together. The use of slicing and execution histories as an aid in software debugging is well established for programming languages like C and C++; however, it is rarely applied in the field of software design specification. We propose a solution by applying the source code level technologies to debugging software designs represented in a high-level specification and description language such as SDL. More specifically, we extend execution slice-based heuristics from source code-based debugging to the software design specification level. Suspicious locations in an SDL specification are prioritized by their likelihood of containing faults. Locations with a higher priority should be examined first rather than those with a lower priority as the former are more likely to contain the faults. A debugging tool, SmartD/sub DSL/, with user-friendly interfaces was developed to support our method. An illustration is provided to demonstrate the feasibility of using our method to effectively debug an architectural design.","0730-3157","0-7695-2020","10.1109/CMPSAC.2003.1245320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245320","","Software debugging;Software design;Testing;Computer bugs;Computer science;Delay;History;Computer languages;Fault detection;Programming profession","software architecture;software process improvement;formal specification;specification languages;software tools;software fault tolerance;program debugging","smart debugging;software architecture;statistical data;software bugs;development process;program slicing;execution histories;software debugging;programming languages;C;C++;software design specification;software designs;high-level specification;description language;execution slice-based heuristics;source code-based debugging;design specification level;SDL specification;containing faults;debugging tool;SmartD/sub DSL/;user-friendly interfaces;architectural design;program execution slicing;software fault detection;SmartDSDL","","1","20","","","","","","IEEE","IEEE Conferences"
"Automatic optimization of wrapper parallel interface constructions applied to digital cores","M. Balaz; E. Gramatova; M. Fischerova","Inst. of Informatics, Slovak Acad. of Sci., Bratislava, Slovakia; Inst. of Informatics, Slovak Acad. of Sci., Bratislava, Slovakia; Inst. of Informatics, Slovak Acad. of Sci., Bratislava, Slovakia","The IEEE Region 8 EUROCON 2003. Computer as a Tool.","","2003","2","","44","47 vol.2","The paper deals with optimization techniques for parallel interface of a test wrapper for internal and external testing of an embedded core. The developed techniques have been implemented in a JAVA applet. This implementation allows not only to find the most optimal constructions of parallel scan lines but also to apply the whole test wrapper construction to a real core modeled by VHDL. The JAVA applet is accessible on the Internet.","","0-7803-7763","10.1109/EURCON.2003.1248143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1248143","","Java;Circuit testing;Design optimization;Internet;System testing;System-on-a-chip;Registers;Object oriented modeling;Informatics;Minimization","software architecture;logic testing;system-on-chip;embedded systems;Java;hardware description languages","automatic optimization;wrapper parallel interface;binoids;digital cores;optimization technique;internal core testing;external core testing;JAVA applet;parallel scan lines;test wrapper construction;VHDL;system on chip;test wrapper architecture","","1","9","","","","","","IEEE","IEEE Conferences"
"Breaking an application specific instruction-set processor: the first step towards embedded software testing","J. T. M. H. Dielissen; B. L. Otero Mathijssen; J. A. Huisken","NA; NA; NA","Seventh IEEE International High-Level Design Validation and Test Workshop, 2002.","","2002","","","89","92","In this paper methods to stop an Application Specific Instruction set Processor (ASIP) are proposed. Constructing the stop criteria for an ASIP on a combination of program counter - and data values is expensive,. and therefore a novel solution, in which the micro program is extended, is investigated. The cost of this extension is limited due to the relative small program size, and optimisations are proposed for even further reduction. Due to the flexible setup of the tool that generates the ASIP both the analysis data of the needed debug hardware and the generation of this hardware can be automated.","","0-7803-7655","10.1109/HLDVT.2002.1224434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1224434","","Application software;Embedded software;Software testing;Application specific processors;Counting circuits;Debugging;Registers;Costs;Hardware;Monitoring","high level synthesis;computer debugging;computer architecture;program debugging","Application Specific Instruction set Processor;ASIP;program counter;debug hardware;stop criterion;complex stop criteria;register file;breakpointing;VHDL","","","5","","","","","","IEEE","IEEE Conferences"
"A variable reordering method for fast optimization of binary decision diagrams","Moon-Bae Song; Hoon Chang","Dept. of Comput. Sci., Soongsil Univ., Seoul, South Korea; NA","Proceedings Sixth Asian Test Symposium (ATS'97)","","1997","","","228","233","In this paper, a new variable ordering algorithm, distributed reordering algorithm that allows more faster solution than existing ones, is presented. Since this method can accomplish fast optimization of BDD in less memory and computation time, the proposed algorithm is more efficient for dynamic variable ordering. Also, the proposed algorithm can achieve more optimized results in combining with other variable ordering method.","1081-7735","0-8186-8209","10.1109/ATS.1997.643963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643963","","Optimization methods;Data structures;Boolean functions;Binary decision diagrams;Design automation;Combinational circuits;Application software;Logic design;Logic testing;Computer science","circuit optimisation","variable reordering;optimization;binary decision diagrams;distributed reordering algorithm;computation time;dynamic variable ordering;window permutation","","1","14","","","","","","IEEE","IEEE Conferences"
"PSCAN'96: new software for simulation and optimization of complex RSFQ circuits","S. Polonsky; P. Shevchenko; A. Kirichenko; D. Zinoviev; A. Rylyakov","Dept. of Phys., State Univ. of New York, Stony Brook, NY, USA; NA; NA; NA; NA","IEEE Transactions on Applied Superconductivity","","1997","7","2","2685","2689","The first version of PSCAN program (Personal Superconductor Circuit ANalyzer) was introduced in 1991. The program is a general purpose superconductor circuit simulator with an emphasis on the design of Rapid Single-Flux-Quantum (RSFQ) circuits. In the intervening years a number of new features were gradually added to the program. In particular, verification of the correct circuit behavior was enhanced using a special hierarchical Single-Flux-Quantum Hardware Description Language (SFQHDL). Next, a fast heuristic algorithm for margin optimization was introduced, which increased the number of parameters that can be simultaneously optimized in reasonable CPU times. Finally, recently we improved the numerical algorithm used for the simulation by using sparse symmetric positive definite matrices (instead of general structure band matrices as before). As a result, simulation speed has increased almost tenfold. Now it takes about 30 seconds of a CPU time on HP716/100 workstation to run a 2000 ps simulation of a 120-Josephson-junction circuit, and about a week to optimize all parameters of a two hundred Josephson junction circuit. We have merged all these improvements in a new version of our simulator, PSCAN'96.","1051-8223;1558-2515;2378-7074","","10.1109/77.621792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=621792","","Circuit simulation;Josephson junctions;Circuit analysis;Circuit testing;Hardware;Circuit synthesis;Analytical models;Central Processing Unit;Sparse matrices;Symmetric matrices","software packages;circuit analysis computing;circuit optimisation;superconducting integrated circuits;hardware description languages;sparse matrices;transient analysis;digital integrated circuits","PSCAN'96;simulation software;optimization software;complex RSFQ circuits;superconductor circuit simulator;rapid single-flux-quantum circuits;verification;SFQHDL;hierarchical HDL;fast heuristic algorithm;margin optimization;numerical algorithm;sparse symmetric positive definite matrices;simulation speed improvement;Josephson-junction circuit","","33","9","","","","","","IEEE","IEEE Journals & Magazines"
"GAMMATELLA: visualization of program-execution data for deployed software","A. Orso; J. A. Jones; M. J. Harrold; J. Stasko","Coll. of Comput., Georgia Inst. of Technol., USA; Coll. of Comput., Georgia Inst. of Technol., USA; Coll. of Comput., Georgia Inst. of Technol., USA; Coll. of Comput., Georgia Inst. of Technol., USA","Proceedings. 26th International Conference on Software Engineering","","2004","","","699","700","To investigate the program-execution data efficiently, we must be able to view the data at different levels of detail. In our visualization approach, we represent software systems at three different levels: statement level, file level, and system level. At the statement level, we represent the actual code. The representation at the file level provides a miniaturized view of the source code similar to the one used in the SeeSoft system (Eick et al., 1992). The system level uses treemaps (Shneiderman, 1992 and Bruls et al., 2000) to represent the software and is the most abstracted level in our visualization. At each level, coloring is used to represent one- or two-dimensional information about the code, using the colors' hue and brightness components. The coloring technique that we apply is a generalization of the coloring technique defined for fault-localization by Jones and colleagues (2001). GAMMATELLA is a toolset that implements our visualization approach and provides capabilities for instrumenting the code, collecting program-execution data from the field, and storing and retrieving the data locally. GAMMATELLA is written in Java, supports the monitoring of Java programs, and consists of three main components: an instrumentation, execution, and coverage tool, a data collection daemon, and a program visualizer.","0270-5257","0-7695-2163","10.1109/ICSE.2004.1317495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317495","","Data visualization;Software safety;Instruments;Monitoring;Data analysis;Educational institutions;Quality assurance;Testing;Performance analysis;Optimization","program visualisation;data visualisation;program testing;Java;system monitoring","GAMMATELLA;data visualization;program-execution data;software systems;statement level;file level;system level;source code;SeeSoft system;treemaps;coloring technique;fault-localization;data retrieval;Java programs;coverage tool;data collection daemon;program visualizer","","4","8","","","","","","IEEE","IEEE Conferences"
"Flexible software protection using hardware/software codesign techniques","J. Zambreno; A. Choudhary; R. Simha; B. Narahari","Dept. of Electr. & Comput. Eng., Northwestern Univ., Evanston, IL, USA; Dept. of Electr. & Comput. Eng., Northwestern Univ., Evanston, IL, USA; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","636","641 Vol.1","A strong level of trust in the software running on an embedded processor is a prerequisite for its widespread deployment in any high-risk system. The expanding field of software protection attempts to address the key steps used by hackers in attacking a software system. In this paper we present an efficient and tunable approach to some problems in embedded software protection that utilizes a hardware/software codesign methodology. By coupling our protective compiler techniques with reconfigurable hardware support, we allow for a greater flexibility of placement on the security-performance spectrum than previously proposed mainly-hardware or software approaches. Results show that for most of our benchmarks, the average performance penalty of our approach is less than 20%, and that this number can be greatly improved upon with the proper utilization of compiler and architectural optimizations.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268916","","Software protection;Hardware;Field programmable gate arrays;Embedded software;Cryptography;Computer hacking;Software systems;Embedded computing;Computer science;Optimizing compilers","hardware-software codesign;integrated circuit design;reconfigurable architectures;embedded systems","hardware-software codesign;embedded processor;software system;embedded software protection;protective compiler techniques;reconfigurable hardware support;security-performance spectrum;architectural optimizations;networking capabilities","","4","13","","","","","","IEEE","IEEE Conferences"
"Detecting equivalent mutants and the feasible path problem","A. J. Offutt; Jie Pan","Dept. of Inf. Syst. & Syst. Eng., George Mason Univ., Fairfax, VA, USA; NA","Proceedings of 11th Annual Conference on Computer Assurance. COMPASS '96","","1996","","","224","236","Mutation testing is a technique for testing software units that has great potential for improving the quality of testing, and thereby increasing our ability to assure the high reliability of critical software. The paper presents a technique that uses mathematical constraints to automatically detect equivalent mutant programs. The paper also describes how the approach is used for the feasible path problem. The paper describes how test criteria are formalized as mathematical constraint systems, how equivalent mutants are represented as infeasible constraints, and how infeasible constraints are detected. A proof of concept implementation has been developed to demonstrate this technique, and experimental results from using this tool are presented. Limitations of the system and the method are described, and proposals for improvements are made.","","0-7803-3390","10.1109/CMPASS.1996.507890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507890","","Genetic mutations;Software testing;Fault detection;System testing;Optimizing compilers;Postal services;Partial response channels;Proposals;Software quality;Automatic testing","program testing;safety-critical software;software quality","test criteria;feasible path problem;mutation testing;software unit testing;high reliability;critical software;mathematical constraints;equivalent mutant programs;mathematical constraint systems;infeasible constraints;proof of concept implementation","","32","21","","","","","","IEEE","IEEE Conferences"
"Experimental test and application of a 2-D finite element calculation for whispering gallery sapphire resonators","D. G. Santiago; R. T. Wang; G. J. Dick; R. A. Osegueda; J. H. Pierluissi; L. M. Gil; A. Revilla; G. J. Villalva","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA; NA; NA; NA; NA","Proceedings of IEEE 48th Annual Symposium on Frequency Control","","1994","","","482","485","This paper discusses the demonstrated accuracy and utility of a 2-D finite element methodology for whispering gallery mode sapphire resonators. The mode solutions obtained by the software compared with experimental results for a wheel-shaped sapphire resonator give an error in mode frequency of less than 0.55%. We also show parts per million agreement with analytical solutions for simple geometries such as an empty coaxial resonator. The CYRES 2D FEM software package developed at The University of Texas at El Paso has proven invaluable for the analysis and identification of modes and mode families for resonators of various geometries. The software also shows promise as a tool for optimization of new resonator designs. Current uses include design of optimum sized dielectric resonators for minimized wall losses, and new resonator geometries for temperature compensated resonators. The operational characteristics of the software and the general methodology for use of the software as a laboratory and design tool are discussed.<<ETX>>","","0-7803-1945","10.1109/FREQ.1994.398293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=398293","","Testing;Finite element methods;Geometry;Software tools;Dielectric losses;Application software;Whispering gallery modes;Frequency;Coaxial components;Software packages","sapphire;crystal resonators;finite element analysis;optimisation;computer aided analysis;CAD;software packages","2D finite element calculation;whispering gallery sapphire resonators;wheel-shaped sapphire resonator;CYRES 2D FEM software package;University of Texas;El Paso;identification of modes;software;optimization;dielectric resonators;minimized wall losses;operational characteristics;Al/sub 2/O/sub 3/","","1","8","","","","","","IEEE","IEEE Conferences"
"New optical switches enable automated testing with true flexibility","M. Bitting","Polatis Ltd., McKinney, TX, USA","Proceedings AUTOTESTCON 2004.","","2004","","","361","366","The proliferation of fiber optic systems in military and avionics platforms is driven by the ever increasing need for higher data rates to support multi-sensor data fusion. Traditionally, the test systems to support these optical deployments are manual and inefficient. Increasingly fast optical components require optical test equipment that is very expensive. To make cost effective test suites, it is essential that these high value resources be used efficiently. This is most effectively accomplished through test architectures that are remotely controlled and automatically scheduled. These test architectures also enable a diverse set of testing applications to be simultaneously executed within an optical test lab or manufacturing environment. The advent of optical matrix switching technology with sub 1dB insertion loss performance and repeatability measured in milli dB's opens up new doors for highly efficient, remotely controlled, automated test systems. The ultra low loss aspects of these switches enable distributed test architectures that were previously unrealizable. Distributed test architectures create a test environment where expensive test equipment can be leveraged over a greater number of test samples in a more timely and automated fashion. This allows the lab manager to prioritize and schedule tests across many users, DUTs, and test equipment bays in an operation that can run 24/7. This paper explores the enabling photonic switch technology and a couple generic test architectures that can be applied in a variety of automated applications to increase test equipment usage and efficiency, thus lowering end costs for deployable fiber optic components and systems.","1088-7725","0-7803-8449","10.1109/AUTEST.2004.1436883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436883","","Optical switches;Automatic testing;Test equipment;System testing;Optical fibers;Optical devices;Costs;Automatic control;Job shop scheduling;Optical losses","photonic switching systems;automatic test equipment;automatic test software;optical engineering computing;optical fibre testing","optical switch;automated testing;optical test equipment;optical matrix switching technology;distributed test architecture;photonic switch technology","","1","","","","","","","IEEE","IEEE Conferences"
"Testability analysis for software components","T. B. Nguyen; M. Delaunay; C. Robach","LCIS-ESISAR, Valence, France; LCIS-ESISAR, Valence, France; LCIS-ESISAR, Valence, France","International Conference on Software Maintenance, 2002. Proceedings.","","2002","","","422","429","In this paper, we propose to use the static single assignment form, which was originally proposed for code optimization in compilation techniques, in order to transform software components into a data-flow representation. Thus, hardware testability concepts can be used to analyze the testability of components that are described by C or Ada programs. Such a testability analysis helps designers during the specification phases of their components and testers during the testing phases to evaluate and eventually to modify the design.","1063-6773","0-7695-1819","10.1109/ICSM.2002.1167799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167799","","Software testing;System testing;Observability;Controllability;Hardware;Data analysis;Software measurement;Software metrics;Process design;Real time systems","program testing;object-oriented programming;software metrics;data flow analysis;formal specification","testability analysis;software components;static single assignment form;code optimization;compilation techniques;data flow representation;hardware testability;Ada programs;C programs;specification phases;testing","","7","13","","","","","","IEEE","IEEE Conferences"
"An embedding debugging architecture for SOCs","R. Leatherman; N. Stollon","NA; NA","IEEE Potentials","","2005","24","1","12","16","Multiple cores embedded debugging architecture for system on chip design (SOC) is presented. It presents an asymmetrical functional test problem. To analyze the problem and optimize performance in multicore operation, debug tools with interfaces are exercised for several cores. HyperJTAG (joint test action group) interface reduces the IO pin interfaces required for debugging several cores. To overcome the wiring problem in hyperJTAG, wire routing and debugging synchronization is proposed. Hyper debug action nodes at each core initiate global or local control actions that synchronously reset the cores. To provide a virtual connection between the processor core in the SoC and its corresponding probe control, MED (multicore embedded debugging) software tool is proposed. This allows a contiguous analysis flow from the system level simulation models of SoC systems through FPGA and emulation prototyping and finally it debug the silicon hardware.","0278-6648;1558-1772","","10.1109/MP.2005.1405795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1405795","","Debugging;Testing;Multicore processing;System-on-a-chip;Performance analysis;Wiring;Wire;Routing;Probes;Software tools","system-on-chip;circuit optimisation;program debugging;computer debugging;software architecture;software tools;field programmable gate arrays;integrated circuit testing;synchronisation;graphical user interfaces;digital signal processing chips;embedded systems","multiple cores embedding debugging software tool architecture;SOC;system on chip design;asymmetrical functional test;optimization;multicore operation;debug tool;hyperJTAG;joint test action group interface;IO pin interface;wire routing;debugging synchronization;local control action;core reset;processor core;probe control;system level simulation model;FPGA;emulation prototyping;silicon hardware debug","","52","3","","","","","","IEEE","IEEE Journals & Magazines"
"Elevating interface device design in the test program set development process","R. Freeman; G. Shoopman","DME Corp., Orlando, FL, USA; NA","1999 IEEE AUTOTESTCON Proceedings (Cat. No.99CH36323)","","1999","","","253","258","This paper describes a Test Program Set (TPS) Interface Device development process which will improve signal performance, lower cost, and enhance maintainability. Hardware is designed as modular components and can be tested prior to incorporation into an interface device. Automated software tools ensure accurate Unit Under Test (UUT) data entry and optimize the use of modular hardware. Once established, this development process produces quick turnaround pre-tested interface devices with high electrical and mechanical integrity.","1080-7725","0-7803-5432","10.1109/AUTEST.1999.800387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=800387","","Production;Circuit testing;Hardware;Wiring;Standards development;Connectors;Libraries;Automatic testing;Electronic equipment testing;Interference","automatic test software;automatic test equipment;peripheral interfaces;software tools","TPS development process;interface device design;signal performance;lower cost;enhanced maintainability;modular components;automated software tools;accurate UUT data entry;modular hardware;quick turnaround pre-tested interface devices;streamlined interface design;crosstalk","","","2","","","","","","IEEE","IEEE Conferences"
"Extraction and simulation of realistic CMOS faults using inductive fault analysis","F. J. Ferguson; J. P. Shen","Comput. Eng., California Univ., Santa Cruz, CA, USA; NA","International Test Conference 1988 Proceeding@m_New Frontiers in Testing","","1988","","","475","484","FXT is a software tool which implements inductive fault analysis for CMOS circuits. It extracts a comprehensive list of circuit-level faults for any given CMOS circuit and ranks them according to their relative likelihood of occurrence. Five commercial CMOS circuits are analyzed using FXT. Of the extracted faults, approximately 50% can be modeled by single-line stuck-at 0/1 fault model. Faults extracted from two circuits are simulated with the switch-level fault simulator FMOSSIM. The test set provided by the circuits' manufacturer, which detects 100% of the single-line stuck-at 0/1 faults, detected between 73% and 89% of the simulated faults.<<ETX>>","1089-3539","0-8186-0870","10.1109/TEST.1988.207759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207759","","Circuit faults;Circuit simulation;Circuit analysis;Semiconductor device modeling;Circuit testing;Electrical fault detection;Fault detection;Software tools;Switching circuits;Virtual manufacturing","automatic testing;CMOS integrated circuits;electronic engineering computing;fault location;integrated circuit testing;software tools","stuck at faults;simulation;CMOS faults;inductive fault analysis;FXT;software tool","","178","30","","","","","","IEEE","IEEE Conferences"
"An approach to generate the thin-threads from the UML diagrams","Xiaoqing Bai; C. P. Lam; Huaizhong Li","Coll. of Electr. Eng., Guangxi Univ., Nanning, China; NA; NA","Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.","","2004","","","546","552 vol.1","Software testing plays a crucial role in assuring software quality. One of the most important issues in software testing research is the generation of the test cases. For scenario-based software testing, the thin-threads, which are the usage scenarios in a software system from the end user's point of view, are frequently used to generate test cases. However, the generation of the thin-threads is not an easy task. A scenario-based business model has to be manually derived or labor-intensive business analysis has to be manually carried out in order to extract the thin-threads from a software system. In this work, we propose an automated approach to directly generate thin-threads from the UML artifacts. The generated thin-threads can be used to generate and to prioritize the test cases for scenario-based software testing.","0730-3157","0-7695-2209","10.1109/CMPSAC.2004.1342893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342893","","Unified modeling language;Software testing;Software systems;System testing;Object oriented modeling;Information science;Software quality;Educational institutions;Computer industry;Software standards","Unified Modeling Language;program testing;object-oriented programming;diagrams;software quality","software quality;UML diagrams;test case generation;scenario-based software testing","","3","20","","","","","","IEEE","IEEE Conferences"
"Cost-Reliability Optimal Release Policies for Software Systems","S. Yamada; S. Osaki","Graduate School of Systems Science; Okayama University of Science; 1-1 Ridai-cho; Okayama 700 JAPAN.; Department of Industrial and Systems Engineering; Faculty of Engineering; Hiroshima University; Higashi-Hiroshima 724, JAPAN.","IEEE Transactions on Reliability","","1985","R-34","5","422","424","This paper extends an optimal software release problem to both cost and reliability requirements. The optimum software release time is determined both by minimizing a total average software cost and satisfying a software reliability requirement. The underlying model is software reliability growth described by a nonhomogeneous Poisson process.","0018-9529;1558-1721","","10.1109/TR.1985.5222222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5222222","Cost-reliability optimization;Optimum software release time;Software reliability","Software systems;Cost function;Software reliability;Software testing;Reliability theory;Reliability engineering;Life testing;Software standards;System testing;Computer errors","","","","99","3","","","","","","IEEE","IEEE Journals & Magazines"
"The importance of life cycle modeling to defect detection and prevention","J. H. van Moll; J. C. Jacobs; B. Freimut; J. J. M. Trienekens","Philips Semicond., Eindhoven, Netherlands; Philips Semicond., Eindhoven, Netherlands; NA; NA","10th International Workshop on Software Technology and Engineering Practice","","2002","","","144","155","In many low mature organizations dynamic testing is often the only defect detection method applied. Thus, defects are detected rather late in the development process. High rework and testing effort, typically under time pressure, lead to unpredictable delivery dates and uncertain product quality. This paper presents several methods for early defect detection and prevention that have been in existence for quite some time, although not all of them are common practice. However, to use these methods operationally and scale them to a particular project or environment, they have to be positioned appropriately in the life cycle, especially in complex projects. Modeling the development life cycle, that is the construction of a project-specific life cycle, is an indispensable first step to recognize possible defect injection points throughout the development project and to optimize the application of the available methods for defect detection and prevention. This paper discusses the importance of life cycle modeling for defect detection and prevention and presents a set of concrete, proven methods that can be used to optimize defect detection and prevention. In particular, software inspections, static code analysis, defect measurement and defect causal analysis are discussed. These methods allow early, low cost detection of defects, preventing them from propagating to later development stages and preventing the occurrence of similar defects in future projects.","","0-7695-1878","10.1109/STEP.2002.1267624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267624","","Testing;Optimization methods;Costs;Consumer electronics;Jacobian matrices;Application software;Concrete;Inspection;Particle measurements;Software measurement","program testing;software reliability;software quality;software performance evaluation;software development management;project management;program diagnostics","life cycle modeling;defect detection;defect prevention;organization dynamic testing;development process;product quality;complex project;development life cycle;project-specific life cycle;defect injection point;development project;optimization;software inspections;static code analysis;defect measurement;defect causal analysis;low cost detection;quality demand","","3","28","","","","","","IEEE","IEEE Conferences"
"Measurement based software design for DVB-T and T-DAB single frequency network planning and coverage prediction","M. M. Velez; P. Angueira; D. De la Vega; A. Arrinda; J. L. Ordiales","Eng. Sch., Univ. of the Basque Country, Bilbao, Spain; NA; NA; NA; NA","2000 5th International Symposium on Antennas, Propagation, and EM Theory. ISAPE 2000 (IEEE Cat. No.00EX417)","","2000","","","607","610","This paper suggests an architecture design for a single frequency network (SFN) planning software. These software tools will be very useful during the planning stages in order to optimize the network design and implementation process. The new digital broadcasting services DVB-T (digital terrestrial video broadcasting) and T-DAB (digital terrestrial audio broadcasting) have already been tested in many countries over the world and commercial emissions have begun in some European and Asian countries. The design described here includes an optimization module that allows an evaluation of the coverage estimation values obtained when some field measurements are available.","","0-7803-6377","10.1109/ISAPE.2000.894860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894860","","Software measurement;Software design;Digital video broadcasting;Digital audio broadcasting;Design optimization;Computer architecture;Frequency;Software tools;Process planning;Process design","digital video broadcasting;digital audio broadcasting;telecommunication computing;telecommunication network planning;software tools;software architecture","measurement based software design;DVB-T;T-DAB;network design optimization;single frequency network coverage prediction;architecture design;SFN;single frequency network planning software;digital broadcasting services;digital terrestrial video broadcasting;digital terrestrial audio broadcasting;Europe;Asia;optimization module;coverage estimation;field measurements;software architecture","","2","7","","","","","","IEEE","IEEE Conferences"
"Advanced software for planning and optimization of spectrum monitoring networks","O. E. Krutova; V. V. Kogan; A. P. Pavliouk","NIIR, Russia; NA; NA","IEEE 6th International Symposium on Electromagnetic Compatibility and Electromagnetic Ecology, 2005.","","2005","","","76","79","Short description of new software for planning and optimization of spectrum monitoring networks or groups of monitoring stations is presented. The software permits to calculate service areas provided by various monitoring functions, such as listening, emission parameters measurement and direction finding, as well as location accuracy templates under triangulation location operations, those are distributions of the location accuracy throughout an overall location service area.","","0-7803-9374","10.1109/EMCECO.2005.1513067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1513067","","Condition monitoring;Testing;Area measurement;Software measurement;Investments;Power generation economics;Directive antennas;Receiving antennas;Radio spectrum management;Software quality","radio spectrum management;telecommunication computing;monitoring;optimisation;telecommunication network planning","advanced software;network planning;optimization;spectrum monitoring network;monitoring station;triangulation location operation;location service area","","3","4","","","","","","IEEE","IEEE Conferences"
"New probabilistic measures for accelerating the automatic test pattern generation algorithm","B. Phillips; S. Ganesan; C. Bacon","Software Div. Tinker AFB, USA; Software Div. Tinker AFB, USA; NA","AUTOTESTCON 93","","1993","","","503","512","In order to approximate the signal controllabilities, the authors introduce new probabilistic measures called signal priorities, whose computation relies on the minimum-value distributions of fanout input variables of a digital circuit. The signal priorities serve the same purpose as do the signal controllabilities. That is, they are used to accelerate the automatic test pattern generation algorithm; however, their computation requires much less effort. This new method is formally defined and tested with several practical example circuits.<<ETX>>","","0-7803-0646","10.1109/AUTEST.1993.396314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396314","","Life estimation;Automatic test pattern generation;Circuit testing;Circuit faults;Automatic testing;Acceleration;Combinational circuits;Digital circuits;Electrical fault detection;Fault detection","automatic test software;probability;automatic programming;controllability;fault location;digital circuits;fault diagnosis;logic testing;signal processing","stuck-at faults;digital circuits;automatic test pattern generation algorithm;signal controllabilities;probabilistic measures;signal priorities;minimum-value distributions;fanout input variables;digital circuit","","","8","","","","","","IEEE","IEEE Conferences"
"Design optimization of a practical ESD protection circuit by CAD: a case study","A. Z. Wang; C. H. Tsay; J. Bielawski; L. DeClue","Dept. of Electr. & Comput. Eng., Illinois Inst. of Technol., Chicago, IL, USA; NA; NA; NA","Proceedings of the Thirteenth Biennial University/Government/Industry Microelectronics Symposium (Cat. No.99CH36301)","","1999","","","116","119","This paper reports design optimization of a practical ESD protection circuit by ESD simulation. A compact ESD protection structure was designed for a transceiver chip. Chip malfunction was observed in tests. ESD simulation and measurement measures showed the malfunction was caused by large substrate current induced early-triggering of the ESD devices. Solutions were provided to optimize the chip. It shows that ESD simulation is a powerful means in conducting practical ESD protection designs.","0749-6877","0-7803-5240","10.1109/UGIM.1999.782835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=782835","","Design optimization;Electrostatic discharge;Protection;Design automation;Computer aided software engineering;Testing;Circuit simulation;Pins;Transceivers;Semiconductor device measurement","electrostatic discharge;circuit optimisation;integrated circuit design;circuit CAD;transceivers;integrated circuit testing;protection;integrated circuit reliability;BiCMOS integrated circuits;mixed analogue-digital integrated circuits","design optimization;ESD protection circuit;CAD;case study;ESD simulation;transceiver chip;chip malfunction;tests;large substrate current;early-triggering;mixed-mode ESD simulation;submicron BiCMOS process","","","4","","","","","","IEEE","IEEE Conferences"
"Software testability measurements derived from data flow analysis","Pu-Lin Yeh; Jin-Cherng Lin","Dept. of Comput. Sci. & Eng., Tatung Inst. of Technol., Taipei, Taiwan; NA","Proceedings of the Second Euromicro Conference on Software Maintenance and Reengineering","","1998","","","96","102","The purpose of the research is to develop formulations to measure the testability of a program. Testability is a program's property which is introduced with the intention of predicting efforts required for testing the program. A program with a high degree of testability indicates that a selected testing criterion could be achieved with less effort and the existing faults can be revealed more easily during testing. We propose a new program normalization strategy that makes the measurement of testability more precise and reasonable. If the program testability metric derived from data flow analysis could be applied at the beginning of a software testing phase, much more effective testing of resource allocation and prioritizing is possible.","","0-8186-8421","10.1109/CSMR.1998.665760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665760","","Software testing;Software measurement;Data analysis;Software metrics;Computer science;Resource management;Electrical capacitance tomography;Size control","program testing;software metrics;resource allocation;data flow analysis","software testability measurements;data flow analysis;program testability;testing criterion;program normalization strategy;program testability metric;software testing phase;resource allocation","","4","19","","","","","","IEEE","IEEE Conferences"
"An efficient array reference analysis for data flow testing","I. Forgacs","Comput. & Autom. Inst., Hungarian Acad. of Sci., Budapest, Hungary","Proceedings of 1994 1st International Conference on Software Testing, Reliability and Quality Assurance (STRQA'94)","","1994","","","131","135","Though lots of data flow analysis techniques were proposed that handle array variables, they are restricted to the application of different compiler optimizations and give no precise result. We present a new method that determines definition-use (du) pairs more precisely and it also gives the program path of each du pair. Therefore, this is the first array analysis technique that is well-applicable for data flow testing.","","0-7803-2608","10.1109/STRQA.1994.526398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526398","","Data analysis;Optimizing compilers;Automatic testing;Data flow computing;Automation;Application software;Flow graphs;Optimization methods","data flow analysis;optimising compilers;program testing;data structures;program diagnostics","array reference analysis;data flow testing;data flow analysis;array variable handling;compiler optimizations;definition-use pairs;du pair","","1","14","","","","","","IEEE","IEEE Conferences"
"Validation of a methodology for assessing software reliability","M. Lil; Y. Wei; D. Desovski; H. Nejad; S. Ghose; B. Cukic; C. Smidts","Center for Reliability Eng., Maryland Univ., College Park, MD, USA; Center for Reliability Eng., Maryland Univ., College Park, MD, USA; NA; NA; NA; NA; NA","15th International Symposium on Software Reliability Engineering","","2004","","","66","76","Software-based digital systems are progressively replacing analog systems in safety-critical applications. However the ability to predict their reliability is not well understood and needs further study. A first step towards systematic resolution of this issue was presented in a recent software engineering measure study. In that study a set of software engineering measures were ranked with respect to their ability in predicting software reliability through an expert opinion elicitation process. This study also proposed a concept of reliability prediction system (RePS) to bridge the gap between software engineering measures and software reliability. The research presented in this paper validates the rankings obtained and the concept of RePS proposed in the previous study.","1071-9458","0-7695-2215","10.1109/ISSRE.2004.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383107","","Software reliability;Software measurement;Software engineering;Reliability engineering;Phase measurement;Application software;Object oriented modeling;Usability;Software testing;Educational institutions","software reliability;program verification;software metrics","software validation;software reliability;software-based digital system;safety-critical application;software engineering measure study;opinion elicitation process;reliability prediction system","","7","23","","","","","","IEEE","IEEE Conferences"
"Worst case tolerance analysis and CLP-based multifrequency test generation for analog circuits","A. Abderrahman; E. Cerny; B. Kaminska","Dept. de Genie Electr. et d'Inf., Ecole Polytech. de Montreal, Que., Canada; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1999","18","3","332","345","We present an algorithm for automatically generating minimal test sets for parametric faults in linear analog circuits. In a previous work we elaborated a multifrequency test generation method (TPG) for such circuit faults. The method was formulated as a series of optimization problems that were solved by sequential quadratic programming (SQP) available in MATLAB. Such a standard optimization method processes local information and, consequently, cannot guarantee that the found solution is global. This may lead to a poor test selection. Furthermore, the method is semiautomatic and depends on various parameters that must be selected by an experienced user. In this paper, we propose a method based on constraint logic programming (CLP) using relational interval arithmetic (RIA) to solve these optimization problems as a series of constraint satisfaction problems (CSPs). The method is fully automatic and provides tight and guaranteed bounds on the true range of a multivariable nonlinear function. The correctness of the bounds stems from the enumeration of subdivisions of the function and its variable domains while discarding those subdivisions containing no solution. The tightness (i.e., the closeness to the true range) of the bounds can be refined to any desired degree by increasing the fineness of subdivisions imposed on the variable domains and the stringency of the termination criterion at the cost of an increased CPU time. The TPG method was implemented in CLP (BNR) prolog. The effectiveness of our approach is illustrated on a number of nonlinear functions known to be difficult, and two realistic electronic circuits in the context of TPG. Our algorithm accelerated the computation of the various parameters related to the test of a biquadratic filter by a factor ranging from 11 to 29 as compared to the Monte Carlo method.","0278-0070;1937-4151","","10.1109/43.748163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=748163","","Tolerance analysis;Circuit testing;Circuit faults;Optimization methods;Automatic testing;Analog circuits;Quadratic programming;MATLAB;Logic programming;Arithmetic","analogue integrated circuits;linear network analysis;tolerance analysis;logic programming;constraint theory;integrated circuit testing;fault diagnosis;arithmetic;circuit analysis computing;automatic test software;optimisation","worst case tolerance analysis;CLP-based multifrequency test generation;linear analog circuits;minimal test sets;automatic test set generation;parametric faults;circuit faults;optimization problems;constraint logic programming;relational interval arithmetic;constraint satisfaction problems;multivariable nonlinear function;termination criterion;CPU time;CLP PROLOG;biquadratic filter","","16","28","","","","","","IEEE","IEEE Journals & Magazines"
"Towards implementing successful software inspections","T. Hall; D. Wilson; N. Baddoo","Hertfordshire Univ., Hatfield, UK; NA; NA","Proceedings International Conference on Software Methods and Tools. SMT 2000","","2000","","","127","136","The authors present their findings of using the Repertory Grid Technique with over two hundred software practitioners in thirteen UK software companies. They use their findings to establish the ways in which inspections can be optimised to be more effective and successful as software engineering tools. The findings suggest that, contrary to conventional wisdom, developers are very positive about using inspections that generate engineering benefit. Often inspections are implemented in such a way as to only benefit managers.","","0-7695-0903","10.1109/SWMT.2000.890428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=890428","","Inspection;Project management;Software engineering;Quality management;Computer industry;Australia;Software development management;Software testing;Software measurement","DP industry;software quality;inspection;software development management;personnel;professional aspects;human factors;software metrics","software inspections;Repertory Grid Technique;software practitioners;UK software companies;software engineering tools;engineering benefit","","2","7","","","","","","IEEE","IEEE Conferences"
"A hybrid heuristic method for global optimization","A. Georgieva; I. Jordanov","Dept. of Comput. Sci. & Software Eng., Portsmouth Univ., UK; Dept. of Comput. Sci. & Software Eng., Portsmouth Univ., UK","Fifth International Conference on Hybrid Intelligent Systems (HIS'05)","","2005","","","3 pp.","","In this paper a new stochastic hybrid technique for unconstrained global optimization (GO) is proposed. It is a combination of an iterative algorithm developed by us (called LP/sub /spl tau//O) that uses low-discrepancy sequences of points and heuristic knowledge to find regions of attraction when searching for a global minimum (GM) and the well-known Nelder-Mead simplex local search. The combination of the two techniques provides a powerful hybrid optimization tool that we call LP/sub /spl tau//SS. The proposed LP/sub /spl tau//SS method is tested on a number of multimodal mathematical functions and results are discussed and compared with such from other stochastic methods.","","0-7695-2457","10.1109/ICHIS.2005.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587797","","Optimization methods;Stochastic processes;Hypercubes;Computer science;Software engineering;Iterative algorithms;Search methods;Benchmark testing;Genetic algorithms;Hybrid power systems","optimisation;search problems;stochastic processes","hybrid heuristic method;global optimization;stochastic hybrid technique;iterative algorithm;LP/sub /spl tau//O;low-discrepancy sequences;global minimum;Nelder-Mead simplex local search;hybrid optimization tool;LP/sub /spl tau//SS method;multimodal mathematical functions","","","7","","","","","","IEEE","IEEE Conferences"
"Power analysis of embedded software: a first step towards software power minimization","V. Tiwari; S. Malik; A. Wolfe","Dept. of Electr. Eng., Princeton Univ., NJ, USA; Dept. of Electr. Eng., Princeton Univ., NJ, USA; Dept. of Electr. Eng., Princeton Univ., NJ, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","1994","2","4","437","445","Embedded computer systems are characterized by the presence of a dedicated processor and the software that runs on it. Power constraints are increasingly becoming the critical component of the design specification of these systems. At present, however, power analysis tools can only be applied at the lower levels of the design-the circuit or gate level. It is either impractical or impossible to use the lower level tools to estimate the power cost of the software component of the system. This paper describes the first systematic attempt to model this power cost. A power analysis technique is developed that has been applied to two commercial microprocessors-Intel 486DX2 and Fujitsu SPARClite 934. This technique can be employed to evaluate the power cost of embedded software. This can help in verifying if a design meets its specified power constraints. Further, it can also be used to search the design space in software power optimization. Examples with power reduction of up to 40%, obtained by rewriting code using the information provided by the instruction level power model, illustrate the potential of this idea.<<ETX>>","1063-8210;1557-9999","","10.1109/92.335012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=335012","","Embedded software;Costs;Embedded computing;Circuits;Application software;Logic;Minimization;Power system modeling;Embedded system;Silicon","real-time systems;software cost estimation;software performance evaluation;computer testing;integrated circuit testing","embedded software;software power minimization;dedicated processor;power constraints;power analysis tools;power analysis technique;microprocessors;Intel 486DX2;Fujitsu SPARClite 934;design space;power reduction;instruction level power model","","485","11","","","","","","IEEE","IEEE Journals & Magazines"
"A design tool for large scale fault-tolerant software systems","C. L. Blackmon; Meng-Lai Yin","California State Polytech Univ., Pomona, CA, USA; California State Polytech Univ., Pomona, CA, USA","Annual Symposium Reliability and Maintainability, 2004 - RAMS","","2004","","","256","260","In order to assist software designers in the application of fault-tolerance techniques to large scale software systems, a computer-aided software design tool has been proposed and implemented that assess the criticality of the software modules contained in the system. This information assists designers in identifying weaknesses in large systems that can lead to system failures. Through analysis and modeling techniques based in graph theory, modules are assessed and rated as to the criticality of their position in the software system. Graphical representation at two levels facilitates the use of cut set analysis, which is our main focus. While the task of finding all cut sets in any graph is NP-complete, the tool intelligently applies cut set analysis by limiting the problem to provide only the information needed for meaningful analysis. In this paper, we examine the methodology and algorithms used in the implementation of this tool and consider future refinements. Although further testing is needed to assess performance on increasingly complex systems, preliminary results look promising. Given the growing demand for reliable software and the complexities involved in the design of these systems, further research in this area is indicated.","","0-7803-8215","10.1109/RAMS.2004.1285457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285457","","Large-scale systems;Fault tolerant systems;Software systems;Fault tolerance;Software design;Software tools;Application software;Software safety;Redundancy;Information analysis","computational complexity;optimisation;graph theory;software fault tolerance;computer aided software engineering","large scale fault-tolerant software systems;computer-aided software design tool;graph theory;NP-complete","","2","7","","","","","","IEEE","IEEE Conferences"
"A demand-driven analyzer for data flow testing at the integration level","E. Duesterwald; R. Gupta; M. L. Soffa","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; Dept. of Comput. Sci., Pittsburgh Univ., PA, USA","Proceedings of IEEE 18th International Conference on Software Engineering","","1996","","","575","584","Data-flow testing relies on static analysis for computing the definition-use pairs that serve as the test case requirements for a program. When testing large programs, the individual procedures are first tested in isolation during unit testing. Integration testing is performed to specifically test the procedure interfaces. The procedures in a program are integrated and tested in several steps. Since each integration step requires data-flow analysis to determine the new test requirements, the accumulated cost of repeatedly analyzing a program can contribute considerably to the overhead of testing. Data-flow analysis is typically computed using an exhaustive approach or by using incremental data-flow updates. This paper presents a new and more efficient approach to data-flow integration testing that is based on demand-driven analysis. We developed and implemented a demand-driven analyzer and experimentally compared its performance during integration testing with the performance of (i) a traditional exhaustive analyzer, and (ii) an incremental analyzer. Our experiments show that demand-driven analysis is faster than exhaustive analysis by up to a factor of 25. The demand-driven analyzer also outperforms the incremental analyzer in 80% of the test programs by up to a factor of 5.","0270-5257","0-8186-7247","10.1109/ICSE.1996.493451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493451","","Data analysis;Data flow computing;Application software;Performance analysis;Software testing;Cost benefit analysis;Computer science;Performance evaluation;Optimizing compilers;Software engineering","data flow analysis;program testing","demand-driven analyzer;data flow testing;program procedure interfaces;static analysis;definition-use pairs;test case requirements;large program testing;overhead;unit testing;integration testing;exhaustive analyzer;incremental data-flow updates;performance;incremental analyzer","","1","21","","","","","","IEEE","IEEE Conferences"
"Logic optimization and equivalence checking by implication analysis","W. Kunz; D. Stoffel; P. R. Menon","Univ. of Potsdam, Germany; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1997","16","3","266","281","This paper proposes a new approach to multilevel logic optimization based on automatic test pattern generation (ATPG). It shows that an ordinary test generator for single stuck-at faults can be used to perform arbitrary transformations in a combinational circuit and discusses how this approach relates to conventional multilevel minimization techniques based on Boolean division. Furthermore, effective heuristics are presented to decide what network manipulations are promising for minimizing the circuit. By identifying indirect implications between signals in the circuit, transformations can be derived which are ""good"" candidates for the minimization of the circuit. A main advantage of the proposed approach is that it operates directly on the structural netlist description of the circuit so that the technical consequences of the performed transformations can be evaluated in an easy way, permitting better control of the optimization process with respect to the specific goals of the designer. Therefore, the presented technique can serve as a basis for optimization techniques targeting nonconventional design goals. This paper only considers area minimization, and our experimental results show that the method presented is competitive with conventional technology-independent minimization techniques. For many benchmark circuits, our tool, the Hannover implication tool, based on learning (HANNIBAL) achieves the best minimization results published to date. Furthermore, the optimization approach presented is shown to be useful in formal verification.","0278-0070;1937-4151","","10.1109/43.594832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=594832","","Automatic test pattern generation;Circuit testing;Performance evaluation;Design optimization;Minimization methods;Automatic logic units;Logic testing;Circuit faults;Combinational circuits;Signal processing","multivalued logic;circuit optimisation;minimisation of switching nets;logic CAD;formal verification;automatic test software;combinational circuits;logic testing;learning (artificial intelligence);integrated logic circuits;circuit CAD;integrated circuit design;integrated circuit testing","multilevel logic optimization;equivalence checking;implication analysis;automatic test pattern generation;ATPG;combinational circuit;multilevel minimization techniques;Boolean division;structural netlist description;area minimization;Hannover implication tool;HANNIBAL;recursive learning;formal verification","","30","38","","","","","","IEEE","IEEE Journals & Magazines"
"Annual Reliability and Maintainability Symposium. 2003 Proceedings (Cat. No.03CH37415)","","","Annual Reliability and Maintainability Symposium, 2003.","","2003","","","","","The following topics are dealt with: reliability; maintainability; software reliability; product design; product assurance; repairable systems modeling; accelerated life testing; safety; risk management; probabilistic risk assessment; reliability prediction; life cycle costing; reliability centered maintenance; reliability and maintainability tools application; Bayesian methods; aging; modeling; optimisation; fault tree analysis; equipment maintenance optimisation; reliability statistical methods; power systems; quality management/six sigma; and lessons learned.","0149-144X","0-7803-7717","10.1109/RAMS.2003.1181754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181754","","Software reliability;Product development;Life estimation;Safety;Risk analysis;Maintenance;Bayes procedures;Aging;Optimization methods;Fault trees;Reliability","software reliability;product development;life testing;safety;risk management;maintenance engineering;life cycle costing;Bayes methods;ageing;optimisation;fault trees;quality management;reliability","reliability;maintainability;software reliability;product design;product assurance;repairable systems modeling;accelerated life testing;safety;risk management;probabilistic risk assessment;reliability prediction;life cycle costing;reliability centered maintenance;reliability and maintainability tools application;Bayesian methods;aging;modeling;optimisation;fault tree analysis;equipment maintenance optimisation;reliability statistical methods;power systems;quality management;six sigma;lessons learned","","","","","","","","","IEEE","IEEE Conferences"
"AICTO: an improved algorithm for planning inter-class test order","Chengying Mao; Yansheng Lu","Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., China; Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., China","The Fifth International Conference on Computer and Information Technology (CIT'05)","","2005","","","927","931","Determining inter-class test order is one of the most difficult tasks in cluster level testing. This paper proposes the concept of weighted object relation diagram (WORD) by extending the traditional object relation diagram (ORD). When generating the inter-class test order based on WORD, minimizing the number of realistic stubs (not the number of classes to be stubbed) is regarded as optimization objective. Some heuristic rules, such as association intensity, cycling weight and direction factors of edges, are used as important criteria to cut association edges to break cycles. An improved algorithm AICTO is presented via analyzing some principles of the existing methods and overcoming some of their drawbacks. This algorithm has some merits such as the fewer number of classes to be stubbed, holding the association edges with higher RD and excellent stability. In practice, the algorithm AICTO has been applied to our C/C++ program testing prototype CppTest and produces favorable effects.","","0-7695-2432","10.1109/CIT.2005.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1562776","","Clustering algorithms;Software testing;Fault detection;Educational institutions;Computer science;Algorithm design and analysis;Stability;Prototypes;Information technology;Sorting","program testing;object-oriented programming;software prototyping","inter-class test order;cluster level testing;weighted object relation diagram;optimization objective;heuristic rules;association intensity;C;C++;program testing prototype;CppTest","","2","11","","","","","","IEEE","IEEE Conferences"
"Optimal scheduling and software pipelining of repetitive signal flow graphs with delay line optimization","F. Depuydt; W. Geurts; G. Goossens; H. De Man","IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; NA","Proceedings of European Design and Test Conference EDAC-ETC-EUROASIC","","1994","","","490","494","Software pipelining can have an enormous impact on the clock cycle count and hence on the performance of a real-time signal processing design. Because it pays off to invest CPU time in the optimal software pipelining of time-critical parts of a design, an integer programming approach is proposed for simultaneous scheduling and software pipelining. The integer programming techniques in the literature do not support cyclic (repetitive) signal flow, graphs, and/or do not allow optimization of the storage cost of delay lines during software pipelining. The new contributions in this paper are the full integration of software pipelining and scheduling, based on a new timing model that supports cyclic signal flow, graphs and optimization of delay line storage costs. Experiments with several real-time signal processing applications have shown the practical applicability of the approach.<<ETX>>","","0-8186-5410","10.1109/EDTC.1994.326831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326831","","Optimal scheduling;Pipeline processing;Signal processing;Linear programming;Cost function;Delay lines;Software performance;Clocks;Process design;Signal design","integer programming;pipeline processing;scheduling;signal processing;real-time systems;delay lines;logic CAD;graph theory","optimal scheduling;software pipelining;repetitive signal flow graphs;delay line optimization;clock cycle count;real-time signal processing design;integer programming;timing model;CAD","","1","12","","","","","","IEEE","IEEE Conferences"
"Preliminary results of an NSF-sponsored software engineering pedagogical laboratory","S. H. Rubin; R. Y. Lee","Dept. of Comput. Sci., Central Michigan Univ., Mount Pleasant, MI, USA; NA","FIE '98. 28th Annual Frontiers in Education Conference. Moving from 'Teacher-Centered' to 'Learner-Centered' Education. Conference Proceedings (Cat. No.98CH36214)","","1998","1","","43","44 vol.1","Five experiments were proposed to the US National Science Foundation: functional decomposition; optimization random-basis testing; software retrieval for reuse; and expert language translation. The purpose of these experiments was to make the study of software engineering cohesive, interesting and, hopefully, fun for the students. They were to learn software engineering principles by doing rather than by memorizing definitions from a course text. Preliminary results evidence that while such learning constitutes effective job training and serves as a basis for further research, it needs to be carefully crafted with regard to student preparation.","0190-5848","0-7803-4762","10.1109/FIE.1998.736799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=736799","","Laboratories;Object oriented programming;Natural languages;Programming profession;Software testing;Functional programming;Contracts;Computer science;Writing;Pattern analysis","computer science education;software engineering;student experiments;educational courses","software engineering pedagogical laboratory;National Science Foundation;functional decomposition;optimization random-basis testing;USA;software retrieval;software retrieval for reuse;expert language translation;students;computer science","","","5","","","","","","IEEE","IEEE Conferences"
"A framework for the management of MCM test strategies","C. Dislis; A. F. Alani; I. P. Jalowiecki","Dept. of Cybern., Reading Univ., UK; NA; NA","Proceedings 1997 International Conference on Multichip Modules","","1997","","","272","277","This paper will present a framework for the management and optimisation of MCM test strategies, and describe a software tool developed for the purpose, based on economics models. MCMs can suffer from low system yield, and the final cost and quality are highly sensitive to the choice of test strategy. The framework presented is an end-to end cost and quality model of the MCM manufacture and test process from die procurement and KGD strategy to final test and rework, and provides a powerful means of test strategy evaluation and optimisation tailored to the project and the organisation. The paper presents an outline of the tool, together with the outcome of a case study evaluated using this framework.","","0-7803-3787","10.1109/ICMCM.1997.581188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581188","","Software development management;Software testing;Software tools;Power generation economics;System testing;Cost function;Power system modeling;Manufacturing processes;Virtual manufacturing;Procurement","multichip modules;integrated circuit testing;production testing;economics;integrated circuit manufacture;quality control","MCM test strategies;software tool;economics models;system yield;cost;quality;die procurement;KGD strategy;rework;strategy evaluation","","1","6","","","","","","IEEE","IEEE Conferences"
"Reducing the cost of package Test","S. R. Shakeri","Adv. Interconnect Technol. Inc., Pleasanton, CA, USA","IEEE/CPMT/SEMI 28th International Electronics Manufacturing Technology Symposium, 2003. IEMT 2003.","","2003","","","225","229","It is no secret that semiconductor manufacturers in today's marketplace face intense pressure to reduce cost while improving quality. However, package test remains an area that is often overlooked in cost-saving initiatives. In this session, you will learn about the elements that make up the total cost of packaging test and discover new ways to reduce costs, such as design for testability. The session will also describe how to optimize use of automatic test equipment (ATE) and determine the most effective test site strategy.","1089-8190","0-7803-7933","10.1109/IEMT.2003.1225905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1225905","","Costs;Circuit testing;Throughput;Semiconductor device manufacture;Automatic testing;Semiconductor device testing;Assembly;Semiconductor device packaging;Profitability;Software testing","cost reduction;semiconductor device manufacture;automatic test equipment;design for testability;integrated circuit testing","cost reduction;package test;semiconductor manufacturers;marketplace face intense pressure;test site strategy;automatic test equipment;ATE;design for testability;cost saving initiatives","","","","","","","","","IEEE","IEEE Conferences"
"Web service group testing with windowing mechanisms","Wei-Tek Tsai; Xiaoying Bai; Yinong Chen; Xinyu Zhou","Comput. Sci. & Eng. Dept., Arizona State Univ., USA; NA; NA; NA","IEEE International Workshop on Service-Oriented System Engineering (SOSE'05)","","2005","","","213","218","ASTRAR provides a framework for testing Web services (WS) using the group testing technique. This paper extends the basic two-phase testing process and introduces the windowing mechanism to further improve testing efficiency. Rather than testing a large number of WS simultaneously, WS are divided into subsets called windows and testing is exercised window by window. Testing results are analyzed for different strategies such as using all of the historical data, using the most recent windows, and using the current window only. Based on the results, test cases are ranked according to their potency to detect faults; and oracles and the confidence level of each oracle are established for individual test cases at runtime. In addition, different strategies are proposed to determine the optimal window size at runtime. By incorporating the windowing mechanism, the two-phase training and volume testing process becomes a continuous learning process and the basic group testing process becomes more adaptive to dynamically changing environment.","","0-7695-2438","10.1109/SOSE.2005.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1551151","Web Services;group testing;verification;ranking","Web services;Service oriented architecture;Runtime;Automatic testing;System testing;Computer science;Fault detection;Software systems;Collaboration;Sun","Internet;program testing","Web service group testing;windowing mechanism;two-phase testing process;historical data;test case ranking;fault detection;oracle confidence level;optimal window size;volume testing process;continuous learning process","","7","12","","","","","","IEEE","IEEE Conferences"
"Next generation test generator (NGTG) for analog circuits","L. Venetsky; S. Singer","Aircraft Div., Naval Air Warfare Center, Lakehurst, NJ, USA; NA","1997 IEEE Autotestcon Proceedings AUTOTESTCON '97. IEEE Systems Readiness Technology Conference. Systems Readiness Supporting Global Needs and Awareness in the 21st Century","","1997","","","113","120","This paper describes a system that automatically generates tests for an analog Unit Under Test (UUT) in learning mode, and then deploys the system for fault detection and isolation in the production mode. The NGTG consists of the following main components: a) minimized input test pattern generator, b) UUT simulator, and last but not least c) evaluation system. The NGTG is a process that utilizes Fuzzy Artmap neural network for fault diagnostics and detection and genetic algorithm for test generation and fault coverage optimization.","","0-7803-4162","10.1109/AUTEST.1997.633580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633580","","Circuit testing;Analog circuits;Automatic testing;System testing;Circuit faults;Electrical fault detection;Production systems;Test pattern generators;Circuit simulation;Fuzzy neural networks","analogue circuits;genetic algorithms;learning (artificial intelligence);fault location;automatic testing;fuzzy logic;electronic equipment testing;software engineering;automatic test equipment;production testing","analog circuits;Unit Under Test;Learning Mode;fault detection;fault isolation;minimized input test pattern generator;UUT simulator;evaluation;Fuzzy Artmap neural network;fault diagnostics;genetic algorithm;test generation;fault coverage optimization;NGTG;gyroscope;servo controller","","2","13","","","","","","IEEE","IEEE Conferences"
"Theory and algorithms for the generation and validation of speculative loop optimizations","Ying Hu; C. Barrett; B. Goldberg","Dept. of Comput. Sci., New York Univ., NY, USA; Dept. of Comput. Sci., New York Univ., NY, USA; Dept. of Comput. Sci., New York Univ., NY, USA","Proceedings of the Second International Conference on Software Engineering and Formal Methods, 2004. SEFM 2004.","","2004","","","281","289","Translation validation is a technique that verifies the results of every run of a translator such as a compiler, instead of the translator itself. Previous papers by the authors and others have described translation validation for compilers that perform loop optimizations (such as interchange, tiling, fusion, etc), using a proof rule that treats loop optimizations as permutations. In this paper we describe an improved permutation proof rule which considers the initial conditions and invariant conditions of the loop. This new proof rule not only improves the validation process for compile-time optimizations, it can also be used to ensure the correctness of speculative loop optimizations, the aggressive optimizations which are only correct under certain conditions that cannot be known at compile time. Based on the new permutation rule, with the help of an automatic theorem prover CVC Lite, an algorithm is proposed for validating loop optimizations. The same permutation proof rule can also be used (within a compiler for example) to generate the runtime tests necessary to support speculative optimizations.","","0-7695-2222","10.1109/SEFM.2004.1347532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347532","","Optimizing compilers;Runtime;Testing;TV;Program processors;Optimization methods;Computer science;Law;Legal factors;Software engineering","optimising compilers;parallel programming;theorem proving;program control structures;formal verification;optimisation","speculative loop optimization generation;speculative loop optimization validation;translation validation;compile-time optimizations;speculative loop optimization correctness;aggressive optimizations;automatic theorem prover;CVC Lite;permutation proof rule;compiler validation;formal method","","","18","","","","","","IEEE","IEEE Conferences"
"A new high-voltage integrated switch: the ""thyristor dual"" function","J. -. Sanchez; M. Breil; P. Austin; J. -. Laur; J. Jalade; B. Rousset; H. Foch","Lab. d'Autom. et d'Anal. des Syst., CNRS, Toulouse, France; NA; NA; NA; NA; NA; NA","11th International Symposium on Power Semiconductor Devices and ICs. ISPSD'99 Proceedings (Cat. No.99CH36312)","","1999","","","157","160","In this paper, a new monolithic integrated device providing the ""thyristor dual"" function without auxiliary supply and based on the functional integration mode is investigated. The influence of the physical and technological parameters of this new structure upon the main electrical characteristics and the physical behaviour has been analyzed using the ATLAS software tool. An optimized device is proposed and test structures have been fabricated.","1063-6854","0-7803-5290","10.1109/ISPSD.1999.764086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=764086","","Switches;Thyristors;MOSFET circuits;Zero voltage switching;Semiconductor diodes;Voltage control;Electric variables;Software tools;Testing;Application software","MOS-controlled thyristors;power semiconductor switches;software tools;semiconductor device models;optimisation;semiconductor device testing","high-voltage integrated switch;thyristor dual function switch;monolithic integrated device;auxiliary supply;functional integration mode;physical parameters;technological parameters;electrical characteristics;physical behaviour;ATLAS software tool;optimized device;test structures","","5","6","","","","","","IEEE","IEEE Conferences"
"Optimal transfer trees and distinguishing trees for testing observable nondeterministic finite-state machines","Fan Zhang; To-yat Cheung","Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China; NA","IEEE Transactions on Software Engineering","","2003","29","1","1","14","The fault-state detection approach for blackbox testing consists of two phases. The first is to bring the system under test (SUT) from its initial state to a targeted state t and the second is to check various specified properties of the SUT at t. This paper investigates the first phase for testing systems specified as observable nondeterministic finite-state machines with probabilistic and weighted transitions. This phase involves two steps. The first step transfers the SUT to some state t' and the second step identifies whether t' is indeed the targeted state t or not. State transfer is achieved by moving the SUT along one of the paths of a transfer tree (TT) and state identification is realized by using diagnosis trees (DT). A theoretical foundation for the existence and characterization of TT and DT with minimum weighted height or minimum average weight is presented. Algorithms for their computation are proposed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166585","","System testing;Software testing;Fault detection;Software systems;Computer Society;Design methodology;Automata;Nonhomogeneous media","finite state machines;program testing;optimisation;trees (mathematics)","optimal transfer trees;distinguishing trees;observable nondeterministic finite-state machine testing;fault-state detection;blackbox testing;probabilistic transitions;weighted transitions;TT;DT;diagnosis trees","","20","24","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing IV &amp; V benefits using simulation","D. M. Raffo; W. Wakeland","Dept. of Comput. Sci., Portland State Univ., OR, USA; NA","28th Annual NASA Goddard Software Engineering Workshop, 2003. Proceedings.","","2003","","","97","101","There is a critical need for cost effective independent verification and validation (IV & V). The goal of this research is to create a flexible tool that NASA IV & V can use to quantitatively assess the economic benefit of performing IV & V on NASA software development projects and to optimize that benefit across alternative IV & V plans. The tool is based on extensive research into software process simulation models (SPSMs) conducted at the Software Engineering Institute (SEI) by Watts Humphrey and Marc Kellner (1989), and Bill Curtis and others (1992). SPSMs can be used to quantify the costs and benefits associated with NASA IV & V practices enabling management to effectively allocate scarce resources for IV & V activities. In addition, SPSMs facilitate the IV & V of NASA software development processes by enabling checks and performance assessments.","","0-7695-2064","10.1109/SEW.2003.1270731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270731","","NASA;Modeling;Costs;Resource management;Software engineering;Programming;Investments;Economic forecasting;Space technology;Software systems","program verification;program testing;software engineering;software performance evaluation;aerospace computing","independent verification;NASA software development;software process simulation models;software engineering;independent validation","","3","20","","","","","","IEEE","IEEE Conferences"
"Design and implementation of the ""G2"" PowerPC/sup TM/ 603e-embedded microprocessor core","C. Hunter; J. Gaither","Motorola Inc., Austin, TX, USA; NA","Proceedings International Test Conference 1998 (IEEE Cat. No.98CH36270)","","1998","","","473","479","New and emerging demands in the embedded market has precipitated in the development of the ""G2"" PowerPC 603e microprocessor into a reusable hard core. The demands and requirements of an embedded core microprocessor require significant changes in the physical design, verification and test strategies. Reuse and standardization in design data and test data present new challenges for an existing family of microprocessors.","1089-3539","0-7803-5093","10.1109/TEST.1998.743188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=743188","","Microprocessors;Testing;Design optimization;Circuits;Routing;Power generation;Clocks;Time to market;Logic devices;Wire","microprocessor chips;embedded systems;reduced instruction set computing;hardware-software codesign;automatic test pattern generation;boundary scan testing;design for testability;integrated circuit testing;formal verification","G2 PowerPC 603e microprocessor;embedded microprocessor core;design;implementation;reusable hard core;test strategies;verification strategies;data standardization;data reuse;HyperMOS3;superscalar processor;JTAG port;boundary scan circuitry;ATPG model;core integration;power bussing;pattern data format;debug processor interface","","1","6","","","","","","IEEE","IEEE Conferences"
"Automatic BIST design tool for mixed-signal circuits","Chujen Lin; L. Haynes; P. Mandava; P. Prasad","Intelligent Autom. Inc., Rockville, MD, USA; NA; NA; NA","1998 IEEE AUTOTESTCON Proceedings. IEEE Systems Readiness Technology Conference. Test Technology for the 21st Century (Cat. No.98CH36179)","","1998","","","97","102","This paper describes the development of a software tool for automating the design process of built-in self-test (BIST) circuits for mixed-signal circuit cards. The purpose of this tool is to generate a complete design of microcontroller-based BIST circuits including hardware and firmware. A typical BIST system generated by this tool consists of a microcontroller, A/D, D/A converters, digital I/Os, digital/analog multiplexers, and a programmable logic device. Several heuristic-based algorithms were developed in this work to optimize the configuration of multiplexers so that the required number of input and output ports is minimized, and the size and number of multiplexers are also reduced. The design tool reported here can greatly simplify the time-consuming procedure of designing BIST circuits and optimize the BIST circuits for mixed-signal circuit cards. A demonstration board is currently under development to evaluate the usability of this tool. The paper focuses on the overall theory and application of the tool, and also describes how if interfaces to the larger tool set we have developed. This larger tool set includes tools to help the designer define the level of built-in test required to meet the system maintainability goals.","1088-7725","0-7803-4420","10.1109/AUTEST.1998.713427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713427","","Built-in self-test;Circuits;Multiplexing;Software tools;Process design;Hardware;Microprogramming;Microcontrollers;Analog-digital conversion;Programmable logic devices","mixed analogue-digital integrated circuits;integrated circuit testing;built-in self test;automatic testing;circuit CAD;design for testability;software tools","mixed-signal circuits;automatic BIST design tool;microcontroller;D/A converters;A/D converters;digital I/O;digital/analog multiplexers;programmable logic device;heuristic-based algorithms;BIST circuits;mixed-signal circuit cards;built-in test;maintainability;MIDIS ABIT","","2","2","","","","","","IEEE","IEEE Conferences"
"Study and application of genetic algorithm in computer test construction","Hanjun Jin; Xiaorong Wang; Yanlin Wang; Yaokun Zhang","Coll. of Hydropower & Inf. Eng., Huazhong Univ. of Sci. & Technol., Wuhan, China; NA; NA; NA","IEEE International Symposium on Communications and Information Technology, 2005. ISCIT 2005.","","2005","1","","424","427","Computer test construction is the basis of computer-based testing. To solve constraint optimization problems on the research of modern test theory and computer-based testing, a novel genetic algorithm to generating test paper, adopting a new decimal system of subsection code, is proposed in this paper. The algorithm provides a set of schemes of making papers of different degree of difficulties display in normal distribution. The result indicates that the algorithm has good performance and practicability.","","0-7803-9538","10.1109/ISCIT.2005.1566884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566884","","Application software;Genetic algorithms;Gaussian distribution;System testing;Constraint optimization;Test pattern generators;Delay;Educational institutions;Hydroelectric power generation;Computer science","genetic algorithms;testing;education","genetic algorithm;computer test construction;computer-based testing;constraint optimization;decimal system;subsection code","","1","5","","","","","","IEEE","IEEE Conferences"
"Supporting product line development","R. Balzer","Inf. Sci. Inst., Marina del Rey, CA, USA","Proceedings 10th International Software Process Workshop","","1996","","","29","31","The author considers how the maturation of the software development field has been inextricably tied to its adoption of a product line orientation. Only by specializing in a particular application domain can software development organizations develop the expertise required for that domain, develop cost effective products, and establish a market niche. He argues that software development processes are fundamentally flawed because maintenance, including enhancement and evolution, is performed on source code. This source code has been hand optimized by programmers and that optimization has spread information and built up implicit dependencies among the parts.","","0-8186-7725","10.1109/ISPW.1996.654362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=654362","","Automatic programming;Computer bugs;Application software;Testing;Debugging;Specification languages;Decision support systems;Refining;Engineering management;Feedback","software engineering","software product line development support;software development organizations;cost effective products;market niche;software development processes;software maintenance;source code;optimization","","","","","","","","","IEEE","IEEE Conferences"
"A test generation algorithm for systems modelled as non-deterministic FSMs","H. AboElFotoh; O. Abou-Rabia; H. Ural","Dept. of Comput. Sci., American Univ., Cairo, Egypt; NA; NA","Software Engineering Journal","","1993","8","4","184","188","A variety of systems can be modelled as finite-state machines (FSM). Several formal methods have been proposed for testing the conformance of an implementation to a given FSM-based specification. However, all of these methods assume that the specification is modelled by a deterministic FSM. The paper presents an efficient algorithm that generates a set of adaptive all state-pair optimised distinguishing sequences for testing systems modelled as non-deterministic FSMs.<<ETX>>","0268-6961","","10.1049/sej.1993.0024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=225553","","Conformance testing;Finite state machines;Software requirements and specifications","conformance testing;finite state machines;formal specification","nondeterministic FSMs;test generation algorithm;finite-state machines;formal methods;conformance;adaptive all state-pair optimised distinguishing sequences","","6","","","","","","","IET","IET Journals & Magazines"
"Using I/sub 2/O and I/O processors in embedded PCI systems. New software standard and processor type speed embedded PCI implementations","R. Robinson","PLX Technol. Inc., Sunnyvale, CA, USA","Wescon/98. Conference Proceedings (Cat. No.98CH36265)","","1998","","","204","209","As the processing power and I/O speeds increase in embedded systems, the implementation of software protocols for communicating within these systems and making efficient use of available bandwidth has become critical. The I/sub 2/O specification, software components and associated hardware, both I/sub 2/O-ready PCI-local bus bridges and I/O processors, solve this problem in an efficient manner, and its wide acceptance in the industry ensures that it is a good architectural choice for embedded designs.","1095-791X","0-7803-5078","10.1109/WESCON.1998.716449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=716449","","Protocols;Embedded software;Software testing;Software standards;Backplanes;Design optimization;Bandwidth;Computer architecture;Control systems;Software performance","system buses;embedded systems;protocols;message passing;reduced instruction set computing;software standards","I/sub 2/O processors;I/O processors;embedded PCI systems;software standard;I/O speeds;software protocols;bandwidth;local bus bridges;architectural choice","","1","","","","","","","IEEE","IEEE Conferences"
"Software assurance for security","G. McGraw","Reliable Software Technol., USA","Computer","","1999","32","4","103","105","The article discusses an approach to security analysis that we have applied successfully over the past several years (to 1999) at Reliable Software Technologies. Our approach is no magic bullet, but it offers a reasoned methodology that has proven to be useful in the trenches. Our methodology, like many useful things, is a mix of art and engineering. The idea is straightforward: design a system with security in mind, analyze the system in light of known and anticipated risks, rank the risks according to their severity, test to the risks, and cycle broken systems back through the design process. The process outlined above has one essential underlying goal: avoiding the unfortunately pervasive penetrate-and-patch approach to computer security-that is, avoiding the problem of desperately trying to come up with a fix to a problem that is being actively exploited by attackers. In simple economic terms, finding and removing bugs in a software system before its release is orders of magnitude cheaper and more effective than trying to fix systems after release.","0018-9162;1558-0814","","10.1109/2.755011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755011","","System testing;Resource management;Solids;Risk analysis;Data security;Software engineering;Protection;Information security;Power system security;Silver","software reliability;software quality;security of data;risk management","software assurance;security analysis;anticipated risks;broken systems;design process;penetrate-and-patch approach;economic terms;software system","","6","","","","","","","IEEE","IEEE Journals & Magazines"
"Data collection and descriptive analysis: a first step for developing quality software","A. M. Shagnea; K. J. Hayhurst; B. E. Withers","Res. Triangle Inst., Research Triangle Park, NC, USA; Res. Triangle Inst., Research Triangle Park, NC, USA; Res. Triangle Inst., Research Triangle Park, NC, USA","COMPASS '91, Proceedings of the Sixth Annual Conference on Computer Assurance","","1991","","","173","179","Software measurement activities can be partitioned into the descriptive, decision support and method effectiveness assessment phases, which parallel the defined, managed, and optimized levels of the US Department of Defense Software Engineering Institute's (SEI's) software maturity framework. The authors describe the descriptive phase of software measurement and give an example of the measurement activities involved in the descriptive phase of the Guidance and Control Software (GCS) project, which is a specific flight-critical software project. The descriptive phase of measurement consists of data collection and descriptive analysis. This analysis can provide insight into the relationship between specific development and test techniques and is a first step toward achieving an optimized software development process.<<ETX>>","","0-7803-0126","10.1109/CMPASS.1991.161057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=161057","","Software quality;Software measurement;Phase measurement;Programming;Feedback;Software safety;NASA;Engineering management;Postal services;Software engineering","aerospace computer control;quality control;software metrics;software reliability","quality software;decision support;method effectiveness assessment phases;optimized levels;software maturity framework;descriptive phase;software measurement;flight-critical software project;data collection;descriptive analysis;optimized software development process","","1","9","","","","","","IEEE","IEEE Conferences"
"START: System Testability Analysis and Research Tool","K. R. Pattipati; S. Deb; M. Dontamsetty; A. Maitra","Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA","IEEE Aerospace and Electronic Systems Magazine","","1991","6","1","13","20","START, a software package for automatic test sequencing and testability analysis of complex, hierarchically described modular systems, is described, and its use in modeling systems is examined. START uses algorithms based on information theory, heuristic search, and graph theory to solve various faces of the test sequencing and testability analysis problems. A system is modeled in the failure space as a hierarchical directed graph with nodes denoting modules and testpoints and with AND nodes denoting redundancy. Interconnections among the nodes denote their immediate functional dependencies. START supports hierarchical testing in accordance with the maintenance strategy; a failure source may be isolated to a component or a module at any level. Other practical features include options to integrate diagnosis with repair (after partial diagnosis) in order to optimize test time, test cost, or test and repair cost. An interactive menu-mouse graphical interface serves as a high-level front end to these algorithms and enables the user to graphically enter and modify hierarchical functional models of systems. START presents the gist of the outputs of the testability analysis algorithms as a concise testability report consisting of important figures of merit.<<ETX>>","0885-8985;1557-959X","","10.1109/62.64988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=64988","","System testing;Software testing;Circuit testing;Automatic testing;Circuit faults;Software packages;Very large scale integration;Algorithm design and analysis;Cost function;Fault detection","automatic test equipment;automatic testing;directed graphs;fault location;information theory;logic testing;maintenance engineering;modules;redundancy;software packages;user interfaces","ATE;VLSI;reachability;dynamic programming;START;software package;automatic test sequencing;testability analysis;hierarchically described modular systems;information theory;heuristic search;graph theory;failure space;hierarchical directed graph;AND nodes;redundancy;hierarchical testing;maintenance strategy;diagnosis;repair;test time;cost;interactive menu-mouse graphical interface;high-level front end","","18","20","","","","","","IEEE","IEEE Journals & Magazines"
"Plug and play testbed to enable responsive space missions","J. Summers","MicroSat Syst. Inc., Littleton, CO, USA","2005 IEEE Aerospace Conference","","2005","","","557","563","There exists a growing need in the DOD for a tactical or responsive space asset to support real-time battlefield intelligence, surveillance, and reconnaissance. The desired attributes include being: 1. responsive /sub e/ployable in days; 2. affordable - expendable tactical resources at a cost comparable to other tactical systems; 3. employable - assets must support the joint force commander (JFC); 4. integrated space/air/terrestrial system-of-systems - full network connectivity, bandwidth on demand, and augment other assets. To the warfighter, a responsive space asset would provide the capability to respond to unanticipated military needs in days, providing flexibility of response to rapidly field tailored payloads and coverage. This capability could also provide rapid reconstitution after a loss from attack or failure and counteract enemy adaptation, through denial or deception, to existing space capabilities. Most importantly the short deployment times and low cost would provide the United States a means for efficiently using the versatility, and relative safety of space to provide real-time support to the war fighter. To be responsive the space element must possess a modular design supporting ""plug and play"" (PnP) architecture, leveraging commercial parts and standards. Lending itself to a lean production and integration environment again utilizing standard interfaces and taking advantage of pre-qualified inventoried subsystems. Rapid deployment of these elements will make use of ""canned"" mission planning tools, tailored orbits for a given theater, built-in health and status monitoring, and autonomous test and checkout software and operations. The two emerging responsive mission objectives include space control and tailored, tactical intelligence, surveillance, reconnaissance (ISR). Space control involves a situational awareness to sense threats against, and provide protection for, US space assets. The tailored, tactical missions offer high tempo ISR operations such as target characterization and emitter location in theater, perform real-time blue force tracking, and provide gap-filler, specialized communications support. The challenge is to develop and qualify the satellite technologies and rapid integration and test processes to support an operational responsive system in the next five years. Under an Air Force Research Laboratory SBIR program, MicroSat Systems is developing a PnP testbed to enable an operational responsive capability through development and ground validation of the various elements. Those elements include the mission definition and CONOPS specification processes, space segment, and the operational prioritization, tasking, processing, exploitation, and dissemination (PTPED) process/infrastructure. Specific to providing an end-to-end mission simulation, the testbed should be equipped with the modeling and simulation tools to develop tactical satellite CONOPS and provide the warfighter with a front end tool for training. To validate utility to the end-user the testbed must be equipped with the capability to simulate the data processing and dissemination infrastructure envisioned to provide the battlefield commander with real-time data. The space segment of the testbed should consist of the hardware and software elements required to simulate the operations of a fully functional satellite system. The approach to developing a responsive mission testbed includes defining requirements, hardware/software architecture, technology development roadmap, starting with a core capability, and incrementally integrating and validating the developing components and processes as they emerge.","1095-323X","0-7803-8870","10.1109/AERO.2005.1559345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559345","","Plugs;Space missions;Space technology;Software testing;Satellites;System testing;Surveillance;Reconnaissance;Costs;Hardware","command and control systems;military aircraft;aerospace simulation;automatic test software;aerospace safety;software tools;hardware-software codesign;surveillance","plug and play testbed;responsive space missions;tactical systems;joint force commander;integrated space-air-terrestrial system-of-systems;responsive space asset;real-time support;war fighter;mission planning tools;health monitoring;status monitoring;autonomous test software;checkout software;space control;tactical intelligence;tactical surveillance;tactical reconnaissance;situational awareness;tactical missions;satellite technologies;operational responsive system;mission simulation;simulation tools;tactical satellite;CONOPS;data processing;dissemination infrastructure;battlefield commander;hardware-software architecture;technology development","","2","6","","","","","","IEEE","IEEE Conferences"
"Extreme programming modified: embrace requirements engineering practices","J. Nawrocki; M. Jasinski; B. Walter; A. Wojciechowski","Poznan Univ. of Technol., Poland; Poznan Univ. of Technol., Poland; Poznan Univ. of Technol., Poland; Poznan Univ. of Technol., Poland","Proceedings IEEE Joint International Conference on Requirements Engineering","","2002","","","303","310","Extreme programming (XP) is an agile (lightweight) software development methodology and it becomes more and more popular. XP proposes many interesting practices, but it also has some weaknesses. From the software engineering point of view the most important issues are: maintenance problems resulting from very limited documentation (XP relies on code and test cases only), and lack of wider perspective of a system to be built. Moreover, XP assumes that there is only one customer representative. In many cases there are several representatives (each one with his own view of the system and different priorities) and then some XP practices should be modified. In the paper we assess XP from two points of view: the capability maturity model and the Sommerville-Sawyer model (1997). We also propose how to introduce documented requirements to XP, how to modify the planning game to allow many customer representatives and how to get a wider perspective of a system to be built at the beginning of the project lifecycle.","1090-705X","0-7695-1465","10.1109/ICRE.2002.1048543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048543","","Capability maturity model;Documentation;Programming profession;Software engineering;Software testing;Software maintenance;Oral communication;Automatic testing;System testing;Electronic mail","formal specification;software engineering","extreme programming;XP;requirements engineering practices;agile software development methodology;software engineering;software maintenance;documentation;capability maturity model;Sommerville-Sawyer model;planning game","","10","","","","","","","IEEE","IEEE Conferences"
"Selecting components: a process for context-driven evaluation","V. Maxville; C. P. Lam; J. Armarego","Edith Cowan Univ., Perth, WA, Australia; Edith Cowan Univ., Perth, WA, Australia; NA","Tenth Asia-Pacific Software Engineering Conference, 2003.","","2003","","","456","465","We describe a process for selecting and evaluating candidates for component based software engineering. The process is aimed at developers sourcing components from third party vendors. Component metadata and a formalised specification of the ideal component, including context information, are used to drive the process. This specification is used to shortlist candidate components from commercial repositories and to generate the tests and adaptations for the candidate components. Metrics from each stage of the selection and evaluation process are then combined to compare and rank components for inclusion in the target application. This approach to component selection, using context information and formal methods, helps address issues with component sourcing, selection and testing of third party components.","","0-7695-2011","10.1109/APSEC.2003.1254401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254401","","Application software;Software engineering;System testing;Risk management;Software testing;Documentation;Computer languages;Computer industry;Programming;Certification","object-oriented programming;formal specification;software metrics;software selection;program testing;meta data","context-driven evaluation;component based software engineering;third party vendor;component metadata;formal specification;software metric","","2","29","","","","","","IEEE","IEEE Conferences"
"Optimal resource allocation for the quality control process","P. Jalote; B. Vishal","Dept. of Comput. Sc. & Engg., Indian Inst. of Technol., Kanpur, India; Dept. of Comput. Sc. & Engg., Indian Inst. of Technol., Kanpur, India","14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.","","2003","","","26","33","Software development project employs some quality control (QC) process to detect and remove defects. The final quality of the delivered software depends on the effort spent on all the QC stages. Given a quality goal, different combinations of efforts for the different QC stages may lead to the same goal. In this paper, we address the problem of allocating resources to the different QC stages, such that the optimal quality is obtained. We propose a model for the cost of QC process and then view the resource allocation among different QC stages as an optimization problem. We solve this optimization problem using non-linear optimization technique of sequential quadratic programming. We also give examples to show how a sub-optimal resource allocation may either increase the resource requirement significantly or lower the quality of the final software.","1071-9458","0-7695-2007","10.1109/ISSRE.2003.1251028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251028","","Resource management;Quality control;Software quality;Cost function;System testing;Quadratic programming;Software performance;Delay;Software reliability;Software testing","software quality;software reliability;quality control;resource allocation;quadratic programming","optimal resource allocation;quality control;software development project;software quality;optimal quality;optimization problem;nonlinear optimization;sequential quadratic programming;suboptimal resource allocation;resource requirement","","","20","","","","","","IEEE","IEEE Conferences"
"Automatic test generation for verifying microprocessors","F. Corno; E. Sanchez; M. S. Reorda; G. Squillero","NA; NA; NA; NA","IEEE Potentials","","2005","24","1","34","37","A pipelined processor with a high-level behavioral HDL description is presented in this paper. It generates a set of effective test programs by using a simulator, which is able to evaluate with respect to an RTL coverage metric. The proposed optimizer is based on a technique called microGP, an evolutionary system able to automatically device and optimizes the program written in an assembly language. Quantitative coverage measurement presented will guide the test-program generation. The approach is fully automatic and broadly applicable. The minimal test set with the programmable coverage is attained.","0278-6648;1558-1772","","10.1109/MP.2005.1405800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1405800","","Automatic testing;Microprocessors;Libraries;Feedback;Assembly;Field programmable gate arrays;Genetic programming;Programming profession;Built-in self-test;Runtime","microprocessor chips;automatic test pattern generation;automatic test software;assembly language;pipeline processing;hardware description languages;genetic algorithms","automatic test generation;microprocessor verification;pipelined processor;high-level behavioral HDL description;test program generation;simulator;RTL coverage metric;optimization;microGP;genetic programming;evolutionary system;assembly language program;quantitative programmable coverage measurement","","3","15","","","","","","IEEE","IEEE Journals & Magazines"
"Parameter estimation of hyper-geometric distribution software reliability growth model by genetic algorithms","T. Minohara; Y. Tohma","Dept. of Comput. Sci., Takushoku Univ., Tokyo, Japan; NA","Proceedings of Sixth International Symposium on Software Reliability Engineering. ISSRE'95","","1995","","","324","329","Usually, parameters in software reliability growth models are not known, and they must be estimated by using observed failure data. Several estimation methods have been proposed, but most of them have restrictions such as the existence of derivatives on evaluation functions. On the other hand, genetic algorithms (GA) provide us with robust optimization methods in many fields. We apply GA to the parameter estimation of the hyper-geometric distribution software reliability growth model. Experimental result shows that GA is effective in the parameter estimation and removes restrictions from software reliability growth models.","1071-9458","0-8186-7131","10.1109/ISSRE.1995.497673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497673","","Parameter estimation;Software reliability;Testing;Software quality;Phase measurement;Fault detection;Genetic algorithms;Genetic engineering;Reliability engineering;Computer science","software reliability;parameter estimation;genetic algorithms;programming theory;program testing;program debugging","parameter estimation;hypergeometric distribution software reliability growth model;genetic algorithms;observed failure data;estimation methods;evaluation functions;robust optimization methods;program testing;program debugging","","18","10","","","","","","IEEE","IEEE Conferences"
"START: System Testability Analysis and Research Tool","K. R. Pattipati; S. Deb; M. Dontamsetty; A. Maitra","Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA","IEEE Conference on Systems Readiness Technology, 'Advancing Mission Accomplishment'.","","1990","","","395","402","A brief overview of the Systems Testability Analysis and Research Tool (START) software package for automatic test sequencing and testability analysis of complex hierarchically described modular systems is presented. It consists of algorithms based on information theory, heuristic search, and graph theory to solve various facets of the test sequencing and testability analysis problems. A system is modeled in the failure space as a hierarchical directed graph with nodes denoting modules and test points and AND nodes denoting redundancy. Interconnections among the nodes denote their immediate functional dependencies. START supports hierarchical testing in accordance with the maintenance strategy; a failure source may be isolated to a component or a module at any level, e.g. LRU (line replaceable unit), SRU (shop replaceable unit), etc. Other practical features include options to integrate diagnosis with repair (after partial diagnosis) and to optimize test time, test cost, or test and repair cost. An interactive menu-mouse graphical interface serves as a high-level front end to these algorithms and enables the user to graphically enter and modify hierarchical functional models of systems.<<ETX>>","","","10.1109/AUTEST.1990.111540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=111540","","System testing;Software testing;Circuit testing;Automatic testing;Circuit faults;Production facilities;Very large scale integration;Software packages;Algorithm design and analysis;Cost function","automatic test equipment;automatic testing;graphical user interfaces;hierarchical systems;interactive systems;maintenance engineering;redundancy;software packages","START;Systems Testability Analysis and Research Tool;software package;automatic test sequencing;testability analysis;complex hierarchically described modular systems;information theory;heuristic search;graph theory;hierarchical directed graph;AND nodes;redundancy;hierarchical testing;maintenance strategy;line replaceable unit;shop replaceable unit;diagnosis;repair;test time;test cost;interactive menu-mouse graphical interface","","9","20","","","","","","IEEE","IEEE Conferences"
"Optimized consistence maintenance strategy in semantic caching","Hao Xiao-Wei; Zhang Tao; Li Lei; Liu Feng-Yu","Software Res. Inst., Sun Yat-Sen Univ., Guangzhou, China; Software Res. Inst., Sun Yat-Sen Univ., Guangzhou, China; Software Res. Inst., Sun Yat-Sen Univ., Guangzhou, China; NA","2005 IEEE International Conference on Systems, Man and Cybernetics","","2005","4","","3392","3397 Vol. 4","Semantic caching is an attractive method when disconnection or connection jam happens in a distributed computing or a mobile computing environment. However, in semantic caching, the consistency maintenance technology is a challengeable technology. The existent methods have many limitations that restrict the practicability of semantic caching in a great extent. This paper extends the existing work in three ways: firstly, corrects relative concepts in previous works and gives a kind of valid consistency maintenance strategy; furthermore, produces related optimization functions to simplify the updating queue; finally, implements the simulation system to test the performance of our works in semantic caching.","1062-922X","0-7803-9298","10.1109/ICSMC.2005.1571671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1571671","Semantic caching;consistency maintenance;updating queue;optimization","Partial response channels;Mobile computing;Sun;Distributed computing;Space technology;System testing;Mobile communication;Database systems;Computational modeling;Wireless communication","cache storage;software maintenance;optimisation;software performance evaluation","optimized consistence maintenance strategy;semantic caching;disconnection jam;connection jam;distributed computing;mobile computing environment;consistency maintenance technology;optimization functions;updating queue","","","10","","","","","","IEEE","IEEE Conferences"
"An execution slice and inter-block data dependency-based approach for fault localization","W. E. Wong; Y. Qi","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA","11th Asia-Pacific Software Engineering Conference","","2004","","","366","373","Localizing a fault in a program is a complex and time-consuming process. In this paper we present a novel approach using execution slice and inter-block data dependency to effectively identify the locations of program faults. An execution slice with respect to a given test case is the set of code executed by this test, and two blocks are data dependent if one block contains a definition that is used by another block or vice versa. Not only can our approach reduce the search domain for program debugging, but also prioritize suspicious locations in the reduced domain based on their likelihood of containing faults. More specifically, the likelihood of a piece of code containing a specific fault is inversely proportional to the number of successful tests that execute it. In addition, the likelihood also depends on whether this piece of code is data dependent on other suspicious code. A debugging tool, DESiD, was developed to support our method. A case study that shows the effectiveness of our method in locating faults on an application developed for the European Space Agency is also reported.","1530-1362","0-7695-2245","10.1109/APSEC.2004.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371939","Software testing;fault localization;program debugging;execution slice;inter-block data dependency","Debugging;Fault diagnosis;Software testing;Computer bugs;Computer science;Application software;Life testing;Programming","program slicing;program debugging;program testing;fault diagnosis;software quality","software testing;execution slice;inter-block data dependency;fault localization;program debugging","","5","14","","","","","","IEEE","IEEE Conferences"
"Testability implications in low-cost integrated radio transceivers: a Bluetooth case study","S. Ozev; C. Gaard; A. Orailoglu","Dept. of Comput. Sci. & Eng., California Univ., San Diego, La Jolla, CA, USA; NA; NA","Proceedings International Test Conference 2001 (Cat. No.01CH37260)","","2001","","","965","974","As the use of wireless communications in daily life increases, attaining low-cost solutions becomes increasingly important due to shrinking profit margins. Cost optimization that solely targets at minimization of the cost of system architecture may result in suboptimal, highly untestable, solutions. Test design and design for testability need to be incorporated into the system design flow to achieve viable solutions. This paper presents an analysis of test requirements, implications and test cost for low-cost Bluetooth systems. Testability problems are identified and possible solutions along with avenues to reduce the test cost by utilizing lower-cost testers are discussed.","1089-3539","0-7803-7169","10.1109/TEST.2001.966721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966721","","Radio transceivers;Bluetooth;Computer aided software engineering;System testing;Radio frequency;Circuit testing;Cost function;Design for testability;Hardware;Bit error rate","transceivers;design for testability;telecommunication equipment testing;phase noise;frequency hop communication;spread spectrum communication;telecontrol","low-cost integrated radio transceivers;wireless communications;design for testability;DFT;RF testing;test design;test cost reduction;low-cost Bluetooth systems;testability problems;receiver architectures;transceiver architectures;synthesizer specifications;transmit architectures;specification-based tests;VCO phase noise;PLL phase noise;RMS phase noise","","16","6","","","","","","IEEE","IEEE Conferences"
"Compiled code dynamic worst case timing simulation tracking multiple causality","K. K. Varma","Mentor Graphics Corp., Wilsonville, OR, USA","Proceedings of 1995 IEEE International Test Conference (ITC)","","1995","","","861","869","Due to the abstract nature of compiled code simulation models, performing accurate worst-case (min-max) simulation with these models presents a complex and a challenging problem. This paper details the functional evaluation, the timing evaluation and the hazard analysis techniques that have been developed to solve this problem. For improved accuracy, a multiple causality approach has been adopted. Issues that came up due to the tracking of multiple causality are highlighted and the solutions devised are described. Potential benefits of using a compiled code technology for min-max are discussed.","1089-3539","0-7803-2992","10.1109/TEST.1995.529918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529918","","Dynamic compiler;Computer aided software engineering;Timing;Testing;Delay;Hazards;Circuit simulation;Performance analysis;Graphics;Logic design","logic testing;logic CAD;discrete event simulation;timing;minimax techniques;optimising compilers;automatic test software;hazards and race conditions","compiled code simulation models;dynamic worst case timing simulation tracking;multiple causality tracking;min-max simulation;functional evaluation;timing evaluation;hazard analysis techniques;logic simulation;design verification;ambiguous event;transition events;stability events;ASIC","","","11","","","","","","IEEE","IEEE Conferences"
"On fault diagnosis of analog circuits with tolerance using simulated annealing optimization algorithm","Li Yan; Weng Xiangying","Northwestern Polytech. Univ., Xi'an, China; Northwestern Polytech. Univ., Xi'an, China","Proceedings of the 1992 International Conference on Industrial Electronics, Control, Instrumentation, and Automation","","1992","","","1472","1475 vol.3","The authors extend the numerical annealing algorithm framework presented by S. Kirkpatrick et al. to the case of continuous multidimensional parameter space. The design an efficient parameter inverse simulated annealing (PI-SA) algorithm, and obtain some important theoretical results on continuous annealing algorithm design. They apply the PI-SA optimization algorithm to fault diagnosis of analog circuits with tolerance. A software package for automatic diagnosis has been developed and applied to fault isolation of the electronic equipment of the remotely piloted vehicles. The diagnosis results obtained with the software package both ground simulation and in flight testing are satisfactory.<<ETX>>","","0-7803-0582","10.1109/IECON.1992.254384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=254384","","Fault diagnosis;Analog circuits;Algorithm design and analysis;Circuit simulation;Simulated annealing;Software packages;Multidimensional systems;Inverse problems;Circuit faults;Electronic equipment","aerospace testing;analogue circuits;automatic testing;fault location;integrated circuit testing;simulated annealing","fault location;aerospace testing;automatic testing;fault diagnosis;analog circuits;tolerance;simulated annealing;optimization algorithm;continuous multidimensional parameter space;design;parameter inverse;software package;fault isolation;ground simulation;in flight testing","","3","2","","","","","","IEEE","IEEE Conferences"
"Formally verified on-line diagnosis","C. J. Walter; P. Lincoln; N. Suri","WW Technol. Group, Ellicott City, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","11","684","721","A reconfigurable fault tolerant system achieves the attributes of dependability of operations through fault detection, fault isolation and reconfiguration, typically referred to as the FDIR paradigm. Fault diagnosis is a key component of this approach, requiring an accurate determination of the health and state of the system. An imprecise state assessment can lead to catastrophic failure due to an optimistic diagnosis, or conversely, result in underutilization of resources because of a pessimistic diagnosis. Differing from classical testing and other off-line diagnostic approaches, we develop procedures for maximal utilization of the system state information to provide for continual, on-line diagnosis and reconfiguration capabilities as an integral part of the system operations. Our diagnosis approach, unlike existing techniques, does not require administered testing to gather syndrome information but is based on monitoring the system message traffic among redundant system functions. We present comprehensive on-line diagnosis algorithms capable of handling a continuum of faults of varying severity at the node and link level. Not only are the proposed algorithms on-line in nature, but are themselves tolerant to faults in the diagnostic process. Formal analysis is presented for all proposed algorithms. These proofs offer both insight into the algorithm operations and facilitate a rigorous formal verification of the developed algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/32.637385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637385","","Fault diagnosis;Fault tolerant systems;Costs;System testing;Fault detection;Monitoring;Algorithm design and analysis;Formal verification;Resource management","online operation;reconfigurable architectures;software fault tolerance;program diagnostics;program verification;program testing","online diagnosis;formal verification;reconfigurable fault tolerant system;operation dependability;fault detection;fault isolation;FDIR paradigm;fault diagnosis;optimistic diagnosis;pessimistic diagnosis;testing;system state information;system message traffic monitoring;redundant system functions","","30","40","","","","","","IEEE","IEEE Journals & Magazines"
"System level memory optimization for hardware-software co-design","K. Danckaert; F. Catthoor; H. De Man","IMEC, Leuven, Belgium; NA; NA","Proceedings of 5th International Workshop on Hardware/Software Co Design. Codes/CASHE '97","","1997","","","55","59","Application studies in the areas of image and video processing systems indicate that between 50 and 80% of the area cost in (application-specific) architectures for real-time multi-dimensional signal processing (RMSP) is due to data storage and transfer of array signals. This is true for both singleand multi-processor realizations, both customized and (embedded) programmable targets. This paper has two main contributions. First, to reduce this dominant cost, we propose to address the system-level storage organization for the multi-dimensional signals as a first step in the overall methodology to map these applications, before the hardware/software partitioning decision. Secondly, we demonstrate the usefulness of this novel approach based on a realistic test vehicle, namely a quad-tree based image coding application.","1092-6100","0-8186-7895","10.1109/HSC.1997.584579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584579","","Costs;Application software;Real time systems;Array signal processing;Multidimensional signal processing;Video signal processing;Memory;Signal processing;Hardware;Testing","high level synthesis;software engineering;optimisation;file organisation;image coding;quadtrees","system-level memory optimization;hardware-software codesign;image processing systems;video processing systems;area cost;application-specific architectures;real-time multi-dimensional signal processing;data storage;array signal transfer;system-level storage organization;multi-dimensional signals;hardware/software partitioning;quad-tree based image coding application","","13","13","","","","","","IEEE","IEEE Conferences"
"Analog automatic test plan generator-Integrating with modular analog IC design environment","R. Naiknaware","Texas Instrum. Pvt. Ltd., Bangalore, India","Proceedings of 1993 IEEE International Conference on Computer Design ICCD'93","","1993","","","318","321","Analog automatic test plan generation (AATPG) was considered to be a difficult task until recently. We have developed a method to automate the test plan generation for analog ICs, which are designed using a modular design concept similar to that of digital ICs. However, it is still not clear exactly how this method can be put in the design and manufacturing process. Overall the automated test program generation process requires intricate knowledge of the design database of the chip, and the destination automatic test equipment (ATE), on which online testing of the ICs is to be performed. At the same time, it should be noted that consideration needs to be given to intermediate stages of extracting information from the design database, selection of appropriate method for test plan generation, test method specifications, test plan format, test plan to test program translator, optimization of the overall test plan and corresponding tester specific test program, deletion and addition of the tests, simulation of the entire test environment including device under test (DUT), extracting information of non-testable areas and, design alteration for higher test coverage. The article addresses the issues involved in each stage of the automatic test program generation process and explains how exactly it needs to be integrated with the design, manufacturing and testing process of analog ICs designed using newly emerging top-down modular design approach.<<ETX>>","","0-8186-4230","10.1109/ICCD.1993.393358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=393358","","Automatic testing;Integrated circuit testing;Analog integrated circuits;Circuit testing;Process design;Databases;Design methodology;Design automation;Data mining;Digital integrated circuits","analogue integrated circuits;circuit CAD;automatic test software;integrated circuit testing","analog automatic test plan generation;modular analog IC design environment;AATPG;modular design concept;manufacturing process;automated test program generation process;design database;automatic test equipment;online testing;test program translator;tester specific test program;device under test;top-down modular design approach","","","12","","","","","","IEEE","IEEE Conferences"
"Automation concept for a new dynamical engine test stand","M. Schmidt; J. -. Kessel","Inst. of Autom. Control, Tech. Hochschule Darmstadt, Germany; NA","Proceedings of the 1999 IEEE International Conference on Control Applications (Cat. No.99CH36328)","","1999","1","","846","851 vol. 1","Dynamical engine test stands are an important tool for the development and optimization of internal combustion (IC) engines. This paper deals with the automation concept for a new dynamical engine test stand considering the graphical user interface, the hard- and software structure, the simulation models for vehicle and driver, and the modes of operation.","","0-7803-5446","10.1109/CCA.1999.807776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=807776","","Automation;Automatic testing;Internal combustion engines;Graphical user interfaces;Software testing;Integrated circuit testing;Integrated circuit modeling;Automatic control;Hardware;Torque measurement","internal combustion engines;automatic testing;dynamic testing;digital simulation","dynamical engine test stand;internal combustion engines;graphical user interface;software structure;hardware structure;simulation models;operation modes","","","6","","","","","","IEEE","IEEE Conferences"
"Diagnosis and failure analysis for scan failure","Z. G. Song; S. P. Neo; T. Tun; C. K. Oh; K. F. Lo","Chartered Semicond. Manuf. Ltd., Singapore; Chartered Semicond. Manuf. Ltd., Singapore; Chartered Semicond. Manuf. Ltd., Singapore; Chartered Semicond. Manuf. Ltd., Singapore; Chartered Semicond. Manuf. Ltd., Singapore","Proceedings of the 12th International Symposium on the Physical and Failure Analysis of Integrated Circuits, 2005. IPFA 2005.","","2005","","","181","184","This paper describes a scan test that is widely used for microelectronic devices nowadays, and scan failure that is often ranked as one of the highest yield loss for most microelectronic devices. Due to its nature, the fault isolation of scan failure can't be achieved by conventional current-based fault isolation techniques. So, establishment of scan failure diagnosis capability is very important. The four cases showed in this paper demonstrated that the scan failure could be diagnosed by diagnosis software effectively and various types of defects could be found by this methodology.","1946-1542;1946-1550","0-7803-9301","10.1109/IPFA.2005.1469157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1469157","","Failure analysis;Circuit testing;Logic testing;Microelectronics;Logic devices;Automatic test pattern generation;Automatic testing;Flip-flops;Latches;Software testing","failure analysis;boundary scan testing;automatic test software;fault diagnosis","scan failure diagnosis;microelectronic devices;failure analysis;automatic test pattern generation;software-based diagnosis","","1","6","","","","","","IEEE","IEEE Conferences"
"The importance of quality requirements in software platform development-a survey","E. Johansson; A. Wesslen; L. Bratthall; M. Host","Ericsson Mobile Commun., Lund, Sweden; Ericsson Mobile Commun., Lund, Sweden; NA; NA","Proceedings of the 34th Annual Hawaii International Conference on System Sciences","","2001","","","10 pp.","","This paper presents a survey where some quality requirements that commonly affect software architecture have been prioritized with respect to cost and lead-time impact when developing software platforms and when using them. Software platforms are the basis for a product-line, i.e. a collection of functionality that a number of products is based on. The survey has been carried out in two large software development organizations using 34 senior participants. The prioritization was carried out using the Incomplete Pairwise Comparison method (IPC). The analysis shows that there are large differences between the importance of the quality requirements studied. The differences between the views of different stakeholders are also analysed and it is found to be less than the difference between the quality requirements. Yet this is identified as a potential source of negative impact on product development cost and lead-time, and rules of thumb for reducing the impact are given.","","0-7695-0981","10.1109/HICSS.2001.927252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=927252","","Software quality;Communication system software;Costs;Mobile communication;Application software;Data analysis;Product development;Testing;Electrical capacitance tomography;Internet","software architecture;software quality;software cost estimation","software quality requirements;software platform development;survey;software architecture;product development cost;lead-time impact;product-line;software development organizations;Incomplete Pairwise Comparison method;software development cost","","15","28","","","","","","IEEE","IEEE Conferences"
"Statistical analysis tools for regression testing of SimSET output","F. P. Jansen; R. M. Manjeshwar; R. L. Harrison","Gen. Electr. Global Res. Center, Niskayuna, NY, USA; Gen. Electr. Global Res. Center, Niskayuna, NY, USA; NA","2002 IEEE Nuclear Science Symposium Conference Record","","2002","2","","835","837 vol.2","SimSET (a Simulation System for Emission Tomography) is widely used for studying PET and SPECT. Techniques to increase simulation efficiency such as importance sampling (IS) have recently been refined with the goal of increasing the efficiency of SimSET by a factor of 20. In order to ensure that IS and other efficiency optimizations do not introduce artifacts in the data, we have developed a new set of tools aimed at confirming equivalence of output from the simulation tool for each revision of the software. Since the efficiency improvement is achieved by performing fewer computations for each detected event, it is not possible to reproduce datasets exactly (by starting with the same random seed) and therefore statistical techniques must be used. In this paper we compare the conventional t-test that is the current standard test, with the chi-squared goodness of fit test and a visual test of the t-statistic.","","0-7803-7636","10.1109/NSSMIC.2002.1239454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1239454","","Statistical analysis;Testing;Monte Carlo methods;Computational modeling;Software tools;Medical simulation;Software algorithms;Positron emission tomography;Event detection;Software packages","importance sampling;positron emission tomography;single photon emission computed tomography;statistical analysis;optimisation","statistical analysis;regression testing;SimSET output;PET;SPECT;importance sampling;efficiency optimizations","","","4","","","","","","IEEE","IEEE Conferences"
"An optimized method for automatic test oracle generation from real-time specification","Xin Wang; Zhi-Chang; Qi Shuhao Li","National Lab. for Parallel & Distributed Process., Changsha, China; National Lab. for Parallel & Distributed Process., Changsha, China; National Lab. for Parallel & Distributed Process., Changsha, China","10th IEEE International Conference on Engineering of Complex Computer Systems (ICECCS'05)","","2005","","","440","449","Test oracles are widely used to verify whether a system under test is running as desired. Since the correctness of real-time systems depends on the logical results of the computation and the time when results are produced at the same time, an optimized model checking-based method for test oracles generation is proposed to check if the system traces satisfy their real-time specifications at run time. Inspired by the idea of real-time model checking, the test oracles can be automatically generated from their specifications in the real-time logic MITL/sub [o,d]/ in a simpler way and modelled by a variant of the timed automata. Assertions are chosen to acquire the traces of real-time systems. A case study is presented to demonstrate the usefulness of the method proposed in this paper.","","0-7695-2284","10.1109/ICECCS.2005.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467926","","Optimization methods;Automatic testing;Real time systems;System testing;Software testing;Laboratories;Logic testing;Automata;Aerospace testing;Distributed processing","program testing;formal specification;program verification;real-time systems;finite automata","automatic test oracle generation;real-time specification;system verification;real-time system correctness;optimized model checking-based method;system checking;real-time specifications;real-time model checking;real-time logic;timed automata;real-time systems","","","13","","","","","","IEEE","IEEE Conferences"
"A framework of greedy methods for constructing interaction test suites","R. C. Bryce; C. J. Colbourn; M. B. Cohen","Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; NA","Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.","","2005","","","146","155","Greedy algorithms for the construction of software interaction test suites are studied. A framework is developed to evaluate a large class of greedy methods that build suites one test at a time. Within this framework are many instantiations of greedy methods generalizing those in the literature. Greedy algorithms are popular when the time for test suite construction is of paramount concern. We focus on the size of the test suite produced by each instantiation. Experiments are analyzed using statistical techniques to determine the importance of the implementation decisions within the framework. This framework provides a platform for optimizing the accuracy and speed of ""one-test-at-a-time"" greedy methods.","0270-5257;1558-1225","1-59593-963","10.1109/ICSE.2005.1553557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553557","","System testing;Software testing;Portable computers;Linux;DSL;Greedy algorithms;Computer science;Permission;Hardware;Optimization methods","program testing;greedy algorithms","interaction test suite construction;one-test-at-a-time greedy methods;mixed-level covering arrays;pair-wise interaction coverage;software interaction testing","","7","33","","","","","","IEEE","IEEE Conferences"
"Empirical studies to identify defect prevention opportunities using process simulation technologies","N. S. Eickelmann","SSERL, Motorola Inc., Schaumburg, IL, USA","Proceedings 26th Annual NASA Goddard Software Engineering Workshop","","2001","","","22","25","Discusses results of a multi-year research project to optimize software and systems test quality throughout Motorola. Our approach is to apply process modeling and simulation technologies to evaluate process improvement, automation and defect prevention strategies. Part of our approach is to conduct experiments using process modeling and simulation technologies to evaluate and characterize technological changes meant to improve quality, reliability, efficiency or schedule. We focus on defect prevention specifically, and process and automation strategies that would result in preventing defects.","","0-7695-1456","10.1109/SEW.2001.992651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992651","","Automation;Software engineering;Productivity;Product development;Technology management;Mirrors;Computational modeling;Programming profession;Software quality;Software systems","software engineering;software quality;software reliability","defect prevention opportunities;process simulation technologies;multi-year research project;software quality;systems test quality;Motorola;process modeling;process improvement;process automation;technological changes;reliability;efficiency","","","8","","","","","","IEEE","IEEE Conferences"
"Architectural-level risk analysis using UML","K. Goseva-Popstojanova; A. Hassan; A. Guedem; W. Abdelmoez; D. E. M. Nassar; H. Ammar; A. Mili","Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; NA","IEEE Transactions on Software Engineering","","2003","29","10","946","960","Risk assessment is an essential part in managing software development. Performing risk assessment during the early development phases enhances resource allocation decisions. In order to improve the software development process and the quality of software products, we need to be able to build risk analysis models based on data that can be collected early in the development process. These models will help identify the high-risk components and connectors of the product architecture, so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product. In this paper, we present a risk assessment methodology which can be used in the early phases of the software life cycle. We use the Unified Modeling Language (UML) and commercial modeling environment Rational Rose Real Time (RoseRT) to obtain UML model statistics. First, for each component and connector in software architecture, a dynamic heuristic risk factor is obtained and severity is assessed based on hazard analysis. Then, a Markov model is constructed to obtain scenarios risk factors. The risk factors of use cases and the overall system risk factor are estimated using the scenarios risk factors. Within our methodology, we also identify critical components and connectors that would require careful analysis, design, implementation, and more testing effort. The risk assessment methodology is applied on a pacemaker case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237174","","Risk analysis;Unified modeling language;Risk management;Connectors;Programming;Software development management;Resource management;Software quality;Computer architecture;Statistics","software architecture;risk management;specification languages;Markov processes","software development;risk assessment;software architecture;dynamic coupling;Unified Modeling Language;UML;software life cycle;severity of failure","","73","33","","","","","","IEEE","IEEE Journals & Magazines"
"Logic model optimization for LSSD structures","S. -. Huang; D. Forlenza; J. Waicukauski; C. Pete","IBM Corp., East Fishkill, NY, USA; IBM Corp., East Fishkill, NY, USA; IBM Corp., East Fishkill, NY, USA; IBM Corp., East Fishkill, NY, USA","International Symposium on VLSI Technology, Systems and Applications,","","1989","","","144","148","A method that optimizes the logical description of an LSSD (level-sensitive scan design) structure is described. The logic model thus created results in improved efficiency of test generation, fault simulation, and failure diagnosis. Extensive testing with this system has resulted in logic-gate-count reduction up to 40%, reduction in simulation CPU time by a factor of 2, and improvement in efficiency and effectiveness in the testing environment. Significant model gate count is achieved with only a minor increase in the CPU time.<<ETX>>","","","10.1109/VTSA.1989.68601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=68601","","Latches;Clocks;Logic gates;Logic testing;Circuit testing;Design optimization;Circuit simulation;Test pattern generators;Logic circuits;Software algorithms","fault location;logic design;logic gates;logic testing","LSSD structures;logical description;level-sensitive scan design;logic model;test generation;fault simulation;failure diagnosis;logic-gate-count reduction;simulation CPU time;efficiency","","","6","","","","","","IEEE","IEEE Conferences"
"Signal-based modeling of ATE and UUTs using COTS based software tools","T. J. Timcho","GDE Syst. Inc., Columbus, OH, USA","1997 IEEE Autotestcon Proceedings AUTOTESTCON '97. IEEE Systems Readiness Technology Conference. Systems Readiness Supporting Global Needs and Awareness in the 21st Century","","1997","","","143","147","This paper discusses a practical approach to the use of Commercial-Off-the-Shelf (COTS) tools to implement Automatic Test Equipment (ATE) and Unit Under Test (UUT) signal-based models to support both new Test Program Set (TPS) development and TPS rehost activities. This modeling technique also provides a standard to which existing and future ATEs and UUTs can be modeled and used for analysis and comparisons. The signal-based model is implemented via a Structured Query Language (SQL) Relational Data Base Management System (RDBMS). COTS Rapid Application Development (RAD) tools, such as Microsoft/sup (R)/ Visual Basic/sup (R)/ Microsoft Access/sup (R)/ and PowerBuilder/sup (R)/, are employed to implement interfaces to the SQL database model and to provide the necessary algorithms for data comparison, model reuse and other manipulations of the model data. After models are established for both the ATE and UUTs, ATU/UUT compatibility analysis can be performed to identify any potential incompatibilities. This information can be used to optimize the selection of a target ATE for a selected group of UUTs. It can also be used to identify any signal deficiencies in the target ATE that require ATE enhancements or Interface Test Adapter (ITA) signal conditioning, to ensure UUT compatibility.","","0-7803-4162","10.1109/AUTEST.1997.633591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633591","","Automatic testing;Power system modeling;Signal processing;Automatic test equipment;Database languages;Power system management;Visual BASIC;Relational databases;Visual databases;Performance analysis","automatic test equipment;software tools;SQL;relational databases","signal-based modeling;ATE;UUTs;COTS based software tools;commercial-off-the-shelf tools;test program set;rehost activities;structured query language;relational database management system;rapid application development tools;data comparison;model reuse;signal deficiencies;ATU/UUT compatibility analysis;interface test adapter","","1","","","","","","","IEEE","IEEE Conferences"
"Development of an e-engineering environment for mechanical systems design and optimization","Qi Hao; Weiming Shen; Zhan Zhang; Seong-Whan Park; Jai-Kyung Lee","Integrated Manuf. Technol. Inst., Nat. Res. Council Canada, London, Ont., Canada; Integrated Manuf. Technol. Inst., Nat. Res. Council Canada, London, Ont., Canada; Integrated Manuf. Technol. Inst., Nat. Res. Council Canada, London, Ont., Canada; NA; NA","8th International Conference on Computer Supported Cooperative Work in Design","","2004","2","","251","258 Vol.2","This work presents an ongoing project on the development of an e-engineering environment for mechanical systems design and optimization. In this project, a prototype of agent-based engineering system is developed based on the AADE (autonomous agent development environment), a FIPA compliant agent platform developed at IMTI. By applying several advanced technologies including intelligent software agents, Internet/Web, workflow and database, the developed system is able to successfully fulfill our target of integrating personnel, design activities and engineering resources along a predefined engineering design project (workflow). A software prototype is implemented to integrate various engineering software tools including CAD, structural analysis (FEA), dynamic analysis, fatigue analysis, and optimization. A wheel-axle-assembly (part of a bogie system) is chosen as a test case for the validation of the prototype system.","","0-7803-7941","10.1109/CACWD.2004.1349193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1349193","","Mechanical systems;Design optimization;Design engineering;Software prototyping;Prototypes;Data engineering;Software tools;Systems engineering and theory;Autonomous agents;Intelligent agent","mechanical engineering computing;software agents;Internet;database management systems;workflow management software;design engineering;engineering information systems;optimisation;finite element analysis","e-engineering;mechanical systems design;mechanical systems optimization;agent-based engineering;autonomous agent development environment;FIPA compliant agent platform;intelligent software agents;Internet;World Wide Web;workflow;database;engineering design project;engineering software tools;CAD;structural analysis;FEA;dynamic analysis;fatigue analysis;wheel-axle-assembly;bogie system;software prototype","","1","22","","","","","","IEEE","IEEE Conferences"
"Assessing staffing needs for a software maintenance project through queuing simulation","G. Antoniol; A. Cimitile; G. A. Di Lucca; M. Di Penta","Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy","IEEE Transactions on Software Engineering","","2004","30","1","43","58","We present an approach based on queuing theory and stochastic simulation to help planning, managing, and controlling the project staffing and the resulting service level in distributed multiphase maintenance processes. Data from a Y2K massive maintenance intervention on a large COBOL/JCL financial software system were used to simulate and study different service center configurations for a geographically distributed software maintenance project. In particular, a monolithic configuration corresponding to the customer's point-of-view and more fine-grained configurations, accounting for different process phases as well as for rework, were studied. The queuing theory and stochastic simulation provided a means to assess staffing, evaluate service level, and assess the likelihood to meet the project deadline while executing the project. It turned out to be an effective staffing tool for managers, provided that it is complemented with other project-management tools, in order to prioritize activities, avoid conflicts, and check the availability of resources.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265735","","Software maintenance;Queueing analysis;Computational modeling;Stochastic processes;Project management;Costs;Computer simulation;Software systems;Counting circuits;Computer Society","software maintenance;queueing theory;discrete event simulation;personnel;project management;stochastic processes;program testing","queuing theory;stochastic simulation;distributed multiphase maintenance process;Y2K massive maintenance;financial software system;distributed software maintenance project;project-management tool;software maintenance staffing;discrete-event simulation;process simulation;schedule estimation","","39","37","","","","","","IEEE","IEEE Journals & Magazines"
"Study on UIO sequence generation for sequential machine's functional test","Haiping Sun; Minglun Gao; Alei Liang","Inst. of VLSI Design, Hefei Univ. of Technol., China; NA; NA","ASICON 2001. 2001 4th International Conference on ASIC Proceedings (Cat. No.01TH8549)","","2001","","","628","632","Unique input/output (UIO) sequence is a basic element to generate vector sequences for a sequential machine's functional test, which arises in many applications, such as VLSI design and communication protocols. A new heuristic algorithm based on distinguishable state group (DSG) is proposed in this paper to optimize UIO sequences generation. The optimizing methods, including a insertion strategy based on a specified 'less' relation, several pruning strategies, and a novel store mechanism of multiple OPEN/CLOSED lists, are also presented. Experimental results show that the proposed algorithm greatly improves the efficiency of the UIO sequence calculation in terms of both the time and space complexities.","","0-7803-6677","10.1109/ICASIC.2001.982642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982642","","Sequential analysis;System testing;Optimization methods;Automata;Very large scale integration;Postal services;Computer science;Protocols;Sun;Application software","sequential machines;logic testing;binary sequences;VLSI;computational complexity;automatic testing;integrated circuit testing","UIO sequence generation;sequential machine;functional test;vector sequences;VLSI design;communication protocols;heuristic algorithm;distinguishable state group;insertion strategy;pruning strategies;store mechanism;multiple OPEN/CLOSED lists;time complexities;space complexities","","","8","","","","","","IEEE","IEEE Conferences"
"Selecting engineering techniques using fuzzy logic based decision support","P. Liggesmeyer","Corp. Res. & Dev., Software & Eng., Siemens AG, Munich, Germany","Proceedings IEEE Symposium and Workshop on Engineering of Computer-Based Systems","","1996","","","427","434","The task of selecting software engineering methods, techniques, metrics, and tools is usually performed manually, based on the expertise of individuals. This paper presents a systematic tool supported approach, that bases its suggestions an the technical situation, the existing goals, and constraints of a specific organization or a particular project. A prototype of the decision support system supports the elaboration of test strategies. The approach uses information about the technical situation that is provided by answering predefined questions with fuzzy data. The objective is to assign ""adequacy values"" to combinations of test methods, techniques, metrics, tools, and quantified test situations. The priorities of goals and constraints have assessed by applying a technique that is based on comparing goals in pairs. This permits to check certain consistency criteria by static analysis. A hierarchy of the importance of goals and constraints is calculated, which provides the basis for the determination of the suitability of test methods; techniques, metrics, and tools with respect to goals and constraints.","","0-8186-7355","10.1109/ECBS.1996.494570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494570","","Fuzzy logic;Software testing;Software engineering;Prototypes;Decision support systems;System testing;Programming;Process control;Research and development;Design engineering","fuzzy logic;decision support systems;software tools;system monitoring;systems software;software metrics","metrics;fuzzy logic based decision support;software engineering method selection;tools;systematic tool supported approach;technical situation;goals;constraints;decision support system;test strategies;predefined questions;fuzzy data;adequacy values;quantified test situations;goal comparison;consistency criteria checking;static analysis","","2","26","","","","","","IEEE","IEEE Conferences"
"A Static Analysis of the NAG Library","M. A. Hennell; J. A. Prudom","Department of Computational Science, University of Liverpool; NA","IEEE Transactions on Software Engineering","","1980","SE-6","4","329","333","This paper reports results obtained from a static analysis of the NAG Mark 4 Fortran numerical algorithms library.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1980.230484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702740","Basic block;compiler;efficiency;Fortrn;LCSAJ optimization","Software libraries;Algorithm design and analysis;Optimizing compilers;Logic testing;ANSI standards;Error correction;Frequency;Automatic control;Application software;Software testing","","Basic block;compiler;efficiency;Fortrn;LCSAJ optimization","","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic parallelization and optimization for irregular scientific applications","M. Guo","Dept. of Comput. Software, Univ. of Aizu, Japan","18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.","","2004","","","228","","Summary form only given. Some automatic parallelization and optimization techniques for irregular scientific computing are proposed. These techniques include communication cost reduction for irregular loop partitioning, interprocedural optimization techniques for communication preprocessing when the irregular code has the procedure call, global vs. local indirection arrays remapping methods, and OpenMP directive extension for irregular computing.","","0-7695-2132","10.1109/IPDPS.2004.1303269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303269","","Concurrent computing;Cost function;Computational fluid dynamics;Optimizing compilers;Testing;Distributed computing;Application software;Cities and towns;Scientific computing;Indexing","parallel programming;message passing;optimisation;program control structures;automatic programming","automatic parallelization;scientific applications;irregular loop partitioning;interprocedural optimization techniques;communication preprocessing;procedure call;arrays remapping methods;OpenMP directive extension","","3","15","","","","","","IEEE","IEEE Conferences"
"Investigation of logistic regression as a discriminant of software quality","N. F. Schneidewind","Naval Postgraduate Sch., Monterey, CA, USA","Proceedings Seventh International Software Metrics Symposium","","2001","","","328","337","Investigates the possibility that logistic regression functions (LRFs), when used in combination with Boolean discriminant functions (BDFs), which we had previously developed, would improve the quality classification ability of BDFs when used alone; this was found to be the case. When the union of a BDF and LRF was used to classify quality, the predictive accuracy of quality and inspection cost was improved over that of using either function alone for the Space Shuttle. Also, the LRFs proved useful for ranking the quality of modules in a build. The significance of these results is that very high-quality classification accuracy (1.25% error) can be obtained while reducing the inspection cost incurred in achieving high quality. This is particularly important for safety-critical systems. Because the methods are general and not particular to the Shuttle, they could be applied to other domains. A key part of the LRF development was a method for identifying the critical value (i.e. threshold) that could discriminate between high and low quality, and at the same time constrain the cost of inspection to a reasonable value.","1530-1435","0-7695-1043","10.1109/METRIC.2001.915540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915540","","Logistics;Software quality;Inspection;Space shuttles;Cost function;Safety;Time factors;Predictive models;Testing;Quality control","software quality;forecasting theory;statistical analysis;Boolean functions;inspection;space vehicles;safety-critical software;software cost estimation;aerospace computing;subroutines;pattern classification;software metrics","logistic regression functions;Boolean discriminant functions;software quality prediction;quality classification ability;predictive accuracy;inspection cost;Space Shuttle;module quality ranking;classification accuracy;safety-critical systems;threshold identification method","","10","12","","","","","","IEEE","IEEE Conferences"
"Dynamic multiobjective optimization problems: test cases, approximations, and applications","M. Farina; K. Deb; P. Amato","STMicroelectronics, Agrate Brianza, Italy; NA; NA","IEEE Transactions on Evolutionary Computation","","2004","8","5","425","442","After demonstrating adequately the usefulness of evolutionary multiobjective optimization (EMO) algorithms in finding multiple Pareto-optimal solutions for static multiobjective optimization problems, there is now a growing need for solving dynamic multiobjective optimization problems in a similar manner. In this paper, we focus on addressing this issue by developing a number of test problems and by suggesting a baseline algorithm. Since in a dynamic multiobjective optimization problem, the resulting Pareto-optimal set is expected to change with time (or, iteration of the optimization process), a suite of five test problems offering different patterns of such changes and different difficulties in tracking the dynamic Pareto-optimal front by a multiobjective optimization algorithm is presented. Moreover, a simple example of a dynamic multiobjective optimization problem arising from a dynamic control loop is presented. An extension to a previously proposed direction-based search method is proposed for solving such problems and tested on the proposed test problems. The test problems introduced in this paper should encourage researchers interested in multiobjective optimization and dynamic optimization problems to develop more efficient algorithms in the near future.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2004.831456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347158","","Testing;Computer aided software engineering;Design optimization;Constraint optimization;Optimal control;Search methods;Optimization methods;Computational modeling;Shape;Mechanical engineering","Pareto optimisation;evolutionary computation","dynamic multiobjective optimization;multiple Pareto optimal solution;dynamic control loop;test problems;evolutionary computation","","206","35","","","","","","IEEE","IEEE Journals & Magazines"
"Resource management in software radio environment","R. Simon; S. Imre","Mobile Commun. & Comput. Lab., Budapest Univ. of Technol. & Econ., Hungary; Mobile Commun. & Comput. Lab., Budapest Univ. of Technol. & Econ., Hungary","Proceedings. Elmar-2004. 46th International Symposium on Electronics in Marine","","2004","","","443","448","The software radio concept is one of the emerging new technologies to result fast progress in the converging telecommunication and information system. This work describes the structure of the software radio system, and the role of optimization algorithms in software radio environment. The article gives overview the principle of the resource management, the alternatives of optimization, the implementing of reconfigurable resource controller and the realizing of a test environment.","1334-2630","953-7044-02","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1356416","","Environmental management;Radio spectrum management;Software radio;Resource management;Software algorithms;GSM;Physical layer;Hardware;Digital signal processing;Signal processing algorithms","4G mobile communication;software radio;optimisation;controllers;telecommunication network management","resource management;software radio system;optimization algorithms;reconfigurable resource controller","","","7","","","","","","IEEE","IEEE Conferences"
"Test coverage analysis based on program slicing","Zhenqiang Chen; Baowen Xu; Hongji Yang; Huowang Chen","Dept. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Dept. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; NA; NA","Proceedings Fifth IEEE Workshop on Mobile Computing Systems and Applications","","2003","","","559","565","Coverage analysis is a structural testing technique, which helps to eliminate gaps in a test suite and determines when to stop testing. To compute test coverage, the paper proposes a gradation model, in which different coverage have different ranks, and the test coverage of the upper layer is compute according to the coverage of all the layers from the lowest to current layer and the rank difference. To distinguish the importance of different variables, the paper proposes a new concept - coverage about variables, based on program slicing, and adds powers according to their importance. Thus we can focus on the important variables to obtain higher test coverage. In most case, the coverage obtained by our method is bigger than that obtained by a traditional measure, because the coverage about a variable takes only the codes related into account, and the gradation model takes more factors into consideration when analyzing test coverage.","","0-7803-8242","10.1109/IRI.2003.1251465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251465","","Computer science;Software testing;Computer science education;Educational programs;Software quality;Fluid flow measurement;Modems;Software engineering;Laboratories;Information processing","program testing;program slicing;software metrics;structured programming","coverage analysis;program slicing;program analysis;software testing;structural testing;gradation model;test coverage;test suite","","","25","","","","","","IEEE","IEEE Conferences"
"Automating regression testing for real-time software in a distributed environment","F. Zhu; S. Rayadurgam; Wei-Tek Tsai","Dept. of Comput. Sci., Minnesota Univ., MN, USA; NA; NA","Proceedings First International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC '98)","","1998","","","373","382","Many real time systems evolve over time due to new requirements and technology improvements. Each revision requires regression resting to ensure that existing functionality is not affected by such changes. Testing these systems often require specialized hardware and software, and both are expensive. While the overall regression testing process is similar across different organizations, the strategies and tools used by them vary according to their product needs. Hence a good framework for regression testing should provide the flexibility to configure it depending on the particular organization's needs while at the same time maximizing utilization. Manual processes are typically slow and error prone and result in under-utilization of valuable test resources. The paper proposes an automated distributed regression testing framework that provides flexibility to the user to configure it to their needs while at the same time optimizing resource usage.","","0-8186-8430","10.1109/ISORC.1998.666810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=666810","","Automatic testing;Software testing;System testing;Real time systems;Hardware;Embedded system;Pacemakers;Microprogramming;Batteries;Distributed computing","real-time systems;program testing;statistical analysis;parallel programming;automatic programming","regression testing automation;real time software;distributed environment;real time systems;regression resting;specialized hardware;product needs;test resources;automated distributed regression testing framework;resource usage;configuration management","","1","12","","","","","","IEEE","IEEE Conferences"
"Module size distribution and defect density","Y. K. Malaiya; J. Denton","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; NA","Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000","","2000","","","62","71","Data from several projects show a significant relationship between the size of a module and its defect density. We address implications of this observation. Does the overall defect density of a software project vary with its module size distribution? Even more interesting is the question can we exploit this dependence to reduce the total number of defects? We examine the available data sets and propose a model relating module size and defect density. It takes into account defects that arise due to the interconnections among the modules as well as defects that occur due to the complexity of individual modules. Model parameters are estimated using actual data. We then present a key observation that allows use of this model for not just estimation of the defect density, but also potentially optimizing a design to minimize defects. This observation, supported by several data sets examined, is that the module sizes often follow exponential distribution. We show how the two models used together provide a way of projecting defect density variation. We also consider the possibility of minimizing the defect density by controlling module size distribution.","1071-9458","0-7695-0807","10.1109/ISSRE.2000.885861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885861","","Testing;Software systems;Computer science;Parameter estimation;Design optimization;Exponential distribution;Size control;Software reliability;Debugging;Hardware","software reliability;exponential distribution;parameter estimation;program testing","software module size distribution;software defect density;software project;data sets;model parameter estimation;exponential distribution;software reliability;program testing","","16","16","","","","","","IEEE","IEEE Conferences"
"Test synthesis of systems-on-a-chip","S. Ravi; N. K. Jha","C&C Res. Labs., NEC, Princeton, NJ, USA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2002","21","10","1211","1217","Embedded systems are increasingly synthesized today as systems-on-a-chip (SOCs), wherein existing functional blocks (also called cores) are used to implement different functions in the system specification. Emphasis during system synthesis is usually on optimizing one or more objectives such as price, area, performance and power. Testability enhancement of the SOC solution so obtained follows as a postprocessing step to enable the application of precomputed test sequences to each embedded core and observe its responses. Unfortunately, performing test modifications after the SOC design has been optimized for target design constraints does not preserve the optimality of the solution obtained. In this paper, the authors present the first system-level synthesis for testability framework that integrates testability considerations into the synthesis process. Their work incorporates finite-state automata (FSA)-based symbolic testability analysis within the framework of an existing multiobjective optimization-based system synthesis tool to provide a viable solution. The experimental results show that the use of FSA-based testability analysis facilitates low test overheads and test application times without sacrificing the test coverage of the embedded cores. System synthesis through the integrated framework for a number of examples indicates that efficient SOC architectures, which tradeoff different architectural features such as integrated circuit price, power consumption, and area under real-time constraints, can now be easily generated with low testability costs.","0278-0070;1937-4151","","10.1109/TCAD.2002.802265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1043905","","System testing;Circuit testing;System-on-a-chip;Integrated circuit synthesis;Embedded system;Performance evaluation;Constraint optimization;Design optimization;Automata;Automatic testing","hardware-software codesign;embedded systems;design for testability;microprocessor chips;integrated circuit design;integrated circuit testing;finite state machines","systems-on-a-chip;embedded systems;test synthesis;functional blocks;system-level synthesis;testability framework;embedded core;finite-state automata;symbolic testability analysis;multiobjective optimization-based tool;hardware-software cosynthesis;task graph specification","","5","22","","","","","","IEEE","IEEE Journals & Magazines"
"Perturbation techniques for detecting domain errors","S. J. Zeil","Dept. of Comput. Sci., Old Dominion Univ., Norfolk, VA, USA","IEEE Transactions on Software Engineering","","1989","15","6","737","746","Perturbation testing is an approach to software testing which focuses on faults within arithmetic expressions appearing throughout a program. This approach is expanded to permit analysis of individual test points rather than entire paths, and to concentrate on domain errors. Faults are modeled as perturbing functions drawn from a vector space of potential faults and added to the correct form of an arithmetic expression. Sensitivity measures are derived which limit the possible size of those faults that would go undetected after the execution of a given test set. These measures open up an interesting view of testing, in which attempts are made to reduce the volume of possible faults which, were they present in the program being tested, would have escaped detection on all tests performed so far. The combination of these measures with standard optimization techniques yields a novel test-data-generation method called arithmetic fault detection.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=24727","","Perturbation methods;Software testing;Error correction;Fault detection;Size measurement;Volume measurement;Measurement standards;Optimization methods;Floating-point arithmetic;Computer science","error detection;perturbation techniques;program testing","error detection;sensitivity measures;software testing;arithmetic expressions;individual test points;domain errors;perturbing functions;vector space;potential faults;test set;standard optimization techniques;novel test-data-generation method;arithmetic fault detection","","22","19","","","","","","IEEE","IEEE Journals & Magazines"
"Study of the subjective performance of a range of MPEG-2 encoders","G. Keesman; A. Cotton; D. Kessler; J. De Lameillieure; J. -. Henot; A. Nicoulin; D. Kalivas","Philips Res., Eindhoven, Netherlands; NA; NA; NA; NA; NA; NA","IBC 95 International Broadcasting Convention","","1995","","","232","237","The paper presents the results of subjective tests organized in November 1994 by the HAMLET subgroup WP2. These tests, carried out in accordance with Rec. ITU-R 500-5, compare the performances of different types of MPEG-2 encoders: 1. The software encoder of the MPEG-2 technical documentation (reference), 2. An exact simulation of the hardware encoder that is being built in the same RACE project, 3. An optimized software encoder and 4. An SNR scalable encoder. The results reveal that the MPEG-2 standard leaves much room for optimization in the encoder. They also illustrate that optimizations carried out for the HAMLET hardware encoder lead to important improvements. Finally, they show that almost comparable quality of the MPEG-2 reference software encoder at 7 Mbit/s and the SNR scalable encoder at 3+4 Mbit/s for the base + enhancement layer.<<ETX>>","","0-85296-644","10.1049/cp:19950958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=475414","","Data compression;Optimization methods;Communication system software;Communication standards;Testing;Video coding","data compression;optimisation;telecommunication computing;telecommunication standards;testing;video coding","subjective performance;MPEG-2 encoders;HAMLET subgroup WP2;Rec. ITU-R 500-5;software encoder;MPEG-2 technical documentation;hardware encoder;RACE project;optimized software encoder;SNR scalable encoder;MPEG-2 standard;quality;7 Mbit/s;3 Mbit/s;4 Mbit/s","","","","","","","","","IET","IET Conferences"
"Evaluation and comparison of fault-tolerant software techniques","J. J. Hudak; B. -. Suh; D. P. Siewiorek; Z. Segall","Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Reliability","","1993","42","2","190","204","Four implementations of fault-tolerant software techniques are evaluated with respect to hardware and design faults. Project participants were divided into four groups, each of which developed fault-tolerant software based on a common specification. Each group applied one of the following techniques: N-version programming, recovery block, concurrent error-detection, and algorithm-based fault tolerance. Independent testing and modeling groups analyzed the software. The testing group subjected it to simulated design and hardware faults. The data were then mapped into a discrete-time Markov model developed by the modeling group. The effectiveness of each technique with respect to availability, correctness, and time to failure given an error, as shown by the model, is contrasted with measured data. The model is analyzed with respect to additional figures of merit identified during the modeling process, and the techniques are ranked using an application taxonomy.<<ETX>>","0018-9529;1558-1721","","10.1109/24.229487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=229487","","Fault tolerance;Fault tolerant systems;Software testing;Computational modeling;Application software;Software algorithms;Software design;Hardware;Transaction databases;Mathematical programming","fault tolerant computing;Markov processes;program testing;programming;software reliability","fault tolerant computing;software reliability;projects;algorithms;program testing;ranking;hardware;design;specification;N-version programming;recovery block;concurrent error-detection;discrete-time Markov model;modeling;availability;correctness;time to failure;error;modeling process;application taxonomy","","19","21","","","","","","IEEE","IEEE Journals & Magazines"
"Building trust into OO components using a genetic analogy","B. Baudry; Vu Le Hanh; J. -. Jezequel; Y. Le Traon","IRISA, Rennes, France; NA; NA; NA","Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000","","2000","","","4","14","Despite the growing interest for component based systems, few works tackle the question of the trust we can bring into a component. The paper presents a method and a tool for building trustable OO components. It is particularly adapted to a design-by-contract approach, where the specification is systematically derived into executable assertions (invariant properties, pre/postconditions of methods). A component is seen as an organic set composed of a specification, a given implementation and its embedded test cases. We propose an adaptation of mutation analysis to the OO paradigm that checks the consistency between specification/implementation and tests. Faulty programs, called ""mutants"", are generated by systematic fault injection in the implementation. The quality of tests is related to the mutation score, i.e. the proportion of faulty programs it detects. The main contribution is to show how a similar idea can be used in the same context to address the problem of effective test optimization. To map the genetic analogy to the test optimization problem, we consider mutant programs to be detected as the initial preys population and test cases as the predators population. The test selection consists of mutating the ""predator"" test cases and crossing them over in order to improve their ability to kill the prey population. The feasibility of component validation using such a ""Darwinian"" model and its usefulness for test optimization are studied.","1071-9458","0-7695-0807","10.1109/ISSRE.2000.885856","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885856","","Automatic testing;Built-in self-test;Contracts;Object oriented modeling;Buildings;Genetic mutations;Fault detection;System testing;Software quality;Context modeling","object-oriented programming;formal specification;program testing;software reliability;genetic algorithms","OO component trust;genetic analogy;component based systems;trustable OO components;design-by-contract approach;executable assertions;invariant properties;pre/postconditions;specification;ts embedded test cases;mutation analysis;OO paradigm;faulty programs;mutants;systematic fault injection;mutation score;test optimization;mutant programs;initial preys population;predators population;test selection;component validation;Darwinian model","","7","17","","","","","","IEEE","IEEE Conferences"
"Si-emulation: system verification using simulation and emulation","Zan Yang; Byeong Min; Gwan Choi","Dept. of Electr. Eng., Texas A&M Univ., USA; NA; NA","Proceedings International Test Conference 2000 (IEEE Cat. No.00CH37159)","","2000","","","160","169","A system-level verification framework is presented that combines the speed of hard-wired (FPGA-based) emulation and the observability of gate-level simulation. A checkpoint approach is developed for (1) periodic capturing of the machine state from an emulation, (2) sampling of the emulation output for error detection, and (3) constructing a piece-wise simulation run, necessary to debug the design in an event of an error detection from the emulation. The checkpoint frequency is optimized to reduce the cost of downloading the state data during a hardware emulation. A sampling of the emulation output also minimizes the network-bandwidth and storage-space requirements associated with instrumenting for error detection.","1089-3539","0-7803-6546","10.1109/TEST.2000.894203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894203","","Emulation;Hardware;Circuit testing;Circuit simulation;Application software;Sampling methods;Event detection;Circuit faults;Software testing;Clocks","digital simulation;observability;fault diagnosis;automatic testing;error detection;electronic equipment testing;formal verification","system verification;emulation;hard-wired emulation;observability;gate-level simulation;periodic capturing;sampling;piece-wise simulation;error detection;checkpoint frequency;cost;hardware emulation;network-bandwidth","","3","17","","","","","","IEEE","IEEE Conferences"
"Trigger condition testing and view maintenance using optimized discrimination networks","E. N. Hanson; S. Bodagala; U. Chadaga","Dept. of Comput. & Inf. Sci., Florida Univ., Gainesville, FL, USA; NA; NA","IEEE Transactions on Knowledge and Data Engineering","","2002","14","2","261","280","Presents a structure that can be used both for trigger condition testing and view materialization in active databases, along with a study of techniques for optimizing the structure. The structure presented is known as a discrimination network. The type of discrimination network introduced and studied in this paper is a highly general type of discrimination network which we call the Gator network. The structure of several alternative Gator network optimizers is described, along with a discussion of optimizer performance, output quality and accuracy. The optimizers can choose an efficient Gator network for testing the conditions of a set of triggers or optimizing maintenance of a set of views, given information about the structure of the triggers or views, database size, predicate selectivity and update frequency distribution. The efficiency of optimized Gator networks relative to alternatives is analyzed. The results indicate that, overall, Gator networks can be optimized effectively and can give excellent performance for trigger condition testing and materialization of views.","1041-4347;1558-2191;2326-3865","","10.1109/69.991716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991716","","Testing","active databases;testing;data structures;optimisation;software performance evaluation","trigger condition testing;database view maintenance;database view materialization;optimized discrimination networks;Gator network optimizer;performance;output quality;accuracy;database size;predicate selectivity;update frequency distribution;efficiency;active database systems;Rete networks;TREAT networks","","4","30","","","","","","IEEE","IEEE Journals & Magazines"
"Test case generation and reduction by automated input-output analysis","P. Saraph; M. Last; A. Kandel","Dept. of Comput. Sci. & Eng., Univ. of South Florida, Tampa, FL, USA; NA; NA","SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)","","2003","1","","768","773 vol.1","In the software testing process, selecting the test cases and verifying their results requires a lot of subjective decisions and human intervention. For a program having a large number of inputs, the number of corresponding combinatorial black-box test cases is huge. A method needs to be established in order to limit the number of test cases and to choose the most important ones. In this research effort we present a novel methodology for identifying important test cases automatically. These test cases involve input attributes which contribute to the value of an output and hence are significant. The reduction in the number of test cases is attributed to identifying input-output relationships. A ranked list of features and equivalence classes for input attributes of a given code are the main outcomes of this methodology. Reducing the number of test cases results directly in the saving of software testing resources.","1062-922X","0-7803-7952","10.1109/ICSMC.2003.1243907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243907","","Automatic testing;Computer aided software engineering;Software testing;Data mining;Computer science;System testing;Humans;Neural networks;Information analysis;Information systems","program testing;neural nets;knowledge acquisition","software testing process;automated input-output analysis;test case generation;combinatorial blackbox test;test case reduction;artificial neural networks","","5","22","","","","","","IEEE","IEEE Conferences"
"The incremental funding method: data-driven software development","M. Denne; J. Cleland-Huang","Sun MicroSysterms Inc., Mountain View, CA, USA; NA","IEEE Software","","2004","21","3","39","47","Software development projects don't get funded unless they return clearly defined value to the business. Demands for shorter investment periods, faster time-to-market, and increased agility require new, radical software development approaches. These approaches must draw on the expertise of both software architects and financial stakeholders and open the traditional black box of software development to rigorous financial analysis. We can accomplish this only by positioning software development as a value-creation activity in which business analysis is integral. The incremental funding method is a financially informed approach to software development. IFM maximizes returns by delivering functionality in ""chunks"" of customer-valued features, carefully sequenced to optimize the project's net present value (NPV). We derived the IFM concepts from several years' experience in winning competitive contracts for large-systems integration and application development projects.","0740-7459;1937-4194","","10.1109/MS.2004.1293071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1293071","","Programming;Costs;Investments;Hardware;Software tools;Sun;Personnel;Application software;Packaging;Testing","software cost estimation;project management;cost-benefit analysis;software development management","software development project;investment;time-to-market;software architects;financial stakeholders;financial analysis;value-creation activity;business analysis;data driven software development;incremental funding method;net present value;application development projects","","45","13","","","","","","IEEE","IEEE Journals & Magazines"
"Codesign model for Xpatchf optimization","B. A. Kadrovach; T. S. Wailes; A. J. Terzuoli; D. S. Gelosh","Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA; NA; NA; NA","IEEE Antennas and Propagation Society International Symposium. 1996 Digest","","1996","3","","1882","1885 vol.3","The Xpatch software package, used to generate very accurate electromagnetic scattering predictions, consists of two programs, Xpatchf and Xpatcht. These programs are for frequency domain and time domain analysis respectively. The research presented focuses on the frequency domain version, Xpatchf, which currently requires a significant amount of time to generate a scattering prediction for an average object. However, real-time radar signature prediction needs to perform these same calculations in less than one second. The objective of this research is to produce a functional codesign model of the Xpatch software to provide significant decrease in processing time.","","0-7803-3216","10.1109/APS.1996.549971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=549971","","Hardware;Algorithms;Electromagnetic scattering;Frequency domain analysis;Radar scattering;Application specific integrated circuits;Software packages;Time domain analysis;System testing;Microprocessors","radar cross-sections;radar computing;software packages;frequency-domain analysis;optimisation;computational complexity","Xpatchf optimization;Xpatch software package;electromagnetic scattering predictions;Xpatchf;Xpatcht;frequency domain analysis;time domain analysis;real-time radar signature prediction;processing time reduction","","2","4","","","","","","IEEE","IEEE Conferences"
"Optimization problems in electromagnetics","J. Simkin; C. W. Trowbridge","Vector Fields Ltd., Oxford, UK; Vector Fields Ltd., Oxford, UK","IEEE Transactions on Magnetics","","1991","27","5","4016","4019","A software environment for optimizing electromagnetic devices is described. The designer can interactively construct a computer model for the device to be optimized, select the variables with their constraints and create a suitable objective function for minimization. The software is designed to exploit the capabilities of a powerful command decoder with an algebraic interpreter. Some preliminary tests using both classical optimization methods involving linear searches and a stochastic method based on the simulated annealing algorithm are described.<<ETX>>","0018-9464;1941-0069","","10.1109/20.104982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=104982","","Constraint optimization;Design optimization;Optimization methods;Shape;Coils;Magnetic resonance imaging;Search methods;Magnetic devices;Software design;Decoding","CAD;electrical engineering computing;electromagnetic devices;electromagnets;optimisation","optimisation;coil design;electromagnetics;software environment;electromagnetic devices;computer model;objective function;minimization;command decoder;algebraic interpreter;linear searches;stochastic method;simulated annealing algorithm","","40","9","","","","","","IEEE","IEEE Journals & Magazines"
"QUEM: an achievement test for knowledge-based systems","C. C. Hayes; M. I. Parzen","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; NA","IEEE Transactions on Knowledge and Data Engineering","","1997","9","6","838","847","This paper describes the Quality and Experience Metric (QUEM), a method for estimating the skill level of a knowledge based system based on the quality of the solutions it produces. It allows one to assess how many years of experience the system would be judged to have if it were a human by providing a quantitative measure of the system's overall competence. QUEM can be viewed as a type of achievement or job placement test administered to knowledge based systems to help system designers determine how the system should be used and by what level of user. To apply QUEM, a set of subjects, experienced judges, and problems must be identified. The subjects should have a broad range of experience levels. Subjects and the knowledge based system are asked to solve the problems; and judges are asked to rank order all solutions, from worst quality to best. The data from the subjects is used to construct a skill function relating experience to solution quality, and confidence bands showing the variability in performance. The system's quality ranking is then plugged into the skill function to produce an estimate of the system's experience level. QUEM can be used to gauge the experience level of an individual system, to compare two systems, or to compare a system to its intended users. This represents an important advance in providing quantitative measures of overall performance that can be applied to a broad range of systems.","1041-4347;1558-2191;2326-3865","","10.1109/69.649311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649311","","System testing;Knowledge based systems;Humans;Problem-solving;Acoustic testing;Knowledge engineering;Databases;Costs;Manufacturing","knowledge based systems;knowledge verification;software performance evaluation;software metrics;software quality","QUEM;achievement test;knowledge based systems;Quality and Experience Metric;system skill level estimation;solution quality;quantitative measure;job placement test;experience levels;skill function;performance","","5","19","","","","","","IEEE","IEEE Journals & Magazines"
"Shifting perspectives on DFM","J. Sawicki","Mentor Graphics Corp., Beaverton, OR, USA","Sixth international symposium on quality electronic design (isqed'05)","","2005","","","19","","Summary form only given. There is one universal truth in terms of design for manufacturing (DFM) - DFM tools and disciplines have always existed. In micron technologies, DFM methodologies were applied to ensure acceptable yield and adequate test coverage. However, nanometer technology has ushered in new and significant yield and manufacturing considerations and constraints. The lack of a major increase in yield improvement between the 350 nm and 180 nm nodes suggests that yield loss mechanisms are not only increasing, but they are increasing fast enough that 'cosmetic' improvements in tools and methodologies are largely offset. If EDA tools are to assist the semiconductor industry at the 90 nm and 65 nm nodes, there must be profound changes to existing tools, and the introduction of new technologies that allow designers to consider and optimize for manufacturing at each stage of the design, verification, tapeout and test process. Where will these new tools and capabilities appear? They will show up in all parts of the design flow, and also on the manufacturing floor. In particular, an immediate focus for the EDA industry must be to deliver new technology in four key areas: process modeling (electrical and lithographic); statistical analysis and visualization; design optimization; test and inspection.","1948-3287;1948-3295","0-7695-2301","10.1109/ISQED.2005.109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410551","","Design for manufacture;Electronic design automation and methodology;Design optimization;Electronics industry;Manufacturing processes;Semiconductor device manufacture;Semiconductor device testing;Manufacturing industries;Statistical analysis;Visualization","design for manufacture;integrated circuit design;nanoelectronics;integrated circuit yield;electronic design automation;software tools;integrated circuit modelling;semiconductor process modelling;statistical analysis;data visualisation;optimisation;integrated circuit testing;inspection","design for manufacturing;DFM tools;nanometer technology;manufacturing considerations;yield loss mechanisms;EDA tools;design flow;manufacturing floor;electrical process modeling;lithographic process modeling;statistical analysis;visualization;design optimization;test;inspection;350 to 65 nm","","","","","","","","","IEEE","IEEE Conferences"
"Common test data language and tools improve quality and reduce cost","D. M. McKinstry","Texas Instrum. Ltd., Lewisville, TX, USA","1997 IEEE Autotestcon Proceedings AUTOTESTCON '97. IEEE Systems Readiness Technology Conference. Systems Readiness Supporting Global Needs and Awareness in the 21st Century","","1997","","","544","550","Parametric test data have been used for years to improve both product design and manufacturing processes. Historically these data have been program specific and have required teams of specialists. Different programs would have different methods of gathering and managing the data. Different people could analyze the same data with different custom tools and come up with different results. Recently we have been using a common test data exchange language to enable common tools and analysis methods. This data language, a precursor to proposed IEEE Standard 1389, has been successfully deployed to several different programs. Each program has its own test equipment, procedures, and goals for data analysis. Yet all programs have benefited from the common test data language and tools.","","0-7803-4162","10.1109/AUTEST.1997.633674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633674","","Testing;Costs;Data analysis;Instruments;Product design;Manufacturing processes;Data engineering;Independent component analysis;Memory;Design engineering","automatic test software;data analysis;military computing;production testing;optimisation;printed circuit testing","product testing;cost;parametric test data;product design;manufacturing processes;common test data exchange language;common tools;IEEE Standard 1389;data analysis;optimisation;US Defense;Texas Instruments","","","2","","","","","","IEEE","IEEE Conferences"
"Applying Moore's technology adoption life cycle model to quality of EDA software","G. Ben-Yaacov; E. P. Stone; R. Goldman","Synopsys Inc., USA; NA; NA","Proceedings of the IEEE 2001. 2nd International Symposium on Quality Electronic Design","","2001","","","76","80","This paper describes a methodology for allocating priority levels and resources to quality activities during the development of EDA software projects. Geoffrey Moore's technology adoption life cycle model is used to provide a baseline understanding of what the market and the target users require at any point in time during the product life cycle. Applying this model, EDA software development teams can make choices and prioritize quality objectives which are based on the customer segment that they are targeting at any point in time during the product life cycle.","","0-7695-1025","10.1109/ISQED.2001.915209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915209","","Electronic design automation and methodology;Software quality;Time to market;Product development;Software tools;Software maintenance;Customer satisfaction;Resource management;Software testing;Filling","electronic design automation;software quality","Moore's technology adoption life cycle model;EDA software quality;priority level allocation;quality activities;EDA software projects;software development teams;product life cycle","","1","","","","","","","IEEE","IEEE Conferences"
"Location of checkpoints in fault-tolerant software","F. Saglietti","Gesellschaft fuer Reaktorsicherheit mbH Forschungsgelande, Garching, Germany","Proceedings of the 5th Jerusalem Conference on Information Technology, 1990. 'Next Decade in Information Technology'","","1990","","","270","277","Information reduction throughout a program is studied, identifying its impact on the effectiveness of checkpoints. The discussion covers failure masking, function classes that reduce information, the impact of information reduction on failure dependence, information reduction for binary values, and location of checkpoints. The conclusions reported and the strategy suggested are intended to support decision-making during the development of fault-tolerant software by identifying which internal variables or intermediate results should be checked by means of diversity in order to optimize fault-tolerance achievement.<<ETX>>","","0-8186-2078","10.1109/JCIT.1990.128295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=128295","","Fault tolerance;Software testing;Parallel programming;Decision making;Fault diagnosis;Software reliability;Reliability engineering;Sequential analysis;Information analysis;Probability","fault tolerant computing;program testing;redundancy;software engineering","fault-tolerant software;failure masking;function classes;failure dependence;information reduction;internal variables;intermediate results","","","4","","","","","","IEEE","IEEE Conferences"
"Cautious optimism for the future (codesign research)","P. A. Subrahmanyam","AT&T Bell Lab., Holmdel, NJ, USA","Computer","","1993","26","1","84","","The expectations, experimental results, and open issues relating to codesign research are discussed. Codesign refers to the integrated design of systems implemented using both hardware and software components. It is argued that the renewed interest in codesign is largely explained by the increasing diversity and complexity of applications employing embedded systems; the need to decrease the cost of designing and testing such systems; and advances in some key enabling technologies.<<ETX>>","0018-9162;1558-0814","","10.1109/2.179165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=179165","","Hardware;System testing;Software testing;Software performance;Computational modeling;Application software;Embedded system;Costs;Integrated circuit testing;Integrated circuit synthesis","concurrent engineering;integrated software","codesign research;integrated design of systems;hardware;software components;diversity;complexity","","11","4","","","","","","IEEE","IEEE Journals & Magazines"
"Logic optimization by an improved sequential redundancy addition and removal technique","U. Glaser; Kwang-Ting Cheng","Syst. Design Technol. Inst., German Nat. Res. Center for Comput. Sci., St. Augustin, Germany; NA","Proceedings of ASP-DAC'95/CHDL'95/VLSI'95 with EDA Technofair","","1995","","","235","240","Logic optimization methods using automatic test pattern generation (ATPG) techniques such as redundancy addition and removal have recently been proposed. We generalize this approach for synchronous sequential circuits. We proposed several new sequential transformations which can be efficiently identified and used for optimizing large designs. One of the new transformations involves adding redundancies across time frames in a sequential circuit. We also suggest a new transformation which involves adding redundancies to block initialization of other wires. We use efficient sequential ATPG techniques to identify more sequential redundancies for either addition or removal. We have implemented a sequential logic optimization system based upon this approach. We show experimental results to demonstrate that this approach is both CPU time efficient and memory efficient and can optimize large sequential designs significantly.","","4-930813-67","10.1109/ASPDAC.1995.486229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=486229","","Logic;Redundancy;Wires;Sequential circuits;Automatic test pattern generation;Circuit faults;Circuit testing;Fault diagnosis;Combinational circuits;Optimization methods","logic testing;sequential circuits;automatic test software;redundancy","logic optimization;improved sequential redundancy addition;automatic test pattern generation;ATPG;redundancy removal;synchronous sequential circuits;sequential transformations;time frames;block initialization;ATPG techniques;sequential redundancies;sequential logic optimization system;CPU time efficient;large sequential design optimisation","","7","14","","","","","","IEEE","IEEE Conferences"
"Design of automated test and evaluation system architecture for a smart proving ground","Kyu-Chang Kang; Dong-Eun Heo","NA; NA","IEEE International Symposium on Virtual Environments, Human-Computer Interfaces and Measurement Systems, 2003. VECIMS '03. 2003","","2003","","","26","31","There are many choices of structuring a test and evaluation system in the proving ground. We need systems that are connected and automated to integrate many different components and parts supplied by different manufacturers. To get a highly optimized system, we need a flexible framework enabling to plug/unplug components in a dynamic fashion, function-based components, and security considerations based on RBAC (role-based access control). OSGi (Open service Gateway initiative) is one solution for a flexible framework. We design the test and evaluation system architecture allowing hot-pluggable function components to be installed/managed and controlled by remote users based on role based access. This system has extensible and scalable characteristics using the OSGi-compatible framework, as well as distributed processing characteristics.","","0-7803-7785","10.1109/VECIMS.2003.1227025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227025","","Automatic testing;System testing;Weapons;Sequential analysis;Fires;Control systems;Projectiles;Delay;Computer architecture;Embedded computing","automatic testing;systems analysis;software architecture;authorisation;weapons;military systems;military computing","system design;automated testing;test system;evaluation system;system architecture;system structure;connected system;automated system;optimized system;plug/unplug component;function-based component;security;RBAC;role-based access control;Open service Gateway initiative;flexible framework;hot-pluggable function component;remote installation;remote management;remote control;remote user;extensible characteristic;scalable characteristic;OSGi-compatible framework;distributed processing characteristic;weapon systems;subsystem role;smart proving ground","","","6","","","","","","IEEE","IEEE Conferences"
"Flip-flop chaining architecture for power-efficient scan during test application","S. Gupta; T. Vaish; S. Chattopadhyay","IIT Guwahati North Guwahati, Assam; NA; NA","14th Asian Test Symposium (ATS'05)","","2005","","","410","413","Power dissipation in CMOS circuits during test time poses a crucial bottleneck for circuit performance and robustness. The power consumption due to switching activity while scan-in of test vectors and scan-out of responses is of particular concern. In this paper a methodology for scan chain modification and test vector adaptation is proposed to effectively reduce the scan test power consumption by controlling this switching activity. Proposed approach, unlike the many in published literature, does not incorporate reordering of scan cells; thus avoiding timing and routing overheads. ATPG software ATALANTA was used for test vector generation. The algorithm was verified for ISCAS&#146;89 benchmark circuits, where it showed as much as 27.3% of reduction in switching activity during scan operations.","1081-7735;2377-5386","0-7695-2481","10.1109/ATS.2005.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575464","","Flip-flops;Circuit testing;Energy consumption;Power dissipation;Circuit optimization;Robustness;Timing;Routing;Automatic test pattern generation;Software testing","","","","14","12","","","","","","IEEE","IEEE Conferences"
"Test signal reduction in radiated EMI measurements","W. Schaefer","Signal Anal. Product Generation Unit, Agilent Technol. Inc., Santa Rosa, CA, USA","IEEE International Symposium on Electromagnetic Compatibility. Symposium Record (Cat. No.00CH37016)","","2000","2","","717","721 vol.2","Radiated EMI compliance measurements are very time consuming, even when a high degree of automation is used. This is particularly true for measurements made on an open area test site (OATS), due to the presence of ambient signals and weather conditions. The overall test time is largely dependent on the number of signals emanating from the equipment under test (EUT), because their amplitudes must be maximized and measured. The maximization process involves changes in antenna height and polarization, as well as rotation of the EUT. It is mandatory to identify only the relevant signals for maximization and final measurement by using adequate test procedures, selecting proper system parameters, and performing tasks like signal discrimination efficiently. This paper first describes how relevant signals can be discerned in data traces of a swept EMI receiver by using a software algorithm. The significance of a user definable parameter of this algorithm, ""peak excursion"", is discussed. The overall measurement process also determines the test time. The application of amplitude comparisons to limit lines or margins, comparisons of signal lists and acoustic identification of signals, further reduce the number of emissions throughout the measurement process. An example of this process is outlined. Other tasks like the discrimination between ambient signals and EUT emissions or between EUT emissions and auxiliary equipment also contribute to the reduction of signals. A tool to efficiently accomplish this discrimination task, the polar plot, is also discussed.","","0-7803-5677","10.1109/ISEMC.2000.874709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874709","","Testing;Electromagnetic interference;Signal processing;Time measurement;Open area test sites;Antenna measurements;Acoustic measurements;Automation;Area measurement;Particle measurements","electromagnetic interference;optimisation;electromagnetic wave polarisation;receivers;software packages;conformance testing","test signal reduction;open area test site;radiated EMI compliance measurements;OATS measurements;equipment under test;maximization process;antenna height;antenna polarization;EUT rotation;test procedures;system parameters;signal discrimination;data traces;swept EMI receiver;software algorithm;test time;acoustic identification;EUT emissions;polar plot;EMI compliance measurements","","","","","","","","","IEEE","IEEE Conferences"
"Optimizing C++ vector expressions","J. F. Blinn","Microsoft Res., USA","IEEE Computer Graphics and Applications","","2000","20","4","97","103","Recently, I've been entertaining myself by studying all the nifty new programming techniques that have been invented since I was in school. I started with C++ and object-oriented programming. I'm now progressing through generic programming, aspect-oriented programming, partial evaluation and generative programming. What I'm really interested in is whether all these tricks work in the real world of graphics programming. My answer so far is ""Yes, but..."". To see ""but what?"", I define one of the problems I want to solve in in this article. I wanted to have a programming language that defined vectors and the arithmetic operations between them. C++ allows me to do this, but there are various pitfalls in doing this well. This article addresses one of these pitfalls: the speed of execution of vector arithmetic. The conventional approach turns out to be somewhat slow, but there's a very tricky technique that can make vector arithmetic very fast. It's based on the work of T. Veldhuizen (1998) and uses the C++ template mechanism in bizarre and unexpected ways.","0272-1716;1558-1756","","10.1109/38.851757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=851757","","Timing;Optimizing compilers;Arithmetic;Books;Testing;Writing;Software engineering","C++ language;object-oriented programming;vectors;arithmetic;mathematics computing;software performance evaluation;optimisation;computer graphics","C++ vector expression optimization;object-oriented programming;generic programming;aspect-oriented programming;partial evaluation;generative programming;graphics programming;programming language;arithmetic operations;execution speed;vector arithmetic;C++ template mechanism","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Generating test cases for GUI responsibilities using complete interaction sequences","L. White; H. Almezen","Dept. of Electr. Eng. & Comput. Sci., Case Western Reserve Univ., Cleveland, OH, USA; NA","Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000","","2000","","","110","121","Testing graphical user interfaces (GUI) is a difficult problem due to the fact that the GUI possesses a large number of states to be tested, the input space is extremely large due to different permutations of inputs and events which affect the GUI, and complex GUI dependencies which may exist. There has been little systematic study of this problem yielding a resulting strategy which is effective and scalable. The proposed method concentrates upon user sequences of GUI objects and selections which collaborate, called complete interaction sequences (CIS), that produce a desired response for the user. A systematic method to test these CIS utilizes a finite-state model to generate tests. The required tests can be substantially reduced by identifying components of the CIS that can be tested separately. Since consideration is given to defects totally within each CIS, and the components reduce required testing further, this approach is scalable. An empirical investigation of this method shows that substantial reduction in tests can still detect the defects in the GUI. Future research will prioritize testing related to the CIS testing for maximum benefit if testing time is limited.","1071-9458","0-7695-0807","10.1109/ISSRE.2000.885865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885865","","Computer aided software engineering;Graphical user interfaces;Computational Intelligence Society;System testing;Collaboration;Application software;Logic;Explosions","program testing;graphical user interfaces;finite state machines","test case generation;GUI;complete interaction sequences;graphical user interfaces;finite-state model;software testing","","64","14","","","","","","IEEE","IEEE Conferences"
"A methodology and a tool for the computer aided design with constraints of electrical devices","F. Wurtz; J. Bigeon; C. Poirson","Lab. d'Electrotech., CNRS, Grenoble, France; Lab. d'Electrotech., CNRS, Grenoble, France; NA","IEEE Transactions on Magnetics","","1996","32","3","1429","1432","A methodology for the computer aided constrained design of electrical devices is presented and validated through the design of a slotless permanent magnet structure. It is based on the use of the analytical design equations of the device. Symbolic calculation is widely used to generate an analysis program and a sensitivity computation program. Those programs are linked with an optimisation algorithm that can take constraints into account. The methodology is tested with an experimental software named PASCOSMA.","0018-9464;1941-0069","","10.1109/20.497516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497516","","Magnetic analysis;Permanent magnets;Equations;Constraint optimization;Software testing","CAD;electrical engineering computing;optimisation;symbol manipulation;computer aided software engineering;permanent magnets;software tools","computer aided design;electrical devices;constrained design;slotless permanent structure;analytical design equations;symbolic calculation;analysis program;sensitivity computation program;optimisation algorithm;experimental software;PASCOSMA","","57","9","","","","","","IEEE","IEEE Journals & Magazines"
"On the effectiveness of the optimally refined proportional sampling testing strategy","F. T. Chan; I. K. Mak; T. Y. Chen; S. M. Shen","Hong Kong Univ., Hong Kong; NA; NA; NA","STEP '99. Proceedings Ninth International Workshop Software Technology and Engineering Practice","","1999","","","95","104","Recently, the effectiveness of subdomain testing and random testing has been studied analytically. T.Y. Chen and Y.T. Yu (1994) found that, for the case of disjoint subdomains, as long as the number of test cases selected from each subdomain is proportional to its size (the proportional sampling strategy), the probability of revealing at least one failure using subdomain testing is not less than that using random testing. This paper investigates the effectiveness of the optimally refined proportional sampling (ORPS) strategy, which is a special case of the proportional sampling strategy. The ORPS strategy is simple in concept, and the implementation cost is usually low. An empirical study has been conducted for a sample of published programs with seeded errors. The performance of this strategy was found to be better than random testing.","","0-7695-0328","10.1109/STEP.1999.798483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=798483","","Sampling methods;Testing;Continuing education;Computer science;Software engineering;Vocational training;Councils;Statistical analysis;Radio access networks;Read only memory","program testing;sampling methods;optimisation;probability","optimally refined proportional sampling testing strategy;subdomain testing;random testing;disjoint subdomains;test case number;failure detection probability;implementation cost;seeded errors;performance;program testing;partition testing","","2","11","","","","","","IEEE","IEEE Conferences"
"RF synthetic instrumentation: ATS technology insertion and implications","K. J. Krizman; J. A. Duvall","Lockheed Martin Inf. Syst., Orlando, FL, USA; Lockheed Martin Inf. Syst., Orlando, FL, USA","Proceedings AUTOTESTCON 2003. IEEE Systems Readiness Technology Conference.","","2003","","","432","435","This paper describes an evaluation of RF synthetic instrumentation (RFSI) technology for potential inclusion into upgrade and optimization efforts for automatic test systems (ATSs). We examine basic software radio technology and its potential to emulate discrete RF measurement instrumentation, and/or RFSI. We discuss performance criteria for RF ATE and constraints imposed on RFSI implementations due to limitations in the digitization process (A-to-D conversion and D-to-A conversion) and digital signal processing (DSP). We discuss trade-offs between discrete and synthetic test system architectures and briefly consider the conceptual outlook for RFSI technology insertion into established ATS product lines.","1080-7725","0-7803-7837","10.1109/AUTEST.2003.1243609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243609","","Radio frequency;Instruments;Software radio;Software testing;System testing;Software measurement;Signal processing;RF signals;Digital signal processing;Computer architecture","virtual instrumentation;automatic test equipment;software radio;analogue-digital conversion;digital-analogue conversion","RF synthetic instrumentation;ATS technology insertion;RFSI technology;automatic test systems;software radio technology;measurement instrumentation emulation;RF test;RF measurement;RF ATE;digitization process limitations;A-to-D conversion;D-to-A conversion;digital signal processing;DSP","","","9","","","","","","IEEE","IEEE Conferences"
"IQ tests for smart databases: the airborne self-protection jammer program performance prediction software","D. Olliff","Electron. Warfare Syst., Point Mugu, CA, USA","IEEE Aerospace and Electronic Systems Magazine","","1992","7","3","16","19","The making of UDFs (user data files), which are loaded into electronic warfare devices on tactical aircraft, is considered a simple case of a flexible manufacturing system. The UDFG (user data file generator) for the ASPJ (airborne self-protection jammer) consists of a set of database and text editing routines to incorporate entries concerning threat data, countermeasures options, and aircraft-dependent information in a user-friendly manner and rapidly merge the inputs into machine-loadable files. Expert system capabilities allow system analysts to tailor search sector strategy for the set of threats involved. Menu selection of threats from a library of data stored in the UDFG's database and tailoring of search sectors allows for an astronomical variety of scenarios to be specified with optimization of the jammer's response to the set of threats included. Whenever running the UDFG, designers are able to select and modify the electronic countermeasures programs to be used against the set of threats, taking advantage of changes in the demand for jamming resources due to changes in the set of threats.<<ETX>>","0885-8985;1557-959X","","10.1109/62.141897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=141897","","Automatic testing;Databases;Jamming;Aerospace electronics;Electronic countermeasures;Electronic warfare;Aircraft manufacture;Flexible manufacturing systems;Expert systems;Libraries","aerospace computing;database management systems;electronic countermeasures;expert systems;jamming;military computing;performance evaluation;software engineering","expert systems;IQ tests;smart databases;airborne self-protection jammer;program performance prediction software;user data files;electronic warfare devices;tactical aircraft;flexible manufacturing system;user data file generator;text editing;threat data;countermeasures","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal and near-optimal test sequencing algorithms with realistic test models","V. Raghavan; M. Shakeri; K. Pattipati","Mathworks Inc., Natick, MA, USA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","","1999","29","1","11","26","In this paper, we first present the formulation and solution of the basic test sequencing problem. We then consider generalized test sequencing problems that incorporate various practical features such as precedence constraints and setup operations for tests, multi-outcome tests, modular diagnosis, and rectification. We develop various AO/sup */ and information heuristic-based algorithms to solve these practical test sequencing problems. We also discuss the issues involved in implementation of the test sequencing algorithms for solving large problems efficiently, and show that our preprocessing techniques result in considerable speed-ups.","1083-4427;1558-2426","","10.1109/3468.736357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=736357","","System testing;Costs;Fault diagnosis;Performance evaluation;Heuristic algorithms;Aircraft;Aerospace electronics;Software testing;Engines;Artificial satellites","fault diagnosis;testing;optimisation;heuristic programming","near-optimal test sequencing algorithms;test models;precedence constraints;setup operations;multi-outcome tests;modular diagnosis;rectification;AO/sup */ algorithms;information heuristic-based algorithms;preprocessing techniques","","42","29","","","","","","IEEE","IEEE Journals & Magazines"
"Power optimization of system-level address buses based on software profiling","W. Fornaciari; M. Polentarutti; D. Sciuto; C. Silvano","Dipt. di Elettronica e Inf., Politecnico di Milano, Italy; NA; NA; NA","Proceedings of the Eighth International Workshop on Hardware/Software Codesign. CODES 2000 (IEEE Cat. No.00TH8518)","","2000","","","29","33","The paper aims at defining a methodology for the optimization of the switching power related to the processor-to memory communication on system-level buses. First, a methodology to profile the switching activity related to system-level buses has been defined, based on the tracing of benchmark programs running on the Sun SPARC V8 architecture. The bus traces have been analyzed to identify temporal correlations between consecutive patterns. Second, a framework has been set up for the design of high-performance encoder/decoder architectures to reduce the transition activity of the system-level buses. Novel bus encoding schemes have been proposed, whose performance has been compared with the most widely adopted power-oriented encodings. The experimental results have shown that the proposed encoding techniques provide an average reduction in transition activity up to 74.11% over binary encoding for instruction address streams. The results indicate the suitability of the proposed techniques for high-capacitance wide buses, for which the power saving due to the transition activity reduction is not offset by the extra power dissipation introduced in the system by the encoding/decoding logic.","","1-58113-268","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=843702","","Encoding;Decoding;Communication switching;Optimization methods;Communication system software;Circuits;Permission;Sun;Pattern analysis;Power dissipation","hardware-software codesign;encoding;decoding;program testing","power optimization;system-level address buses;software profiling;switching power;processor-to memory communication;benchmark programs;Sun SPARC V8 architecture;temporal correlations;high-performance encoder/decoder architectures;power-oriented encodings;encoding/decoding logic","","13","9","","","","","","IEEE","IEEE Conferences"
"Diagnostic tests for communicating nondeterministic finite state machines","R. Belhassine-Cherif; A. Ghedamsi","ENIT, Le Belvedere, Tunisia; NA","Proceedings ISCC 2000. Fifth IEEE Symposium on Computers and Communications","","2000","","","424","429","Systematic test sequence generation for conformance testing of communication protocol implementations, has been an active research area during the last decade. Methods were developed to produce optimized test sequences for detecting faults in such systems. However the application of these methods gives only limited information about the location of detected faults. In this paper we propose a complementary step, which localizes the fault, once detected. It consists of a generalized diagnostic algorithm for the case where distributed system specifications (implementations) are given in the form of communicating nondeterministic finite state machines. Such algorithm localizes the faulty transition once the fault has been detected. The algorithm guarantees the correct diagnosis of any single (output and/or transfer) fault. A simple example is used to demonstrate the functioning of the proposed algorithm. The complexity of each step in the algorithm and hence, the overall complexity are calculated.","","0-7695-0722","10.1109/ISCC.2000.860675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860675","","Automata;System testing;Fault detection;Protocols;Fault diagnosis;Software testing;Artificial intelligence;Medical diagnostic imaging;Joining processes","finite state machines;conformance testing;sequences;fault location;program testing;fault diagnosis","diagnostic tests;communicating nondeterministic finite state machines;test sequence generation;conformance testing;communication protocol implementations;optimized test sequences;fault detection;generalized diagnostic algorithm;distributed system specifications;complexity;software domain","","","11","","","","","","IEEE","IEEE Conferences"
"Optimizing HW/SW codesign towards reliability for critical-application systems","F. Vargas; E. Bezerra; L. Wulff; D. Barros","Electr. Eng. Dept., Catholic Univ., Porto Alegre, Brazil; NA; NA; NA","Proceedings Seventh Asian Test Symposium (ATS'98) (Cat. No.98TB100259)","","1998","","","52","57","This work presents an innovative approach for hardware/software codesign of safety-critical computing systems. The proposed approach is based on system reliability requirements to decide which parts of the system are partitioned into hardware or software. The approach considers as input a complete software description of the design. In our case, we use as the initial description the C language and then, for those parts compiled to hardware, the Handel-C language is applied. After partitioning, we verify system reliability based on an adaptation of the weak mutation analysis technique. This technique was originally proposed for software testing by means of verifying the adequacy of a test vector set for a given program. We also present a case study in order to illustrate the proposed approach.","1081-7735","0-8186-8277","10.1109/ATS.1998.741584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=741584","","Algorithm design and analysis;Partitioning algorithms;Costs;Hardware;Digital systems;Application software;Time factors;System testing;Software safety;Genetic mutations","hardware-software codesign;safety-critical software;C language","HW/SW codesign;reliability;critical-application systems;safety-critical computing systems;system reliability requirements;C language;Handel-C language;weak mutation analysis technique;test vector set","","8","18","","","","","","IEEE","IEEE Conferences"
"Effects of refactoring legacy protocol implementations: a case study","B. Geppert; F. Rossler","NA; NA","10th International Symposium on Software Metrics, 2004. Proceedings.","","2004","","","14","25","We report on our experience of applying collaboration-based protocol design in combination with software refactoring as enabling technologies for re-engineering legacy protocol implementations. We have re-engineered a subsystem of a large enterprise communications product. The subsystem implements a standards-based communication protocol with numerous proprietary extensions. Due to many enhancements which the code has undergone, it showed clear signs of design degradation. The business purpose of the re-engineering project was to improve intelligibility and changeability of the code without changing or breaking existing functionality and without imposing a significant performance penalty. We used the re-engineering effort as experimental context for evaluating the enabling technologies. This article reports on our findings and discusses why collaboration-based protocol design in combination with software refactoring worked well in achieving success with our re-engineering effort.","1530-1435","0-7695-2129","10.1109/METRIC.2004.1357887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357887","","Protocols;Computer aided software engineering;Collaborative software;Collaboration;Collaborative work;Degradation;Process design;Standards organizations;Testing;Design optimization","systems re-engineering;software development management;project management;software metrics;protocols;groupware;software maintenance","collaboration-based protocol;software refactoring;software re-engineering;communication protocol;project management;software metrics","","2","14","","","","","","IEEE","IEEE Conferences"
"Datapath optimization using feedback","D. W. Knapp","Illinois Univ., Urbana, IL, USA","Proceedings of the European Conference on Design Automation.","","1991","","","129","134","Describes recent experience with Fasolt, a software tool that automatically optimizes a register-level datapath. Fasolt uses a model of layout to drive the choice of optimizing transformations at the levels of scheduling and allocation; hence it is a feedback-driven optimization system. In choosing transformations, Fasolt takes placement and wiring into account in a way that has not been demonstrated in any other high-level synthesis system. Fasolt is also cyclic in that high-level transformations trigger changes at lower levels, which after analysis trigger further high-level changes. This implementation of Fasolt has an expanded set of transformation rules, timing-driven and area-driven transformations, and improved layout modeling capability. The authors present experimental results on three basic test cases and two major variations on the layout software.<<ETX>>","","","10.1109/EDAC.1991.206375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=206375","","Feedback;Costs;Wiring;High level synthesis;Logic testing;Software tools;Software testing;Very large scale integration;Hardware design languages;Software algorithms","circuit layout CAD;feedback;software tools;VLSI","register level datapath optimisation;Fasolt;software tool;model of layout;choice of optimizing transformations;levels of scheduling and allocation;feedback-driven optimization system;choosing transformations;high-level synthesis system;high-level transformations;expanded set of transformation rules;area-driven transformations;layout modeling capability;experimental results;test cases","","9","18","","","","","","IEEE","IEEE Conferences"
"Evaluation of Maintenance Software in Real-Time Systems","Gay","Department of Advanced Switching Networks, Bell Laboratories","IEEE Transactions on Computers","","1978","C-27","6","576","582","Statistical theory can be applied to data collected by physical fault insertion of small, random fault samples to estimate the critical parameters of software responsible for fault recovery, detection, and resolution in real-time systems. Estimates of the critical parameters can be used to determine if system reliability objectives are satisfied. This method of software evaluation, modified because of the impracticality of selecting and physically inserting truly random fault samples, was verified against a diagnostic/ Trouble Locating Program (TLP) whose critical parameters were known and was then used to evaluate and optimize a new diagnostic/ TLP program of unknown quality. Empirical evidence indicates that there is a close correlation between system performance against real faults and against the selected subset of real faults used for sampling.","0018-9340;1557-9956;2326-3814","","10.1109/TC.1978.1675151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1675151","Critical parameter estimation;diagnostic programs;maintenance software performance evaluation;physical fault insertion;software reliability;software testing and evaluation","","","Critical parameter estimation;diagnostic programs;maintenance software performance evaluation;physical fault insertion;software reliability;software testing and evaluation","","1","5","","","","","","IEEE","IEEE Journals & Magazines"
"A 'crystal ball' for software liability","J. Voas; G. McGraw; L. Kassab; L. Voas","Reliable Software Technologies, 21515 Ridgetop Cir., Suite 250, Sterling, VA, USA; NA; NA; NA","Computer","","1997","30","6","29","36","Software developers are living in a liability grace period, but it won't last. To adequately insure themselves against potential liability, developers need tools to identify worst-case scenarios and help them quantify the risks associated with a piece of software. For assessing such risks associated with software, the authors recommend fault injection, which provides worst-case predictions about how badly a piece of code might behave and how frequently it might behave that way. By contrast, software testing states how good software is. But even correct code can have ""bad days"", when external influences keep it from working as desired. Fault injection is arguably the next best thing to having a crystal ball, and it certainly beats facing the future with no predictions at all. It should be a regular part of risk assessment. The greatest benefit from fault injection occurs when a piece of software does not tolerate injected anomalies. False optimism gives way to the only honest claim-that the software presents risks.","0018-9162;1558-0814","","10.1109/2.587545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=587545","","Insurance;Law;Legal factors;Software performance;Companies;Costs;Computer industry;Information systems;Software quality;Programming","DP industry;risk management;software fault tolerance;software engineering;product liability;system monitoring","software developers;software liability;potential liability insurance;worst-case scenarios;risk quantification;risk assessment;fault injection;software testing;correct code;external influences;injected anomaly tolerance","","16","12","","","","","","IEEE","IEEE Journals & Magazines"
"Testing and optimizing ADC performance: a probabilistic approach","N. Giaquinto; M. Savino; A. Trotta","Dept. of Electr. & Electron., Bari Polytech., Italy; Dept. of Electr. & Electron., Bari Polytech., Italy; Dept. of Electr. & Electron., Bari Polytech., Italy","IEEE Transactions on Instrumentation and Measurement","","1996","45","2","621","626","A novel approach to the topic of analog-to-digital converter (ADC) characterization is proposed. The key idea is to describe the behaviour of the device via a suitable conditional probability function, estimated through a modified version of the popular histogram test. Any traditional figure of merit for ADC's can be accurately evaluated from the proposed probabilistic characterization. Besides, this allows one to optimize the ADC overall performance, determining the best allocation of the output reconstruction levels. The parameters of the modified histogram test are determined as a function of the desired accuracy. Finally, computer simulations illustrate the performance of the method.","0018-9456;1557-9662","","10.1109/19.492799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492799","","Histograms;System testing;Legged locomotion;Analog-digital conversion;Computer simulation;Software measurement;Employment;Computer errors;Error correction","analogue-digital conversion;probability;optimisation;digital simulation;simulation;electronic equipment testing;computerised instrumentation;automatic testing;performance evaluation","ADC performance;analog-to-digital converter;conditional probability function;behaviour;histogram test;probabilistic characterization;output reconstruction;modified histogram test;computer simulation;nonlinearities","","20","12","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical results from an experiment on value-based review (VBR) processes","K. Lee; B. Boehm","Center for Software Eng., Univ. of Southern California, Los Angeles, CA, USA; Center for Software Eng., Univ. of Southern California, Los Angeles, CA, USA","2005 International Symposium on Empirical Software Engineering, 2005.","","2005","","","10 pp.","","As part of our research on value-based software engineering, we conducted an experiment on the use of value-based review (VBR) processes. We developed a set of VBR checklists with issues ranked by success-criticality, and a set of VBR processes prioritized by issue criticality and stakeholder-negotiated product capability priorities. The experiment involved 28 independent verification and validation (IV&V) subjects (full-time working professionals taking a distance learning course) reviewing specifications produced by 18 real-client, full-time student e-services projects. The IV&V subjects were randomly assigned to use either the VBR approach or our previous value-neutral checklist-based reading (CBR) approach. The difference between groups was not statistically significant for number of issues reported, but was statistically significant for number of issues per review hour, total issue impact, and cost effectiveness in terms of total issue impact per review hour. For the latter, the VBRs were roughly twice as cost-effective as the CBRs.","","0-7803-9507","10.1109/ISESE.2005.1541809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541809","","Software engineering;Costs;Computer aided instruction;Bioreactors;Software quality;Computer architecture;Software performance;Application software;Software testing;Investments","software engineering;statistical analysis","empirical analysis;value-based review process;VBR checklist;value-based software engineering;full-time student e-services project;value-neutral checklist-based reading approach;CBR approach","","2","16","","","","","","IEEE","IEEE Conferences"
"Getting the whole team into usability testing","K. Ehrlich; M. B. Butler; K. Pernice","Lotus Dev. Corp.; Lotus Dev. Corp.; Lotus Dev. Corp.","IEEE Software","","1994","11","1","89","91","Pressure from customers, the trade press, and the competition is causing companies to focus more on delivering usable products. But even when the commitment is there, companies still find it difficult to get their developers to believe in the results of usability tests and to make improvements based on them. The authors report on what Lotus has done to bring developers closer to the process, describing tools, techniques, and concepts to optimize user interfaces. They have found four approaches particularly valuable in getting developers and other members of the product team involved in usability testing: informal setups, sit-in sessions, usability nights, and videotapes. Their experience shows that these methods are more effective than conventional practices, in which developers merely receive reports of key problems from the test sessions.<<ETX>>","0740-7459;1937-4194","","10.1109/52.251216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=251216","","Usability;Testing;User interfaces","software metrics","teamwork;usability testing;usable products;user interface optimization;Lotus;product team;informal setups;sit-in session;usability nights;videotapes","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Application of the software for BWA systems for analysis and design","A. B. Shelkovnikov; D. Budimir; B. N. Shelkovnikov","Nat. Tech. Univ. of Ukraine, Ukraine; NA; NA","12th International Conference Microwave and Telecommunication Technology","","2002","","","267","270","The paper observes the algorithm of the BWA design on the base of stated task of multistage optimization. The software employment is presented with simulation of a one-layer and two-layer structure of BW. Advantages of the two-layer structure are shown and the optimal parameters being entry requirements for the following design stages are determined.","","966-7968-12","10.1109/CRMICO.2002.1137236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137236","","Application software;Software systems;Algorithm design and analysis;Design automation;Hardware;Base stations;Optimization methods;Design optimization;Costs;Low-noise amplifiers","broadband networks;optimisation;telecommunication computing;cellular radio","BWA;broadband wireless access;multistage optimization;two-layer structure;one-layer structure;optimal parameters;design stages","","","13","","","","","","IEEE","IEEE Conferences"
"Structural properties of post-dominator trees","T. Y. Chen; Y. Y. Cheung","Dept. of Comput. Sci., Melbourne Univ., Parkville, Vic., Australia; NA","Proceedings of Australian Software Engineering Conference ASWEC 97","","1997","","","158","165","The concepts of post dominators and post dominator trees are extensively used in code optimisation (J. Ferrante et al., 1987), program slicing (H. Agrawal and J.R. Horgan, 1990) and test suite reduction (R. Gupta and M.L. Soffa, 1093). The paper studies some characteristics of post dominator trees. These results can form the basis for the development of a more efficient construction algorithm of post dominator tree.","","0-8186-8081","10.1109/ASWEC.1997.623767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=623767","","Tree graphs;Flow graphs;Testing;Computer science","trees (mathematics);tree data structures;optimising compilers;program diagnostics;program testing","structural properties;post dominator trees;code optimisation;program slicing;test suite reduction;construction algorithm","","","9","","","","","","IEEE","IEEE Conferences"
"Compiler optimization and its impact on development of real-time systems","K. Tucker; E. Solomon; K. Littlejohn","DDC-I Inc., Phoenix, AZ, USA; NA; NA","17th DASC. AIAA/IEEE/SAE. Digital Avionics Systems Conference. Proceedings (Cat. No.98CH36267)","","1998","1","","C12/1","C12/6 vol.1","Real-time systems are increasing in size and complexity. Software developers seek to maximize the performance, as well as minimize the memory requirements in order to meet the constraints of these systems. Traditionally, software developers have overcome these constraints by using an optimizing compiler. However, optimization carries a hidden price. Many Commercial-Off-The-Shelf (COTS) debuggers are unable to work effectively with optimized applications, because the one-to-one mapping of source to object code is often disturbed by optimization. While optimizing, a compiler is free to reorder or interleave the code for statements, so long as the semantics of the program are preserved. This causes two major problems for a debugger. First, selecting a representative instruction to be used when setting a breakpoint on a source-level statement becomes difficult. Second, at a given breakpoint, the value of a variable may differ from what it would be in the non-optimized version. This condition is known as non-currency. Each of these problems becomes a serious issue if a debugger is used to debug the deliverable code or to perform verification testing. This paper presents an in-depth study of the effects of loop-invariant optimization on debugging user applications. The paper will show how to significantly improve the performance of the debugger, with respect to breakpoints and non-current variables, when operating on code affected by this optimization. A summary of the research done, including specific implementation issues and lessons learned while enhancing the DDC-I SPARCStation Solaris native Ada compiler system, is presented.","","0-7803-5086","10.1109/DASC.1998.741479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=741479","","Optimizing compilers;Real time systems;Force sensors;Debugging;Sensor systems;Laboratories;Software performance;Software systems;Application software;Program processors","program debugging;optimising compilers;real-time systems","compiler optimization;real-time systems development;memory requirements;system constraints;loop-invariant optimization;debugging user applications;specific implementation issues;DDC-I SPARCStation Solaris;native Ada compiler system;setting breakpoints;hoisted statement","","","5","","","","","","IEEE","IEEE Conferences"
"Modular rocket engine control software (MRECS)","C. Tarrant; J. Crook","Lockheed Martin Space Mission Syst. & Services, Houston, TX, USA; NA","16th DASC. AIAA/IEEE Digital Avionics Systems Conference. Reflections to the Future. Proceedings","","1997","2","","8.3","24","The Modular Rocket Engine Control Software (MRECS) Program is a technology demonstration effort designed to advance the state-of-the-art in launch vehicle propulsion systems. Its emphasis is on developing and demonstrating a modular software architecture for a generic, advanced engine control system that will result in lower software maintenance (operations) costs. It effectively accommodates software requirements changes that occur due to hardware technology upgrades and engine development testing. Ground rules directed by MSFC were to optimize modularity and implement the software in the Ada programming language. MRECS system software and the software development environment utilize Commercial-Off-the-Shelf (COTS) products. This paper presents the objectives and benefits of the program. The software architecture, design, and development environment are described. MRECS tasks are defined and timing relationships given. Major accomplishments are listed. MRECS offers benefits to a wide variety of advanced technology programs in the areas of modular software architecture, reuse software, and reduced software reverification time related to software changes. Currently, the program is focused on supporting MSFC in accomplishing Space Shuttle Main Engine (SSME) and Low Cost Boost Technology (LCBT) Program hot-fire tests.","","0-7803-4150","10.1109/DASC.1997.637278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637278","","Rockets;Engines;Software architecture;Control systems;Costs;Space technology;Vehicles;Propulsion;Software maintenance;Hardware","rocket engines;aerospace control;software reusability;economics;Ada;software maintenance","rocket engine control software;launch vehicle propulsion;modular software architecture;software maintenance costs;Ada programming language;COTS;timing relationships;software reverification time;Space Shuttle Main Engine","","","","","","","","","IEEE","IEEE Conferences"
"Random testing in Isabelle/HOL","S. Berghofer; T. Nipkow","Inst. fur Informatik, Tech. Univ. Munchen, Garching, Germany; Inst. fur Informatik, Tech. Univ. Munchen, Garching, Germany","Proceedings of the Second International Conference on Software Engineering and Formal Methods, 2004. SEFM 2004.","","2004","","","230","239","When developing non-trivial formalizations in a theorem prover, a considerable amount of time is devoted to ""debugging"" specifications and conjectures by failed proof attempts. To detect such problems early in the proof and save development time, we have extended the Isabelle theorem prover with a tool for testing specifications by evaluating propositions under an assignment of random values to free variables. Distribution of the test data is optimized via mutation testing. The technical contributions are an extension of earlier work with inductive definitions and a generic method for randomly generating elements of recursive datatypes.","","0-7695-2222","10.1109/SEFM.2004.1347524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347524","","Software testing;Software engineering","theorem proving;optimisation;formal specification;recursive functions;program testing","random testing;nontrivial formalizations;debugging specifications;Isabelle theorem prover;specification testing;random values;free variables;mutation testing;inductive definitions;recursive datatypes;Isabelle/HOL","","9","14","","","","","","IEEE","IEEE Conferences"
"Application of CBR in VR-based test and simulation system","Tian-Tai Guo; Xiao-Jun Zhou; Gen-Xing Zhu","Dept. of Mech. Eng., Zhejiang Univ., Hangzhou, China; Dept. of Mech. Eng., Zhejiang Univ., Hangzhou, China; Dept. of Mech. Eng., Zhejiang Univ., Hangzhou, China","Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)","","2003","4","","2337","2340 Vol.4","This paper first introduces the concept of VR-based test and simulation. In a VR-based test and simulation system (VTSS), the test processes are interactively planned, optimized and simulated in a virtual test environment generated by computer, aiming at eventually performing tests completely in virtual test environments. To make VTSS intelligent and practical, the technique of case-based reasoning (CBR) is introduced into the system. With ultrasonic non-destructive testing (NDT) as the technical background, a prototype of the system is detailed in the paper. The conclusion is that CBR can play a very important role in VTSS.","","0-7803-7865-20-7803-8131","10.1109/ICMLC.2003.1259899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259899","","System testing;Nondestructive testing;Software testing;Discrete event simulation;Computational modeling;Virtual reality;Computer simulation;Performance evaluation;Machine learning;Cybernetics","virtual reality;case-based reasoning;ultrasonic materials testing","test processes;virtual test environment;case-based reasoning;ultrasonic nondestructive testing;virtual reality;simulation system","","2","6","","","","","","IEEE","IEEE Conferences"
"Test sequence generation methods for protocol conformance testing","Chul Kim; J. S. Song","Dept. of Comput. Sci., Yonsei Univ., Seoul, South Korea; Dept. of Comput. Sci., Yonsei Univ., Seoul, South Korea","Proceedings Eighteenth Annual International Computer Software and Applications Conference (COMPSAC 94)","","1994","","","169","174","The paper presents a survey of test sequence generation methods for testing the conformance of a protocol implementation to its specification which is modeled as a finite state machine (FSM). The past decade (1984-94) has been a period of intense research in test sequence generation, since it is one of the most important issues in the area of protocol conformance testing. There have been four major methods of conformance test generation reported in the literature: transition tours, distinguishing sequences, characterizing sequences, and unique input/output sequences. These methods are used to test the control portion of a protocol specification. Applications of these methods to the finite state machine model are discussed and then comparison and analysis are made in terms of two criteria: fault coverage and test sequence length. Also, some issues which are related to test generation for protocol conformance testing are identified.<<ETX>>","","0-8186-6705","10.1109/CMPSAC.1994.342812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=342812","","Protocols;Automata;System testing;Computer network reliability;Telecommunication network reliability;Optimization methods;Computer science;Humans;Computer errors;Hardware","protocols;conformance testing;formal specification;program verification;finite state machines","test sequence generation methods;protocol conformance testing;protocol implementation;finite state machine;FSM;transition tours;distinguishing sequences;characterizing sequences;unique input/output sequences;control portion;protocol specification;fault coverage;test sequence length","","1","25","","","","","","IEEE","IEEE Conferences"
"Performance Criteria for Constrained Nonlinear Programming Codes","H. W. Robb; H. R. Weistroffer","Unicorn Lines (Pty) Ltd.; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","479","489","A set of performance criteria for evaluating optimization software with respect to efficiency, reliability, and accuracy is presented and discussed. A numerical comparison of five constrained nonlinear programming codes is described, which was carried out in order to test the usefulness and general applicability of the proposed performance criteria. The results of the numerical comparison are discussed, and the proposed criteria are compared to the criteria traditionally used in comparative evaluations of nonlinear programming codes, with particular reference to machine dependence and the applicability to test problems with unknown solutions. A separate small scale computational experiment is described which was carried out specifically to test the machine dependence of the criteria. The observed deficiencies of the proposed new criteria are also discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702239","Constrained optimization;evaluation;mathematical programming;nonlinear programming;numerical comparison;optimization software;performance criteria","Mathematical programming;Robustness;Software performance;Africa;Writing;Councils;Guidelines;Life testing;Particle measurements;Time measurement","","Constrained optimization;evaluation;mathematical programming;nonlinear programming;numerical comparison;optimization software;performance criteria","","1","26","","","","","","IEEE","IEEE Journals & Magazines"
"An optimization scheme for motor parameter adaptation in IFO vector controlled induction motor drive","Hu Xingbo; Qu Wenlong","Dept. of Electr. Eng., Tsinghua Univ., Beijing, China; NA","ICEMS'2001. Proceedings of the Fifth International Conference on Electrical Machines and Systems (IEEE Cat. No.01EX501)","","2001","2","","1109","1112 vol.2","Crucial to the success of the IFO vector control is the proper calculation of the slip frequency, which depends greatly on the motor parameters. Many schemes have been proposed for parameter adaptation in induction motor drives over the past years. However these schemes cannot be well applied in a practical drive system. In this paper, a new and practical scheme for motor parameter adaptation is proposed. This new scheme converts the adaptation problem into in optimization problem. A simple and reliable optimization algorithm that is expressly contrived for this application is employed to solve the optimization problem. Computer simulations and experiments are carried out to verify the proposed scheme.","","7-5062-5115","10.1109/ICEMS.2001.971872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971872","","Induction motors;Induction motor drives;Machine vector control;Rotors;Application software;Computer simulation;Mathematical model;Frequency;Motor drives;Optimization methods","induction motor drives;machine vector control;control system synthesis;machine theory;parameter estimation;control system analysis computing;electric machine analysis computing;optimisation;slip (asynchronous machines);machine testing","indirect field-oriented vector control;induction motor drive;control design;optimization scheme;slip frequency;motor parameter adaptation;optimization algorithm;computer simulation;control simulation;control performance","","1","5","","","","","","IEEE","IEEE Conferences"
"On optimizing test sequence generation for communicating systems","V. Carchiolo; A. Faro","Instituto di Inf. e Telecomunicazioni, Catania Univ., Italy; Instituto di Inf. e Telecomunicazioni, Catania Univ., Italy","[1990] Proceedings. Second IEEE Workshop on Future Trends of Distributed Computing Systems","","1990","","","325","332","The authors present a method to derive an optimal testing sequence from Extended Calculus for Communicating Systems (ECCS) specifications. The method is based on the theory of experiments which deals with the idea of defining a characterization set for each state of the protocol under test. The authors propose a definition of the characterization set tailored for ECCS processes and present algorithms to derive a less time-consuming characterization set for recognizing that the system under test is in a given state. Two main problems are considered in the formal approach to protocol testing: one deals with a faster testing session; the second deals with less time-consuming computation methods to derive the sequences to be used in the optimal testing session. The authors first justify the importance of the optimization problem and then present a solution by using ECCS as a formal description technique.<<ETX>>","","0-8186-2088","10.1109/FTDCS.1990.138340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=138340","","System testing;Protocols;Calculus;Character recognition;Software testing;Computer network reliability;Fault detection;Automatic testing;Telecommunications;Software safety","formal specification;protocols","test sequence generation optimisation;communicating systems;optimal testing sequence;Extended Calculus for Communicating Systems;ECCS;specifications;protocol;optimal testing session;formal description technique","","1","9","","","","","","IEEE","IEEE Conferences"
"A new yield optimization algorithm and its applications","Z. Wang; H. Yang; R. Liu; C. Fan","Dept. of Electron. Eng., Tsinghua Univ., Beijing, China; Dept. of Electron. Eng., Tsinghua Univ., Beijing, China; Dept. of Electron. Eng., Tsinghua Univ., Beijing, China; Dept. of Electron. Eng., Tsinghua Univ., Beijing, China","1991., IEEE International Sympoisum on Circuits and Systems","","1991","","","1996","1999 vol.4","The authors propose a novel yield optimization algorithm for IC design. To design a practical yield optimization system, two efforts must be made. One is to get a suitable convergence criterion, the other is to propose an efficient optimization method, by which one can reach the maximum yield point as soon as possible. A convergence criterion based on sequential tests of a hypothesis is presented. This algorithm was implemented in a CADS (computer-aided design system). Applications of this algorithm show its high efficiency.<<ETX>>","","0-7803-0050","10.1109/ISCAS.1991.176055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=176055","","Convergence;Design optimization;Circuits;Application software;Algorithm design and analysis;Design automation;Equations;Yield estimation;Optimization methods;Design engineering","circuit CAD;circuit layout CAD;electronic engineering computing;integrated circuit manufacture;optimisation","yield optimization algorithm;IC design;convergence criterion;optimization method;sequential tests;CADS;computer-aided design system","","2","7","","","","","","IEEE","IEEE Conferences"
"Direct algorithm and its application to slider air bearing surface optimization","Hong Zhu; D. B. Bogy","Dept. of Mech. Eng., California Univ., Berkeley, CA, USA; Dept. of Mech. Eng., California Univ., Berkeley, CA, USA","2002 IEEE International Magnetics Conference (INTERMAG)","","2002","","","DP9","","Summary form only given. Slider air bearing surface (ABS) designs that satisfy very strict multi-objective goals are of great importance for magnetic hard disk drives. Finding such optimal designs is a strongly nonlinear problem. In slider ABS optimization, the evaluation of the objective function for every single sample design takes substantial computation time. It is desirable to reduce the number of sample designs evaluated without losing the global property of the optimization algorithm. In this paper, we make use of the DIRECT (DIviding RECTangle) optimization technique, which is a deterministic global optimization technique developed by Jones et al. (1993) and which is used to find the minimum of a Lipschitz continuous function without knowing the Lipschitz constant.","","0-7803-7365","10.1109/INTMAG.2002.1001042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1001042","","Design optimization;Equations;Testing;Application software;Laboratories;Mechanical engineering;Magnetic levitation;Hard disks;Algorithm design and analysis;Performance evaluation","hard discs;optimisation;disc drives","direct algorithm;slider air bearing;surface optimization;multi-objective goals;magnetic hard disk drives;strongly nonlinear problem;objective function;computation time;sample designs;global property;DIRECT;dividing rectangle optimization technique;Lipschitz continuous function","","","","","","","","","IEEE","IEEE Conferences"
"Hardware/software strategies in DC brushless motor development","M. Allan; I. J. Kemp; T. Westwood; B. E. Pitches","Sch. of Eng., Glasgow Coll., UK; Sch. of Eng., Glasgow Coll., UK; Sch. of Eng., Glasgow Coll., UK; NA","Fifth Annual Proceedings on Applied Power Electronics Conference and Exposition","","1990","","","401","405","An integrated approach involving software/hardware strategies to DC brushless motor development is presented. These strategies are intended to allow complete optimization of motor performance over a wide range of applications. A series of tests performed on the motor confirmed the competence of the basic hardware (a saturation problem was addressed via change in lamination material from a nickel-iron alloy to a cobalt-based alloy) as an integral part of the overall hardware/software system envisaged.<<ETX>>","","","10.1109/APEC.1990.66441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=66441","","Hardware;Brushless motors;Cobalt alloys;Nickel alloys;Application software;Materials testing;Software testing;System testing;Performance evaluation;Lamination","DC motors;power engineering computing","motor performance optimisation;software strategies;DC brushless motor;hardware strategies;saturation problem;lamination material","","1","12","","","","","","IEEE","IEEE Conferences"
"A novel solution for chip-level functional timing verification","R. Jayabharathi; Kyung Tek Lee; J. A. Abraham","Dept. of Design Technol., Intel Corp., Folsom, CA, USA; NA; NA","Proceedings. 15th IEEE VLSI Test Symposium (Cat. No.97TB100125)","","1997","","","137","142","Existing timing verification tools can provide methodologies for identifying and optimizing critical true paths in a embedded combinational module; however the problem of justifying these paths to the chip level is a very difficult one. This paper addresses the problem of timing verification at the entire chip level. We use a critical path tool, CRITIC, to obtain critical paths in an embedded combinational module. In order to reduce the complexity of checking whether the module-level critical path is indeed critical at the chip level, we use techniques from formal verification to extract the control behavior of the circuit, and check whether there is any control sequence which will justify the path to the chip level. The results of the experiments on several processor designs show that our approach is very effective in large sequential circuits such as microprocessors, where conventional ATPG techniques require inordinate amounts of CPU time. The experiments also show that the execution time remains reasonable as the circuit size increases, since we deal with a reduced control space rather than the entire state space of the circuit.","1093-0167","0-8186-7810","10.1109/VTEST.1997.599465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599465","","Timing;Optimization methods;Formal verification;Process design;Sequential circuits;Microprocessors;Automatic test pattern generation;Central Processing Unit;Size control;State-space methods","design for testability;combinational circuits;logic testing;timing;automatic test software;formal verification;computer testing;integrated circuit testing;critical path analysis;circuit optimisation;logic CAD","chip-level functional timing verification;entire chip level;critical path tool;CRITIC tool;embedded combinational module;formal verification;control behavior;large sequential circuits;microprocessors;reduced control space;automated procedure;vigorous sensitization;extracted flow control machine","","1","23","","","","","","IEEE","IEEE Conferences"
"TiGeR, the Transmeta Instruction GEneratoR: A Production Based, Pseudo Random Instruction x86 Test Generator","A. S. Nadkarni; T. Kenville","Transmeta Corporation; NA","Fifth International Workshop on Microprocessor Test and Verification (MTV'04)","","2004","","","2","7","Real world software applications, in addition to carefully crafted test cases, are often used in microprocessor and system verification as they exercise the interaction of many different functional blocks in the processor In the case of dynamic binary translation based microprocessors like Efficeon and Crusoe, they also exercise the code morphing software (CMS), the firmware layer that provides x86 compatibility. However this approach has limitations in that it is exceedingly difficult to identify, isolate, reproduce and debug failures and no single application may exercise most of the architectural corner cases of the CPU. Hence we have developed an x86 random instruction generator that generates code that has the characteristics of typical 32 bit protected mode programs. The instruction generator is highly configurable and deterministic, and has been successfully used in the verification of the Efficeon line of processors through all its phases of development and productization. The instruction generator is production based, in the sense that it generates code based on a set of rules called productions","1550-4093","0-7695-2320","10.1109/MTV.2004.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563066","","Production;Collision mitigation;Microprocessors;Character generation;System testing;Software testing;Application software;VLIW;Optimizing compilers;Stress","firmware;formal verification;instruction sets;microprocessor chips;random number generation","TiGeR;transmeta instruction generator;production based pseudo random instruction x86 test generator;system verification;dynamic binary translation based microprocessors;code morphing software;firmware;that;code generation;Efficeon","","","7","","","","","","IEEE","IEEE Conferences"
"Identification of test process improvements by combining fault trigger classification and faults-slip-through measurement","L. -. Damm; L. Lundberg","Ericsson AB, Karlskrona, Sweden; NA","2005 International Symposium on Empirical Software Engineering, 2005.","","2005","","","10 pp.","","Successful software process improvement depends on the ability to analyze past projects and determine which parts of the process that could become more efficient. One source of such an analysis is the faults that are reported during development. This paper proposes how a combination of two existing techniques for fault analysis can be used to identify where in the test process improvements are needed, i.e. to pinpoint which activities in which phases that should be improved. This was achieved by classifying faults after which test activities that triggered them and which phase each fault should have been found in, i.e. through a combination of orthogonal defect classification (ODC) and faults-slip-through measurement. As a part of the method, the paper proposes a refined classification scheme due to identified problems when trying to apply ODC classification schemes in practice. The feasibility of the proposed method was demonstrated by applying it on an industrial software development project at Ericsson AB. The obtained measures resulted in a set of quantified and prioritized improvement areas to address in consecutive projects.","","0-7803-9507","10.1109/ISESE.2005.1541824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541824","","Fault diagnosis;Testing;Feedback;Software measurement;Phase measurement;Programming;Refining;Computer industry;Area measurement;Costs","software process improvement;software development management;software fault tolerance","fault trigger classification;faults-slip-through measurement;software process improvement;fault analysis;orthogonal defect classification;industrial software development project","","3","30","","","","","","IEEE","IEEE Conferences"
"Assessing system performance using component level performance specifications","J. R. Mingrone; A. Farahat; D. King","Naval Electron. & Surveillance Syst., Lockheed Martin, Moorestown, NJ, USA; NA; NA","Proceedings of the IEEE 2000 National Aerospace and Electronics Conference. NAECON 2000. Engineering Tomorrow (Cat. No.00CH37093)","","2000","","","469","475","Performance of software components may be characterized in a way that permits software architects to predict response times that result after integrating multiple components. Using information about individual component execution time and invocation rates, this method predicts processor utilization and ""thread"" latency (where a thread is an executed string of components). The method derives component budgets which can be individually verified via empirical tests and which assure system response times meet specified requirements. In the event budgets do not support satisfactory system response times, the method determines what components should be optimized in order to produce the desired system result. When calculating budgets or when identifying optimization goals for components, the method considers difficulty of component optimization. The method is based on a simple application of mathematical concepts from queuing theory and optimization theory and may be implemented using a spreadsheet. This method was used to develop component based budgets for a large complex software intensive system.","","0-7803-6262","10.1109/NAECON.2000.894947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894947","","System performance;Yarn;Delay;Software performance;Optimization methods;Real time systems;System testing;Queueing analysis;Software systems;Surveillance","software performance evaluation;delays;optimisation;queueing theory","system performance;component level performance specification;software components;response times;empirical tests;optimization goals;queuing theory;optimization theory;spreadsheet;complex software intensive system;performance budgets;mathematical model","","","5","","","","","","IEEE","IEEE Conferences"
"Thermal analysis of power MOSFETs using Rebeca-3D thermal modeling software (from Epsilon Ingenierie) versus physical measurements and possible extractions","K. Pandya; S. Jaunay","Vishay Siliconix, Santa Clara, CA, USA; Vishay Siliconix, Santa Clara, CA, USA","EuroSimE 2005. Proceedings of the 6th International Conference on Thermal, Mechanial and Multi-Physics Simulation and Experiments in Micro-Electronics and Micro-Systems, 2005.","","2005","","","394","397","Shrinking semiconductor packages, increasing die densities, process changes, and the requirements for optimizing electro-mechanical assemblies are making sound thermal designs more important than ever. Depending on the application scenario, thermal considerations can be approached from two angles. Rebeca-3D, a semi-FEA based software tool, provides the ability to develop a thermal model using detailed device geometry and material characteristics. The tool's versatility allows it to be used to analyze a design in a range of application scenarios. Using this platform, this paper discusses actual examples of thermal model development and results. The results are first validated by comparing with datasheet information and experiments in an application lab. Following simulation, the platform is used to extrapolate the measurements done in the application lab. The results enable development of thermal performance profiles for different packages and for different electrical and assembly conditions. Several examples are discussed with figures and pictures. The study compares the results of simulations and tests on actual devices.","","0-7803-9062","10.1109/ESIME.2005.1502836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1502836","","MOSFETs;Power measurement;Software measurement;Assembly;Application software;Semiconductor device packaging;Design optimization;Software tools;Solid modeling;Geometry","power MOSFET;semiconductor device models;thermal analysis;finite element analysis;software tools;semiconductor device measurement;semiconductor device packaging;electronic engineering computing","thermal analysis;power MOSFET;Rebeca-3D thermal modeling;semiconductor packages;semi-FEA based software tool;thermal model development;thermal performance profile;semiconductor device models;finite element analysis;semiconductor device measurement;semiconductor device packaging;electronic engineering computing","","1","4","","","","","","IEEE","IEEE Conferences"
"Modelization of cracks in steel subsea structures-application to the optimization of a new electromagnetic detection method","P. Giordano; J. M. Ittel; J. Cahouet; S. Daste","CEA-DTA-LETI-CENG, Grenoble, France; CEA-DTA-LETI-CENG, Grenoble, France; NA; NA","IEEE Transactions on Magnetics","","1991","27","6","5408","5410","A novel contactless electromagnetic nondestructive testing technique has been developed for the detection of cracks in offshore steel structures. In order to optimize the detection device, a crack model using existing software based on a finite elements method has been proposed. The electrical activity of the modeled crack, interpreted as a back electromotive force (BEMF) source, is investigated. A two-level approach leading to a simplified modeling makes the investigation of this BEMF possible by reducing the size of the numerical problem. Results lead to the understanding of defect magnetic signal shapes and thus to the design of an optimized detection device.<<ETX>>","0018-9464;1941-0069","","10.1109/20.278854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=278854","","Steel;Contacts;Electromagnetic forces;Nondestructive testing;Optimization methods;Finite element methods;Magnetic devices;Shape;Signal design;Design optimization","crack detection;finite element analysis;nondestructive testing;oil technology","steel subsea structures;electromagnetic detection method;contactless electromagnetic nondestructive testing technique;offshore steel structures;crack model;finite elements method;electrical activity;back electromotive force;defect magnetic signal shapes;optimized detection device","","5","5","","","","","","IEEE","IEEE Journals & Magazines"
"Development and application of innovational drop impact modeling techniques","Tong Yan Tee; Jing-en Luan; Hun Shen Ng","STMicroelectronics, Singapore; STMicroelectronics, Singapore; STMicroelectronics, Singapore","Proceedings Electronic Components and Technology, 2005. ECTC '05.","","2005","","","504","512 Vol. 1","Due to demand for short time-to-market, drop testing has become a bottleneck for semiconductor and telecommunication industry. Therefore, there is a need for a faster and cheaper solution, i.e. validated drop impact model, which is accurate, reliable, and enables understanding of physics-of-failure for design improvement. Currently, there has been increasing interest and effort by researchers on board level drop test studies by numerical modeling. Several different modeling methods have been developed to satisfy the requirements in package design analysis, product qualification, and impact life prediction. In this paper, for the first time, various advanced drop test modeling techniques developed are systematically introduced, integrated, compared, and recommended for various applications, consisting of analysis type (dynamic vs. static), loading method (free-fall vs. input-G), and solver algorithm (explicit vs. implicit). Each combination of modeling techniques has its unique advantages, depending on applications. All the models are validated to show excellent first level correlation on the dynamic responses of PCB, second level correlation on the solder joint stress and failure mode, and third level correlation on impact life prediction. Dynamic model is required for accurate drop impact simulation, whereas static model is useful for quick design analysis and optimization. The free-fall drop model can be applied to provide fundamental understanding of different drop test conditions, i.e. drop height, contact surface, drop block, felt layer on the impact pulse. Once the impact pulse has been established, the input-G method can be used as a standard ""numerical drop test"" to perform design analysis and optimization for product qualification. Implicit solver is an alternative to explicit solver, saving software cost by maintaining only one type of solver, but the technical challenge is greater.","0569-5503;2377-5726","0-7803-8907","10.1109/ECTC.2005.1441312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1441312","","Predictive models;Qualifications;Design optimization;Software maintenance;Time to market;Semiconductor device testing;Communication industry;Numerical models;Packaging;Product design","integrated circuit modelling;impact testing;test equipment;integrated circuit testing;integrated circuit design;circuit optimisation;printed circuits;solders;life testing","drop impact modeling technique;drop testing;board level drop test;numerical model;package design analysis;product qualification;life prediction;loading method;solver algorithm;dynamic response;PCB;solder joint stress;dynamic model;drop impact simulation;free-fall drop model;drop height;contact surface;drop block;input-G method;numerical drop test;implicit solver","","14","33","","","","","","IEEE","IEEE Conferences"
"20 V LDMOS optimized for high drain current condition. Which is better, n-epi or p-epi?","K. Kinoshita; Y. Kawaguchi; T. Sano; A. Nakagawa","Res. & Dev. Center, Toshiba Corp., Kawasaki, Japan; NA; NA; NA","11th International Symposium on Power Semiconductor Devices and ICs. ISPSD'99 Proceedings (Cat. No.99CH36312)","","1999","","","59","62","This paper discusses whether n-epi or p-epi substrates are better for 20 V range LDMOSFETs. We present four optimized 20 V LDMOSFETs and compare them. The best compromise is the LDMOS on n-epi with a high dose n-implant layer which achieves a sufficiently low on-resistance of 17.2 m/spl Omega//spl middot/mm/sup 2/ and a high static breakdown voltage of 24.0 V without breakdown voltage degradation under large drain current flow conditions. The device on-state breakdown voltage for a 5 V gate voltage is 24.5 V.","1063-6854","0-7803-5290","10.1109/ISPSD.1999.764049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=764049","","MOSFETs;Degradation;Implants;Breakdown voltage;Design optimization;Research and development;Application software;Computer applications;Bipolar transistors;Petroleum","power MOSFET;optimisation;doping profiles;substrates;semiconductor epitaxial layers;electric current;semiconductor device testing;semiconductor device breakdown;ion implantation","LDMOS optimization;drain current;p-epi substrates;n-epi substrates;optimized LDMOSFETs;high dose n-implant layer;on-resistance;static breakdown voltage;breakdown voltage degradation;drain current flow;device on-state breakdown voltage;gate voltage;20 V;24 V;5 V;24.5 V;Si","","3","3","","","","","","IEEE","IEEE Conferences"
"Diagnostics and maintenance testing-a case for relevancy","R. Krupa; D. Kennedy","NA; NA","Proceedings of the IEEE 1991 National Aerospace and Electronics Conference NAECON 1991","","1991","","","1201","1205 vol.3","The authors advocate diagnostics and testing in a back to basics way by stressing the importance of establishing goals which are absolutely relevant to operational capabilities and constraints. The evolution of support methods and concepts resulted in the application of technology to diagnostics and testing without first establishing the operational relevancy for such support. The trend is insensitive to the fact that maintenance testing is not the primary mission of operational users and that ineffective testing can be more a liability than an asset to the mission. Without relevant objectives for support tasks, their development can be dominated by technical challenge. The result can be overly complex, ineffective support. The authors highlight a fundamental approach to design and diagnostics to select and balance maintenance tasks and optimize operational support.<<ETX>>","","0-7803-0085","10.1109/NAECON.1991.165913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=165913","","Testing;Computer aided software engineering;Aircraft;Design optimization;Performance evaluation;Government;Protection;Fuels;Petroleum;Weapons","electronic equipment testing;maintenance engineering;management;military systems","mission equipment;military aircraft;diagnostics;testing;goals;operational capabilities;maintenance;objectives","","","","","","","","","IEEE","IEEE Conferences"
"A novel built-in self-repair approach to VLSI memory yield enhancement","P. Mazumder; J. S. Yih","Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA; Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA","Proceedings. International Test Conference 1990","","1990","","","833","841","The feasibility of implementing electronic neural networks as intelligent hardware for memory array repair is demonstrated. In particular, it is shown that the neural network control possesses a robust and degradable computing capability under various fault conditions. A yield analysis performed on 64K DRAMs shows that the yield can be improved from as low as 20% to near 99% owing to the self-repair design, with an overhead of no more than 7%. Simulation shows that the neural net algorithms are superior to the Repair Most algorithm.<<ETX>>","","0-8186-9064","10.1109/TEST.1990.114101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114101","","Very large scale integration;Circuit faults;Neural networks;Built-in self-test;Hardware;Optical arrays;Software algorithms;Throughput;Fault diagnosis;Fault tolerance","built-in self test;digital simulation;DRAM chips;electronic engineering computing;integrated circuit testing;integrated memory circuits;maintenance engineering;neural nets;optimisation;VLSI","digital simulation;Hill Climbing algorithm;combinatorial optimisation;built-in self-repair;VLSI memory yield enhancement;neural networks;intelligent hardware;degradable computing capability;yield analysis;DRAMs;overhead;64*10/sup 3/ bit","","21","20","","","","","","IEEE","IEEE Conferences"
"Portable maintenance aid instrument pack (PIP)","M. Gooding; S. DeSantis","ARGOSyst. Inc., Sunnyvale, CA, USA; NA","1998 IEEE AUTOTESTCON Proceedings. IEEE Systems Readiness Technology Conference. Test Technology for the 21st Century (Cat. No.98CH36179)","","1998","","","247","253","Flight line component false removals are costly and add to the logistics tail of every weapon system. VXI offers opportunity to consolidate test needs into a user-friendly application that can significantly reduce flight line false removal rates. VXI has grown substantially in applications from testing in the lab to field support of weapon systems. Early criticism of VXI came that it was only intended for lab or depot environments. Recently, developers have been taking on the challenge of extending VXI use into field environments. This paper addresses false removals, describes a VXI test system approach, and describes the challenges of creating an enclosure for COTS VXI instruments that protect them from very harsh flight-line environmental extremes. This effort was accomplished for the RAH-66 Comanche Helicopter program, and has created two working prototypes.","1088-7725","0-7803-4420","10.1109/AUTEST.1998.713452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713452","","Instruments;Weapons;System testing;Isolation technology;Aerospace electronics;Logistics;Tail;Prototypes;Aircraft;Manuals","aircraft maintenance;aircraft testing;helicopters;military aircraft;automatic test equipment;automatic test software;diagnostic expert systems;fault location;portable instruments;aerospace computing","portable maintenance aid instrument pack;flight line component false removals;weapon system logistics tail;test needs;user-friendly application;VXI test system approach;field support;COTS VXI instruments;RAH-66 Comanche Helicopter program;embedded diagnostics;shadow-of-the-aircraft;external diagnostics;fault isolation;diagnostic expert;optimized test strategy;legacy systems;software engineering environment;ATLAS language;packaging","","","","","","","","","IEEE","IEEE Conferences"
"enVision: the inside story","D. Organ","LTX/Trillium, San Jose, CA, USA","Proceedings. International Test Conference 1990","","1990","","","530","536","It is pointed out that two long-standing concerns of device test engineers have been run-time efficiency and ease of modification. Traditionally test languages optimizing one have sacrificed the other. The author examines the implementation of the enVision visual test programming language from this perspective. In analyzing the nature of a test program from an implementation perspective, four components are recognized: flow control, static background, dynamic background, and executable statements. The dynamic background component must optimize both run-time efficiency and flexibility and is therefore a critical component in implementing enVision. The information flow from specification sheet through timing equations and into the tester registers may be represented as a DAG (directed acyclic graph). It is noted that the nature of DAGs allows certain significant optimization to be achieved.<<ETX>>","","0-8186-9064","10.1109/TEST.1990.114064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114064","","Timing;Force measurement;Software testing;Runtime;Flowcharts;Mice;Libraries;Books;Current measurement;Computer architecture","automatic testing;computer graphics;high level languages;programming environments;user interfaces;visual programming","spreadsheet model;directed graph;programming environment;enVision;test languages;visual test programming language;flow control;static background;dynamic background;executable statements;run-time efficiency;flexibility;specification sheet;timing equations;directed acyclic graph;optimization","","3","10","","","","","","IEEE","IEEE Conferences"
"Hot chips and soggy software","S. C. Johnson","Stardent Comput. Corp., Sunnyvale, CA, USA","IEEE Micro","","1990","10","1","23","26","The author discusses the bottlenecks that impair performance of a computer system and discusses the success of the RISC (reduced-instruction-set computer) approach. He attributes it, at least in part, to the fact that all the seminal work on the RISC chips was carried out in close conjunction with a strong compiler team. He discusses issues that designers of computer systems must consider and examines trends that will affect the optimum design points for future systems. The author then addresses what he refers to as 'soggy software', i.e. the slow pace of progress in software development as compared to hardware, identifying standardization and reuse as necessary components of any solution to the problem.<<ETX>>","0272-1732;1937-4143","","10.1109/40.46765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46765","","Hardware;Software design;Costs;Testing;Reduced instruction set computing;Springs;Semiconductor device measurement;Software measurement;Computational modeling;Optimizing compilers","reduced instruction set computing;software engineering","bottlenecks;performance;computer system;RISC;software development;standardization","","1","","","","","","","IEEE","IEEE Journals & Magazines"
"Hardware self-tuning and circuit performance monitoring","T. Kehl","Dept. of Comput. Sci. & Eng., Washington Univ., Seattle, WA, USA","Proceedings of 1993 IEEE International Conference on Computer Design ICCD'93","","1993","","","188","192","Self-tuning is a new clocking methodology borrowing heavily from both the synchronous and self-timed disciplines. A self-tuned system has an adjustable clock and measurement logic. During the tuning process the adjustable clock is made to run faster and faster until before the system fails. After tuning and during operation each cycle is measured and, if a failure is imminent, the system is retuned. During the tuning phase test vectors-either hardware embedded or software-select near maximum speed for a particular instance of the system. As self-tuning is predicated on self-test, it is essential to build in self-test features. These same self-test features are useful in circuit level performance monitoring. Two extremes on the continuum of self-tuning are discussed: at one extreme is purely hardware self-tuning and at the other, nearly purely software. Data is given from an experimental self-tuned primary memory indicating 70 ns access time DRAM can be operated at 45 ns or less.<<ETX>>","","0-8186-4230","10.1109/ICCD.1993.393383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=393383","","Hardware;Circuit optimization;Clocks;Tuning;Built-in self-test;Condition monitoring;Logic;Circuit testing;Software testing;System testing","performance evaluation;clocks;fault tolerant computing;reliability;built-in self test;DRAM chips","hardware self-tuning;circuit performance monitoring;clocking methodology;self-tuned system;adjustable clock;measurement logic;failure;test vectors;hardware;software;near maximum speed;self-test features;circuit level performance monitoring;self-tuned primary memory;access time;DRAM","","20","4","","","","","","IEEE","IEEE Conferences"
"Steady-state evolutionary algorithm for solving constrained optimization problems","Chen Ziyi; Kang Lishan","State Key Lab. of Software Eng., Wuhan Univ., China; State Key Lab. of Software Eng., Wuhan Univ., China","IEEE International Symposium on Communications and Information Technology, 2005. ISCIT 2005.","","2005","2","","1267","1270","A novel steady-state evolutionary algorithm (MEA) is proposed to solve the constrained global optimization problems. MEA adopts the partial ordering scheme to handle the equality constraints and inequality constraints in a universal way. Meanwhile,a novel multi-parent crossover operator which can instruct its search direction using statistical information is presented to accelerate the convergence. Experiments have been carried on several benchmark functions to test the performance of the presented MEA. Numerical results show that MEA is highly competitive with other algorithms in effectiveness and generality.","","0-7803-9538","10.1109/ISCIT.2005.1567098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1567098","","Steady-state;Evolutionary computation;Constraint optimization;Laboratories;Software engineering;Electronic mail;Acceleration;Convergence;Benchmark testing;Functional programming","evolutionary computation;numerical analysis;statistical analysis","constrained global optimization problems;steady-state evolutionary algorithm;partial ordering scheme;equality constraints;inequality constraints;multiparent crossover operator;statistical information","","","8","","","","","","IEEE","IEEE Conferences"
"The viability of using COCOMO in the special application software bidding and estimating process","J. E. Helm","Motorola SED, Chandler, AZ, USA","IEEE Transactions on Engineering Management","","1992","39","1","42","58","The purpose of the author is to correlate actual data with COCOMO (constructive cost model) estimated values and determine if the COCOMO method accurately reflects documented program expenditures. Because spaceborne microprocessing is a relatively new arena, the primary constraint associated with developing a model is the limited available database. Data collection to support a statistical analysis is presented along with a discussion of calculated COCOMO results. In the analysis, the use of nonparametric statistics for small samples is addressed. Wilcoxon Signed-Rank and Kendall-Rank statistics support distribution free analyses of the data.<<ETX>>","0018-9391;1558-0040","","10.1109/17.119662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=119662","","Application software;Microprocessors;Software testing;Statistical analysis;System testing;Databases;Microelectronics;Control systems;Statistical distributions;Costs","aerospace computing;DP management;software engineering","software estimating process;data collection;Wilcoxon Signed Rank statistics;distribution free data analyses;COCOMO;software bidding;constructive cost model;documented program expenditures;spaceborne microprocessing;statistical analysis;Kendall-Rank statistics","","6","15","","","","","","IEEE","IEEE Journals & Magazines"
"Integrated design environment for DC/DC converter FET optimization","R. Sodhi; S. Brown; D. Kinzer","Int. Rectifier Corp., El Segundo, CA, USA; NA; NA","11th International Symposium on Power Semiconductor Devices and ICs. ISPSD'99 Proceedings (Cat. No.99CH36312)","","1999","","","241","244","This paper presents a new integrated design environment for device optimization for DC/DC converter applications. The tool, developed for a synchronous buck converter, combines a physical device model with a power loss model and uses the results to evaluate optimum die specifications. This integrated environment has been used for several different applications and has led to significant improvements in the final in-circuit efficiency.","1063-6854","0-7803-5290","10.1109/ISPSD.1999.764108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=764108","","DC-DC power converters;FETs;Design optimization;MOSFETs;Capacitance;Buck converters;Parametric statistics;Testing;Packaging;Flowcharts","power MOSFET;DC-DC power convertors;circuit CAD;circuit optimisation;semiconductor device models;software tools","integrated design environment;DC/DC converter FET optimization;device optimization;DC/DC converter applications;synchronous buck converter;physical device model;power loss model;optimum die specifications;integrated environment;final in-circuit efficiency;power MOSFET","","20","5","","","","","","IEEE","IEEE Conferences"
"Full chip false timing path identification: applications to the PowerPC/sup TM/ microprocessors","Jing Zeng; M. S. Abadir; J. Bhadra; J. A. Abraham","ASP Somerset Design Center, Motorola Inc., Austin, TX, USA; NA; NA; NA","Proceedings Design, Automation and Test in Europe. Conference and Exhibition 2001","","2001","","","514","518","Static timing anaylsis sets the industry standard in the design methodology of high speed/performance microprocessors to determine whether timing requirements have been met. Unfortunately, not all the paths identified using such analysis can be sensitized. This leads to a pessimistic estimation of the processor speed. Also, no amount of engineering effort spent on optimizing such paths can improve the timing performance of the chip. In the past we demonstrated initial results of how ATPG techniques can be used to identify false paths efficiently. Due to the gap between the physical design on which the static timing analysis of the chip is bused and the test view on which the ATPG techniques are applied to identify false paths, in many cases only sections of some of the paths in the full-chip were analyzed in our initial results. In this paper, we will fully analyze all the timing paths using the ATPG techniques, thus overcoming the gap between the testing and timing analysis techniques. This enables us to do false path identification at the full-chip level of the circuit. Results of applying our technique to the second generation G4 PowerPC/sup TM/ will be presented.","1530-1591","0-7695-0993","10.1109/DATE.2001.915072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915072","","Timing;Microprocessors;Circuits;Automatic test pattern generation;Performance analysis;Delay;Design optimization;Application software;Jacobian matrices;Application specific processors","timing;microprocessor chips;fault diagnosis;automatic test pattern generation;integrated circuit testing;logic testing","false timing path identification;PowerPC;microprocessors;static timing anaylsis;industry standard;design methodology;processor speed;timing performance;ATPG techniques","","4","11","","","","","","IEEE","IEEE Conferences"
"COTS-based systems top 10 list","V. R. Basili; B. Boehm","Dept. of Comput. Sci., Maryland Univ., MD, USA; NA","Computer","","2001","34","5","91","95","Presents a COTS-based system (CBS) software defect-reduction list as hypotheses, rather than results, that also serve as software challenges for enhancing our empirical understanding of CBSs. The hypotheses are: (1) more than 99% of all executing computer instructions come from COTS products (each instruction passed a market test for value); (2) more than half the features in large COTS software products go unused; (3) the average COTS software product undergoes a new release every 8-9 months, with active vendor support for only its latest three releases; (4) CBS development and post-deployment efforts can scale as high as the square of the number of independently developed COTS products targeted for integration; (5) CBS post-deployment costs exceed CBS development costs; (6) although glue-code development usually accounts for less than half the total CBS software development effort, the effort per line of glue code averages about three times the effort per line of developed applications code; (7) non-development costs, such as licensing fees, are significant, and projects must plan for and optimize them; (8) CBS assessment and tailoring efforts vary significantly by COTS product class (operating system, database management system, user interface, device driver, etc.); (9) personnel capability and experience remain the dominant factors influencing CBS development productivity; and (10) CBS is currently a high-risk activity, with effort and schedule overruns exceeding non-CBS software overruns, yet many systems have used COTS successfully for cost reduction and early delivery.","0018-9162;1558-0814","","10.1109/2.920618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=920618","","Cost function;Software systems;Computer aided instruction;Computer interfaces;Software testing;System testing;Programming;Application software;Licenses;Operating systems","software packages;software quality;software management","commercial off-the shelf systems;COTS-based systems;software defect reduction;hypotheses;software challenges;computer instructions;unused features;software releases;active vendor support;system development efforts;post-deployment effort;independently developed products;products integration;costs;glue code development;software development effort;personnel capability;personnel experience;development productivity;high-risk activity;schedule overruns;early delivery","","37","","","","","","","IEEE","IEEE Journals & Magazines"
"Design, fabrication, and testing of silicon microgimbals for super-compact rigid disk drives","V. Temesvary; Shuyun Wu; W. H. Hsieh; Yu-Chong Tai; D. K. Miu","Dept. of Mech. Eng., California Univ., Los Angeles, CA, USA; NA; NA; NA; NA","Journal of Microelectromechanical Systems","","1995","4","1","18","27","This paper documents results related to design optimization, fabrication process refinement, and micron-level static/dynamic testing of silicon micromachined microgimbals that have applications in super-compact computer disk drives as well as many other engineering applications of microstructures and microactuators requiring significant out-of-plane motions. The objective of the optimization effort is to increase the in-plane to out-of-plane stiffness ratio in order to maximize compliance and servo bandwidth and to increase the displacement to strain ratio to maximize the shock resistance of the microgimbals, while that of the process modification effort is to simplify in order to reduce manufacturing cost. The testing effort is to characterize both the static and dynamic performance using precision instrumentation in order to compare various prototype designs.<<ETX>>","1057-7157;1941-0158","","10.1109/84.365366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=365366","","Fabrication;Testing;Silicon;Application software;Design optimization;Computer applications;Disk drives;Design engineering;Microstructure;Microactuators","silicon;micromechanical devices;hard discs;micromachining;dynamic testing;mechanical testing;testing","fabrication;Si microgimbals;supercompact rigid disk drives;design optimization;micron-level static testing;micron-level dynamic testing;micromachined microgimbals;stiffness ratio;compliance;servo bandwidth;displacement to strain ratio;shock resistance;Si","","12","13","","","","","","IEEE","IEEE Journals & Magazines"
"How much information is needed for usage-based reading? A series of experiments","T. Thelin; P. Runeson; C. Wohlin; T. Olsson; C. Andersson","Dept. of Commun. Syst., Lund Univ., Sweden; Dept. of Commun. Syst., Lund Univ., Sweden; NA; NA; NA","Proceedings International Symposium on Empirical Software Engineering","","2002","","","127","138","Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts of a software document by using prioritized use cases. This paper presents a series of three UBR experiments on design specifications, with focus on the third. The first experiment evaluates the prioritization of UBR and the second compares UBR against checklist-based reading. The third experiment investigates the amount of information needed in the use cases and whether a more active approach helps the reviewers to detect more faults. The third study was conducted at two different places with a total of 82 subjects. The general result from the experiments is that UBR works as intended and is efficient as well as effective in guiding reviewers during the preparation phase of software inspections. Furthermore, the results indicate that use cases developed in advance are preferable compared to developing them as part of the preparation phase of the inspection.","","0-7695-1796","10.1109/ISESE.2002.1166932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166932","","Inspection;Software engineering;Bioreactors;Communication system software;Computer science;Technology planning;Fault detection;Testing;Engineering management","software engineering;inspection;program debugging","usage-based reading;experiments;software inspections;software development fault detection;software document;prioritized use cases;design specifications;checklist-based reading","","8","23","","","","","","IEEE","IEEE Conferences"
"Optimizing shadow recovery algorithms","J. Kent; H. Garcia-Molina","Dept. of Electr. Eng. & Comput. Sci., Princeton Univ., NJ, USA; Dept. of Electr. Eng. & Comput. Sci., Princeton Univ., NJ, USA","IEEE Transactions on Software Engineering","","1988","14","2","155","168","Experiments conducted on a database testbed at Princeton indicate excessive page-table I/O is the major performance drawback of shadow recovery. In light of this, a method for parameterizing shadow recovery that minimize page-table I/O without sacrificing to much disk utilization is proposed. Using a simple model, the mechanism is analyzed and evaluated, comparing it to two conventional ones.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4635","","Computer crashes;Software algorithms;Shadow mapping;Testing;Database systems;File systems;Performance analysis;Application software;Transaction databases","input-output programs;optimisation;program testing;system recovery","system recovery;optimisation;shadow recovery algorithms;database testbed;Princeton;page-table I/O;disk utilization","","6","10","","","","","","IEEE","IEEE Journals & Magazines"
"A transputer-based waveform synthesizer for protection relay test and parameter estimation applications","L. R. Dann; H. J. Vermeulen","Dept. of Electr. Eng., Stellenbosch Univ., South Africa; Dept. of Electr. Eng., Stellenbosch Univ., South Africa","Proceedings of IECON'94 - 20th Annual Conference of IEEE Industrial Electronics","","1994","3","","1824","1827 vol.3","Power system faults can give rise to highly complex transient voltage and current waveforms, particularly during the periods immediately following the faults. Modern microprocessor-based protection relays can in principle respond intelligently to such input signals. This gives rise to the need for test equipment with the capability to evaluate protection relay performance for representative waveform data recorded for actual field conditions, or obtained alternatively by simulation using software packages such as the Electromagnetic Transients Program (EMTP). This paper describes a computer-based, multiphase, arbitrary waveform synthesizer for protection relay test applications. The system distinguishes itself from other such arrangements in that it features a highly programmable and versatile transputer-based digital to analog converter facility developed primarily with the view to obtain optimized performance for protection test applications.<<ETX>>","","0-7803-1328","10.1109/IECON.1994.398093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=398093","","Synthesizers;Protective relaying;Testing;Power system protection;Power system relaying;EMTP;Application software;Power system faults;Power system transients;Voltage","power system relaying;relay protection;automatic test equipment;power engineering computing;transputer systems;transputers;waveform generators;digital-analogue conversion;digital simulation;simulation;parameter estimation;power system transients","transputer-based waveform synthesizer;protection relay testing;parameter estimation;transient voltage waveforms;transient current waveforms;microprocessor-based protection relays;simulation software packages;Electromagnetic Transients Program;EMTP;digital to analog converter","","1","7","","","","","","IEEE","IEEE Conferences"
"A configurable logic architecture for dynamic hardware/software partitioning","R. Lysecky; F. Vahid","Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","480","485 Vol.1","In previous work, we showed the benefits and feasibility of having a processor dynamically partition its executing software such that critical software kernels are transparently partitioned to execute as a hardware coprocessor on configurable logic - an approach we call warp processing. The configurable logic place and route step is the most computationally intensive part of such hardware/software partitioning, normally running for many minutes or hours on powerful desktop processors. In contrast, dynamic partitioning requires place and route to execute in just seconds and on a lean embedded processor. We have therefore designed a configurable logic architecture specifically for dynamic hardware/software partitioning. Through experiments with popular benchmarks, we show that by specifically focusing on the goal of software kernel speedup when designing the FPGA architecture, rather than on the more general goal of ASIC prototyping, we can perform place and route for our architecture 50 times faster, using 10,000 times less data memory, and 1,000 times less code memory, than popular commercial tools mapping to commercial configurable logic. Yet, we show that we obtain speedups (2x on average, and as much as 4x) and energy savings (33% on average, and up to 74%) when partitioning even just one loop, which are comparable to commercial tools and fabrics. Thus, our configurable logic architecture represents a good candidate for platforms that will support dynamic hardware/software partitioning, and enables ultra-fast desktop tools for hardware/software partitioning, and even for fast configurable logic design in general.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268892","","Computer architecture;Hardware;Logic design;Software tools;Kernel;Coprocessors;Software performance;Field programmable gate arrays;Application specific integrated circuits;Software prototyping","logic partitioning;hardware-software codesign;system-on-chip;reconfigurable architectures;dynamic programming;field programmable gate arrays;just-in-time","configurable logic architecture;dynamic partitioning;processor partitioning;warp processing;hardware-software partitioning;embedded processor;software kernel;FPGA architecture;FPGA fabric;field programmable gate arrays;ASIC prototyping;system-on-chip;dynamic optimization;just-in-time compilation;reconfigurable computing;software kernels;desktop processors","","25","33","","","","","","IEEE","IEEE Conferences"
"Dynamic performance optimization mechanism for parallel object-oriented database programming languages","K. Kimura; H. Amano; A. Makinouchi","Graduate Sch. of Inf. Sci. & Electr. Eng., Kyushu Univ., Fukuoka, Japan; NA; NA","Proceedings 2000 International Database Engineering and Applications Symposium (Cat. No.PR00789)","","2000","","","405","409","Discusses dynamic performance optimization mechanisms for parallel object-oriented database programming languages. When a large number of objects are distributed on a distributed-memory parallel processor, referencing a remote object requires inter-processor communication. If those objects are not allocated properly, it may cause total performance degradation. When those objects are allocated with a biased distribution among the processing elements, it may cause a load imbalance. However, the information necessary for relocating objects must be obtained at run time, since the number of objects on a processor and the topology of the objects in a database are unknown at compilation time. This paper proposes a new approach to dynamic performance optimization method, and evaluates the results through simulation tests.","","0-7695-0789","10.1109/IDEAS.2000.880624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=880624","","Dynamic programming;Optimization;Object oriented databases;Object oriented programming;Parallel programming;Computer languages;Data structures;Runtime;Concurrent computing;Spatial databases","parallel languages;object-oriented languages;database languages;software performance evaluation;optimisation;distributed memory systems;resource allocation;parallel databases;object-oriented databases","dynamic performance optimization;parallel object-oriented database programming languages;distributed-memory parallel processor;remote object referencing;inter-processor communication;object allocation;performance degradation;biased distribution;load imbalance;object relocation;topology;simulation tests","","","7","","","","","","IEEE","IEEE Conferences"
"Laboratory and computer tests for Carson's FM bandwidth rule","R. J. Pieper","Dept. of Electr. & Comput. Eng., Naval Postgraduate Sch., Monterey, CA, USA","Proceedings of the 33rd Southeastern Symposium on System Theory (Cat. No.01EX460)","","2001","","","145","149","The commonly cited significance for Carson's FM bandwidth is that it defines spectral limits which contain 98% or more of the spectrum power. The 98% rule typically requires enough computations to discourage its application for in-situ laboratory type measurements. Furthermore, the 98% rule will only provide an upper bound for Carson's bandwidth rule. What is suggested in this paper is a more direct, simpler approach which provides both upper and lower bounds. The sideband reduction method described, is based on taking visual cues for sideband reduction levels measured with respect to the unmodulated carrier. Computations required by the method are not extensive. In addition, the sideband reduction method is confirmable using an FFT test on the computer.","","0-7803-6661","10.1109/SSST.2001.918507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918507","","Laboratories;Testing;Bandwidth;Frequency modulation;Amplitude modulation;Power engineering computing;Upper bound;Phase modulation;Application software;Electric variables measurement","frequency modulation;bandwidth compression;optimisation;testing","Carson bandwidth rule;upper bound;FM bandwidth;lower bounds;sideband reduction;unmodulated carrier;FFT test","","2","2","","","","","","IEEE","IEEE Conferences"
"Simulation of the remote unit assembly and test: a case study","J. Fields; D. Davis; A. Taylor","AT&T Wireless Services, Redmond, WA, USA; NA; NA","2000 Winter Simulation Conference Proceedings (Cat. No.00CH37165)","","2000","2","","1351","1354 vol.2","This paper presents a case study on the use of simulation to develop and implement an assembly line for the assembly and test of customer located telephony equipment. The simulation model was used as a tool to assist in development and integration of the assembly and test processes with a focus on capacity, material flow optimization, and equipment layout. The authors discuss how the model affected the facilities layout, equipment specifications, and material flow.","","0-7803-6579","10.1109/WSC.2000.899108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899108","","Assembly;Computer aided software engineering;Product design;Consumer electronics;Materials testing;Telephony;Optimized production technology;Optimization methods;Team working;Electronic equipment testing","assembling;computer aided facilities layout;digital simulation","remote unit assembly and test simulation;assembly line;simulation model;material flow optimization;equipment layout;facilities layout;equipment specifications;material flow","","1","2","","","","","","IEEE","IEEE Conferences"
"Variable precision reaching definitions analysis for software maintenance","P. Tonella; G. Antoniol; R. Fiutem; E. Merlo","Ist. per la Ricerca Sci. e Tecnologica, Trento, Italy; NA; NA; NA","Proceedings. First Euromicro Conference on Software Maintenance and Reengineering","","1997","","","60","67","A flow analyzer can be very helpful in the process of program understanding, by providing the programmer with different views of the code. As the documentation is often incomplete or inconsistent, it is extremely useful to extract the information a programmer may need directly from the code. Program understanding activities are interactive, thus program analysis tools may be asked for quick answers by the maintainer. Therefore the control on the trade-off between accuracy and efficiency should be given to the user. The paper presents an approach to interprocedural reaching definitions flow analysis based on three levels of precision depending on the sensitivity to the calling context and the control flow. A lower precision degree produces an overestimate of the data dependencies in a program. The result is anyhow conservative (all dependencies which hold are surely reported), and definitely faster than the more accurate counterparts. A tool supporting reaching definition analysis in the three variants has been developed. The results on a test suite show that three orders of magnitude can be gained in execution times by the less accurate analysis, but 57.4% extra dependencies are on average added. The intermediate variant is much more precise (1.6% extra dependencies), but gains less in times (one order of magnitude).","","0-8186-7892","10.1109/CSMR.1997.583007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=583007","","Software maintenance;Flow graphs;Couplings;Algorithm design and analysis;Testing;Electric variables control;Software algorithms;Optimizing compilers;Data mining;Debugging","reverse engineering;software maintenance;data flow analysis;reachability analysis;programming;software tools","variable precision reaching definitions analysis;software maintenance;flow analyzer;program understanding;programmer;code views;documentation;information extraction;program analysis tools;efficiency;accuracy;interprocedural reaching definitions flow analysis;calling context;control flow;data dependencies;execution times","","5","22","","","","","","IEEE","IEEE Conferences"
"Real-time processing of a software defined W-CDMA modem","Gweon-Do Jo; Kyung-Seok Kim; Jin-Up Kim","SDR Res. Team, Korea Electron. & Telecommun. Res. Inst., Daejeon, South Korea; SDR Res. Team, Korea Electron. & Telecommun. Res. Inst., Daejeon, South Korea; SDR Res. Team, Korea Electron. & Telecommun. Res. Inst., Daejeon, South Korea","IEEE 60th Vehicular Technology Conference, 2004. VTC2004-Fall. 2004","","2004","3","","1959","1962 Vol. 3","As the code division multiple access (CDMA) based third generation cellular infrastructure requires high performance signal processing in the baseband modem, a common approach of chip rate processing has been to use an application specific integrated circuit (ASIC) or field programmable gate array (FPGA) implementation. The use of digital signal processors (DSP) is explored for a wideband CDMA (W-CDMA) channel modem with the goal of increasing flexibility. Design concepts of a software defined radio (SDR) platform we implemented are presented, including its hardware-software architecture and test results. Practical issues for the optimization of DSP software are also discussed in detail.","1090-3038","0-7803-8521","10.1109/VETECF.2004.1400380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1400380","","Multiaccess communication;Modems;Application specific integrated circuits;Field programmable gate arrays;Signal generators;Array signal processing;Baseband;Digital signal processors;Digital signal processing chips;Wideband","software radio;field programmable gate arrays;digital signal processing chips;code division multiple access;modems;3G mobile communication;cellular radio;hardware-software codesign;optimisation","real-time processing;software defined modem;software defined W-CDMA modem;third generation cellular infrastructure;baseband modem;chip rate processing;ASIC;FPGA;digital signal processors;DSP;wideband CDMA;software defined radio;SDR;hardware-software architecture","","","8","","","","","","IEEE","IEEE Conferences"
"Built-in self-test with weighted random pattern hardware","F. Brglez; G. Gloster; G. Kedem","Microelectron. Center of North Carolina, Research Triangle Park, NC, USA; Microelectron. Center of North Carolina, Research Triangle Park, NC, USA; Microelectron. Center of North Carolina, Research Triangle Park, NC, USA","Proceedings., 1990 IEEE International Conference on Computer Design: VLSI in Computers and Processors","","1990","","","161","166","The authors address scan-based built-in self-test (BIST) of digital circuits that are highly resistant to testing with uniform random patterns. Introducing a procedure, the precompute test patterns for random-pattern resistant faults and generate optimized distributions of weights that guarantee pattern coverage in a given number of random trials. The software implementation offers a tradeoff in the number of distributions (hardware memory) and the length of the total test time. The hardware implementation is based on a canonic weighting circuit that interfaces to a circulating memory and a pseudo-random source.<<ETX>>","","0-8186-2079","10.1109/ICCD.1990.130190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=130190","","Built-in self-test;Hardware;Circuit testing;Circuit faults;Test pattern generators;Benchmark testing;Automatic testing;Costs;Circuit synthesis;Partitioning algorithms","built-in self test;digital integrated circuits;integrated circuit testing;logic testing","scan-based built-in self-test;BIST;digital circuits;uniform random patterns;precompute test patterns;random-pattern resistant faults;canonic weighting circuit;circulating memory;pseudo-random source","","29","11","","","","","","IEEE","IEEE Conferences"
"Validation and reliability estimation of a fingerprint image registration software","D. Desovski; V. Gandikota; Yan Liu; Yue Jiang; B. Cukic","Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA; Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA; Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA; Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA; Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA","15th International Symposium on Software Reliability Engineering","","2004","","","306","313","The application of biometric devices and systems is experiencing significant growth, primarily due to the need for reliable authentication. Verification and validation techniques applicable to these systems are rather immature and ad hoc, yet the consequences of the wide deployment of biometric systems could be significant. In this paper we discuss an approach to validation and reliability estimation of a fingerprint registration software. Our validation approach includes the following three steps; a) The validation of the source code with respect to the system requirements specification; b) the validation of the optimization algorithm, which is in the core of the registration system and c) the automation of testing. Since the optimization algorithm is heuristic in nature, mathematical analysis and test results are used to estimate the reliability of the image registration module.","1071-9458","0-7695-2215","10.1109/ISSRE.2004.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383127","","Fingerprint recognition;Image matching;Biometrics;Application software;Authentication;Automation;Automatic testing;System testing;Heuristic algorithms;Mathematical analysis","image registration;message authentication;software reliability;fingerprint identification;program verification;formal specification","software reliability estimation;Fingerprint Image Registration Software;biometric device;authentication;verification technique;validation technique;source code;system requirements specification;optimization algorithm;image registration module","","1","16","","","","","","IEEE","IEEE Conferences"
"A unified symbolic evaluation framework for parallelizing compilers","T. Fahringer; B. Scholz","Inst. for Software Sci., Wien Univ., Austria; NA","IEEE Transactions on Parallel and Distributed Systems","","2000","11","11","1105","1125","The quality of many optimizations and analyses of parallelizing compilers depends significantly on the ability to evaluate symbolic expressions and on the amount of information available about program variables at arbitrary program points. In this paper, we describe an effective and unified symbolic evaluation framework that statically determines the values of variables and symbolic expressions, assumptions about and constraints between variable values, and the condition under which control flow reaches a program statement. We introduce the program context, a novel representation for comprehensive and compact control and data flow analysis information. Program contexts are described as first order logic formulas, which allows us to use public domain software for standard symbolic manipulation. Computations are represented as algebraic expressions defined over a program's problem size. Our symbolic evaluation techniques comprise accurate modeling of assignment and input/output statements, branches, loops, recurrences, arrays, and procedures. All of our techniques target both linear, as well as nonlinear, expressions and constraints. Efficiency of symbolic evaluation is highly improved by aggressive simplification techniques. A variety of examples, including program verification, dependence analysis, array privatization, communication vectorization, and elimination of redundant communication, are used to illustrate the effectiveness of our approach. We present results from a preliminary implementation of our framework, which is used as part of a parallelizing compiler that demonstrates the potential performance gains achievable by employing symbolic evaluation to support program parallelization.","1045-9219;1558-2183;2161-9883","","10.1109/71.888633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888633","","Program processors;Information analysis;Optimizing compilers;Data analysis;Privatization;Testing;Logic;Software standards;Performance gain;Context","parallelising compilers;data flow analysis;public domain software;program verification;symbol manipulation","unified symbolic evaluation framework;parallelizing compilers;optimizations;symbolic expressions;program context;data flow analysis;first order logic formulas;public domain software;algebraic expressions;program verification;dependence analysis;array privatization;communication vectorization","","8","57","","","","","","IEEE","IEEE Journals & Magazines"
"Design and integration of new software for the Robot Controller Test Station","B. Mack; M. M. Bayoumi","Dept. of Electr. Eng., Queen's Univ., Kingston, Ont., Canada; Dept. of Electr. Eng., Queen's Univ., Kingston, Ont., Canada","Proceedings, 1989 International Conference on Robotics and Automation","","1989","","","866","873 vol.2","A description is given of the Robot Controller Test Station (RCTS), a software environment for implementing, testing, and evaluating robot control and sensor algorithms in both a simulated and a real robot system. RCTS is designed for flexibility, portability, ease of modification and ease of use. The authors discuss many of the design and implementation issues for RCTS. They outline the steps that a researcher can follow in order to integrate a control, sensing hardware interface or data analysis algorithm into an RCTS-based robot system. These steps do not require the researcher to be an expert programmer or even to comprehend the remainder of the robot software, so he or she can concentrate on algorithm design and optimization.<<ETX>>","","0-8186-1938","10.1109/ROBOT.1989.100091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=100091","","Robot control;Robot sensing systems;Software testing;System testing;Software algorithms;Sensor phenomena and characterization;Sensor systems;Control systems;Hardware;Data analysis","programming environments;robots","programming environment;algorithm implementation;algorithm testing;algorithm evaluation;Robot Controller Test Station;RCTS;software environment","","2","18","","","","","","IEEE","IEEE Conferences"
"An optimistic method for updating information in distributed collaborative work","T. Okubo; T. Matsutsuka; Y. Tanaka; H. Hara; S. Uehara","Fujitsu Labs. Ltd., Kawasaki, Japan; NA; NA; NA; NA","Proceedings. The Twenty-Second Annual International Computer Software and Applications Conference (Compsac '98) (Cat. No.98CB 36241)","","1998","","","400","405","Proposes a new method for updating information in distributed collaborative work. The effective performance of collaborative work distributed among several locations requires that information be replicated and freely updated in each distributed server. This is difficult to achieve with existing information-sharing methods such as the WWW and workflow. Our optimistic lock control method enables the direct updating of replicated information in any server while maintaining consistency among all the servers. A server failure is not critical to other servers because there is no master server. This method differs from the traditional master-slave method in which updates are performed in a specific master server. This paper also addresses the dynamic deployment of a new server. Our method is being used on a trial basis for an international software development project between Japan and India.","0730-3157","0-8186-8585","10.1109/CMPSAC.1998.716687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=716687","","Optimization methods;Collaborative work;Laboratories;World Wide Web;Internet;Read only memory;Master-slave;Electronic switching systems;Design methodology;Testing","groupware;replicated databases;file servers;fault tolerant computing","consistency maintenance;information updating;distributed collaborative work;performance;replicated information;distributed server;information-sharing methods;optimistic lock control method;server failure;dynamic server deployment;international software development project;Japan;India","","2","6","","","","","","IEEE","IEEE Conferences"
"Performance of runtime optimization on BLAST","A. Das; Jiwei Lu; H. Chen; J. Kim; Pen-Chung Yew; Wei-Chung Hsu; Dong-Yuan Chen","Minnesota Univ., Duluth, MN, USA; Minnesota Univ., Duluth, MN, USA; Minnesota Univ., Duluth, MN, USA; Minnesota Univ., Duluth, MN, USA; Minnesota Univ., Duluth, MN, USA; Minnesota Univ., Duluth, MN, USA; NA","International Symposium on Code Generation and Optimization","","2005","","","86","96","Optimization of a real world application BLAST is used to demonstrate the limitations of static and profile-guided optimizations and to highlight the potential of runtime optimization systems. We analyze the performance profile of this application to determine performance bottlenecks and evaluate the effect of aggressive compiler optimizations on BLAST. We find that applying common optimizations (e.g. O3) can degrade performance. Profile guided optimizations do not show much improvement across the board, as current implementations do not address critical performance bottlenecks in BLAST. In some cases, these optimizations lower performance significantly due to unexpected secondary effects of aggressive optimizations. We also apply runtime optimization to BLAST using the ADORE framework. ADORE is able to detect performance bottlenecks and deploy optimizations resulting in performance gains up to 58% on some queries using data cache prefetching.","","0-7695-2298","10.1109/CGO.2005.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402079","","Runtime;Optimizing compilers;Program processors;Testing;Computer science;Microprocessors;Application software;Performance analysis;Degradation;Performance gain","optimising compilers;cache storage;performance evaluation","runtime optimization performance;BLAST;real world application;compiler optimization;profile guided optimization;ADORE;data cache prefetching","","4","28","","","","","","IEEE","IEEE Conferences"
"The far ultraviolet spectroscopic explorer (FUSE) instrument data system","B. K. Heggestad; R. C. Moore","Appl. Phys. Lab., Johns Hopkins Univ., Laurel, MD, USA; NA","Gateway to the New Millennium. 18th Digital Avionics Systems Conference. Proceedings (Cat. No.99CH37033)","","1999","2","","7.C.2","7.C.2","This paper describes the architecture for the IDS flight hardware and its real-time embedded flight software. The design uses commercial off-the-shelf (COTS) software components as much as possible, to reduce cost and software development time. The features of the IDS design that provide radiation hardness and fault tolerance are described. Implementation of software to meet the functional requirements is accomplished using a relatively small number of prioritized real-time tasks. A commercial real-time operating system kernel manages and supports these tasks. Inter-task communication is described, as are the software test and validation methods. The paper shows how custom ground support equipment was developed to facilitate software development and testing. Reliable communications between the IDS and the FUSE spacecraft bus are accomplished using a MIL-STD-1553B bus that has an imposed, deterministic real-time protocol. Similarly, communication between the IDS and the other instrument subsystems uses a second MIL-STD-1553B bus that has its own time division multiplex real-time protocol. The design of these real-time protocols is described, with particular attention to reliability and testability.","","0-7803-5749","10.1109/DASC.1999.821995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821995","","Spectroscopy;Fuses;Instruments;Intrusion detection;Protocols;Programming;Software testing;Computer architecture;Hardware;Embedded software","aerospace instrumentation;ultraviolet spectroscopy;spectroscopy computing;aerospace computing;embedded systems;software architecture;system buses;computer architecture;radiation hardening (electronics);space vehicle electronics;software fault tolerance;astronomy computing;satellite telemetry","FUSE;architecture;IDS flight hardware;real-time embedded flight software;COTS software;cost;development time;radiation hardness;fault tolerance;prioritized real-time tasks;real-time operating system kernel;inter-task communication;custom ground support equipment;software development;testing;MIL-STD-1553B bus;real-time protocol;time division multiplex real-time protocol;testability;reliability","","","","","","","","","IEEE","IEEE Conferences"
"Multilayer transient-mode CNN for solving optimization problems","P. R. Bakic; B. D. Reljin; N. S. Vujovic; D. P. Brazakovic; P. D. Kostic","Dept. of Comput. Sci. & Electr. Eng., Lehigh Univ., Bethlehem, PA, USA; NA; NA; NA; NA","1996 Fourth IEEE International Workshop on Cellular Neural Networks and their Applications Proceedings (CNNA-96)","","1996","","","25","30","An implementation of analog parallel network for solving image optimization problems is presented. The implementation is based on a multilayer cellular neural network (CNN). A general optimization procedure is divided into subfunctions each of which is realized by a network layer. The problem of multilevel halftoning of images is used as an example of the optimization procedure. The resulting network which consists of four layers is described in the paper. The network was simulated on a digital computer and its performance was evaluated using a set of test images. Results of the tests and other potential applications of the proposed network are discussed.","","0-7803-3261","10.1109/CNNA.1996.566484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566484","","Nonhomogeneous media;Cellular neural networks;Iterative algorithms;Multi-layer neural network;Testing;Image processing;Design optimization;Application software;Electronic mail;Computational modeling","computer vision;image enhancement;optimisation;transients;parallel processing;performance evaluation;cellular neural nets","multilayer transient-mode CNN;image optimization;analog parallel network;multilayer cellular neural network;subfunctions;multilevel halftoning;performance evaluation","","4","9","","","","","","IEEE","IEEE Conferences"
"Efficient object-oriented integration and regression testing","Y. Le Traon; T. Jeron; J. -. Jezequel; P. Morel","IRISA, Rennes, France; NA; NA; NA","IEEE Transactions on Reliability","","2000","49","1","12","25","This paper presents a model, a strategy and a methodology for planning integration and regression testing from an object-oriented model. It shows how to produce a model of structural system test dependencies which evolves with the refinement process of the object-oriented design. The model (test dependency graph) serves as a basis for ordering classes and methods to be tested for regression and integration purposes (minimization of test stubs). The mapping from unified modeling language to the defined model is detailed as well as the test methodology. While the complexity of optimal stub minimization is exponential with the size of the model, an algorithm is given that: computes a strategy for integration testing with a quadratic complexity in the worst case; and provides an efficient testing order for minimizing the number of stubs. Various integration strategies are compared with the optimized algorithm (a real-world case study illustrates this comparison). The results of the experiment seem to give nearly optimal stubs with a low cost despite the exponential complexity of getting optimal stubs. As being a part of a design-for-testability approach, the presented methodology also leads to the early repartition of testing resources during system integration for reducing integration duration.","0018-9529;1558-1721","","10.1109/24.855533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855533","","Object oriented modeling;System testing;Unified modeling language;Minimization methods;Software testing;Refining;Strategic planning;Cost function;Software algorithms;Charge coupled devices","software reliability;program testing;object-oriented methods","object-oriented integration;regression testing;object-oriented design;test dependency graph;unified modeling language;optimal stub minimization;quadratic complexity;integration strategies;design-for-testability approach","","59","15","","","","","","IEEE","IEEE Journals & Magazines"
"Computerized exercise ECG testing and measurement for optimizing the prediction of coronary artery disease","J. N. Froning; V. F. Froelicher","Sunnyside Biomed. Syst. & Software, Carlsbad, Palo Alto, CA, USA; NA","Proceedings Computers in Cardiology","","1992","","","95","98","A series of studies has been undertaken to evaluate computer processing of the exercise electrocardiogram (ECG) for predicting the presence and severity of angiographic coronary artery disease (CAD). Over 400 patients at the Long Beach VA Medical Center have undergone both exercise treadmill testing and cardiac catheterization with exercise ECG being stored on optical disks. Preliminary analyses have focused on visual-versus-computer measures, the contribution of traditional ST measures, and proposed exercise ECG scores. A summary of key findings is reported. Both discriminant function analyses and receiver operating characteristic curves have been used to make comparisons. The strongest predictors of CAD have been found to be ST changes during recovery.<<ETX>>","","0-8186-3552","10.1109/CIC.1992.269438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=269438","","Electrocardiography;Arteries;Coronary arteriosclerosis;Biomedical measurements;Computer displays;Measurement standards;Biomedical computing;Medical tests;System testing;Medical diagnostic imaging","electrocardiography;medical signal processing","coronary artery disease prediction;discriminant function analysis;receiver operating characteristic curves;Long Beach VA Medical Center;exercise treadmill testing;cardiac catheterization;ST measures","","","16","","","","","","IEEE","IEEE Conferences"
"Software-controlled cache architecture for energy efficiency","Chia-Lin Yang; Hung-Wei Tseng; Chia-Chiang Ho; Ja-Ling Wu","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA","IEEE Transactions on Circuits and Systems for Video Technology","","2005","15","5","634","644","Power consumption is an important design issue of current multimedia embedded systems. Data caches consume a significant portion of total processor power for multimedia applications because they are data intensive. In an integrated multimedia system, the cache architecture cannot be tuned specifically for an application. Therefore, a significant amount of cache energy is actually wasted. In this paper, we propose the software-controlled cache architecture that improves the energy efficiency of the shared cache in an integrated multimedia system on an application-specific base. Data types in an application are allocated to different cache regions. On each access, only the allocated cache regions need to be activated. We test the effectiveness of the software-controlled cache of the MPEG-2 software decoder. The results show up to 40% of cache energy reduction on an ARM-like cache architecture without sacrificing performance.","1051-8215;1558-2205","","10.1109/TCSVT.2005.846444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1425528","Dynamic power dissipation;energy consumption;MPEG-2;multimedia applications;software-controlled cache","Energy efficiency;Energy consumption;Multimedia systems;Application software;Computer architecture;Embedded system;Costs;Energy dissipation;Space exploration;Software testing","cache storage;embedded systems;optimisation;power consumption;multimedia systems;software engineering;control engineering computing","software-controlled cache architecture;energy efficiency optimization;multimedia embedded system;dynamic power dissipation","","5","35","","","","","","IEEE","IEEE Journals & Magazines"
"System optimization without numerical target","H. Takagi","Dept. of Acoust. Design, Kyushu Inst. of Design, Fukuoka, Japan","Proceedings of North American Fuzzy Information Processing","","1996","","","351","354","Describes the importance of the challenging task of system optimization based on human preferences or subjective evaluation, and shows how to approach the task. As an example of this type of approach, an interactive genetic algorithm (GA) is introduced, and its advantages and disadvantages are discussed. To solve the disadvantages, this paper proposes two methods to reduce the burden of interactive GA operators, and evaluates their performances. Subjective tests have shown that the proposed methods are significantly better for human operators than conventional interfaces from a statistical test point of view.","","0-7803-3225","10.1109/NAFIPS.1996.534758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534758","","Humans;Design optimization;Fuzzy systems;Neural networks;Performance evaluation;System testing;Genetic algorithms;Control systems;Consumer products;Washing machines","genetic algorithms;interactive systems;human factors;user interfaces;software performance evaluation;statistical analysis;systems engineering;fuzzy systems;engineering computing","system optimization;numerical targetless optimization;human preferences;subjective evaluation;interactive genetic algorithm;human operator burden;performance evaluation;user interfaces;statistical tests","","6","17","","","","","","IEEE","IEEE Conferences"
"Practical Priorities in System Testing","N. H. Petschenik","Bell Communications Research, Inc","IEEE Software","","1985","2","5","18","23","During the system test phase, ""thorough testing"" can pass the limits of practicality. Test case selection, based on simple priority rules, is one solution to the problem of practicality vs. thoroughness.","0740-7459;1937-4194","","10.1109/MS.1985.231755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1695401","","System testing;Software testing;Size measurement;Application software;Centralized control;Control systems;Central office;Manuals","","","","16","11","","","","","","IEEE","IEEE Journals & Magazines"
"An optimization technique for ordered (binary) decision diagrams","V. Dvorak","Tech. Univ. of Brno, Czechoslovakia","CompEuro 1992 Proceedings Computer Systems and Software Engineering","","1992","","","1","4","The minimum-cost ordered (binary) decision diagram (OBDD) (also a reduced OBDD or ROBDD) is a canonical representation for a logic function, given an ordering on its variables (R. Bryant, 1986). A new optimization technique is presented for suboptimal synthesis of ODDs of complete as well as partial multiple-output Boolean functions. The method is based on iterative decomposition. The central notion in this process is that of subfunctions, whereas in ODDs there are decision nodes. There is, however, 1:1 mapping between them: a level of decision nodes in the ODD corresponds to a set of subfunctions recognized in a corresponding decomposition step. The technique is computationally effective and deals with incomplete functions frequently used in practice. A small synthesis example is given to introduce a new technique for ROBDDs. The results and some experience with the optimization program are described.<<ETX>>","","0-8186-2760","10.1109/CMPEUR.1992.218496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=218496","","Boolean functions;Data structures;Network synthesis;Iterative methods;Circuit testing;Cost function;Binary decision diagrams;Computational efficiency;Digital systems;Sequential circuits","Boolean functions;decision theory;logic design;optimisation","reduced OBDD;ROBDD;canonical representation;logic function;ordering;optimization technique;suboptimal synthesis;ODDs;partial multiple-output Boolean functions;iterative decomposition;subfunctions;decomposition step;computationally effective;incomplete functions;synthesis example;optimization program","","2","9","","","","","","IEEE","IEEE Conferences"
"Virtual instrument parameter calibration with particle swarm optimization","Peng Yu; Peng Xiyuan; Meng Shengwei","Harbin Inst. of Technol., China; Harbin Inst. of Technol., China; Harbin Inst. of Technol., China","Proceedings of the 2003 IEEE Swarm Intelligence Symposium. SIS'03 (Cat. No.03EX706)","","2003","","","42","45","In virtual instrument designs and applications, lots of functional parameters can be set through software methods. Currently, most parameter settings methods are lightly linked with the knowledge of instruments and basic principles related to specific applications. However, it is difficult for some end users to deal with those advanced operations. By adopting the particle swarm optimization (PSO) algorithm, the adaptive set and calibration of instrument parameters can be achieved by software with computational intelligence. Experiments and applications showed that the adaptive parameter calibration method based on the PSO can enhance the effectiveness of debugging and maintenance of virtual instrument and test system.","","0-7803-7914","10.1109/SIS.2003.1202245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202245","","Instruments;Calibration;Particle swarm optimization;Optimization methods;Artificial neural networks;Industrial training;Partial response channels;Temperature;Moisture;Knowledge engineering","evolutionary computation;optimisation;calibration;virtual instrumentation;program debugging;software maintenance","virtual instrument parameter calibration;particle swarm optimization;PSO algorithm;computational intelligence;adaptive parameter calibration;debugging;maintenance","","1","11","","","","","","IEEE","IEEE Conferences"
"Genetic multiway partitioning","K. Shahookar; P. Mazumder","Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA; Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA","Proceedings of the 8th International Conference on VLSI Design","","1995","","","365","369","This research investigates a new software tool for Genetic Partitioning. The Genetic Algorithm is used to perform the partitioning with a significant improvement in result quality. Furthermore, it can optimize a cost function with multiple objectives and constraints. Separate algorithms have been developed, fine-tuned for bipartitioning and multiway partitioning. The bipartitioning problem is represented as a binary chromosome. Efficient bit-mask operations perform crossover, mutation, and net cut evaluation 32 bits at a time, without unpacking. The multiway partitioning algorithm has a global view of the problem, and generates/optimizes all the necessary partitions simultaneously. The algorithms were tested on the MCNC benchmark circuits, and the cut size obtained was lower than that for the conventional Fiduccia-Mattheyses algorithm.","1063-9667","0-8186-6905","10.1109/ICVD.1995.512140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=512140","","Partitioning algorithms;Circuit testing;Software tools;Genetic algorithms;Constraint optimization;Cost function;Biological cells;Performance evaluation;Genetic mutations;Benchmark testing","VLSI;circuit CAD;cellular arrays;logic CAD;logic partitioning;genetic algorithms;software tools;circuit optimisation","genetic multiway partitioning;software tool;result quality;cost function;multiple objectives;bipartitioning;binary chromosome;bit-mask operations;crossover;mutation;net cut evaluation;MCNC benchmark circuits;cut size;CAD;VLSI","","3","15","","","","","","IEEE","IEEE Conferences"
"Next Generation Test Generator (NGTG) for digital circuits","S. Singer; L. Vanetsky","NAWCAD, Lakehurst, NJ, USA; NA","1997 IEEE Autotestcon Proceedings AUTOTESTCON '97. IEEE Systems Readiness Technology Conference. Systems Readiness Supporting Global Needs and Awareness in the 21st Century","","1997","","","105","112","The process outlined in this paper describes the system developed to meet the goals of the Next Generation Test Generator program, funded by the Office of Naval Research. This system takes advantage of an unsupervised pattern classification algorithm (Adaptive Resonance Theory (ART)) and a Genetic Algorithm (GA) that is combined to form an optimizing control system. The GA generates a population of test patterns (individuals). Each individual is provided as a set of timed inputs to behavior based simulations representing good and faulty systems. The response of each model (good and faulty) is recombined in the form of an image matrix with each row representing a signature of each of the different circuits. FuzzyART (Fuzzy Logic Based ART) provides a method of image recognition, extracting those images that are distinctly different from any other. Each individual generated by the GA is provided as input to the list of models, then evaluated by FuzzyART and a fitness representing the number of separate classes is formed. New test sequences evolve with increasing fault isolation and detection. The process is repeated until a maximum number of models have been identified and separated. A selective breading algorithm was included to reduce the need for large populations, thus increasing the speed to converge to the ""best test"". The process was demonstrated using a commercial simulator based on Verilog HDL with a simple master/slave flip-flop and a moderately complex digital circuit (real UUT).","","0-7803-4162","10.1109/AUTEST.1997.633572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633572","","Circuit testing;Digital circuits;Circuit faults;Subspace constraints;Circuit simulation;Hardware design languages;System testing;Pattern classification;Classification algorithms;Programmable control","unsupervised learning;image classification;automatic test equipment;genetic algorithms;fuzzy logic;fault location;image sequences;digital simulation;circuit testing;logic testing;software engineering;digital circuits","digital circuits;Office of Naval Research;unsupervised pattern classification algorithm;Adaptive Resonance Theory;Genetic Algorithm;optimizing control system;test patterns;simulations;image matrix;Fuzzy Logic;image recognition;image extraction;selective breading algorithm;Verilog HDL;commercial simulator;master/slave flip-flop;complex digital circuit","","1","8","","","","","","IEEE","IEEE Conferences"
"An Automatic System Coupling Large Signal Measurement Set up and an Optimization Software for Power Amplifier Design","J. M. Nebus; J. P. Villotte; J. F. Vidalou; M. Aubourg; L. Hagerman; H. Jallageas","IRCOM - Universit de Limoges - 123, Avenue Albert-Thomas 87060 LIMOGES CEDEX - FRANCE; IRCOM - Universit de Limoges - 123, Avenue Albert-Thomas 87060 LIMOGES CEDEX - FRANCE; IRCOM - Universit de Limoges - 123, Avenue Albert-Thomas 87060 LIMOGES CEDEX - FRANCE; IRCOM - Universit de Limoges - 123, Avenue Albert-Thomas 87060 LIMOGES CEDEX - FRANCE; IRCOM - Universit de Limoges - 123, Avenue Albert-Thomas 87060 LIMOGES CEDEX - FRANCE; IRCOM - Universit de Limoges - 123, Avenue Albert-Thomas 87060 LIMOGES CEDEX - FRANCE","1988 18th European Microwave Conference","","1988","","","327","332","An automatic active source and load pull system has been designed. The behaviour of devices under large signal conditions is given by a data file. Associated optimization software gives the optimum operating conditions.","","","10.1109/EUMA.1988.333836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4132524","","Power measurement;Software measurement;Design optimization;Power amplifiers;Signal design;Calibration;Power harmonic filters;Frequency measurement;System testing;Reflection","","","","1","6","","","","","","IEEE","IEEE Conferences"
"A study of the speedups and competitiveness of FPGA soft processor cores using dynamic hardware/software partitioning","R. Lysecky; F. Vahid","Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA","Design, Automation and Test in Europe","","2005","","","18","23 Vol. 1","Field programmable gate arrays (FPGAs) provide designers with the ability to create hardware circuits quickly. Increases in FPGA configurable logic capacity and decreasing FPGA costs have enabled designers to incorporate FPGAs more readily in their designs. FPGA vendors have begun providing configurable soft processor cores that can be synthesized onto their FPGA products. While FPGAs with soft processor cores provide designers with increased flexibility, such processors typically have degraded performance and energy consumption compared to hard-core processors. Previously, we proposed warp processing, a technique capable of optimizing a software application by dynamically and transparently re-implementing critical software kernels as custom circuits in on-chip configurable logic. We now study the potential of a MicroBlaze soft-core based warp processing system to eliminate the performance and energy overhead of a soft-core processor compared to a hard-core processor. We demonstrate that the soft-core based warp processor achieves average speedups of 5.8 and energy reductions of 57% compared to the soft core alone. Our data shows that a soft-core based warp processor yields performance and energy consumption competitive with existing hard-core processors, thus expanding the usefulness of soft processor cores on FPGAs to a broader range of applications.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395522","Hardware/software partitioning;warp processing;FPGA;dynamic optimization;soft cores;MicroBlaze","Field programmable gate arrays;Hardware;Energy consumption;Programmable logic arrays;Logic design;Costs;Circuit synthesis;Process design;Degradation;Application software","field programmable gate arrays;hardware-software codesign;power consumption","speedup;competitiveness;FPGA soft processor cores;dynamic hardware/software partitioning;field programmable gate arrays;hardware circuits;configurable logic capacity;configurable soft processor cores;energy consumption;hard-core processors;warp processing;software kernels;on-chip configurable logic;soft-core processor","","37","28","","","","","","IEEE","IEEE Conferences"
"Microprocessor design verification by two-phase evolution of variable length tests","J. E. Smith; M. Bartley; T. C. Fogarty","Fac. of Comput. Studies & Math., Univ. of the West of England, Bristol, UK; NA; NA","Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)","","1997","","","453","458","This paper discusses the use of a genetic algorithm to generate test programs for the verification of the design of a modern microprocessor. The algorithm directly learns sequences of assembly-code instructions which satisfy a coverage metric for one specific part of a design. The complexity of the design is such that it is not simple to predict in advance the length of the program needed to achieve coverage, and there is a severe time penalty for evaluating long tests. This has led to the development of a genetic algorithm which uses a two phase mechanism for variation in string length, through maintenance of a diverse population with varying lengths coupled with a ""meta-algorithm"" for periodic larger increases.","","0-7803-3949","10.1109/ICEC.1997.592354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=592354","","Microprocessors;Testing;Genetic algorithms;Algorithm design and analysis;Logic design;Stress;Mathematics;Microelectronics;Computer science;Distributed control","computer testing;integrated circuit testing;microprocessor chips;formal verification;circuit CAD;genetic algorithms;circuit optimisation;automatic test software;assembly language;instruction sets","microprocessor design verification;two-phase evolution;variable length tests;genetic algorithm;test program generation;assembly-code instructions;coverage metric;design complexity;time penalty;string length;meta-algorithm","","7","11","","","","","","IEEE","IEEE Conferences"
"RSCS: a parallel simplex algorithm for the Nimrod/O optimization toolset","A. Lewis; D. Abramson; T. Peachey","Div. of Inf. Services, Griffith Univ., Brisbane, Qld., Australia; NA; NA","Third International Symposium on Parallel and Distributed Computing/Third International Workshop on Algorithms, Models and Tools for Parallel Computing on Heterogeneous Networks","","2004","","","71","78","This paper describes a method of parallelisation of the popular Nelder-Mead simplex optimization algorithms that can lead to enhanced performance on parallel and distributed computing resources. A reducing set of simplex vertices are used to derive search directions generally closely aligned with the local gradient. When tested on a range of problems drawn from real-world applications in science and engineering, this reducing set concurrent simplex (RSCS) variant of the Nelder-Mead algorithm compared favourably with the original algorithm, and also with the inherently parallel multidirectional search algorithm (MDS). All algorithms were implemented and tested in a general-purpose, grid-enabled optimization toolset.","","0-7695-2210","10.1109/ISPDC.2004.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1372051","Parallel programming;optimization;Nelder-Mead algorithm","Optimization methods;Distributed computing;Design optimization;Computer science;Testing;Design engineering;Process design;Software engineering;Algorithm design and analysis;Numerical simulation","optimisation;grid computing;parallel programming;parallel algorithms;search problems;concurrency control","RSCS;parallel simplex algorithm;Nimrod/O optimization toolset;parallelisation method;Nelder-Mead simplex optimization algorithms;parallel computing;distributed computing;simplex vertices;search directions;reducing set concurrent simplex;parallel multidirectional search algorithm;grid-enabled optimization toolset;parallel programming","","6","19","","","","","","IEEE","IEEE Conferences"
"Swarm intelligence for permutation optimization: a case study of n-queens problem","Xiaohui Hu; R. C. Eberhart; Yuhui Shi","Dept. of Biomed. Eng., Purdue Univ., West Lafayette, IN, USA; NA; NA","Proceedings of the 2003 IEEE Swarm Intelligence Symposium. SIS'03 (Cat. No.03EX706)","","2003","","","243","246","This paper introduces a modified particle swarm optimizer which deals with permutation problems. Particles are defined as permutations of a group of unique values. Velocity updates are redefined based on the similarity of two particles. Particles change their permutations with a random rate defined by their velocities. A mutation factor is introduced to prevent the current pBest from becoming stuck at local minima. Preliminary study on the n-queens problem shows that the modified PSO is promising in solving constraint satisfaction problems.","","0-7803-7914","10.1109/SIS.2003.1202275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202275","","Particle swarm optimization;Computer aided software engineering;Artificial intelligence;Optical computing;Biomedical engineering;Genetic mutations;Benchmark testing;Artificial neural networks;Genetic algorithms;Concurrent computing","evolutionary computation;optimisation;constraint theory;problem solving;search problems","swarm intelligence;permutation optimization;n-queens problem;modified particle swarm optimizer;velocity updates;unique values;mutation factor;pBest;PSO;constraint satisfaction problems","","24","11","","","","","","IEEE","IEEE Conferences"
"An approach to designing accelerated life-testing experiments","J. A. Clark; U. S. Garganese; R. S. Swarz","MITRE Corp., Bedford, MA, USA; NA; NA","Annual Reliability and Maintainability Symposium","","1997","","","242","248","This paper presents and evaluates an approach to designing accelerated life testing (ALT) experiments. We believe that knowing the design limits of the test item is critical to successful ALT. Unfortunately, effective methods do not exist for analytically predicting the design limits of most electronic items; therefore, the basis of our approach is a destructive evaluation performed on a small number of test items to measure their design limits. Once the design limits have been established, environmental stress levels can be tailored to achieve the objectives of the accelerated life test, which may include optimizing it for greater acceleration, accuracy, and/or statistical confidence. The approach is oriented toward ALT of electronic systems using multiple stresses and is most applicable to low-cost, high-volume production items. We have evaluated our approach by applying it to a commercial off-the-shelf single-board computer. Results from this application demonstrate that the approach can be quite effective for designing successful ALT experiments.","0149-144X","0-7803-3783","10.1109/RAMS.1997.571715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=571715","","Acceleration;Life estimation;Life testing;Stress;Performance analysis;Performance evaluation;Electronic equipment testing;Design optimization;Production systems;Application software","computer testing;life testing;design of experiments;environmental stress screening;microcomputers","accelerated life-testing experiments;experiments design;design limits;test item;electronic items;destructive evaluation;environmental stress levels;statistical confidence;electronic systems;multiple stresses;high-volume production;off-the-shelf single-board computer","","5","15","","","","","","IEEE","IEEE Conferences"
"Using curve-fitting optimisation technique to estimate power MOSFET model parameters for PECT II system","I. B. Aris; L. N. Hulley; N. B. Mariun; R. K. Z. Sahbudin","Dept. of Electr. & Electron. Eng., Putra Malaysia Univ., Serdang, Malaysia; NA; NA; NA","ICSE'98. 1998 IEEE International Conference on Semiconductor Electronics. Proceedings (Cat. No.98EX187)","","1998","","","157","161","This paper presents a proposed new structure of the power MOSFET model and its implementation in the HSPICE and PECT II (Power Electronics and Control Tool) packages. A fast and accurate technique for determining power MOSFET model parameters which introduces a curve-fitting optimisation technique is discussed in detail. The optimisation process generates automatically a set of device parameters based on the input specification, measured data and manufacturer's data sheets. The input procedure involves a netlist specifying parameter tolerances, component voltage and current limits and an initial guess to the selected parameter. Using curve fitting and iterative methods, the ""correct"" model is generated for utilisation in the simulation phase of both the HSPICE and PECT II packages. This method is then compared to that of a normal method which uses specific formulae to obtain power MOSFET model parameters. The results show that the curve-fitting optimisation technique produces more accurate power MOSFET model parameters than the other method. Both simulation and experimental results of the power MOSFET characteristics are also included in the discussion.","","0-7803-4971","10.1109/SMELEC.1998.781171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781171","","Curve fitting;MOSFET circuits;Power MOSFET;Power electronics;Automatic control;Electronics packaging;Manufacturing automation;Manufacturing processes;Voltage;Iterative methods","power MOSFET;semiconductor device models;SPICE;software tools;curve fitting;iterative methods;optimisation;parameter estimation;semiconductor device testing","curve-fitting optimisation technique;power MOSFET model parameters estimation;PECT II system;power MOSFET model;HSPICE;Power Electronics and Control Tool package;power MOSFET model parameters;optimisation proces;device parameters;input specification;measured data;data sheets;netlist;parameter tolerances;component voltage limits;component current limits;curve fitting;iterative methods;PECT II package;MOSFET characteristics","","","6","","","","","","IEEE","IEEE Conferences"
"The importance of interfaces: a HW/SW codesign case study","D. C. R. Jensen; J. Madsen; S. Pedersen","Dept. of Inf. Technol., Tech. Univ., Lyngby, Denmark; NA; NA","Proceedings of 5th International Workshop on Hardware/Software Co Design. Codes/CASHE '97","","1997","","","87","91","This paper presents a codesign case study in image analysis. The main objective is to stress the importance of handling HW/SW interfaces more precisely at the system level. In the presented case study, there is an intuitive and simple HW/SW interface, which is based upon the functional modules in the application. However, it is found that this seemingly sound choice caused a number of practical problems and sub-optimal solutions during the implementation of the prototype system.","1092-6100","0-8186-7895","10.1109/HSC.1997.584584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584584","","Computer aided software engineering;Weather forecasting;Kernel;Image motion analysis;Convolution;Information technology;Stress;Image analysis;Testing;Image sequence analysis","image sequences;high level synthesis;software engineering;optimisation","hardware software codesign;hardware software interface;case study;image analysis;system level;functional modules;optimal solutions;prototype system;optical flow","","3","15","","","","","","IEEE","IEEE Conferences"
"A Lagrangian optimized rate control algorithm for the H.264/AVC encoder","M. M. Ghandi; M. Ghanbari","Dept. of Electron. Syst. Eng., Essex Univ., Colchester, UK; Dept. of Electron. Syst. Eng., Essex Univ., Colchester, UK","2004 International Conference on Image Processing, 2004. ICIP '04.","","2004","1","","123","126 Vol. 1","The H.264/AVC video coding standard has been recently presented by the Joint Video Team of ITU-T and MPEG experts. The AVC encoder achieves video bitstreams at the same quality as the previous standard coders, while requiring typically 50% of the bit rate. This paper proposes a Lagrangian optimized rate control algorithm for the H.264/AVC video encoder. It controls the bit rate by adjusting the Lagrangian multiplier for every picture and specifying the quantizer parameter for every macroblock. In the proposed method, the accuracy of desired bit rate is tunable by allocating more searching time to the encoder. Experimental results show that the proposed method gives a quality improvement of about 0.5 dB when compared to the rate control method utilized in the JM7.4 test model, with an acceptable encoder complexity.","1522-4880","0-7803-8554","10.1109/ICIP.2004.1418705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1418705","","Lagrangian functions;Automatic voltage control;Quadratic programming;Bit rate;Encoding;Optimization methods;Video coding;Rate-distortion;Software testing;Systems engineering and theory","video coding;code standards;optimisation;variable rate codes;video streaming;data compression","Lagrangian rate control algorithm;optimized rate control algorithm;H.264/AVC encoder;video coding standard;Joint Video Team;ITU-T;encoder complexity;MPEG experts;video bitstreams;Lagrangian multiplier;quantizer parameter","","7","8","","","","","","IEEE","IEEE Conferences"
"Gate oxide degradation due to plasma damage related charging while ILD cap oxide deposition - detection, localization and resolution","S. Schulte; G. Dubois; D. Basso","ALTIS Semicond., Corbeil-Essonnes, France; ALTIS Semicond., Corbeil-Essonnes, France; ALTIS Semicond., Corbeil-Essonnes, France","2003 8th International Symposium Plasma- and Process-Induced Damage.","","2003","","","93","96","The article reports on the flow of detection, localization and resolution of a plasma damage related problem in a logic chip production line. The problem was observed on standard 0.25 /spl mu/m logic technology. The introduction and optimization of a voltage breakdown (VBD) test in ILT (in line test) routines led to the detection of an insufficient gate oxide quality. Using data-mining application software and taking into consideration the structure of the test routine, the root cause for the degradation of the gate-oxide was found to be ILD (inter-layer-dielectric) cap oxide deposition. A matrix design of experiment was used to optimize the plasma deposition process in order to minimize charging effects by paying attention to wafer uniformity and reproducibility. It is shown that the principal detractor for the quality of gate oxide was eliminated by introducing the new ILD cap oxide process.","","0-7803-7747","10.1109/PPID.2003.1200927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1200927","","Degradation;Logic testing;Voltage;Plasma applications;CMOS technology;Electric breakdown;Plasma measurements;System testing;Optimized production technology;Dielectric breakdown","CMOS logic circuits;integrated circuit manufacture;plasma CVD;integrated circuit reliability;surface charging;design of experiments;data mining;process monitoring;semiconductor device breakdown;integrated circuit testing;dielectric thin films;integrated circuit interconnections","gate oxide degradation;plasma damage related charging;ILD cap oxide deposition;plasma damage detection;plasma damage localization;logic chip production line;voltage breakdown test;inline test;data-mining application software;test routine structure;inter-layer-dielectric cap oxide deposition;matrix design of experiment;plasma deposition process optimization;wafer uniformity;wafer reproducibility;gate oxide quality;CMOS technology;0.25 micron","","","3","","","","","","IEEE","IEEE Conferences"
"Uncovering database access optimizations in the middle tier with TORPEDO","B. E. Martin","NA","21st International Conference on Data Engineering (ICDE'05)","","2005","","","916","926","A popular architecture for enterprise applications is one of a stateless object-based server accessing persistent data through object-relational mapping software. The reported benefits of using object-relational mapping software are increased developer productivity, greater database portability and improved runtime performance over hand-written SQL due to caching. In spite of these supposed benefits, many software architects are suspicious of the ""black box"" nature of O-R mapping software. Discerning how O-R mapping software actually accesses a database is difficult. The testbed of object relational products for enterprise distributed objects (TORPEDO) is designed to reveal the sophistication of O-R mapping software in accessing databases in single server and clustered environments. TORPEDO defines a set of realistic application level operations that detect a significant set of database access optimizations. TORPEDO supports two standard Java APIs for O-R mapping, namely, container managed persistence (CMP 2.0) and Java data objects (JDO). TORPEDO also supports the TopLink and Hibernate APIs. There are dozens of commercial and open-source O-R mapping products supporting these APIs. Results from running TORPEDO on different O-R mapping systems are comparable. We provide sample results from running TORPEDO on popular O-R mapping solutions. We describe why the optimizations TORPEDO reveals are important and how the application level operations detect the optimizations.","1063-6382;2375-026X","0-7695-2285","10.1109/ICDE.2005.149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410203","","Relational databases;Application software;Java;Computer architecture;Software performance;Productivity;Runtime;Software testing;Distributed databases;Containers","object-oriented databases;relational databases;SQL;Java;software architecture;distributed databases;public domain software","database access optimization;TORPEDO;object relational product;enterprise distributed object;object-relational mapping software;SQL;Java API;container managed persistence;Java data object","","","12","","","","","","IEEE","IEEE Conferences"
"Introduction of BSS into VR-based test and simulation-with application in NDE","Tian-Tai Guo; Xiao-Jun Zhou; Gen-Xing Zhu","Dept. of Mech. Eng., Zhejiang Univ., Hangzhou, China; Dept. of Mech. Eng., Zhejiang Univ., Hangzhou, China; Dept. of Mech. Eng., Zhejiang Univ., Hangzhou, China","International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003","","2003","2","","1647","1649 Vol.2","VR-based test and simulation system(VTSS) is a concept put forward in the authors' previous works. In a VTSS, the test processes are interactively planned, optimized and simulated in a virtual test environment generated by computer, aiming at eventually performing tests completely in virtual test environments. To make VTSS intelligent and practical, the technique of blind source separation (BSS) is introduced into VTSS. With ultrasonic non-destructive evaluation (NDE) as the technical background, a prototype of the system has been described in this paper. BSS is adopted in VTSS to serve three purposes: defect classification, system modeling and noise reduction. The conclusion is that BSS can play a very important role in VTSS.","","0-7803-7702","10.1109/ICNNSP.2003.1281198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281198","","Source separation;Computational modeling;Application software;System testing;Computer simulation;Discrete event simulation;Performance evaluation;Blind source separation;Prototypes;Noise reduction","blind source separation;virtual reality;prototypes;digital simulation;test equipment","ultrasonic nondestructive evaluation;virtual reality;VR based test and simulation system;blind source separation;prototype;defect classification;system modeling;noise reduction;virtual test environment;test equipment","","","17","","","","","","IEEE","IEEE Conferences"
"Improving industrial application's performances with an Historian","A. Fras; T. Dang","Performance Optimization of Ind. Process Dept., EDF Res. & Dev., Chatou, France; Performance Optimization of Ind. Process Dept., EDF Res. & Dev., Chatou, France","2004 IEEE International Conference on Industrial Technology, 2004. IEEE ICIT '04.","","2004","2","","718","721 Vol. 2","The Centralized Power Generation Optimization Center of CDF uses several applications dealing with real time data from power generating stations and consummation trends. Helicooptere is the application that acquires real time power generation data from all the EDF nuclear, conventional thermal and hydraulic power plans. The particularity is that these data are first acquired by the Transmission System Operator, who retransmits them to Helicooptere using TASE.2 protocol. Helicooptere was developed on a specific EDF solution to response to immediate needs. After 4 years of utilization, we noted an inadequate level of performance of the application, mainly due to the increasing number of web based clients and an untimely redundancy of the database Moreover, the maintenance costs are getting higher than initially foreseen and the application can't cope with the rapid evolutions of the user needs. Our work's objective was to propose several solutions for rebuilding Helicooptere with MES based data Historian software. We made two testing models based on two data Historians leaders: 1P21 (by Aspentech) and PI (by OSIsoft). Their abilities to integrate the three stages (Acquisition task, Application logic and data Publication), to interpret timestamps, to support multiple acquisition protocols, to support complex data models, and to propose a high availability and secured architecture, make the testing models so conclusive that EDF decided to rebuild Helicooptere with a data Historian.","","0-7803-8662","10.1109/ICIT.2004.1490163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1490163","","Power generation;Application software;Nuclear power generation;Research and development;Protocols;Costs;Logic testing;Electricity supply industry;Production;Prototypes","power markets;power generation economics;power transmission economics;power system analysis computing;maintenance engineering;protocols;redundancy","centralized power generation optimization center;CDF;power generating stations;real time power generation data;EDF nuclear;transmission system operator;Helicooptere;TASE.2 protocol;maintenance costs;Historian software;testing models;Historians leaders;1P21 model;PI model;interpret timestamps;multiple acquisition protocols;complex data models;secured architecture;redundancy","","3","","","","","","","IEEE","IEEE Conferences"
"Improved strategy for adaptive rank estimation with spherical subspace trackers","B. Champagne; W. Kang; H. C. Tam","Dept. of Electr. & Comput. Eng., McGill Univ., Montreal, Que., Canada; Dept. of Electr. & Comput. Eng., McGill Univ., Montreal, Que., Canada; Dept. of Electr. & Comput. Eng., McGill Univ., Montreal, Que., Canada","CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)","","2003","3","","2029","2034 vol.3","An improved adaptive rank detection algorithm for on-line estimation and tracking of the signal subspace dimension in applications of spherical subspace trackers is presented. The proposed algorithm uses different adaptive thresholds for the rank increase (up) and decrease (down) tests as well as a special set of fast tracking eigenvalue estimates in the rank decrease test, which can be obtained at little extra cost. It is based on an original investigation of the detection performance for the up and down tests that takes into account the exponential nature of the eigenvalue update in spherical subspace trackers. Through computer experiments in multiuser detection, it is shown that with the proposed algorithm, the time required to detect a rank decrease is significantly less than with existing methods.","0840-7789","0-7803-7781","10.1109/CCECE.2003.1226314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226314","","Testing;Eigenvalues and eigenfunctions;Covariance matrix;Multiuser detection;Vectors;Detection algorithms;Application software;Costs;Working environment noise;Matrix decomposition","adaptive estimation;code division multiple access;eigenvalues and eigenfunctions;multiuser detection;tracking;adaptive signal detection","adaptive rank estimation;spherical subspace tracker applications;adaptive rank detection algorithm;on-line estimation;signal subspace dimension tracking;fast tracking eigenvalue estimate;rank decrease test;rank increase test;computer experiment;multiuser detection","","1","5","","","","","","IEEE","IEEE Conferences"
"Assigning a value to a communication protocol test case using sensitivity analysis","A. Saekow; R. Lai","Dept. of Comput. Sci. & Comput. Eng., La Trobe Univ., Bundoora, Vic., Australia; NA","Proceedings Twelfth International Conference on Information Networking (ICOIN-12)","","1998","","","462","467","There is an enormous number of test cases that can be derived from any complex communication protocol. In order to test an implementation, in a reasonable amount of time and consuming only a reasonable amount of resources, the number of test cases in a test suite has to be within a reasonable limit. Some selection criteria need to be set in order to reduce the number of test cases. By assigning a variable to a test case, the relative importance of test cases can be ranked and an optimal test suite can then be designed. This paper describes a method of assigning a value to a test case in a test suite generated from an Estelle specification; the method is based on sensitivity analysis.","","0-8186-7225","10.1109/ICOIN.1998.648428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648428","","Protocols;Computer aided software engineering;Sensitivity analysis;Computer science;Mathematics;Australia;Automatic testing;Computer bugs;Software testing;Software reliability","protocols","communication protocol test case;sensitivity analysis;selection criteria;Estelle specification","","","11","","","","","","IEEE","IEEE Conferences"
"A cognitive-based mechanism for constructing software inspection teams","J. Miller; Zhichao Yin","Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada; Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada","IEEE Transactions on Software Engineering","","2004","30","11","811","825","Software inspection is well-known as an effective means of defect detection. Nevertheless, recent research has suggested that the technique requires further development to optimize the inspection process. As the process is inherently group-based, one approach to improving performance is to attempt to minimize the commonality within the process and the group. This work proposes an approach to add diversity into the process by using a cognitively-based team selection mechanism. The paper argues that a team with diverse information processing strategies, as defined by the selection mechanism, maximize the number of different defects discovered.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359772","Index Terms- Planning for SQA and V&amp;V;code inspections and walkthroughs;programming teams;software psychology.","Inspection;Costs;NIST;Information processing;Psychology;Computer bugs;Economic indicators;Investments;Testing;Personnel","program testing;program verification;cognition","cognitive-based mechanism;software inspection team;defect detection;code inspection;code walkthrough;programming team;software psychology","","22","42","","","","","","IEEE","IEEE Journals & Magazines"
"Optimum dwell time determination for RF immunity test","A. N. Vinod; C. Subramanian; S. K. Das","SAMEER-Centre for Electromagnetics, Taramani, India; NA; NA","Proceedings of the International Conference on Electromagnetic Interference and Compatibility '99 (IEEE Cat. No. 99TH 8487)","","1997","","","57","60","The EMC regulations call for a radio frequency immunity test for the majority of electronic equipment. The relevant IEC 1000-4 series standard specifies the maximum permitted rate of sweep and the step size to carry out the tests. The optimum choice of dwell time at sensitive frequencies is an important factor when conducting the RF immunity tests to ensure repeatability. Considering the complexity of arriving at an optimum dwell time for a wide variety of electronic equipment, the general tendency may be to carry out the test with the maximum sweep rate specified in the standard. The sweep rate and corresponding dwell time chosen must be compatible with the response time of the EUT and operational modes. The inadequacy of choosing an arbitrary dwell time is highlighted in this paper. The relevant details of a radiated immunity case study (power line carrier communication equipment) conducted using an experimental approach, to optimize the dwell time is presented. An analysis of arriving at a dwell time parameter at a typical sensitive frequency is included.","","81-900652-0","10.1109/ICEMIC.1997.669760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669760","","Radio frequency;Immunity testing;IEC standards;Electronic equipment testing;Delay;Legged locomotion;Time factors;Electromagnetic compatibility;Electromagnetic compatibility and interference;Computer aided software engineering","electromagnetic compatibility;radiofrequency interference;standards;telecommunication equipment testing;carrier transmission on power lines","optimum dwell time determination;RF immunity test;EMC regulations;radio frequency immunity test;electronic equipment;IEC 1000-4 series standard;maximum permitted rate;step size;sensitive frequencies;repeatability;EUT;operational modes;power line carrier communication equipment","","","7","","","","","","IEEE","IEEE Conferences"
"Implicit state transition graphs: applications to sequential logic synthesis and test","P. Ashar; A. Ghosh; S. Devadas; A. R. Newton","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; NA; NA","1990 IEEE International Conference on Computer-Aided Design. Digest of Technical Papers","","1990","","","84","87","Implicit state enumeration is used in developing strategies to solve key problems in sequential logic synthesis and test. It is shown that it is possible to extract implicit state transition graphs (ISTGs) from logic-gate and flip-flop descriptions of sequential circuits that allow equivalent states to be represented by cubes, and edges from different states to be coalesced into one, thereby decreasing significantly the CPU time and memory requirements of the extraction process. Coupled with the enumeration technique, synthesis strategies are proposed for FSMs (finite state machines) described at the logic level. As is illustrated, these synthesis strategies allow the authors to optimize large FSMs. The authors apply an ISTG traversal algorithm for verifying equivalence and detecting redundancies in logic-level sequential circuits. This algorithm is more efficient than previously developed sequential test generation algorithms when used to detect equivalent-state redundancies present in some classes of circuits.<<ETX>>","","0-8186-2055","10.1109/ICCAD.1990.129847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=129847","","Logic testing;Sequential analysis;Circuit testing;Sequential circuits;Circuit synthesis;Central Processing Unit;Application software;Flip-flops;Very large scale integration;Coupling circuits","finite automata;logic CAD;logic testing;sequential circuits","implicit state transition graphs;sequential logic synthesis;test;logic-gate;flip-flop;cubes;edges;memory requirements;finite state machines;logic-level sequential circuits","","10","12","","","","","","IEEE","IEEE Conferences"
"A critical look at software capability evaluations","T. B. Bollinger; C. McGowan","NEC America, Dallas, TX, USA; NA","IEEE Software","","1991","8","4","25","41","The methods used by the Software Engineering Institute (SEI's) Software Capability Evaluation program (SCE) are assessed. The goal of the SCE program is to provide the US Defense Department with a method by which it can rank the overall capability of organizations to produce software in a timely, repeatable fashion. Because SEI assessments are a preparation for SCEs, the authors first describe the major steps in an SEI process assessment and highlight its strengths and weaknesses. The SCE grading methods are described, focusing on their statistical reliability. It is concluded that the system is so seriously and fundamentally flawed that it should be abandoned rather than modified or updated. SEI's overall process-improvement paradigm, that is, the set of goals and directions it is trying to impart to the industry through its assessment and evaluation programs, is examined. It is suggested that the assessment program needs some type of structured, preferably graphical, method for recording the details of an organization's existing processes.<<ETX>>","0740-7459;1937-4194","","10.1109/52.300034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=300034","","Software tools;Personnel;US Government agencies;Performance analysis;National electric code;Productivity;Software standards;Management training;Process design;Testing","DP industry;software reliability","software grading methods;Software Engineering Institute;Software Capability Evaluation program;SCE program;US Defense Department;process-improvement paradigm","","58","3","","","","","","IEEE","IEEE Journals & Magazines"
"An incremental version of iterative data flow analysis","L. L. Pollock; M. L. Soffa","Dept. of Comput. Sci., Rice Univ., Houston, TX, USA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1537","1549","A technique is presented for incrementally updating solutions to both union and intersection data-flow problems in response to program edits and transformations. For generality, the technique is based on the iterative approach to computing data-flow information. The authors show that for both union and intersection problems, some changes can be incrementally incorporated immediately into the data-flow sets while others are handled by a two-phase approach. The first phase updates the data-flow sets to overestimate the effect of the program change, enabling the second phase to incrementally update the affected data-flow sets to reflect the actual program change. An important problem that is addressed is the computation of the data-flow changes that need to be propagated throughout a program, based on different local code changes. The technique is compared to other approaches to incremental data-flow analysis.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=58766","","Data analysis;Data flow computing;Information analysis;Application software;Iterative methods;Computer science;Optimizing compilers;Software testing;Optimization methods;Programming environments","iterative methods;parallel programming;set theory;systems analysis","union data flow problems;incremental version;iterative data flow analysis;intersection data-flow problems;program edits;iterative approach;data-flow information;data-flow sets;two-phase approach;data-flow sets;local code changes","","29","19","","","","","","IEEE","IEEE Journals & Magazines"
"Test planning and test resource optimization for droplet-based microfluidic systems","Fei Su; S. Ozev; K. Chakrabarty","Duke University; NA; NA","Proceedings. Ninth IEEE European Test Symposium, 2004. ETS 2004.","","2004","","","72","77","","","0-7695-2119","10.1109/ETSYM.2004.1347609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347609","","System testing;Microfluidics;Electrodes;Integer linear programming;Biomedical computing;Biomedical engineering;Reliability engineering;Application software;Optimization methods;Scalability","","","","9","21","","","","","","IEEE","IEEE Conferences"
"On software development to support statistical simulation of analogue circuits","E. Driouk; O. Jarov; A. Sukhodolsky","Dept. of Microelectron., Byelorussian State Univ. of Inf. & Radioelectron., Minsk, Byelorussia; Dept. of Microelectron., Byelorussian State Univ. of Inf. & Radioelectron., Minsk, Byelorussia; Dept. of Microelectron., Byelorussian State Univ. of Inf. & Radioelectron., Minsk, Byelorussia","Proceedings the European Design and Test Conference. ED&TC 1995","","1995","","","539","543","A system for statistical circuit analysis, yield estimation and design centering is presented. The system architecture is based on the decomposition of the simulation process into three logically independent layers. A dedicated language is a significant part of the system. Its syntax implements an advanced technique that allows one to create flexible circuit performance extraction procedures. An application example demonstrates the system capabilities.<<ETX>>","","0-8186-7039","10.1109/EDTC.1995.470348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470348","","Programming;Circuit simulation;Yield estimation;Analytical models;Data mining;Circuit optimization;Circuit synthesis;Statistical analysis;Manufacturing;Production","circuit analysis computing;integrated circuit yield;statistical analysis;analogue integrated circuits;software engineering","software development;statistical simulation;analogue circuits;statistical circuit analysis;yield estimation;design centering;dedicated language;circuit performance extraction procedures","","","11","","","","","","IEEE","IEEE Conferences"
"Shape design optimization of shielding electrodes for reducing radio interference of HV devices","S. Dessanti; F. Dianin; F. Moro; R. Turri","AREVA T&D S.p.A, Italy; AREVA T&D S.p.A, Italy; NA; NA","39th International Universities Power Engineering Conference, 2004. UPEC 2004.","","2004","1","","165","169 Vol. 1","Developments in computational methods for solving boundary value problems, coupled with the advances in computer hardware, has led to the commercial availability of very efficient computer aided engineering (CAE) tools. The aim of this work is to define a suitable criterion for the shape design optimization of HV shielding electrodes for reducing the radio interference. The experimental part of the investigation consists of standard radio interference voltage (RIV) measurements on test geometries with different sized toroidal and spherical electrodes. A detailed 3D modelling of the test geometries has been carried out using a BEM based commercial code, in order to precisely compute the electric field distribution near the electrode surfaces. It has been observed that, despite the different geometries and applied voltages, a unique electric field value is attained in all configurations in correspondence to the RIV threshold voltage. Therefore, this value can be used as a design parameter for CAE tools once the field distribution near the HV device is computed. The proposed approach may greatly improve the efficiency of HV shield design, which nowadays is normally based on empirical considerations.","","1-86043-365","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1491984","","Shape;Design optimization;Electrodes;Electromagnetic interference;Computer aided engineering;Testing;Geometry;Distributed computing;Threshold voltage;Boundary value problems","electrodes;computer aided engineering;software tools;power system CAD;electromagnetic shielding;power system protection;switchgear protection;boundary-elements methods;boundary-value problems;radiofrequency interference","shape design optimization;shielding electrodes;radio interference reduction;HV devices;computational methods;boundary value problems;CAE;computer aided engineering tools;radio interference voltage measurements;test geometries;toroidal electrodes;spherical electrodes;3D modelling;BEM based commercial code;electric field distribution;electrode surfaces;RIV threshold voltage;design parameter;HV device field distribution;HV shield design;HV switchgear","","","5","","","","","","IEEE","IEEE Conferences"
"Global implementation of ERP software - critical success factors on upgrading technical infrastructure","S. Ghosh","IEEE, Columbia, MD, USA","IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change","","2003","","","320","324","Implementing an Enterprise Resource planning (ERP) software in a global environment, executive sponsors face two key challenges. While business processes are to be re-engineered to align with the ERP software best practices, technical architecture and infrastructure needs to be in place globally as per specifications of the packaged software. In the legacy environment, different countries or different business units use different systems, based on local standards supported by local resources. In the new ERP world, globally all the countries must conform to same technical infrastructure. Technical managers face multiple critical issues implementing a global solution. Most of the ERP software are developed in technically advanced countries, standards are often too high for under developed or developing countries. In an effort to bring the global organization to a common platform different countries needs different levels of upgrades. In this paper the authors review key technical issues faced is a global upgrade process to support a global ERP implementation and how to resolve those. We conclude although technical infrastructure and business process reengineering both are equally important and each implementation is unique, but following some simple steps it is easy to prioritize each ones during different phases of the project. Also time lines of two sub-projects must converge after initial phase and must follow a common plan for the project to be successful. Multiple scenarios are described to facilitate the process.","","0-7803-8150","10.1109/IEMC.2003.1252285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252285","","Enterprise resource planning;Business process re-engineering;Project management;Internet;USA Councils;Computer architecture;Software packages;Packaging;Life testing;Environmental management","enterprise resource planning;business process re-engineering;software packages;globalisation;international trade;technology management","enterprise resource planning software;business process reengineering;software package;business managers;technology management;business processes life cycle;global data traffic;global business;global organization;global implementation;technical infrastructure;technical architecture;legacy environment;business units","","6","6","","","","","","IEEE","IEEE Conferences"
"Preliminary studies of discrete tomography in neutron imaging","A. Kuba; L. Rusko; L. Rodek; Z. Kiss","Dept. of Image Process. & Comput. Graphics, Univ. of Szeged, Hungary; Dept. of Image Process. & Comput. Graphics, Univ. of Szeged, Hungary; Dept. of Image Process. & Comput. Graphics, Univ. of Szeged, Hungary; Dept. of Image Process. & Comput. Graphics, Univ. of Szeged, Hungary","IEEE Transactions on Nuclear Science","","2005","52","1","380","385","Discrete tomography (DT) is a new technique to reconstruct discrete images from their projections (like neutron images). The reconstruction methods in DT are different from the conventional ones, because the created images may contain only a few numbers of given discrete values. One of the main reasons to apply DT is that hopefully we need only a few numbers of projections. In many applications we have a situation where we know the material components of the object to be studied, that is, we know the discrete values of the image to be reconstructed. Using discreteness and some a priori information we can apply several DT methods in neutron imaging. Most of the DT reconstruction methods are reducing the problem to an optimization task. We tried two such methods on software and physical phantoms. In these experiments we investigated the effects of the following parameters: number of projections, noise levels, and complexity of the object to be reconstructed. We also developed a software system, called DIRECT, for testing different DT methods, to compare them and to present the reconstructed objects.","0018-9499;1558-1578","","10.1109/TNS.2005.843657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1417171","Discrete tomography;image reconstruction;neutron imaging","Tomography;Neutrons;Image reconstruction;Reconstruction algorithms;Optimization methods;Imaging phantoms;Noise level;Software systems;Software testing;System testing","phantoms;image reconstruction;tomography;materials testing","discrete tomography;discrete image reconstruction;material components;neutron imaging;optimization task;software phantoms;physical phantoms;noise levels;DIRECT;reconstructed objects","","5","9","","","","","","IEEE","IEEE Journals & Magazines"
"Application of fuzzy classification by evolutionary neural network in incipient fault detection of power transformer","Jingen Wang; Lin Shang; Shifu Chen; Yanfei Wang","Nat. Lab. for Novel Software Technol., Nanjing, China; Nat. Lab. for Novel Software Technol., Nanjing, China; Nat. Lab. for Novel Software Technol., Nanjing, China; Nat. Lab. for Novel Software Technol., Nanjing, China","2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)","","2004","3","","2279","2283 vol.3","Aiming at the incipient fault detection of power transformer, the paper proposes a novel fuzzy classification by evolutionary neural network. The method models the membership fuctions of all fuzzy sets by utilizing a three-layer feedforward neural network, and trains a group of neural networks by combining the modified Evolutionary Strategy with Levenberg-Marquardt optimization method in order to accelerate convergence and avoid falling into local minima. Thus each trained neural network denotes an ""expert"" model. The classification results obtained from all ""expert"" models are integrated according to the absolute-majority-voting rule. A lot of samples are tested, and the testing results demonstrate that the novel method is much better in neural network structure, classification accuracy, generalization capability, fault-tolerance ability and robustness that then other traditional methods.","1098-7576","0-7803-8359","10.1109/IJCNN.2004.1380978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380978","","Fuzzy neural networks;Neural networks;Fault detection;Power transformers;Feedforward neural networks;Testing;Fuzzy sets;Optimization methods;Acceleration;Convergence","pattern classification;fuzzy set theory;feedforward neural nets;fault tolerance;power transformer testing;learning (artificial intelligence);generalisation (artificial intelligence);convergence;optimisation;evolutionary computation","fuzzy classification;evolutionary neural network;incipient fault detection;power transformer;membership functions;fuzzy sets;three layer feedforward neural network;Levenberg-Marquardt optimization method;convergence acceleration;expert model;absolute majority voting rule;sample testing;neural network structure;generalization capability;fault tolerance;neural network training","","","13","","","","","","IEEE","IEEE Conferences"
"A method for comparison of optimization algorithms used in compartmental modelling","M. L. Moulin; F. Frouin; J. P. Baazin; H. Benali; L. Manil; R. Di Paola","INSERM, Inst. Gustave-Roussy, Villejuif, France; INSERM, Inst. Gustave-Roussy, Villejuif, France; INSERM, Inst. Gustave-Roussy, Villejuif, France; INSERM, Inst. Gustave-Roussy, Villejuif, France; NA; NA","IEEE Conference on Nuclear Science Symposium and Medical Imaging","","1992","","","1141","1143 vol.2","A protocol is proposed to compare the performance of five minimization algorithms used in compartmental modeling. Kinetics are computed from the model being tested, and noise added to them with different signal-to-noise ratios. These noisy curves undergo compartmental analysis in the case of good a priori and poor a priori knowledge of the parameters to be estimated. The protocol makes it possible to guide the choice of an algorithm and to test numerical identifiability of a model. It also emphasizes the unreliability of the conventional confidence intervals computation.<<ETX>>","","0-7803-0884","10.1109/NSSMIC.1992.301055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=301055","","Optimization methods;Iterative algorithms;Minimization methods;Least squares approximation;Testing;Algorithm design and analysis;Software algorithms;Sensitivity analysis;Kinetic theory;Protocols","computerised tomography;modelling;optimisation;radioisotope scanning and imaging","comparison;optimization algorithms;compartmental modelling;performance;minimization algorithms;signal-to-noise ratios;noisy curves;compartmental analysis;confidence intervals computation","","","16","","","","","","IEEE","IEEE Conferences"
"Mechanical-strength reliability evaluation using an iterative approach","T. -. Dao; Z. Liu; M. Massoud","Ecole de Technol. Superieure, Quebec Univ., Montreal, Que., Canada; NA; NA","Annual Reliability and Maintainability Symposium. 1991 Proceedings","","1991","","","446","450","An iterative approach for using microcomputer software for the evaluation of the mechanical strength reliability of a product at the design stage is presented. The first software, DEREL, is interactive with color graphic capabilities, allowing the user to change the values of the design variables at will and to observe their effect on the design reliability value. The user will thus be able to judge the sensitivity of the reliability value to any changes in the design variables. The second software, DERELOP, was conceived in an effort to rationalize the iterative process of selecting the design variables. This software combines the classical optimization iterative search algorithm to a modified version of the previous software to search for the optimal reliability design. Illustrations and flow charts of the main operations are given.<<ETX>>","","0-87942-661","10.1109/ARMS.1991.154478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=154478","","Iterative methods;Stress;Reliability engineering;Design engineering;Testing;Microcomputers;Design optimization;Iterative algorithms;Algorithm design and analysis;Prototypes","engineering graphics;failure analysis;iterative methods;mechanical engineering computing;mechanical strength;microcomputer applications;optimisation;reliability;software packages","failure analysis;software packages;engineering computing;reliability;iterative approach;microcomputer software;mechanical strength;product;design;DEREL;color graphic;sensitivity;DERELOP;optimization;algorithm","","","8","","","","","","IEEE","IEEE Conferences"
"A method for automatic optimization of dynamic memory management in C++","D. Haggander; P. Liden; L. Lundberg","Dept. of Software Eng. & Comput. Sci., Blekinge Inst. of Technol., Ronneby, Sweden; Dept. of Software Eng. & Comput. Sci., Blekinge Inst. of Technol., Ronneby, Sweden; Dept. of Software Eng. & Comput. Sci., Blekinge Inst. of Technol., Ronneby, Sweden","International Conference on Parallel Processing, 2001.","","2001","","","489","498","In C++, the memory allocator is often a bottleneck that severely limits performance and scalability on multiprocessor systems. The traditional solution is to optimize the C library memory allocation routines. An alternative is to attack the problem on the source code level, i.e. modify the applications source code. Such an approach makes it possible to achieve more efficient and customized memory management. To implement and maintain such source code optimizations is however both laborious and costly, since it is a manual procedure. Applications developed using object-oriented techniques, such as frameworks and design patterns, tend to use a great deal of dynamic memory to offer dynamic features. These features are mainly used for maintainability reasons, and temporal locality often characterizes the run-time behavior of the dynamic memory operations. We have implemented a pre-processor based method, named Amplify, which is a completely automated procedure optimizes (object-oriented) C++ applications to exploit the temporal locality in dynamic memory usage. Test results show that Amplify can obtain significant speed-up for synthetic applications and that it was useful for a commercial product.","0190-3918","0-7695-1257","10.1109/ICPP.2001.952096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952096","","Optimization methods;Memory management;Power system management;Programming profession;Computer science;Scalability;Multiprocessing systems;Libraries;Runtime;Testing","storage management;C++ language;optimising compilers","automatic optimization;dynamic memory management;C++;memory allocator;performance;scalability;multiprocessor systems;C library memory allocation routines;source code level;customized memory management;source code optimizations;object-oriented techniques;frameworks;design patterns;dynamic memory;Amplify","","2","12","","","","","","IEEE","IEEE Conferences"
"Multimedia tools and applications in the development of a large complex network system","S. Bhattacharya; S. Palangala; S. P. Tolety","Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; NA; NA","Proceedings. Twenty-Third Annual International Computer Software and Applications Conference (Cat. No.99CB37032)","","1999","","","226","231","The paper demonstrates the capability and use of integrating multimedia tools and events in the life-cycle development of large, complex networks. Network engineering (NE), detailed by the authors' previous research, defines process steps for the requirement specification, design, evaluation, testing, layout and maintenance of computer networks (S. Palangala et al., 1998). The paper shows that the integration of multimedia in the process steps for NE results in better networks being designed. The paper details a multimedia based simulation executed on the network prototype, prior to submitting the designed network to a more rigorous simulation software, like OPNET (OPtimized Network Engineering Tools). The tool detailed in the paper allows the user to specify events during the network simulation. Events are user defined conditions that can identify and alert the user regarding abnormal behavior of the network system. The key ideas presented in the paper include the usage of multimedia in the NE process, and the role of user-defined events in identifying design aspects of the network.","0730-3157","0-7695-0368","10.1109/CMPSAC.1999.812705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812705","","Design engineering;Computational modeling;Complex networks;Testing;Computer networks;Software prototyping;Virtual prototyping;Software tools;Design optimization;Discrete event simulation","computer networks;telecommunication computing;multimedia systems;digital simulation","large complex network systems development;multimedia tools;life-cycle development;network engineering;process steps;requirement specification;computer networks;multimedia based simulation;network prototype;rigorous simulation software;OPNET;OPtimized Network Engineering Tools;network simulation;user defined conditions;abnormal behavior;network system;NE process;user-defined events","","1","5","","","","","","IEEE","IEEE Conferences"
"Automatic array alignment in parallel Matlab scripts","I. Z. Milosavljevic; M. A. Jabri","Comput. Eng. Lab., Sydney Univ., NSW, Australia; NA","Proceedings 13th International Parallel Processing Symposium and 10th Symposium on Parallel and Distributed Processing. IPPS/SPDP 1999","","1999","","","285","289","We present the ParAL system which compiles Matlab scripts into C programs with calls to a parallel run-time library. The novel feature of the compiler is the optimisation of array alignment which reduces or eliminates unnecessary communication overheads. We have evaluated this technique on several Matlab codes. For comparison, the same applications were hand-coded using the PBLAS library. The aligned codes were on average 43% faster then the misaligned codes, with the speedup factor of almost 4 achieved in some cases. This optimisation technique enabled ordinary Matlab scripts to run at a similar speed as manually optimised PBLAS codes.","","0-7695-0143","10.1109/IPPS.1999.760489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=760489","","MATLAB;Software libraries;Testing;Laboratories;Australia;Optimizing compilers;Application software;Software tools;Algorithms;Computer languages","optimising compilers;parallelising compilers","ParAL system;Matlab scripts;parallel run-time library;compiler;optimisation;array alignment;optimisation technique;optimised PBLAS codes","","4","18","","","","","","IEEE","IEEE Conferences"
"Optimal selection and simulation software package for fan/pump and AC motor systems","Zhang Yang; Liu Congwei; Sun Xudong; Li Fahai","Dept. of Electr. Eng., Tsinghua Univ., Beijing, China; NA; NA; NA","ICEMS'2001. Proceedings of the Fifth International Conference on Electrical Machines and Systems (IEEE Cat. No.01EX501)","","2001","1","","432","435 vol.1","Using variable frequency drives (VFD) to control electric driven fans and pumps is an effective way for saving energy. But this technology is not widely used in this field, the most common reason is lack of knowledge of such drives. The software package presented in this paper is aimed to provide users corresponding knowledge and guide them to design optimized fan and pump applications using VFD. At this same time, a simulation tool is offered to test their performance.","","7-5062-5115","10.1109/ICEMS.2001.970704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=970704","","Software packages;AC motors;Fans;Pumps;Frequency;Valves;Electric variables control;Application software;Energy consumption;Induction motors","pumps;variable speed drives;energy conservation;electric machine CAD;machine theory;optimisation;AC motor drives","fan/pump AC motor systems;selection optimisation;computer simulation;software package;variable frequency drives;energy saving;machine design optimisation;CAD","","","9","","","","","","IEEE","IEEE Conferences"
"Specification back-propagation and its application to DC fault simulation for analog/mixed-signal circuits","Jiun-Lang Huang; Chen-Yang Pan; Kwang-Ting Cheng","Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA; NA; NA","Proceedings 17th IEEE VLSI Test Symposium (Cat. No.PR00146)","","1999","","","220","225","In this paper we present the specification backpropagation technique which enables one to derive the constraint of an internal functional block with respect to a given DC specification for an analog/mixed-signal system. Based on this technique, we implement an efficient fault simulator which reduces the required efforts by (1) removing undetectable faults from the fault list, and (2) performing fault simulation only locally for the fault block. Simulation results on an industrial design show a speedup factor of 7.2 with 98% correct classification of detected and undetected faults as compared with full-chip DC fault simulation.","1093-0167","0-7695-0146","10.1109/VTEST.1999.766669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=766669","","Circuit faults;Circuit simulation;Sufficient conditions;Design optimization;Application software;Computational modeling;Computer simulation;Hip;Costs;Analog circuits","analogue integrated circuits;mixed analogue-digital integrated circuits;fault simulation;backpropagation;integrated circuit testing;automatic testing;circuit analysis computing","specification backpropagation;DC fault simulation;analog circuits;mixed-signal circuits;internal functional block;fault simulator","","15","13","","","","","","IEEE","IEEE Conferences"
"Fast hardware-software coverification by optimistic execution of real processor","Sungjoo Yoo; Jong-Eun Lee; Jinyong Jung; Kyungseok Rha; Youngchul Cho; Kiyoung Choi","Sch. of Electr. Eng., Seoul Nat. Univ., South Korea; NA; NA; NA; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","663","668","To achieve fast verification of the software part of an embedded system, we propose to run the target processor optimistically, which effectively reduces the synchronization overhead with other simulators. For the optimistic processor execution, we present a processor execution platform and state saving/restoration methods. We performed optimistic execution of ARM710A processor in the coverification of an IS-95 CDMA cellular phone system and obtained up to orders of magnitude higher performance compared with the case that the processor runs conservatively.","","0-7695-0537","10.1109/DATE.2000.840857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840857","","Hardware;Optimization methods;Multiaccess communication;System-on-a-chip;Instruction sets;Design automation;Laboratories;Timing;Emulation;Registers","hardware-software codesign;formal verification;synchronisation;embedded systems;integrated circuit design;application specific integrated circuits","hardware-software coverification;optimistic execution;real processor;embedded system;target processor;synchronization overhead;processor execution platform;state saving/restoration methods;ARM710A processor;IS-95 CDMA cellular phone system","","","17","","","","","","IEEE","IEEE Conferences"
"Transforming linear systems for joint latency and throughput optimization","M. B. Srivastava; M. Potkonjak","AT&T Bell Labs., Murray Hill, NJ, USA; NA","Proceedings of European Design and Test Conference EDAC-ETC-EUROASIC","","1994","","","267","271","We present algorithm transformations to simultaneously optimize for throughput and latency for the important case of linear time-invariant DSP systems. Although throughput alone can be arbitrarily improved using previously published techniques, none of them is effective when latency constraints are considered. We have used a state-space based approach which treats various algorithm transformations in an integrated fashion, and answers analytically whether it is possible to simultaneously meet any given combination of constraints on latency and throughput. The analytic approach is optimum and constructive in nature, and produces a complete implementation when feasibility conditions are fulfilled. We also present a sub-optimal but hardware efficient heuristic approach. On all benchmarks the new approaches show much superior results than published ones.<<ETX>>","","0-8186-5410","10.1109/EDTC.1994.326866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326866","","Linear systems;Delay;Throughput;Digital signal processing;Software algorithms;Costs;Hardware;Very large scale integration;Filtering algorithms;Optimizing compilers","active filters;signal processing;circuit CAD;state-space methods;directed graphs","algorithm transformations;throughput optimization;latency optimization;linear time-invariant DSP systems;state-space based approach;analytic approach;suboptimal heuristic approach;VLSI synthesis;directed control dataflow graph;biquad filters","","14","12","","","","","","IEEE","IEEE Conferences"
"Economic resource sharing in ATM network","Jiann-Liang Chen; Ginkou Ma; Bao-Shuh Lin","Comput. & Commun. Res. Labs., Ind. Technol. Res. Inst., Hsinchu, Taiwan; Comput. & Commun. Res. Labs., Ind. Technol. Res. Inst., Hsinchu, Taiwan; Comput. & Commun. Res. Labs., Ind. Technol. Res. Inst., Hsinchu, Taiwan","Proceedings of 3rd International Workshop on the Economics of Design, Test and Manufacturing","","1994","","","157","161","Based on the optimization theory and LaGrange multipliers concept, a novel strategy for ""fair"" and ""economic"" resource sharing in an ATM network is proposed in the paper. The main essence of proposed strategy is to confirm the minimal cost waste, that is the minimal cell loss in the ATM network, under the various negotiated Quality of Services (QoS). By doing so, consumers (senders of services) can obtain a fair share of the resources under their QoS requirements and the provider of broadband ISDN services will possess an economic operation. The tactics are realized by using the MatLab tool in a workstation.","","0-8186-6595","10.1109/ICEDTM.1994.496084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=496084","","Asynchronous transfer mode;Broadband communication;Integrated services digital networks;Resource management;Optimization methods;Communication system software;Communication system traffic;Traffic control (communication);Communication system planning;Communication system operations and management","asynchronous transfer mode;B-ISDN;resource allocation;optimisation;telecommunication computing;telecommunication traffic;telecommunication congestion control;telecommunication network management","economic resource sharing;ATM network;optimization theory;LaGrange multipliers;minimal cost waste;minimal cell loss;quality of services;broadband ISDN services;MatLab tool","","","","","","","","","IEEE","IEEE Conferences"
"A pulp and paper simulator for operator training and process optimization","C. Lindberg; E. Dahlquist; H. Ekwall","ABB Corporate Research, 721 78 V&#x00E4;ster&#x00E5;s, Sweden; ABB Automation Systems, 721 67 V&#x00E4;ster&#x00E5;s, Sweden; ABB Automation Systems, 721 67 V&#x00E4;ster&#x00E5;s, Sweden","1999 European Control Conference (ECC)","","1999","","","1196","1200","A new dynamic simulator for pulp and paper applications has been developed. The simulator is useful for process optimization and operator training, especially when constructing a new paper machine, or rebuilding an existing one, or introducing new employees. The main advantage of the simulator is that the same control system (Distributed Control System source code) and the same displays in the Operator stations are used in the simulator as in the real plant. In this way process operators get very realistic training of various events. Optimization of an existing process line can be done by testing different ways of operation, and then selecting the best way of operation. A suitable rebuild of an existing mill could be found in a similar manner, i.e. by simulating different rebuild possibilities. A DCS functionality checkout can be done before startup of a real mill, since identical DCS code is used in the simulator as in the real mill.","","978-3-9524173-5","10.23919/ECC.1999.7099472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7099472","Simulation;Operator training;Process optimization;Pulp and Paper","Process control;IP networks;Graphical user interfaces;Density estimation robust algorithm","control engineering computing;distributed control;industrial training;optimisation;paper industry;paper making machines;paper mills;process control;source code (software)","pulp simulator;paper simulator;operator training;process optimization;paper machine;distributed control system source code;DCS functionality checkout","","","9","","","","","","IEEE","IEEE Conferences"
"Program Optimization Using Invariants","S. Katz","IBM-Israel Scientific Center","IEEE Transactions on Software Engineering","","1978","SE-4","5","378","389","Optimizing a computer program is defined as improving the execution time without disturbing the correctness. We show how to use invariants from a proof of correctness in order to change the statement in and around the program's loops. This approach is shown to systematize existing optimization methods, and to sometimes allow stronger optimizations than are possible under the standard transformation approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1978.233858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702553","Invariants;program optimization;proof of correctness;transformations of programs","Optimization methods;Program processors;Optimizing compilers;Testing;Cities and towns;Registers;Flowcharts;Law;Legal factors","","Invariants;program optimization;proof of correctness;transformations of programs","","3","16","","","","","","IEEE","IEEE Journals & Magazines"
"Control flow optimization for fast system simulation and storage minimization /spl lsqb/real-time multidimensional signal processing/spl rsqb/","F. Franssen; L. Nachtergaele; H. Samsom; F. Catthoor; H. De Man","IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium","Proceedings of European Design and Test Conference EDAC-ETC-EUROASIC","","1994","","","20","24","This paper addresses the important problem of efficient system-level evaluation for real-time multi-dimensional signal processing systems, as occurring in image, speech and video processing. We solve both the difficult task of finding a correct procedural ordering for the evaluation (without expanding the code to scalars) and the optimisation of the loop organisation, leading to an acceptable amount of memory within the system-level evaluation and/or software/hardware synthesis environment. The effectiveness of our solution is substantiated with several realistic test cases.<<ETX>>","","0-8186-5410","10.1109/EDTC.1994.326904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326904","","Real time systems;Image storage;Random access memory;Speech analysis;Hardware;Application software;Signal synthesis;Multidimensional signal processing;Speech processing;Control system synthesis","real-time systems;signal processing;speech analysis and processing;image processing;digital simulation;computer testing;integrated circuit testing;application specific integrated circuits;storage management;digital signal processing chips","real-time multi-dimensional signal processing systems;system-level evaluation;fast system simulation;storage minimization;image processing;speech processing;video processing;procedural ordering;loop organisation optimisation;hardware synthesis environment;software synthesis environment;control flow optimization;application specific architectures;polyhedral dependence graph model","","10","13","","","","","","IEEE","IEEE Conferences"
"How to future-proof your ATS using LAN-based synthetic instruments","J. Stratton","Agilent Technol. Inc., Santa Rosa, CA, USA","Proceedings AUTOTESTCON 2004.","","2004","","","428","433","With the current trend to drive down the total cost of ownership of automatic test systems (ATS), industry-standard open architectures have been seen as a way of driving down the cost of test (for both design and manufacturing) and of reducing the size of ATS platforms (by eliminating redundant hardware and software). Since these open architectures have been based on rapidly changing commercial computer standards, this large investment in hardware is quickly becoming a support problem-the very problem it was supposed to fix. Additionally, these architectures have been optimized for digital or low frequency analog signals, making it a technical challenge to implement high performance microwave testing.","1088-7725","0-7803-8449","10.1109/AUTEST.2004.1436919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436919","","Instruments;Computer architecture;Costs;Automatic testing;Software testing;System testing;Hardware;Computer industry;Manufacturing industries;Manufacturing automation","automatic test equipment;automatic testing;local area networks;standards;digital signals","LAN-based synthetic instrument;automatic test system;industry-standard open architecture;redundant hardware-software;commercial computer standards;digital signal;low frequency analog signal;high performance microwave testing","","","1","","","","","","IEEE","IEEE Conferences"
"Automated software engineering using concurrent class machines","R. Grosu; Y. A. Liu; S. Smolka; S. D. Stoller; Jingyu Yan","State Univ. of New York, Stony Brook, NY, USA; State Univ. of New York, Stony Brook, NY, USA; State Univ. of New York, Stony Brook, NY, USA; State Univ. of New York, Stony Brook, NY, USA; State Univ. of New York, Stony Brook, NY, USA","Proceedings 16th Annual International Conference on Automated Software Engineering (ASE 2001)","","2001","","","297","304","Concurrent Class Machines are a novel state-machine model that directly captures a variety of object-oriented concepts, including classes and inheritance, objects and object creation, methods, method invocation and exceptions, multithreading and abstract collection types. The model can be understood as a precise definition of UML activity diagrams which, at the same time, offers an executable, object-oriented alternative to event-based statecharts. It can also be understood as a visual, combined control and data flow model for multithreaded object-oriented programs. We first introduce a visual notation and tool for Concurrent Class Machines and discuss their benefits in enhancing system design. We then equip this notation with a precise semantics that allows us to define refinement and modular refinement rules. Finally, we summarize our work on generation of optimized code, implementation and experiments, and compare with related work.","1938-4300","0-7695-1426","10.1109/ASE.2001.989816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989816","","Software engineering;Object oriented modeling;Unified modeling language;Power system modeling;Embedded software;Multithreading;Design automation;Automotive engineering;Medical simulation;Aerospace testing","specification languages;software engineering;object-oriented programming;multi-threading;exception handling","automated software engineering;concurrent class machines;state-machine model;object-oriented concepts;classes;inheritance;object creation;method invocation;UML activity diagrams;event-based statecharts;data flow model;multithreaded object-oriented programs;visual notation;system design;semantics;optimized code;exceptions;multithreading;abstract collection types","","","19","","","","","","IEEE","IEEE Conferences"
"A Self-Testing Fully Pipelined Implementation for the Advanced Encryption Standard","M. Nazm-Bojnordi; N. Sedaghati-Mokhtari; S. Mehdi Fakhraie","Silicon Intelligence and VLSI Signal Processing Laboratory ECE Department, University of Tehran, Tehran, IRAN. m.bojnordi@ece.ut.ac.ir; NA; NA","2005 International Conference on Microelectronics","","2005","","","260","263","In contrast to software implementation, hardware implementation of encryption protocols provides a higher level of security and cryptography speed at some flexibility cost. In this paper, different existing implementations of Advanced Encryption Standard (AES) are considered and a fully pipelined implementation for the AES is presented. Implementation considers both encryption and decryption. The design is optimized for achieving higher speed and lower area cost. The Selected algorithm for our design is Rijndael. The major part of an AES design is designing substitute boxes (S-box). S-boxes in our design are implemented at a lower cost rather than the existing implementations. Throughput of up to 6 Gbps is gained by our proposed architecture. This implementation is equipped with BIST architecture for self testing.","2159-1660;2159-1679","0-7803-9262","10.1109/ICM.2005.1590080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1590080","AES;self-testing;BIST;Rijndael;fully pipeline implementation","Built-in self-test;Cryptography;Hardware;Cryptographic protocols;Security;Design optimization;Cost function;Algorithm design and analysis;Throughput;Automatic testing","","AES;self-testing;BIST;Rijndael;fully pipeline implementation","","","8","","","","","","IEEE","IEEE Conferences"
"CIRG@UP OptiBench: a statistically sound framework for benchmarking optimisation algorithms","E. S. Peer; A. P. Engelbrecht; F. van den Bergh","Dept. of Comput. Sci., Pretoria Univ., South Africa; Dept. of Comput. Sci., Pretoria Univ., South Africa; NA","The 2003 Congress on Evolutionary Computation, 2003. CEC '03.","","2003","4","","2386","2392 Vol.4","This article is a proposal, by the Computational Intelligence Research Group at the University of Pretoria (CIRG@UP), for a framework to benchmark optimisation algorithms. This framework, known as OptiBench, was conceived out of the necessity to consolidate the efforts of a large research group. Many problems arise when different people work independently on their own research initiatives. These problems range from duplicating effort to, more seriously, having conflicting results. In addition, less experienced members of the group are sometimes unfamiliar with the necessary statistical methods required to properly analyse their results. These problems are not limited internally to CIRG@UP but are also prevalent in the research community at large. This proposal aims to standardise the research methodology used by CIRG@UP internally (initially in the optimisation subgroup and later in subgroups working in other paradigms of computational research). Obviously this article cannot dictate the methodologies that should be used by other members of the broader research community, however, the hope is that this framework can be found useful and that others would willingly contribute and become involved.","","0-7803-7804","10.1109/CEC.2003.1299386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1299386","","Particle swarm optimization;Africa;Computer science;Proposals;Testing;Mobile computing;Peer to peer computing;Competitive intelligence;Research initiatives;Statistical analysis","research initiatives;benchmark testing;software performance evaluation;evolutionary computation;statistical testing;optimisation","Computational Intelligence Research Group;University of Pretoria;benchmark optimisation algorithm;research initiative;research community;research methodology;CIRG@UP OptiBench;statistically sound framework","","1","34","","","","","","IEEE","IEEE Conferences"
"Program dependence analysis of concurrent logic programs and its applications","J. Zhao; J. Cheng; K. Ushijima","Dept. of Comput. Sci. & Commun. Eng., Kyushu Univ., Fukuoka, Japan; Dept. of Comput. Sci. & Commun. Eng., Kyushu Univ., Fukuoka, Japan; Dept. of Comput. Sci. & Commun. Eng., Kyushu Univ., Fukuoka, Japan","Proceedings of 1996 International Conference on Parallel and Distributed Systems","","1996","","","282","291","In this paper a formal model for program dependence analysis of concurrent logic programs is proposed with the following contributions. First, two language-independent program representations are presented for explicitly representing control flows and/or data flows in a concurrent logic program. Then based on these representations, program dependences between literals in concurrent logic programs are defined formally, and a dependence-based program representation named the Literal Dependence Net (LDN) is presented for explicitly representing primary program dependences in a concurrent logic program. Finally, as applications of the LDNs, some important software engineering activities including program slicing, debugging, testing, complexity measurement, and maintenance are discussed in a programming environment for concurrent logic programs.","","0-8186-7267","10.1109/ICPADS.1996.517574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=517574","","Logic programming;Communication system control;Logic testing;Application software;Software engineering;Programming environments;Optimizing compilers;Computer science;Software testing;Software measurement","programming environments;software maintenance;program debugging;software metrics;logic programming;parallel programming","program dependence analysis;concurrent logic programs;formal model;language-independent program representations;control flows;data flows;dependence-based program representation;Literal Dependence Net;software engineering;program slicing;debugging;testing;complexity measurement;maintenance;programming environment","","6","27","","","","","","IEEE","IEEE Conferences"
"Air traffic control improvement using prioritized CSMA","D. C. Robinson","Glenn Res. Center, NASA, Cleveland, OH, USA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","3","","3/1359","3/1365 vol.3","Version 7 simulations of the industry-standard network simulation software ""OPNET"" are presented of two applications of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) and Automatic Dependent Surveillance-Broadcast mode (ADS-B), over VHF Data Link mode 2 (VDL-2). Communication is modeled for air traffic between just three cities. All aircraft are assumed to have the same equipage. The simulation involves Air Traffic Control (ATC) ground stations and 105 aircraft taking off, flying realistic free-flight trajectories, and landing in a 24-hr period. All communication is modeled as unreliable. Collision-less, prioritized carrier sense multiple access (CSMA) is successfully tested. The statistics presented include latency, queue length, and packet loss. This research may show that a communications system simpler than the currently accepted standard envisioned may not only suffice, but also surpass performance of the standard at a lower cost of deployment.","","0-7803-6599","10.1109/AERO.2001.931366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931366","","Air traffic control;Multiaccess communication;Aircraft;Communication standards;Aerospace industry;Communication industry;Computer industry;Industrial control;Application software;Automatic control","carrier sense multiple access;air traffic control;aerospace simulation","air traffic control;OPNET Version 7 software;computer simulation;Aeronautical Telecommunications Network;Controller Pilot Data Link Communications;Automatic Dependent Surveillance-Broadcast mode;VHF Data Link mode 2;collisionless prioritized CSMA;latency;queue length;packet loss;communication network","","1","3","","","","","","IEEE","IEEE Conferences"
"Searching protection relay response time extremes using genetic algorithm-software quality by optimization","J. T. Alander; T. Mantere; G. Moghadampour; J. Matila","Dept. of Inf. Technol. & Ind. Econ., Vaasa Univ., Finland; NA; NA; NA","1997 Fourth International Conference on Advances in Power System Control, Operation and Management, APSCOM-97. (Conf. Publ. No. 450)","","1997","1","","95","99 vol.1","In this work, the authors studying the possibilities of automating the searching and measuring of response time extremes of power system protection relay software using genetic algorithm optimization. The idea is to produce test cases in order to find potentially problematic situations causing processing time extremes in the software of an electric network protection relay. The testing was done using a relay software simulator. In the comparison, the genetic algorithm based method was clearly better than a random test method.","","0-85296-912","10.1049/cp:19971811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726850","","","power system protection","response time extremes;genetic algorithm optimisation;software quality;power system protection relay software;relay software simulator;computer simulation;protection automation","","","","","","","","","IET","IET Conferences"
"Optimisation techniques based on the use of genetic algorithms (GAs) for logic implementation on FPGAs","P. Thomson; J. F. Miller","Dept. of Electr. & Electron. & Comput. Eng., Napier Polytech. of Edinburgh, UK; Dept. of Electr. & Electron. & Comput. Eng., Napier Polytech. of Edinburgh, UK","IEE Colloquium on Software Support and CAD Techniques for FPGAs,","","1994","","","4/1","4/4","The work described in this paper began some time ago as an investigation into two problems associated with logic minimisation or optimisation. These are respectively, the state assignment problem in the design of finite state machines, and the optimisation of combinational logic circuits using Reed-Muller (RM) techniques. When faced with such designs, the use of FPGAs to implement circuits is clearly appropriate. However, because of the limited resources available on FPGA parts, in terms of the number of available CLBs, and the increased difficulty that place and route software will experience in the layout of increasingly complex designs, it is felt that some form of optimisation of the design before implementation is still a necessary stage in the design process. This paper describes the implementation of algorithms which attempt to provide this type of optimisation for the two previously mentioned problems. The resultant software uses genetic algorithms to select, breed and test the fitness of potential solutions, and thereby recommend a near-optimal solution. In practice, these recommended solutions represent a considerable saving (in terms of gate count) on many circuit implementations, as experimental results demonstrate.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=369838","","Circuit optimization;Combinational logic circuits;Field programmable gate arrays;Finite state machines;Genetic algorithms;Design automation;Minimization methods;Reed-Muller codes;State assignment","circuit optimisation;combinational circuits;field programmable gate arrays;finite state machines;genetic algorithms;logic CAD;minimisation of switching nets;Reed-Muller codes;state assignment","genetic algorithms;logic implementation;FPGAs;logic minimisation;state assignment problem;finite state machines;combinational logic circuits;Reed-Muller techniques;near-optimal solution;gate count","","1","","","","","","","IET","IET Conferences"
"Parametric optimization of measuring systems according to the joint error criterion","J. Gajda; M. Szyper","Dept. of Instrum. & Meas., Univ. of Min. & Metall., Cracow, Poland; Dept. of Instrum. & Meas., Univ. of Min. & Metall., Cracow, Poland","Quality Measurement: The Indispensable Bridge between Theory and Reality (No Measurements? No Science! Joint Conference - 1996: IEEE Instrumentation and Measurement Technology Conference and IMEKO Tec","","1996","1","","174","179 vol.1","The paper presents a method and some results of the modelling and design of measuring systems. The minimum of designed system errors is achieved using parametric optimization methods of the measuring system model. A ""structural method"" of measuring system modelling is used. The used equipment properties, as well as data processing algorithms are taken into account.","","0-7803-3312","10.1109/IMTC.1996.507370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507370","","Electromagnetic measurements;Optimization methods;Hardware;Parametric statistics;Input variables;Equations;Instrumentation and measurement;Electronic mail;Data processing;Software measurement","measurement theory;measurement errors;modelling;optimisation;parameter estimation;error analysis;sensitivity analysis;measurement systems","parametric optimization;measuring systems;joint error criterion;modelling;design;structural method;data processing algorithms;measuring errors;vector state equations;sensitivity error;shock absorber testing;damping factor;variability intervals;system quality criterion;simulation","","","4","","","","","","IEEE","IEEE Conferences"
"An automated contingency management simulation environment for integrated health management and control","Jianhua Ge; M. J. Roemer; G. Vachtsevanos","Impact Technol., LLC, Rochester, NY, USA; Impact Technol., LLC, Rochester, NY, USA; NA","2004 IEEE Aerospace Conference Proceedings (IEEE Cat. No.04TH8720)","","2004","6","","3725","3732 Vol.6","This work presents an automated contingency management (ACM) software simulation test bed developed in Simulink that can be applied to various unmanned platforms for developing; testing and verifying automated fault accommodation strategies. Specifically, this paper introduces the required software components and integrated health management and control architecture for performing these tasks and applies it to the unmanned combat armed rotorcraft (UCAR). The ACM Simulink Blockset provides for ''plug in play"" integration of UAV models, prognostic and health management (PHM) algorithms, adaptive flight control logic, and intelligent agents into a simulation-based design environment. By integrating advanced health management, intelligent control, and intelligent reasoner strategies, the ACM simulation test bench allows designers to examine optimal fault accommodation techniques that can increases availability, improve safety, and optimize maintenance resource planning for complex vehicle systems. The demonstrated graphical user interface enables the end users to initially configure the ACM system and monitor the effectiveness of the vehicle simulation under various fault scenarios. It also enables users to rapidly design and simulate ACM components for integrated health monitoring and control system for helicopter robotics, UCARs, reusable launch vehicles, propulsion, aircraft, automotive, etc.","1095-323X","0-7803-8155","10.1109/AERO.2004.1368190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1368190","","Contingency management;Environmental management;Automatic control;Unmanned aerial vehicles;Automatic testing;Software testing;Prognostics and health management;Aerospace control;Intelligent control;Vehicle safety","aerospace simulation;aerospace testing;condition monitoring;adaptive control;aerospace control;planning;software tools;graphical user interfaces;intelligent control;software agents;remotely operated vehicles;fault simulation","automated contingency management;simulation based design environment;health management and control architecture;software simulation test bed;automated fault accommodation strategy;software components;unmanned combat armed rotorcraft;Simulink Blockset;prognostic health management algorithms;adaptive flight control logic;maintenance resource planning;unmanned aerial vehicle model;intelligent agents;intelligent control;intelligent reasoner;complex vehicle systems;graphical user interface","","3","12","","","","","","IEEE","IEEE Conferences"
"A multi-platform support environment","G. Miyahara; C. P. Satterthwaite","Hughes Aircraft Co., Los Angeles, CA, USA; NA","Proceedings of the IEEE 1997 National Aerospace and Electronics Conference. NAECON 1997","","1997","2","","815","819 vol.2","Legacy weapon systems, such as attack aircraft, have taken advantage of embedded computers and software to provide enormous capabilities for flexibility and expandability. The provision of these capabilities has been at a cost, and that is in the dedicated software development facilities which have sprung up to support these legacy systems. Unfortunately, the costs of these dedicated facilities is becoming prohibitive. The Advanced Avionics Multi-Radar Software Support Study (AAMRSSS) project offers experience in handling the above problem. AAMRSSS studied the feasibility of using a dedicated Software Development Facility (SDF) to support multiple software system platforms. Issues of the study were: commonality; unique requirements of the new system to be added; platform priorities; and future expansion. In particular, this study has addressed supporting the AC-130U Gunship's Radar Operational Flight Program (OFF) in the F-15's Radar Software Development Facility.","","0-7803-3725","10.1109/NAECON.1997.622734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622734","","Weapons;Embedded software;Software systems;Radar;Costs;Aerospace electronics;Aircraft;Software testing;System testing;Embedded system","military avionics;real-time systems;radar computing;project support environments;military computing;weapons;aircraft computers;aerospace simulation;airborne radar","multi-platform support environment;legacy weapon systems;attack aircraft;AAMRSSS project;dedicated software development facility;multiple software system platforms;commonality;platform priorities;future expansion;radar operational flight program support;radar software development facility;embedded computers;weapon system embedded software;shared platform support;target generator;simulation models;avionics software","","2","5","","","","","","IEEE","IEEE Conferences"
"On relevant quality criteria for optimized partitioning methods","D. C. De Souza; M. A. De Barros; L. A. B. Naviner; B. G. A. Neto","Departamento de Engenharia Eletrica, Univ. Fed. de Campina Grande, Brazil; NA; NA; NA","2003 46th Midwest Symposium on Circuits and Systems","","2003","3","","1502","1505 Vol. 3","This paper deals with HW/SW partitioning methods. The authors employed the concepts of ""quality requisites"" and a method based on QFD (quality function deployment) as references to both represent the advantages and disadvantages of existing methods as well as to define a set of features for an optimized partitioning algorithm.","1548-3746","0-7803-8294","10.1109/MWSCAS.2003.1562581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1562581","","Optimization methods;Partitioning algorithms;Quality function deployment;Knowledge management;Costs;Knowledge representation;Ontologies;Telecommunication computing;System testing;Collaboration","hardware-software codesign;quality function deployment;knowledge representation;knowledge management","quality criteria;optimized partitioning;quality requisites;quality function deployment;hardware-software codesign","","2","23","","","","","","IEEE","IEEE Conferences"
"Using coupling measurement for impact analysis in object-oriented systems","L. C. Briand; J. Wust; H. Lounis","Fraunhofer Inst. for Exp. Software Eng., Kaiserlautern, Germany; NA; NA","Proceedings IEEE International Conference on Software Maintenance - 1999 (ICSM'99). 'Software Maintenance for Business Change' (Cat. No.99CB36360)","","1999","","","475","482","Many coupling measures have been proposed in the context of object oriented (OO) systems. In addition, due to the numerous dependencies present in OO systems, several studies have highlighted the complexity of using dependency analysis to perform impact analysis. An alternative is to investigate the construction of probabilistic decision models based on coupling measurement to support impact analysis. In addition to providing an ordering of classes where ripple effects are more likely, such an approach is simple and can be automated. In our investigation, we perform a thorough analysis on a commercial C++ system where change data has been collected over several years. We identify the coupling dimensions that seem to be significantly related to ripple effects and use these dimensions to rank classes according to their probability of containing ripple effects. We then assess the expected effectiveness of such decision models.","1063-6773","0-7695-0016","10.1109/ICSM.1999.792645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792645","","Electrical capacitance tomography;Collaboration;Performance evaluation;Performance analysis;Software engineering;Encapsulation;System testing;History;Fault diagnosis;Identity management systems","object-oriented programming;software metrics;software maintenance;C++ language;decision theory","coupling measurement;impact analysis;object oriented systems;coupling measures;OO systems;dependency analysis;probabilistic decision models;ripple effects;commercial C++ system;change data;probability;expected effectiveness","","68","17","","","","","","IEEE","IEEE Conferences"
"The top ten list: dynamic fault prediction","A. E. Hassan; R. C. Holt","Software Archit. Group, Waterloo Univ., Ont., Canada; Software Archit. Group, Waterloo Univ., Ont., Canada","21st IEEE International Conference on Software Maintenance (ICSM'05)","","2005","","","263","272","To remain competitive in the fast paced world of software development, managers must optimize the usage of their limited resources to deliver quality products on time and within budget. In this paper, we present an approach (the top ten list) which highlights to managers the ten most susceptible subsystems (directories) to have a fault. Managers can focus testing resources to the subsystems suggested by the list. The list is updated dynamically as the development of the system progresses. We present heuristics to create the top ten list and develop techniques to measure the performance of these heuristics. To validate our work, we apply our presented approach to six large open source projects (three operating systems: NetBSD, FreeBSD, OpenBSD; a window manager: KDE; an office productivity suite: KOffice; and a database management system: Postgres). Furthermore, we examine the benefits of increasing the size of the top ten list and study its performance.","1063-6773","0-7695-2368","10.1109/ICSM.2005.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510122","","Resource management;Programming;Financial management;Quality management;Software development management;Testing;Operating systems;Project management;Productivity;Database systems","software fault tolerance;software development management","top ten list;dynamic fault prediction;software development;open source project;NetBSD;FreeBSD;OpenBSD;KDE;KOffice;database management system","","75","15","","","","","","IEEE","IEEE Conferences"
"Robust search algorithms for test pattern generation","J. O. M. Silva; K. A. Sakallah","Cadence Eur. Lab., Inst. Superior Tecnico, Lisbon, Portugal; NA","Proceedings of IEEE 27th International Symposium on Fault Tolerant Computing","","1997","","","152","161","In recent years several highly effective algorithms have been proposed for Automatic Test Pattern Generation (ATPG). Nevertheless, most of these algorithms too often rely on different types of heuristics to achieve good empirical performance. Moreover there has not been significant research work on developing algorithms that are robust, in the sense that they can handle most faults with little heuristic guidance. In this paper we describe an algorithm for ATPG that is robust and still very efficient. In contrast with existing algorithms for ATPG, the proposed algorithm reduces heuristic knowledge to a minimum and relies on an optimized search algorithm for effectively pruning the search space. Even though the experimental results are obtained using an ATPG tool built on top of a Propositional Satisfiability (SAT) algorithm, the same concepts can be integrated on application-specific algorithms.","0731-3071","0-8186-7831","10.1109/FTCS.1997.614088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614088","","Robustness;Test pattern generators;Automatic test pattern generation;Circuit faults;Electrical fault detection;Fault diagnosis;Laboratories;Automatic testing;Decision making;Fault detection","automatic test software;search problems;logic CAD;combinational circuits;logic testing","test pattern generation;robust search algorithms;heuristics;heuristic knowledge;optimized search algorithm;propositional satisfiability algorithm;application-specific algorithms","","37","19","","","","","","IEEE","IEEE Conferences"
"An optimistic locking technique for concurrency control in distributed databases","U. Halici; A. Dogac","Middle East Tech. Univ., Ankara, Turkey; Middle East Tech. Univ., Ankara, Turkey","IEEE Transactions on Software Engineering","","1991","17","7","712","724","A method called optimistic method with dummy locks (ODL) is suggested for concurrency control in distributed databases. It is shown that by using long-term dummy locks, the need for the information about the write sets of validated transactions is eliminated and, during the validation test, only the related sites are checked. The transactions to be aborted are immediately recognized before the validation test, reducing the costs of restarts. Usual read and write locks are used as short-term locks during the validation test. The use of short-term locks in the optimistic approach eliminates the need for the system-wide critical section and results in a distributed and parallel validation test. The performance of ODL is compared with strict two-phase locking (2PL) through simulation, and it is found out that for the low conflict cases they perform almost the same, but for the high conflicting cases, ODL performs better than strict 2PL.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=83907","","Concurrency control;Distributed databases;System recovery;System testing;Costs;Optimization methods;Transaction databases;Database systems;Certification;Protocols","concurrency control;distributed databases;system recovery;transaction processing","optimistic locking technique;concurrency control;distributed databases;optimistic method;dummy locks;write sets;validated transactions;validation test;short-term locks;ODL;strict two-phase locking;low conflict cases;strict 2PL","","3","39","","","","","","IEEE","IEEE Journals & Magazines"
"Real-time software development in multiprocessor, multifunction systems","M. E. Minges","Wright Res. & Dev. Center, Wright-Patterson AFB, OH, USA","IEEE Conference on Aerospace and Electronics","","1990","","","675","680 vol.2","It is pointed out that the real-time requirements placed on embedded avionics systems are large and, to avoid costly redesigns of the code (especially during integration and test of the system), a sound development strategy during the design of the system is paramount. The use of systems analysis, both manually and via simulation, will help guarantee the success of the hardware/software partnership. The Integrated Communication, Navigation, Identification, Avionics (ICNIA) system is considered as a model for the development. The real-time interaction of multiple functions running in the ICNIA multiprocessor system is examined. The critical importance of developing a system simulation that can provide timing and sizing data is demonstrated. If the simulation is developed early in the program, there is a valuable opportunity to keep it up to date and use it effectively in static and dynamic systems analyses. These analyses are critical for system optimization and, in turn, are the key to efficient time and resource expenditure. The data provided by the simulator are most valuable during the integration and test phase of the project.<<ETX>>","","","10.1109/NAECON.1990.112847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=112847","","Programming;Real time systems;Aerospace electronics;Analytical models;Acoustic testing;System testing;Hardware;Navigation;Multiprocessing systems;Timing","aerospace computing;digital simulation;multiprocessing programs;real-time systems;software engineering;systems analysis","software development;embedded avionics;systems analysis;real-time interaction;ICNIA;system simulation;dynamic systems analyses;system optimization","","","","","","","","","IEEE","IEEE Conferences"
"Interactive cosimulation with partial evaluation","P. Schaumont; I. Verbauwhede","Dept. of Electr. Eng., Califonia Univ., Los Angeles, CA, USA; Dept. of Electr. Eng., Califonia Univ., Los Angeles, CA, USA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","642","647 Vol.1","We present a technique to improve the efficiency of hardware-software cosimulation, using design information known at simulator compile-time. The generic term for such optimization is partial evaluation. Our contribution is that we apply the optimization transparently to the user, and at multiple abstraction levels in the simulation. We use the technique to create an interactive codesign environment, and evaluate it on several designs including an AES encryption coprocessor and a Viterbi decoder, and for several instruction-set simulators. Compared to SystemC-based cosimulation, we achieve comparable cosimulation performance at only a fraction of the model-build time.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268917","","Hardware;Decoding;Cryptography;Coprocessors;Computer languages;Embedded software;Design automation;Automatic testing;Software testing;System testing","hardware-software codesign;circuit simulation;circuit optimisation;integrated circuit design","interactive cosimulation;partial evaluation;hardware-software cosimulation;simulator compile-time;abstraction levels;interactive codesign environment;AES encryption coprocessor;Viterbi decoder;instruction-set simulators;SystemC-based cosimulation","","15","13","","","","","","IEEE","IEEE Conferences"
"Time optimization of soft real-time virtual instrument design","P. Bilski; W. Winiecki","Inst. of Radioelectron., Warsaw Univ. of Technol., Poland; Inst. of Radioelectron., Warsaw Univ. of Technol., Poland","IEEE Transactions on Instrumentation and Measurement","","2005","54","4","1412","1416","The paper presents a detailed description of the real-time conditions for the virtual instrumentation. Aspects of the instrument design are explained, and the soft real-time (RT) mode is considered to be the most likely to achieve. The requirements for it are shown and verified, resulting in certain conditions for the designers. The premises for future research are included.","0018-9456;1557-9662","","10.1109/TIM.2005.851213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1468546","Real-time (RT) mode;time analysis;virtual instrument (VI)","Design optimization;Instruments;Data acquisition;Signal processing;Spectral analysis;Testing;Buffer storage;Timing;Time measurement;Application software","virtual instrumentation;real-time systems;optimisation","time optimization;virtual instrumentation;instrument design;soft real-time mode;time analysis","","9","6","","","","","","IEEE","IEEE Journals & Magazines"
"Using Untampered Metrics to Decide When to Stop Testing Software","Y. Levendel","AT&T Bell Laboratories","TENCON '91. Region 10 International Conference on EC3-Energy, Computer, Communication and Control Systems","","1991","2","","352","356","Exhaustive testing of large software systems is prohibitively expensive and practically impossible. However, most software development process managers erroneously assume completeness of the test programs. This has often resulted in an incorrect system evaluation and the release of poor quality products to the field. The understanding of this limitation can become a strength by driving the testers to optimize the positioning of the test programs (robust testing). The observation that software errors tend to form clusters (design holes) permits the use of statistical failure analysis to pinpoint the largest and most likely defective areas. It is essential to periodically halt testing and analyze the test failure data in order to optimize the use of the remaining testing resources and interval. The entire test program can be stopped when the likelihood of residual design errors is small enough.","","0-7803-0538","10.1109/TENCON.1991.729675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=729675","","Software testing;Process control;Debugging;Software quality;System testing;Failure analysis;Costs;Process design;Software systems;Programming","","","","1","12","","","","","","IEEE","IEEE Conferences"
"Ispike: a post-link optimizer for the Intel/spl reg/ Itanium/spl reg/ architecture","C. -. Luk; R. Muth; Harish Patil; R. Cohn; G. Lowney","Massachusetts Microprocessor Design Center, Intel Corp., USA; Massachusetts Microprocessor Design Center, Intel Corp., USA; Massachusetts Microprocessor Design Center, Intel Corp., USA; Massachusetts Microprocessor Design Center, Intel Corp., USA; Massachusetts Microprocessor Design Center, Intel Corp., USA","International Symposium on Code Generation and Optimization, 2004. CGO 2004.","","2004","","","15","26","Ispike is a post-link optimizer developed for the Intel/spl reg/ Itanium Processor Family (IPF) processors. The IPF architecture poses both opportunities and challenges to post-link optimizations. IPF offers a rich set of performance counters to collect detailed profile information at a low cost, which is essential to post-link optimization being practical. At the same time, the predication and bundling features on IPF make post-link code transformation more challenging than on other architectures. In Ispike, we have implemented optimizations like code layout, instruction prefetching, data layout, and data prefetching that exploit the IPF advantages, and strategies that cope with the IPF-specific challenges. Using SPEC CINT2000 as benchmarks, we show that Ispike improves performance by as much as 40% on the ltanium/spl reg/2 processor, with average improvement of 8.5% and 9.9% over executables generated by the Intel/spl reg/ Electron compiler and by the Gcc compiler, respectively. We also demonstrate that statistical profiles collected via IPF performance counters and complete profiles collected via instrumentation produce equal performance benefit, but the profiling overhead is significantly lower for performance counters.","","0-7695-2102","10.1109/CGO.2004.1281660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281660","","Service oriented architecture;Counting circuits;Linux;Prefetching;Monitoring;Electrons;Design optimization;Instruments;Computer architecture;Software performance","computer architecture;optimising compilers;storage management;benchmark testing;program control structures;operating systems (computers);instruction sets","Ispike;post-link optimizer;Intel/spl reg/ Itanium/spl reg/ architecture;Itanium Processor Family;performance counters;code transformation;code layout;instruction prefetching;data layout;data prefetching;SPEC CINT2000;Intel/spl reg/ Electron compiler;Gcc compiler;statistical profiles","","2","27","","","","","","IEEE","IEEE Conferences"
"A DRAM compiler for fully optimized memory instances","G. Harling","NA","Proceedings 2001 IEEE International Workshop on Memory Technology, Design and Testing","","2001","","","3","8","System-on-Chip (SoC) designs will soon be dominated by on-chip memory so there is an urgent need for customization of memory semiconductor intellectual property (SIP) to increase product differentiation. This paper describes a software compiler tool which can be used to customize DRAM memory arrays in both pure logic and merged logic processes. This compiler optimizes memory macrocells for speed, power, and area to obtain radically reduced area and power when compared to SRAM implementations. It can also create custom memories with very fine granularity.","","0-7695-1242","10.1109/MTDT.2001.945221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=945221","","Random access memory;Optimizing compilers;System-on-a-chip;Logic arrays;Macrocell networks;Tiles;Bandwidth;Intellectual property;Integrated circuit manufacture;SRAM chips","DRAM chips;random-access storage;high level synthesis;circuit CAD;timing;circuit layout CAD;integrated circuit design;cellular arrays;application specific integrated circuits;VLSI","DRAM compiler;system-on-chip designs;SoC designs;on-chip memory;memory semiconductor intellectual property;software compiler tool;DRAM memory array customisation;merged logic processes;pure logic processes;memory macrocells optimisation;speed;power;area;custom memories;very fine granularity;EDA tool;document generator;VHDL netlists;Verilog netlists;timing;assembler module;netlister","","1","3","","","","","","IEEE","IEEE Conferences"
"Phased array shaped multi-beam optimization for LEO satellite communications using a genetic algorithm","K. N. Sherman","Satellite Software Inc., Carson City, NV, USA","Proceedings 2000 IEEE International Conference on Phased Array Systems and Technology (Cat. No.00TH8510)","","2000","","","501","504","LEO communications satellite antennas may require hundreds of high gain beams to achieve sufficient link margin, especially for mobile systems where the ground terminals have very low EIRP. Generally, the procedure for optimizing antenna beams for shaped coverage areas starts with a set of polygons defined in antenna angle space. These polygons are filled with synthesis stations at which the desired gain is prescribed. An optimization program is then used to synthesize the excitation of the antenna in order to achieve the desired gain at each station. For multibeam coverages, if the number and size of the coverage polygons are not optimal, pattern performance will be poor. Layout is difficult at LEO because a large variation in cell size is dictated by the substantial path length variation from nadir to edge of coverage. A genetic algorithm was developed to optimize the number and size of cells for a circularly symmetric grid. Cells were then filled with synthesis stations and a least squares optimizer used to shape the antenna pattern for each cell. A phased array antenna with circular aperture and cos/sup 1.3//spl theta/ element power pattern was used. The genetic optimizer was found to quickly produce optimal cell layouts for arbitrary altitude, field of view, and directivity requirements. It was also a very good way to quickly and accurately determine the number of beams needed for particular set of requirements.","","0-7803-6345","10.1109/PAST.2000.859006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859006","","Phased arrays;Low earth orbit satellites;Satellite communication;Genetic algorithms;Artificial satellites;Satellite antennas;Solids;Testing;Mobile communication;Communication system software","genetic algorithms;satellite antennas;antenna phased arrays;antenna radiation patterns;multibeam antennas;mobile satellite communication;least squares approximations;directive antennas;aperture antennas","multibeam optimization;LEO satellite communications;genetic algorithm;satellite antennas;high gain beams;link margin;mobile systems;shaped coverage areas;polygons;antenna angle space;synthesis stations;antenna excitation;optimization program;pattern performance;cell size;circularly symmetric grid;least squares optimizer;phased array antenna;circular aperture;directivity","","6","4","","","","","","IEEE","IEEE Conferences"
"Design optimization of multi-cluster embedded systems for real-time applications","P. Pop; P. Eles; Z. Peng; V. Izosimov; M. Hellring; O. Bridal","Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","2","","1028","1033 Vol.2","We present an approach to design optimization of multi-cluster embedded systems consisting of time-triggered and event-triggered clusters, interconnected via gateways. In this paper, we address design problems which are characteristic to multi-clusters: partitioning of the system functionality into time-triggered and event-triggered domains, process mapping, and the optimization of parameters corresponding to the communication protocol. We present several heuristics for solving these problems. Our heuristics are able to find schedulable implementations under limited resources, achieving an efficient utilization of the system. The developed algorithms are evaluated using extensive experiments and a real-life example.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269028","","Design optimization;Embedded system;Real time systems;Time division multiple access;Application software;Message passing;Electronic equipment testing;Communication system control;Software architecture;Kernel","optimisation;embedded systems;logic partitioning","design optimization;multicluster embedded systems;real time applications;time triggered clusters;event triggered clusters;time triggered domain;event triggered domain;process mapping;communication protocol","","9","15","","","","","","IEEE","IEEE Conferences"
"Using case-based reasoning as a reinforcement learning framework for optimisation with changing criteria","Dajun Zeng; K. Sycara","Robotics Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA; Robotics Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA","Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence","","1995","","","56","62","Practical optimization problems such as job-shop scheduling often involve optimization criteria that change over time. Repair-based frameworks have been identified as flexible computational paradigms for difficult combinatorial optimization problems. Since the control problem of repair-based optimization is severe, reinforcement learning (RL) techniques can be potentially helpful. However, some of the fundamental assumptions made by traditional RL algorithms are not valid for repair-based optimization. Case-based reasoning compensates for some of the limitations of traditional RL approaches. We present a case-based reasoning RL approach, implemented in the C/sub A/B/sub I/NS system, for repair-based optimization. We chose job-shop scheduling as the testbed for our approach. Our experimental results show that C/sub A/B/sub I/NS is able to effectively solve problems with changing optimization criteria which are not known to the system and only exist implicitly in a extensional manner in the case base.","1082-3409","0-8186-7312","10.1109/TAI.1995.479378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479378","","Learning;Problem-solving;Optimization methods;Robots;Processor scheduling;Testing;Artificial intelligence;Signal processing;Search methods;Design optimization","learning by example;case-based reasoning;optimisation;scheduling;production control;software agents","case-based reasoning;reinforcement learning framework;optimization problems;job-shop scheduling;optimization criteria;repair-based frameworks;flexible computational paradigms;combinatorial optimization;case-based reasoning RL approach;CABINS system","","1","20","","","","","","IEEE","IEEE Conferences"
"Contingency screening and ranking algorithm using two different sets of security performance indices","C. I. F. Agreira; C. M. M. Ferreira; J. A. D. Pinto; F. P. M. Barbosa","Dept. of Electr. Eng., Instituto Superior de Engenharia de Coimbra, Portugal; Dept. of Electr. Eng., Instituto Superior de Engenharia de Coimbra, Portugal; Dept. of Electr. Eng., Instituto Superior de Engenharia de Coimbra, Portugal; NA","2003 IEEE Bologna Power Tech Conference Proceedings,","","2003","4","","5 pp. Vol.4","","In this paper it is studied and analyzed the impact of different security performance indices on a contingency screening and ranking algorithm. The proposed filtering technique consists of three modules, with different complexity levels. The contingencies are ranked in accordance with severity indices that allow assessing the influence of the overloads and voltage limit violations in the power network. The developed software package was applied to the IEEE 118 test power system. The simulation results that were produced using two different sets of security indices were compared, showing a satisfactory agreement. The simulation was carried out considering a monotonous increase of the load level as well as a random one. For the different solutions the computing times were also obtained. Finally, some conclusions that provide a valuable contribution to the understanding of the electric power system security analysis are highlighted.","","0-7803-7967","10.1109/PTC.2003.1304716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304716","","Power system simulation;Power system security;Computational modeling;Performance analysis;Algorithm design and analysis;Filtering;Voltage;Software packages;Software testing;System testing","power system analysis computing;IEEE standards;power system security;power transmission lines","contingency screening;ranking algorithm;filtering technique;power network;software package;IEEE 118 test power system;electric power system security analysis;steady-state security analysis","","3","14","","","","","","IEEE","IEEE Conferences"
"Optimization of Power Added Efficiency and Third Order Intermodulation in Solid State Power Amplifiers, using Data Processing Software Coupled to Single Tone Active Source and Load-Pull System. Extension to Class F Operation","P. Bouysse; J. M. Nebus; J. P. Villotte; M. Aubourg","IRCOM - University of LIMOGES - 123, Avenue Albert-Thomas - 87060 LIMOGES, CEDEX FRANCE; IRCOM - University of LIMOGES - 123, Avenue Albert-Thomas - 87060 LIMOGES, CEDEX FRANCE; IRCOM - University of LIMOGES - 123, Avenue Albert-Thomas - 87060 LIMOGES, CEDEX FRANCE; IRCOM - University of LIMOGES - 123, Avenue Albert-Thomas - 87060 LIMOGES, CEDEX FRANCE","1990 20th European Microwave Conference","","1990","2","","1228","1233","A fully calibrated and automatized single-tone test set-up, using both active source and load-pull techniques allows to built an accurate data-file, characterizing any non-linear two-port device. Appropriate data processing routines provide optimized operating conditions of the device under test in terms of added power, efficiency and third order intermodulation. Therefore such a measurement system reveals to be a precious laboratory tool to aid optimized power-circuit design.","","","10.1109/EUMA.1990.336234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4136171","","Solid state circuits;Power amplifiers;Data processing;Frequency;Automatic testing;Design optimization;Power generation;Impedance;Circuit testing;Laboratories","","","","3","4","","","","","","IEEE","IEEE Conferences"
"Designing decentralized software for a wireless network environment: evaluating patterns of mobility for a mobile agent swarm","V. A. Cicirello; A. Mroczkowski; W. Regli","Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA; Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA; Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA","IEEE 2nd Symposium on Multi-Agent Security and Survivability, 2005.","","2005","","","49","57","Designing decentralized software applications for a wireless network environment offers harsh challenges to the software engineer. All of the usual difficulties associated with a distributed system are present, but are amplified by the inherent dynamics and uncertainty of the wireless network. This paper takes an agent-oriented software engineering perspective in considering how to design decentralized software systems for a mobile ad hoc network (MANET) of resource-constrained devices. Specifically, the authors codify within the context of a software design pattern the concept of an agent swarm. Swarms of mobile agents have been used in the development of applications to support coordination and collaboration in a live MANET test bed. Work is underway to transition some of this technology into use by public protectors as part of the Philadelphia area urban wireless network testbed. The objectives of this paper include motivating the need for a swarm-based approach to distributed software for wireless environments and discussing the critical issues involved with mobile agents swarming on a MANET. For example, one such design issue is the selection of migration patterns for use by the swarming agents. Several different types of itinerary patterns are evaluated within the context of a mobile agent swarm.","","0-7803-9447","10.1109/MASSUR.2005.1507047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507047","","Software design;Wireless networks;Mobile agents;Mobile ad hoc networks;Application software;Testing;Design engineering;Uncertainty;Software engineering;Software systems","mobile agents;object-oriented methods;particle swarm optimisation;ad hoc networks;mobile computing;mobility management (mobile radio)","decentralized software design pattern;wireless network environment;mobile agent swarm;agent-oriented software engineering;mobile ad hoc network;MANET;resource-constrained device;public protector;distributed software","","1","29","","","","","","IEEE","IEEE Conferences"
"The single event upset response of the Analog Devices, ADSP2100A, digital signal processor","R. Harboe-Sorensen; H. Seran; P. Armbruster; L. Adams","European Space Agency/ESTEC, Noordwijk, Netherlands; European Space Agency/ESTEC, Noordwijk, Netherlands; European Space Agency/ESTEC, Noordwijk, Netherlands; European Space Agency/ESTEC, Noordwijk, Netherlands","RADECS 91 First European Conference on Radiation and its Effects on Devices and Systems","","1991","","","457","461","The authors present the results of a radiation evaluation program carried out on the Analog Devices, ADSP2100A, which is a single chip microprocessor optimized for 12.5 Mips Digital Signal Processing (DSP). Single Event Upset/Latch-up (SEU/SEL) testing using Californium-252 was the primary aim of this program, however, accelerator heavy ion and proton SEU/SEL data as well as total ionising dose data are also presented. In order to perform these tests, in particular the SEU tests, a dedicated test system was required. Special test hardware and software were developed. The hardware design and software used are described and details of the various tests and test facilities are given. Finally, the authors report on the use of the SEU data for the calculation of expected in-orbit upset rates using the CREME suite of programs.<<ETX>>","","0-7803-0208","10.1109/RADECS.1991.213555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213555","","Single event upset;System testing;Digital signal processing chips;Software testing;Hardware;Microprocessors;Life estimation;Ion accelerators;Proton accelerators;Performance evaluation","aerospace instrumentation;digital signal processing chips;integrated circuit testing;ion beam effects;logic testing;proton effects","proton test data;latch up testing;heavy ion test data;single event upset response;Analog Devices;ADSP2100A;digital signal processor;single chip microprocessor;total ionising dose data;dedicated test system;hardware design;software;in-orbit upset rates;CREME suite of programs;12.5 MIPS","","","7","","","","","","IEEE","IEEE Conferences"
"Briki: an optimizing Java compiler","M. Cierniak; Wei Li","Dept. of Comput. Sci., Rochester Univ., NY, USA; NA","Proceedings IEEE COMPCON 97. Digest of Papers","","1997","","","179","184","We have developed a research compiler for Java class files. The compiler, which we call Briki, is designed to test new compilation techniques. We focus on optimizations which are only possible or which are much easier to perform on a high-level intermediate representation (IR). We have designed such a representation (JavaIR) and have written a front-end which recovers high-level structure from the information from the class file. Some of the high-level optimizations can be performed by the Java compiler which produces the class file. There is, however, a set of machine-dependent optimizations which have to be customized for the specific architecture and so can only be performed when the machine code is generated from the byte codes, e.g. in a just-in-time (JIT) compiler. We choose memory hierarchy optimizations as an example of machine-dependent techniques. We show that there is an intersection of the set of machine-dependent optimizations and the set of high-level optimizations. One such example is array remapping, which requires multi-dimensional array references which are not present in the byte codes and at the same time requires information about the memory organization. We develop a set of optimizations for accessing array elements and object fields and show their impact on set of benchmarks which we run on two machines with a JIT compiler. The execution times are reduced by as much as 50%, and we argue that the improvement could be even higher with a more mature JIT technology.","1063-6390","0-8186-7804","10.1109/CMPCON.1997.584697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584697","","Optimizing compilers;Java;Computer languages;Application software;Virtual machining;Computer science;Testing;Joining processes;Computer aided instruction;Program processors","object-oriented languages;optimising compilers;arrays;abstract data types","Briki;optimizing Java compiler;Java class files;compilation technique testing;high-level intermediate representation;JavaIR;front-end;high-level structure recovery;high-level optimizations;machine-dependent optimizations;customization;byte codes;just-in-time compiler;memory hierarchy optimizations;array remapping;multi-dimensional array references;memory organization;object fields;benchmarks","","1","8","","","","","","IEEE","IEEE Conferences"
"Momentum particle swarm optimizer","L. Yul; Q. Zheng; W. Xianghua; H. Xingshi","Dept. of Computer Science, Xi'an Jiaotong Univ., XTan 710049, P. R. China; Dept. of Computer Science, Xi'an Jiaotong Univ., XTan 710049, P. R. China; School of Software, Tsinghua Univ, Beijing 100084, P. R. China; Dept. of Computer Science, Xi'an Jiaotong Univ., XTan 710049, P. R. China; Dept. of Mathematics, XTan Univ. of Engineering Science and Technology, XTan 710048, P. R. China","Journal of Systems Engineering and Electronics","","2005","16","4","941","946","The previous particle swarm optimizers lack direct mechanism to prevent particles beyond predefined search space, which results in invalid solutions in some special cases. A momentum factor is introduced into the original particle swarm optimizer to resolve this problem. Furthermore, in order to accelerate convergence, a new strategy about updating velocities is given. The resulting approach is mromentum-PSO which guarantees that particles are never beyond predefined search space without checking boundary in every iteration. In addition, linearly decreasing wight PSO (LDW-PSO) equipped with a boundary checking strategy is also discussed, which is denoted as LDWBOPSO. LDW-PSO, LDWBCPSO and momentum-PSO are compared in optimization on five test functions. The experimental results show that in some special cases LDW-PSO finds invalid solutions and LDWBC-PSO has poor performance, while momentum-PSO not only exhibits good performance but also reduces computational cost for updating velocities.","1004-4132","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071274","evolutionary computation;particle swarm optimization;optimization algorithm","Optimization;Particle swarm optimization;Convergence;Navigation;Search problems;Helium;Tuning","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"An exact array reference analysis for data flow testing","I. Forgacs","Comput. & Autom. Inst., Hungarian Acad. of Sci., Budapest, Hungary","Proceedings of IEEE 18th International Conference on Software Engineering","","1996","","","565","574","Data-flow testing is a well-known technique, and it has proved to be better than the commercially-used branch testing. The problem with data-flow testing is that, apart from scalar variables, only approximate information is available. This paper presents an algorithm that precisely determines the definition-use pairs for arrays within a large domain. There are numerous methods addressing the array data-flow problem; however, these methods are only used in the optimization or parallelization of programs. Data-flow testing, however, requires at least one real solution of the problem for which the necessary program path is executed. Contrary to former precise methods, we avoid negation in formulae, which seems to be the biggest problem in all previous methods.","0270-5257","0-8186-7247","10.1109/ICSE.1996.493450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493450","","Data analysis;Testing;Optimizing compilers;Input variables","program testing;data flow analysis;arrays","exact array reference analysis;data flow testing;approximate information;definition-use pairs;program optimization;program parallelization;program path execution;precise method;formulae negation avoidance","","1","18","","","","","","IEEE","IEEE Conferences"
"Software Performance Modeling and Management","D. J. Simkins","International Business Machines Corporation; Federal Systems Division; Owego, New York 13827 USA.","IEEE Transactions on Reliability","","1983","R-32","3","293","298","This paper addresses methods to assess the impact of software on weapon system performance parameters such as reliability and operability/suitability. The latter is emphasized in major weapon-system go-ahead decisions. This paper discusses a system reliability model primarily intended to provide management insight and guidelines for identifying out-of-tolerance situations and needed corrective actions. Guidelines are discussed for judging if ``independent verification and test'' and ``weapon-system proof-of-compliance testing'' are successful. Guidelines are provided for comparing software and hardware in terms of total valid problems reported, resolution rates, and comparable difficulty of implementing and verifying the resolutions. This is done with respect to severity levels in MIL-STD-1679. The management of the operability/suitability issue is discussed and recommendations are made to both the procuring agency and the prime contractor. Software is an attractive medium in comparison to hardware in implementing complex functions because: a) there are more controllable means to reduce severe software defects, and b) it is easier to effect change. Properly managed software will have minimal difficulties with system reliability and operability/suitability. Proper software management includes: a) the application, during development, of proper design tools such as top-down design, structural programming, and programming teams; b) aggressive, independent testing and problem tracking activities; and c) application of the management elements presented in this paper. Such application requires contractor familiarity with user needs and capabilities as well as with the mission and operations of the system, so as to optimize the man/machine interface.","0018-9529;1558-1721","","10.1109/TR.1983.5221654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5221654","System software reliability;Operability/ suitability;Software management;Human factors management;User analysis;Prototyping","Software performance;Weapons;Guidelines;Application software;Reliability;Testing;Hardware;Software development management;Software systems;System performance","","","","1","6","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing the design examples of wide-band antennas","Suidong Yang; A. Michaelides; C. Riley; J. Archer; M. Hook; J. Simkin","Vector Fields Ltd., Oxford, UK; Vector Fields Ltd., Oxford, UK; Vector Fields Ltd., Oxford, UK; Vector Fields Ltd., Oxford, UK; Vector Fields Ltd., Oxford, UK; Vector Fields Ltd., Oxford, UK","IEE Wideband and Multi-band Antennas and Arrays 2005 (Ref. No. 2005/11059)","","2005","","","185","190","The examples described in this paper illustrate the application of computational electromagnetics in the design of realistic multi- and wideband antennas. With the model generation, solver, optimization and post processing utilities of modern software tools, and the capabilities of today's desktop computers, the CONCERTO rf design process can be very rapid. The number of build and test iterations can be dramatically reduced, with consequent reduction in cost and time scale. Relevant antenna design examples are given to illustrate its capability and utility.","0537-9989","0-86341-562","10.1049/ic:20050310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524842","","","broadband antennas;computational electromagnetics;multifrequency antennas;optimisation;software tools","wide-band antennas;computational electromagnetics;multiband antennas;optimization;rf design process;software tools","","1","","","","","","","IET","IET Conferences"
"Precis: a usercentric word-length optimization tool","M. L. Chang; S. Hauck","Franklin W. Olin Coll. of Eng., Needham, MA, USA; NA","IEEE Design & Test of Computers","","2005","22","4","349","361","Translating an algorithm designed for a general-purpose processor into an algorithm optimized for custom logic requires extensive knowledge of the algorithm and the target hardware. Precis lets designers analyze the precision requirements of algorithms specified in Matlab. The design time tool combines simulation, user input, and program analysis to help designers focus their manual precision optimization efforts.","0740-7475;1558-1918","","10.1109/MDT.2005.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492295","D.2.2 Design Tools and Techniques;J.6.a Computer-aided design","Design optimization;Algorithm design and analysis;Analytical models;Hardware design languages;Software performance;Software algorithms;Performance analysis;Very large scale integration;Libraries;Logic design","optimising compilers;mathematics computing;redundant number systems;program interpreters;computer architecture","Precis software tool;usercentric word-length optimization;general-purpose processor;precision requirements;algorithm translation;design time tool;Matlab;slack-analysis","","12","29","","","","","","IEEE","IEEE Journals & Magazines"
"Static analysis of object references in RMI-based Java software","M. Sharp; A. Rountev","Ohio State Univ., Columbus, OH, USA; Ohio State Univ., Columbus, OH, USA","21st IEEE International Conference on Software Maintenance (ICSM'05)","","2005","","","101","110","Distributed applications provide numerous advantages related to software performance, reliability, interoperability, and extensibility. This paper focuses on distributed Java programs built with the help of the remote method invocation (RMI) mechanism. We consider points-to analysis for such applications. Points-to analysis determines the objects pointed to by a reference variable or a reference object field. Such information plays a fundamental role as a prerequisite for many other static analyses. We present the first theoretical definition of points-to analysis for RMI-based Java applications, and an algorithm for implementing a flow- and context-insensitive points-to analysis for such applications. We also discuss the use of points-to information for computing call graph information, for understanding data dependencies due to remote memory locations, and for identifying opportunities for improving the performance of object serialization at remote calls. The work described in this paper solves one key problem for static analysis of RMI programs, and provides a starting point for future work on improving the understanding, testing, verification, and performance of RMI-based software.","1063-6773","0-7695-2368","10.1109/ICSM.2005.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510107","","Java;Application software;Information analysis;Software performance;Algorithm design and analysis;Performance analysis;Software testing;Middleware;Software maintenance;Optimization","Java;distributed programming;remote procedure calls;program diagnostics","RMI program static analysis;object references;RMI-based Java software;distributed Java programs;remote method invocation;flow-insensitive points-to analysis;context-insensitive points-to analysis;call graph information computing;data dependencies;remote memory locations;object serialization;remote calls","","1","19","","","","","","IEEE","IEEE Conferences"
"Advanced test cell diagnostics for gas turbine engines","M. J. Roemer; G. J. Kacprzynski; M. Schoeller; R. Howe; R. Friend","Impact Technol., Rochester, NY, USA; NA; NA; NA; NA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","6","","2915","2925 vol.6","Improved test cell diagnostics capable of detecting and classifying engine mechanical and performance faults as well as instrumentation problems is critical to reducing engine operating and maintenance costs while optimizing test cell effectiveness. Proven anomaly detection and fault classification techniques utilizing engine Gas Path Analysis (GPA) and statistical/empirical models of structural and performance related engine areas can now be implemented for real-time and post-test diagnostic assessments. Integration and implementation of these proven technologies into existing USAF engine test cells presents a great opportunity to significantly improve existing engine test cell capabilities to better meet today's challenges. A suite of advanced diagnostic and troubleshooting tools have been developed and implemented for gas turbine engine test cells as part of the Automated Jet Engine Test Strategy (AJETS) program. AJETS is an innovative USAF program for improving existing engine test cells by providing more efficient and advanced monitoring, diagnostic and troubleshooting capabilities. This paper describes the basic design features of the AJETS system; including the associated data network, sensor validation and anomaly detection/diagnostic software that was implemented in both a real-time and post-test analysis mode. These advanced design features of AJETS are currently being evaluated and advanced utilizing data from TF39 test cell installations at Travis AFB and Dover AFB.","","0-7803-6599","10.1109/AERO.2001.931313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931313","","Turbines;Fault detection;Automatic testing;Instruments;Cost function;Performance analysis;Jet engines;Monitoring;Sensor phenomena and characterization;Sensor systems","aerospace engines;aircraft testing;gas turbines;military aircraft;automatic test equipment;vibration measurement;automatic testing;feature extraction;belief networks;aerospace computing;fault diagnosis","test cell diagnostics;gas turbine engines;mechanical faults;performance faults;engine operating costs;maintenance costs;anomaly detection;fault classification;engine gas path analysis;statistical models;real-time diagnostic;USAF engine test cells;troubleshooting tools;automated jet engine test;sensor validation;Travis AFB;Dover AFB;TF39 test cell;engine vibration;feature extraction;Bayesian network","","3","3","","","","","","IEEE","IEEE Conferences"
"Modelling and simulation of indoor radio channels","U. Dersch; R. Ruegg; H. Kaufmann; R. Rufener","Ascom Tech Ltd., Baden, Switzerland; Ascom Tech Ltd., Baden, Switzerland; Ascom Tech Ltd., Baden, Switzerland; Ascom Tech Ltd., Baden, Switzerland","Proceedings of ICC '93 - IEEE International Conference on Communications","","1993","3","","1970","1974 vol.3","Starting from the physical propagation mechanisms in indoor environments, a channel model for wideband, indoor propagation channels is developed. The model is flexible enough to accommodate various parameters such as system bandwidth, spatial distribution of scatterers, and maximum delay of propagation paths. A software simulation tool allows the generation of samples of the time-invariant complex channel impulse response. These can be used to evaluate and optimize the system performance by software simulation. In addition to the software simulator, a hardware channel simulator is built. It provides the possibility for real-time tests of radio receivers under realistic, yet well defined, channel conditions.<<ETX>>","","0-7803-0950","10.1109/ICC.1993.397623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=397623","","Indoor radio communication;Indoor environments;Wideband;Bandwidth;Scattering parameters;Propagation delay;Software tools;System performance;Software performance;Software systems","indoor radio;land mobile radio;simulation;modelling;software tools;telecommunication channels;delays;transient response;telecommunication equipment testing;radio receivers;real-time systems","indoor radio channels;physical propagation mechanisms;channel model;system bandwidth;spatial distribution of scatterers;maximum delay;software simulation tool;time-invariant complex channel impulse response;system performance;hardware channel simulator;real-time tests;radio receivers","","2","9","","","","","","IEEE","IEEE Conferences"
"MicroMultitest: Ranking Differentially-Expressed Genes in Microarray Data","Li Xiao; Linfeng Cao; J. Iqbal; Guimei Zhou; W. C. Chan; S. Sherman","University of Nebraska Medical Center, Omaha, NE; NA; NA; NA; NA; NA","Proceedings of the 38th Annual Hawaii International Conference on System Sciences","","2005","","","279a","279a","The important purpose of the microarray gene expression data analysis is to identify significantly differentially-expressed genes between two groups of samples which are in two different experimental states. In this work, we propose to use several statistical test methods for a given microarray data set and cross-refer the results of different statistical test methods. The accuracy of different statistical methods is estimated by Receiver Operation Characteristic (ROC) technique. A new software tool, MicroMultitest, was developed. A number of statistical testing methods (such as t-test, adapted SAM method, p-value adjustments), as well as the ROC analysis technique were implemented in this software. Using the MicroMultitest one has the ability to evaluate the performance of different statistical testing methods by applying each to the same given microarray data set, optimize the cutoff values and permutation times for these statistical testing methods, and select relative reliable differentially-expressed gene set.","1530-1605","0-7695-2268","10.1109/HICSS.2005.410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1385811","","Statistical analysis;Error analysis;Data analysis;Gene expression;System testing;Cancer;Pathology;Software tools;Optimization methods;DNA","","","","","9","","","","","","IEEE","IEEE Conferences"
"A case study in design space exploration: the Tosca environment applied to a telecommunication link controller","A. Allara; M. Bombana; W. Fornaciari; F. Salice","Siemens ICN, Italy; NA; NA; NA","IEEE Design & Test of Computers","","2000","17","2","60","72","The concept of system design, or codesign, includes a variety of possible definitions according to the considered relevant aspects, the application field, and the system granularity of the analysis. The novelty of codesign with respect to the design of pure hardware and software, which are well-known subjects, arises from the tight integration between the two types of design and from the global scope of the design constraints. Since such applications strive for high volumes, there is a payoff for size, power, and speed optimization techniques. This article presents a system-level design methodology to specify, analyze, and explore different hardware/software solutions, whose benefits have been tested by redesigning a commercial device.","0740-7475;1558-1918","","10.1109/54.844335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844335","","Computer aided software engineering;Space exploration;Hardware;Electronic design automation and methodology;System testing;Control systems;Telecommunication control;Software testing;Application software;Telecommunication switching","hardware-software codesign;formal specification","design space exploration;Tosca environment;telecommunication link controller;system design;codesign;application field;system granularity;design constraints;speed optimization;system-level design methodology;hardware/software solutions","","1","8","","","","","","IEEE","IEEE Journals & Magazines"
"Self-optimizing Web system","S. Kato; J. Shimizu; T. Hama; T. Fukuda; T. Yamane","Tokyo Res. Lab., IBM Res., Tokyo, Japan; Tokyo Res. Lab., IBM Res., Tokyo, Japan; Tokyo Res. Lab., IBM Res., Tokyo, Japan; Tokyo Res. Lab., IBM Res., Tokyo, Japan; Tokyo Res. Lab., IBM Res., Tokyo, Japan","IEEE International Conference on Industrial Informatics, 2003. INDIN 2003. Proceedings.","","2003","","","480","482","We propose a new architecture for a self-optimizing Web system that adaptively tunes its performance according to changes in workload characteristics. To realize the system, the authors have developed the integrated performance analysis tool (IPAT) to monitor and simulate the performance of Web systems. IPAT can be used as a testing tool for constructing Web sites. It also performs a comprehensive analysis of Web system performance and detects bottleneck candidates within Web systems. The road map and technical problems for the realization of the system are also discussed.","","0-7803-8200","10.1109/INDIN.2003.1300382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1300382","","Monitoring;Performance analysis;Control systems;Service oriented architecture;Roads;Analytical models;Testing;System performance;Companies;Web page design","Internet;Web sites;performance evaluation;optimisation;software tools","self-optimizing Web system;workload characteristics;integrated performance analysis tool;Web sites;road map problem","","","2","","","","","","IEEE","IEEE Conferences"
"iCDMdt: focused the model mapping and performance optimization in embedded system design","Jing Luan; Xuan Cheng; Junzhong Gu","Comput. Dept., East China Normal Univ., Shanghai, China; Comput. Dept., East China Normal Univ., Shanghai, China; Comput. Dept., East China Normal Univ., Shanghai, China","The Fifth International Conference on Computer and Information Technology (CIT'05)","","2005","","","771","775","This paper proposes a method of model-driven HW/SW co-design in embedded system design and discusses the key technology of model mapping, automatic generating codes and performance optimization. Using the developed iCDMdt platform, a series of design tasks can be integrative fulfilled, such as modeling, mapping from DCDM to systecC model, generating executable systemC codes and co-verifying in different abstract levels. After performance optimizing, the virtual prototype that offers for low-level chops design is gained. This can shorten the design time of product. Finally, an instance of application is introduced.","","0-7695-2432","10.1109/CIT.2005.128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1562750","","Embedded system;Hardware;Testing;Computational modeling;Independent component analysis;Embedded computing;Design optimization;Software prototyping;Virtual prototyping;Product design","embedded systems;optimisation;hardware-software codesign;virtual prototyping;optimising compilers","model mapping;performance optimization;embedded system design;model-driven HW/SW co-design;automatic generating code;iCDMdt platform;DCDM;systemC model;virtual prototype;low-level chops design","","","11","","","","","","IEEE","IEEE Conferences"
"Virtual environment for robots interfaces design and testing","E. Franti; D. Tufis; S. Goschin; M. Dascalu; P. L. Milea; G. Stefan; T. Balan; C. Slav; R. Demco","Nat. Inst. for Res. & Dev. in Microtechnol., Bucharest, Romania; NA; NA; NA; NA; NA; NA; NA; NA","CAS 2005 Proceedings. 2005 International Semiconductor Conference, 2005.","","2005","2","","463","466 vol. 2","This paper refers to the implementation of a virtual environment for the robot interfaces testing. This software environment is very useful because, comparing to the experiments with real robots, it allow the testing and evaluation of different types of interfaces and different working environments with diverse configurations. A very important facility of this interactive software environment is the fact that the designers of the robots sensors and interfaces are able to work in parallel to design test, optimize and realize different control devices for the robot","1545-827X;2377-0678","0-7803-9214","10.1109/SMICND.2005.1558827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1558827","","Virtual environment;Robot sensing systems;Artificial intelligence;Intelligent robots;Mobile robots;Electronic equipment testing;Software testing;Parallel robots;Robot control;Education","control engineering computing;mobile robots","robot interface design;robot interface testing;virtual environment;interactive software environment;robot sensor designers;control devices","","2","5","","","","","","IEEE","IEEE Conferences"
"The T9 transputer: A practical example of the application of standard test techniques","G. Frearson","Inmos Ltd., Bristol, UK","Proceedings of 1993 IEEE International Workshop on Defect and Fault Tolerance in VLSI Systems","","1993","","","231","238","The authors outline a practical approach adopted to integrated current design-for-test methodologies into a VLSI chip containing several processing elements and embedded RAM. It is shown that optimizing design styles to suit the applications has resulted in the need for more than one test strategy. Global design rules concerning clocking and reset to allow scan and behavioral test are discussed. Using the T9000 virtual channel processor as an example, the integration of some of these techniques is explained.","1550-5774","0-8186-3502","10.1109/DFTVS.1993.595806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=595806","","Design for testability;Design methodology;Very large scale integration;Software testing;System testing;Random access memory;Design optimization;Clocks;Automatic test pattern generation;Software tools","design for testability","global design;ASIC;scan test;ROM signature analysis;control logic scan;T9 transputer;standard test;integrated current design-for-test methodologies;VLSI chip;embedded RAM;clocking;reset;virtual channel processor","","","3","","","","","","IEEE","IEEE Conferences"
"Combining static analysis and model checking for software analysis","G. Brat; W. Visser","Kestrel/NASA Ames Res. Center, Moffett Field, CA, USA; NA","Proceedings 16th Annual International Conference on Automated Software Engineering (ASE 2001)","","2001","","","262","269","We present an iterative technique in which model checking and static analysis are combined to verify large software systems. The role of the static analysis is to compute partial order information which the model checker uses to reduce the state space. During exploration, the model checker also computes aliasing information that it gives to the static analyzer which can then refine its analysis. The result of this refined analysis is then fed back to the model checker which updates its partial order reduction. At each step of this iterative process, the static analysis computes optimistic information which results in an unsafe reduction of the state space. However, we show that the process converges to a fixed point at which time the partial order information is safe and the whole state space is explored.","1938-4300","0-7695-1426","10.1109/ASE.2001.989812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989812","","State-space methods;Space exploration;Algorithm design and analysis;Information analysis;Java;Explosions;Software systems;Aerospace industry;Computer industry;Software safety","formal verification;program testing;program verification","static analysis;model checking;software analysis;iterative technique;partial order information;aliasing information;software verification","","12","13","","","","","","IEEE","IEEE Conferences"
"Reformulating software engineering as a search problem","J. Clarke; J. J. Dolado; M. Harman; R. Hierons; B. Jones; M. Lumkin; B. Mitchell; S. Mancoridis; K. Rees; M. Roper; M. Shepperd","Univ. of York, Heslington, UK; Univ. of York, Heslington, UK; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEE Proceedings - Software","","2003","150","3","161","175","Metaheuristic techniques such as genetic algorithms, simulated annealing and tabu search have found wide application in most areas of engineering. These techniques have also been applied in business, financial and economic modelling. Metaheuristics have been applied to three areas of software engineering: test data generation, module clustering and cost/effort prediction, yet there remain many software engineering problems which have yet to be tackled using metaheuristics. It is surprising that metaheuristics have not been more widely applied to software engineering; many problems in software engineering are characterised by precisely the features which make metaheuristics search applicable. In the paper it is argued that the features which make metaheuristics applicable for engineering and business applications outside software engineering also suggest that there is great potential for the exploitation of metaheuristics within software engineering. The paper briefly reviews the principal metaheuristic search techniques and surveys existing work on the application of metaheuristics to the three software engineering areas of test data generation, module clustering and cost/effort prediction. It also shows how metaheuristic search techniques can be applied to three additional areas of software engineering: maintenance/evolution system integration and requirements scheduling. The software engineering problem areas considered thus span the range of the software development process, from initial planning, cost estimation and requirements analysis through to integration, maintenance and evolution of legacy systems. The aim is to justify the claim that many problems in software engineering can be reformulated as search problems, to which metaheuristic techniques can be applied. The goal of the paper is to stimulate greater interest in metaheuristic search as a tool of optimisation of software engineering problems and to encourage the investigation and exploitation of these technologies in finding near optimal solutions to the complex constraint-based scenarios which arise so frequently in software engineering.","1462-5970","","10.1049/ip-sen:20030559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214740","","","software engineering;simulated annealing;genetic algorithms;search problems;program testing;formal specification","software engineering;search problem;metaheuristic techniques;genetic algorithms;simulated annealing;tabu search;test data generation;module clustering;cost/effort prediction;maintenance/evolution system integration;requirements scheduling;legacy systems;complex constraint-based scenarios","","59","","","","","","","IET","IET Journals & Magazines"
"Robust optimization of experimental designs in microelectronics processes using a stochastic approach","F. Pasqualini; E. Josse","Central R&D, Crolles, France; Central R&D, Crolles, France","13th Annual IEEE/SEMI Advanced Semiconductor Manufacturing Conference. Advancing the Science and Technology of Semiconductor Manufacturing. ASMC 2002 (Cat. No.02CH37259)","","2002","","","187","192","Design of Experiments (DOE) is a structured approach widely used in the Microelectronics industry for over 20 years to study physical phenomena with simultaneous factors and responses. Today this methodology is in daily use to optimize process and products. With this in mind most DOE software's available on the market introduced since around five years ago have included the Desirability functions of Derringer (1980). Desirability functions permit the optimization simultaneously of several characteristics in the same experimental space. This first step in multi-response optimization was absolutely essential for industrial use of DOE, but this approach's weakness is that it is based on a deterministic optimization model. The provided optimum does not guarantee the robustness of the solution because it does not take into account uncertainty on factors and Response model coefficients. For this reason we are deploying at STMicroelectronics a multi-response optimization solution based on a stochastic approach of optimum's research. It takes into account uncertainty on factors and on coefficients of all the response models. The obtained solution provides a distribution function for the optimized criteria, which permits us to appreciate the statistically determined robustness. The different steps of optimization will be detailed. An example of application, for an advanced metal etch process in 0.18 /spl mu/m technology, will be presented. This permits us to point out the contribution of the stochastic solution to the process robustness and to compare it to the deterministic solution. In this example, the localization between optimums was different in the experimental space. The two solutions were tested and the physical results concluded that the better prediction was obtained with the stochastic optimum. The retained solution for industrialization was obviously the stochastic optimum.","","0-7803-7158","10.1109/ASMC.2002.1001601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1001601","","Robustness;Design optimization;Design for experiments;Microelectronics;Stochastic processes;US Department of Energy;Uncertainty;Space technology;Optimization methods;Distribution functions","optimisation;semiconductor process modelling;stochastic processes;sputter etching;design of experiments","robust process optimization;experimental design;microelectronics processing;stochastic method;desirability function;multi-response optimization;response model;distribution function;metal etching;0.18 micron","","","5","","","","","","IEEE","IEEE Conferences"
"ATLAST deployment &amp; push pack spares optimization module","N. Gurvitz; S. Borodetsky; P. van Eck","Clockwork Solutions Inc., Austin, TX, USA; Clockwork Solutions Inc., Austin, TX, USA; Clockwork Solutions Inc., Austin, TX, USA","Annual Reliability and Maintainability Symposium, 2005. Proceedings.","","2005","","","55","60","This article discusses ATLAST (aircraft total life-cycle assessment software tool) developed to support life-cycle logistics impact forecasting for new and aging weapon-system fleets. The latest version of ATLAST consists of a ""Deployment & Push Pack Spares"" optimization module that utilizes a hybrid analytical-simulation optimization approach to rank spare parts by their effectiveness in increasing availability. Decision makers can use ATLAST to determine the optimal level of spares for the deployment of aircraft at a specific location. Detailed mathematical formulation and three numerical examples are presented in this paper.","0149-144X","0-7803-8824","10.1109/RAMS.2005.1408338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408338","","Logistics;Weapons;Predictive models;Discrete event simulation;Military aircraft;Clocks;Availability;Aircraft propulsion;Assembly systems;Monte Carlo methods","aircraft maintenance;ageing;aircraft testing;life testing;aerospace computing;optimisation;weapons;software tools;Monte Carlo methods","deployment & push pack spares optimization module;aircraft total life-cycle assessment software tool;ATLAST;life-cycle logistic;weapon-system aging;spare parts;decision making;Monte Carlo simulation;maintenance resource","","","10","","","","","","IEEE","IEEE Conferences"
"Design of an optimized IC for control algorithms of AC machines: system testing and application","M. Fathallah; J. P. Chante; F. Calmon; M. H. El-husseini","NA; NA; NA; NA","Conference Record of the 2001 IEEE Industry Applications Conference. 36th IAS Annual Meeting (Cat. No.01CH37248)","","2001","1","","103","109 vol.1","The paper deals with the integration of a control algorithm for alternative current (AC) motors using a co-design methodology for mixed hardware/software implementation. The authors mainly focus on two points: the co-design flow with the associated tools and a full hardware integration with a programmable FPGA device. They also present the interest in using the CORDIC (COrdinate Rotation DIgital Computer) algorithm as a mathematical processor. This study is part of project that investigates firstly, the co-design methodology in order to optimize the integration of control algorithms using hardware, software or mixed resources, and secondly the development of machine diagnosis routines.","0197-2618","0-7803-7114","10.1109/IAS.2001.955399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=955399","","Algorithm design and analysis;Design optimization;Application specific integrated circuits;Control systems;AC machines;System testing;Machine vector control;Pulse width modulation inverters;Software algorithms;AC motors","AC machines;machine control;control system synthesis;digital control;power engineering computing;field programmable gate arrays;programmable controllers;signal processing;digital arithmetic","AC machine control algorithms;optimized IC design;system testing;co-design methodology;mixed hardware/software implementation;co-design flow;programmable FPGA device;CORDIC algorithm;mathematical processor","","1","11","","","","","","IEEE","IEEE Conferences"
"Electrical ground support equipment design for SABER","K. Paskett","Space Dynamics Lab., Utah State Univ., Logan, UT, USA","1997 IEEE Aerospace Conference","","1997","3","","321","331 vol.3","The SDL/USU is building a broadband earthlimb-viewing radiometer for the SABER (Sounding of the Atmosphere using Broadband Emission Radiometry) instrument. The radiometer will be tested and calibrated before launch in a test chamber that closely simulates the in-flight operational environment of the instrument. SDL/USU has designed electrical ground support equipment (GSE) for this testing to optimize the monitor and control functions of the sensor and calibration sources, and to reduce data collection and processing time. This paper presents the electrical GSE hardware and software designs for the SABER instrument. Data collection, processing, and archival methods are given, as well as a description of the real-time displays that show instrument status and performance. Hardware and software used to control the instrument, test chamber, and calibration sources are presented. The integration of these control systems into a simple user interface is also described.","","0-7803-3741","10.1109/AERO.1997.574885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=574885","","Ground support;Instruments;Radiometry;Testing;Calibration;Hardware;Buildings;Atmosphere;Atmospheric modeling;Design optimization","aerospace instrumentation;radiometers;radiometry;aerospace simulation;aerospace test facilities;ground support equipment;calibration;remote sensing;aerospace computing;telescopes;infrared imaging;aerospace testing","electrical ground support equipment design;SABER instrument;atmosphere sounding;broadband emission radiometry;broadband earthlimb-viewing radiometer;in-flight operational environment;radiometer testing;control functions;calibration sources;software designs;hardware designs;data collection;real-time displays;instrument status;instrument performance;simple user interface;control systems integration;TIMED mission;test chamber;command and data handling;GUI;on-orbit environment;full-aperture blackbody;high off-axis rejection telescope;full-field collimator;simulated spacecraft housing","","","3","","","","","","IEEE","IEEE Conferences"
"Generating optimization-based decision support systems","A. Geoffrion; S. Maturana","John E. Anderson Graduate Sch. of Manage., California Univ., Los Angeles, CA, USA; NA","Proceedings of the Twenty-Eighth Annual Hawaii International Conference on System Sciences","","1995","3","","439","448 vol.3","This paper discusses the implementation of optimization based DSSs. An approach is proposed that will enable OR/MS analysts to develop this kind of system much more efficiently than is possible today. The aim is to develop systems with modern GUIs, that interact with DBMSs, and that can be built with little programming effort. Many of the tools required for this approach were developed to support SML, which was chosen to represent the models because it has ample expressive power, completely specified syntax and semantics, total structure/data independence, and lends itself to the surface/deep structure distinction, which makes the approach feasible. The approach is being implemented in a research project funded by the Chilean Government and several firms and is being tested on problems taken from these firms.<<ETX>>","","0-8186-6930","10.1109/HICSS.1995.375626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=375626","","Decision support systems;Engines;Government;System testing;User interfaces;Spread spectrum communication;Power system modeling;Design optimization;Machinery;Graphics","decision support systems;optimisation;graphical user interfaces;software engineering;database management systems;business data processing;software tools","optimization-based decision support systems;GUI;graphical user interfaces;DBMS;programming effort;software tools;SML;completely specified syntax;semantics;total structure/data independence;surface/deep structure distinction","","4","17","","","","","","IEEE","IEEE Conferences"
"A new methodology for concurrent technology development and cell library optimization","M. Chew; S. Saxena; T. F. Cobourn; P. K. Mozumder; A. J. Strojwas","PDF Solutions Inc., San Jose, CA, USA; NA; NA; NA; NA","Proceedings Twelfth International Conference on VLSI Design. (Cat. No.PR00013)","","1999","","","18","24","To minimize the time to market and cost of new sub 0.2 um process technologies and products, PDF Solutions, Inc. has developed a new comprehensive approach based on the use of predictive simulation roots combined with highly efficient experimental design techniques and special test structures. This paper focuses on our approach for concurrent development of new technologies and optimization of cell libraries for these technologies. We present a software system called Circuit Surfer which performs this library optimization in a highly automated fashion and with guaranteed correctness in silicon. We demonstrate several examples of Circuit Surfer applications to cell library design to optimize such objective functions as performance, cell area or yield.","1063-9667","0-7695-0013","10.1109/ICVD.1999.745118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=745118","","Software libraries;Time to market;Costs;Predictive models;Circuit simulation;Design for experiments;Circuit testing;Software systems;Silicon;Application software","concurrent engineering;circuit optimisation;circuit simulation;integrated circuit yield;design for manufacture;integrated circuit design","concurrent technology development;cell library optimization;PDF Solutions;predictive simulation roots;test structures;Circuit Surfer;library optimization;cell area;yield","","","14","","","","","","IEEE","IEEE Conferences"
"Portable avionics test suite design and operation","S. M. McGovern; K. F. Chin","NA; NA","Digital Avionics Systems Conference, 2003. DASC '03. The 22nd","","2003","1","","4.C.7","4.1-7 vol.1","Optimization and acceptance testing of an airport surface surveillance system requires a comparison be performed between the data collected by the system and the truth data (known, precise data provided by the vehicle being tracked by the sensor). Truth data systems can be costly and difficult to transport. The Volpe Center was tasked with rapidly designing and developing a portable suite of equipment to simulate a taxiing, takeoff roll or landing rollout aircraft and provide truth data. The design concept for the portable airport surveillance verification system (PASVS) required that the unit be portable, easily deployed in a government or rental vehicle, and intuitive to operate. A modular design was selected for the PASVS consisting primarily of an integrated aircraft VHF radio, a differential global positioning system receiver, two aviation transponders (an air traffic control radar beacon system-only transponder and a mode S transponder) and a computer with data logging software. The PASVS has shown to be a capable platform for simulating a taxiing, takeoff or landing rollout aircraft and for providing ground truth data to evaluate airport surface surveillance systems. PASVS also demonstrated its ability to be easily and quickly shipped to various continental U.S. and overseas locations and be rapidly installed and easily operated from any ground vehicle by a single operator with minimal training. In this paper, the requirements and design processes are described and PASVS performance data is presented along with collected sample truth data.","","0-7803-7844","10.1109/DASC.2003.1245842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5731092","","","Global Positioning System;VHF radio propagation;air traffic control;aircraft computers;airports;data loggers;optimisation;portable instruments;transponders","Volpe Center;acceptance testing;air traffic control radar;airport surface surveillance system;aviation transponders;data logging software;differential global positioning system receiver;integrated aircraft VHF radio;landing rollout aircraft;mode S transponder;modular design;optimization;portable airport surveillance verification system;portable avionics test suite;portable suite;radar beacon system-only transponder;takeoff roll;taxiing simulation;truth data systems","","","8","","","","","","IEEE","IEEE Conferences"
"A set of radiating structures for a european antenna software benchmarking","R. Gillard; G. A. E. Vandenbosch","NA; NA","34th European Microwave Conference, 2004.","","2004","1","","417","420","This paper will present some preliminary results from the softwarr activity of ACE (Antenna Centre of Excellence), the FP6 Network of Excellence dedicated to the restructuring of European antenna R&D. The paper foeuses on the benchmarking task that has been initiated to facilitate the assessment of antenna software.","","1-58053-992","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1412616","","Software tools;Benchmark testing;Research and development;Application software;Software testing;Design optimization;Antenna theory;Heart;Collaborative software;Documentation","","","","1","2","","","","","","IEEE","IEEE Conferences"
"Resource-aware scientific computation on a heterogeneous cluster","J. D. Teresco; J. Fair; J. E. Flaherty","Dept. of Comput. Sci., Williams Coll., Williamstown, MA, USA; NA; NA","Computing in Science & Engineering","","2005","7","2","40","50","Although researchers can develop software on small, local clusters and move it later to larger clusters and supercomputers, the software must run efficiently in both environments. Two efforts aim to improve the efficiency of scientific computation on clusters through resource-aware dynamic load balancing. The popularity of cost-effective clusters built from commodity hardware has opened up a new platform for the execution of software originally designed for tightly coupled supercomputers. Because these clusters can be built to include any number of processors ranging from fewer than 10 to thousands, researchers in high-performance scientific computation at smaller institutions or in smaller departments can maintain local parallel computing resources to support software development and testing, then move the software to larger clusters and supercomputers. As promising as this ability is, it has also led to the need for local expertise and resources to set up and maintain these clusters. The software must execute efficiently both on smaller local clusters and on larger ones. These computing environments vary in the number of processors, speed of processing and communication resources, and size and speed of memory throughout the memory hierarchy as well as in the availability of support tools and preferred programming paradigms. Software developed and optimized using a particular computing environment might not be as efficient when it's moved to another one. In this article, we describe a small cluster along with two efforts to improve the efficiency of parallel scientific computation on that cluster. Both approaches modify the dynamic load-balancing step of an adaptive solution procedure to tailor the distribution of data across the cooperating processes. This modification helps account for the heterogeneity and hierarchy in various computing environments.","1521-9615;1558-366X","","10.1109/MCSE.2005.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401801","cluster computing;grid computing;resource-aware computing;dynamic load balancing;heterogeneous clusters","Supercomputers;Concurrent computing;Load management;Hardware;Software design;Parallel processing;Programming;Software testing;Software maintenance;Availability","workstation clusters;resource allocation;software engineering;program testing;distributed processing","resource-aware scientific computation;heterogeneous cluster;supercomputers;resource-aware dynamic load balancing;parallel computing;software development;software testing","","13","22","","","","","","IEEE","IEEE Journals & Magazines"
"Software Process Instantiation And The Planning Paradigm","K. E. Huff","GTE Laboratories","[1989] Proceedings of the 5th International Software Process Workshop","","1989","","","71","73","","","0-8186-2104","10.1109/ISPW.1989.690422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=690422","","Process planning;Programming profession;Artificial intelligence;Laboratories;Control systems;Steady-state;Debugging;Optimization methods;System testing;Program processors","","","","3","3","","","","","","IEEE","IEEE Conferences"
"BestFit, distribution fitting software by Palisade Corporation","L. Jankauskas; S. McLafferty","Palisade Corp., Newfield, NY, USA; Palisade Corp., Newfield, NY, USA","Winter Simulation Conference Proceedings, 1995.","","1995","","","457","461","BestFit is distribution-fitting software for Microsoft Windows that finds the statistical distribution function that best fits a data set. BestFit provides a flexible, easy-to-use environment for analysis that allows users to focus on their data, not their software. Users of simulation software can use distributions produced by BestFit to define uncertainty in their simulation models. BestFit helps you find the best representation of randomness in your model, making your simulation results more accurate. Users of RISK, ProModel or any other simulation software will find BestFit an invaluable tool for defining uncertainty.","","0-78033018","10.1109/WSC.1995.478774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=478774","","Statistical distributions;Uncertainty;Software tools;Distribution functions;Optimization methods;Impedance matching;Tail;Displays;Testing;Probability distribution","graphical user interfaces;statistical analysis;digital simulation;data analysis;maximum likelihood estimation","BestFit;distribution fitting software;Palisade Corporation;Microsoft Windows;statistical distribution function;simulation software;uncertainty;randomness;RISK;ProModel;maximum likelihood estimation","","1","7","","","","","","IEEE","IEEE Conferences"
"Modeling Software Driven Power Consumption","G. Cameron","ATI Technologies Inc., 33 Commerce Valley Drive East, Markham, Ontario, Canada. Phone: 905 8822600 x3826, Fax: 905 8822567, E- mail: gcameron@ati.com, gfcameron@gmail.com.","2005 IEEE Instrumentationand Measurement Technology Conference Proceedings","","2005","3","","2082","2087","Power management issues are a major factor that threatens to curtail the progress of Moore's law in the 21st century (Borkar and Malone, 2004). Techniques to calculate and optimize the power consumption of hardware components are well known and understood, yet there exists no formal technique for modeling the effect of running software on the power dissipation for an arbitrary system. System designers have been forced to over design systems based on worst case estimates, or make design changes based on empirical measurements after the hardware and software is complete. Since there exists no formal method for modeling how software affects power dissipation, there is no way to optimize the software to minimize power consumption. This paper demonstrates a formal approach towards establishing a power consumption model for any processor running on an arbitrary target. Power consumption models are developed and tested for three DSPs with radically different internal architecture through the use of modeling, measurement, and statistical techniques","1091-5281","0-7803-8879","10.1109/IMTC.2005.1604540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604540","software;power;measurement;modeling","Energy consumption;Hardware;Power system modeling;Power dissipation;Energy management;Moore's Law;Force measurement;Software measurement;Optimization methods;Testing","computerised instrumentation;digital signal processing chips;hardware-software codesign;low-power electronics;statistical analysis","software driven power consumption;power management;Moore law;hardware components;power dissipation;formal method;power consumption model;DSP","","3","7","","","","","","IEEE","IEEE Conferences"
"A reflective practice of automated and manual code reviews for a studio project","Jun-Suk Oh; Ho-Jin Choi","KT Operations Support Syst. Lab, Inf. & Commun. Univ., Deajeon, South Korea; NA","Fourth Annual ACIS International Conference on Computer and Information Science (ICIS'05)","","2005","","","37","42","In this paper, the target of code review is project management system (PMS), developed by a studio project in a software engineering master's program, and the focus is on finding defects not only in view of development standards, i.e., design rule and naming rule, but also in view of quality attributes of PMS, i.e., performance and security. From the review results, a few lessons are learned. First, defects which had not been found in the test stage of PMS development could be detected in this code review. These are hidden defects that affect system quality and that are difficult to find in the test. If the defects found in this code review had been fixed before the test stage of PMS development, productivity and quality enhancement of the project would have been improved. Second, manual review takes much longer than an automated one. In this code review, general check items were checked by automation tool, while project-specific ones were checked by manual method. If project-specific check items could also be checked by automation tool, code review and verification work after fixing the defects would be conducted very efficiently. Reflecting on this idea, an evolution model of code review is studied, which eventually seeks fully automated review as an optimized code review.","","0-7695-2296","10.1109/ICIS.2005.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515372","","Inspection;Software engineering;Security;Testing;Automation;Project management;Code standards;Software standards;Standards development;Communication standards","project management;software development management;program verification;software quality;software standards;software performance evaluation;computer science education","automated code review;manual code review;studio project;project management system;software engineering;development standard;design rule;naming rule;system quality;PMS development;PMS productivity;quality enhancement;automation tool;project-specific check item;verification work","","3","7","","","","","","IEEE","IEEE Conferences"
"Tradeoffs in synthetic instrument design and application","J. Fleagle","Syst. & Electron. Inc., St. Louis, MO, USA","IEEE Autotestcon, 2005.","","2005","","","166","170","There are efforts underway within industry to develop synthetic instrumentation (SI) as a complement to traditional monolithic instrumentation designs. This provides flexibility in the selection of SI components to match the performance level required and to also overcome obsolescence issues. The synthetic instrument concept is attractive because signal processing and generation can be handled by platform independent software routines. The capability of an instrument can be changed or extended by simply down loading a new program or upgrading a driver. Common programming standards such as IVI can simplify transitioning to replacement hardware and software when obsolescence becomes an issue. With the explosion in personal computer development, processing power has become an inexpensive commodity. A/D and D/A converters are becoming commodities as well, with increasing speed and resolution every year. Signal conditioning remains a custom element as it usually must be tailored for specific applications. Leveraging COTS hardware and software can reduce cost and shorten system development times. As with any new concept, application of synthetic instrument raises issues that must be considered. Synthetic instruments impact overall test system architecture. The impact depends on the complexity of synthetic instrument function and where the signal processing functionality (software) is located. Emulation of legacy instrumentation is another issue. Since the SI measurement processes used are typically different from the legacy instrument, the synthetic instrument may not perfectly emulate the legacy device. For military test systems where longevity and user maintainability are prime requirements, the logistics tail must be considered. Development cycles for military test equipment are long by commercial standards. The world of COTS changes rapidly and it is not uncommon for COTS technology to be obsolete before the system is fielded. Another concern is environmental standards. Military systems typically have stringent requirements for shock, vibration, temperature, humidity, EMI/EMC, and other environmental requirements that commercial systems are not required to meet. This paper examines some of these issues related to synthetic instrumentation. As always with engineering, there is no perfect answer, there are only tradeoffs to be optimized","1088-7725;1558-4550","0-7803-9101","10.1109/AUTEST.2005.1609121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609121","","Instruments;Signal processing;Hardware;Application software;System testing;Military standards;Signal generators;Software standards;Explosions;Microcomputers","automatic test equipment;military equipment;signal processing","synthetic instrument design;synthetic instrumentation;monolithic instrumentation design;programming standards;A/D converters;D/A converters;signal conditioning;COTS hardware;COTS software;signal processing functionality;SI measurement processes;legacy instrument;military test systems;military test equipment;environmental standards","","5","","","","","","","IEEE","IEEE Conferences"
"Design optimization of time- and cost-constrained fault-tolerant distributed embedded systems","V. Izosimov; P. Pop; P. Eles; Z. Peng","Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden","Design, Automation and Test in Europe","","2005","","","864","869 Vol. 2","In this paper we present an approach to the design optimization of fault tolerant embedded systems for safety-critical applications. Processes are statically scheduled and communications are performed using the time-triggered protocol. We use process re-execution and replication for tolerating transient faults. Our design optimization approach decides the mapping of processes to processors and the assignment of fault-tolerant policies to processes such that transient faults are tolerated and the timing constraints of the application are satisfied. We present several heuristics which are able to find fault-tolerant implementations given a limited amount of resources. The developed algorithms are evaluated using extensive experiments, including a real-life example.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395691","","Design optimization;Fault tolerant systems;Embedded system;Fault detection;Time division multiple access;Software architecture;Protocols;Communication system control;Real time systems;Job shop scheduling","fault tolerance;embedded systems;distributed algorithms;scheduling;optimisation;safety-critical software;protocols","design optimization;time constraints;cost constraints;fault-tolerant distributed embedded systems;safety-critical applications;static process scheduling;time-triggered protocol;process re-execution;process replication;transient faults;process mapping;fault-tolerant policy assignment","","51","21","","","","","","IEEE","IEEE Conferences"
"Feedback-directed switch-case statement optimization","Peng Zhao; J. N. Amaral","Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada; Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada","2005 International Conference on Parallel Processing Workshops (ICPPW'05)","","2005","","","295","302","This paper presents two new feedback-guided techniques to generate code for switch-case statements: hot default case promotion (DP) and switch-case statement partitioning (SP). DP improves case dispatch while SP simplifies case dispatch, improves instruction layout and enables further inlining. An extensive experimental study reveals up to 4.9% performance variations among different strategies. The largest performance improvement of DP and SP over existing O3 optimization in the open research compiler (ORC) is 1.7%. A micro-architecture level performance study provides insights on the basis for this performance improvement.","0190-3918;2332-5690","0-7695-2381","10.1109/ICPPW.2005.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488708","","Testing;Switches;Optimizing compilers;Feedback;Frequency;Runtime;Microarchitecture;Virtual machining;Indexing;Electric breakdown","optimising compilers;software architecture;feedback","feedback guided techniques;switch-case statement optimization;code generation;hot default case promotion;switch-case statement partitioning;instruction layout;O3 optimization;open research compiler;optimizing compilers;software architecture;feedback-directed optimization","","1","14","","","","","","IEEE","IEEE Conferences"
"Efficient implementations of mobile video computations on domain-specific reconfigurable arrays","S. Khawam; S. Baloch; A. Pai; I. Ahmed; N. Aydin; T. Arslan; F. Westall","School of Electronics and Engineering, University of Edinburgh, Edinburgh, UK; NA; NA; NA; NA; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","2","","1","6","Mobile video processing as defined in standards like MPEG-4 and H.263 contains a number of timeconsuming computations that cannot be efficiently executed on current hardware architectures. The authors recently introduced a reconfigurable SoC platform that permits a low-power, high-throughput and flexible implementation of the motion estimation and DCT algorithms. The computations are done using domainspecific reconfigurable arrays that have demonstrated up to 75% reduction in power consumption when compared to generic FPGA architecture, which makes them suitable for portable devices. This paper presents and compares different configurations of the arrays to efficiently implementing DCT and motion estimation algorithms. A number of algorithms are mapped into the various reconfigurable fabrics demonstrating the flexibility of the new reconfigurable SoC architecture and its ability to support a number of implementations having different performance characteristics.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269064","","Mobile computing;Discrete cosine transforms;Signal processing algorithms;Energy consumption;Field programmable gate arrays;Computer architecture;Motion estimation;Portable computers;MPEG 4 Standard;Hardware","formal verification;fault diagnosis;mixed analogue-digital integrated circuits;analogue-digital conversion;high level synthesis;C language;hardware description languages;low-power electronics;automatic test pattern generation;pipeline processing;memory architecture;smart cards;system-on-chip;digital simulation;nanotechnology;integrated circuit testing;integrated circuit reliability;microprocessor chips","architectural-level power management;formal verification;functional information;structural information;diagnosis constrained testing;mixed-signal circuits;communication-centric optimisations;source-level optimisations;high-level synthesis;SystemC;SystemVerilog;low power systems;TPG;memory hierarchies;high security smartcards;low-power design;SAT;analogue test;high-frequency test;energy efficiency;memory usage;interconnect technology scaling;system level design;system level modelling;system level analysis;SoC testing;analogue system performance modelling;circuit-level performance modelling;scheduling;reconfigurable computing;power aware design;digital systems simulation;on-line testing;reliability;nanometer technologies;parasitic-aware analogue design;hardware-software system design;architecture exploration;low-power logic","","5","20","","","","","","IEEE","IEEE Conferences"
"Coordinate optimization for bi-convex matrix inequalities","J. W. Helton; O. Merino","Dept. of Math., California Univ., San Diego, La Jolla, CA, USA; NA","Proceedings of the 36th IEEE Conference on Decision and Control","","1997","4","","3609","3613 vol.4","We consider optimization of the largest eigenvalue of a smooth selfadjoint matrix valued function /spl Gamma/(X, Y) of two vector or matrix variables X and Y. We assume that /spl Gamma/ is concave or convex in Y and separately in X, but possibly has bad joint behavior. A typical problem one faces in control design are matrix versions of minimizing in Y and maximizing in X. Also minimizing in X and Y is an important problem. When joint behavior in X and Y is bad existing commercial software must be applied to each coordinate separately, and so can be used only to give a coordinate optimization algorithm. We give strong evidence in this article that on ""well behaved /spl Gamma/"" coordinate optimization always gives a local optimum for the min/sub Y/ max/sub X/ problem and that it almost never gives a local solution to the min/sub Y/ min/sub X/ problem.","0191-2216","0-7803-4187","10.1109/CDC.1997.652414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=652414","","Linear matrix inequalities;Minimax techniques;Eigenvalues and eigenfunctions;Mathematics;Software algorithms;Terminology;Level set;Testing","optimisation;matrix algebra;vectors","coordinate optimization;bi-convex matrix inequalities;largest eigenvalue;smooth selfadjoint matrix valued function;control design;local optimum","","7","7","","","","","","IEEE","IEEE Conferences"
"Code optimization for code compression","M. Drinic; D. Kirovski; H. Vo","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; NA; NA","International Symposium on Code Generation and Optimization, 2003. CGO 2003.","","2003","","","315","324","With the emergence of software delivery platforms such as Microsoft's .NET, the reduced size of transmitted binaries has become a very important system parameter, strongly affecting system performance. We present two novel pre-processing steps for code compression that explore program binaries' syntax and semantics to achieve superior compression ratios. The first preprocessing step involves heuristic partitioning of a program binary into streams with high auto-correlation. The second preprocessing step uses code optimization via instruction rescheduling in order to improve prediction probabilities for a given compression engine. We have developed three heuristics for instruction rescheduling that explore tradeoffs of the solution quality versus algorithm run-time. The pre-processing steps are integrated with the generic paradigm of prediction by partial matching (PPM) which is the basis of our compression codec. The compression algorithm is implemented for x86 binaries and tested on several large Microsoft applications. Binaries compressed using our compression codec are 18-24% smaller than those compressed using the best available off-the-shelf compressor.","","0-7695-1913","10.1109/CGO.2003.1191555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191555","","Compression algorithms;Codecs;Application software;Bandwidth;Computer architecture;Computer science;Software performance;System performance;Autocorrelation;Engines","optimising compilers;codecs;network operating systems;data compression","code optimization;code compression;software delivery platforms;Microsoft NET;system parameter;system performance;pre-processing steps;program binaries;compression ratios;preprocessing step;heuristic partitioning;high auto-correlation;instruction rescheduling;prediction probabilities;compression engine;prediction by partial matching;PPM;compression codec;compression algorithm;large Microsoft applications;off-the-shelf compressor","","1","29","","","","","","IEEE","IEEE Conferences"
"Hardware/software co-design of a fuzzy RISC processor","V. Salapura; M. Gschwind","Tech. Univ. Wien, Austria; NA","Proceedings Design, Automation and Test in Europe","","1998","","","875","882","In this paper, we show how hardware/software co-evaluation can be applied to instruction set definition. As a case study, we show the definition and evaluation of instruction set extensions for fuzzy processing. These instructions are based on the use of subword parallelism to fully exploit the processor's resources by processing multiple data streams in parallel. The proposed instructions are evaluated in software and hardware to gain a balanced view of the costs and benefits of each instruction. We have found that a simple instruction optimized to perform fuzzy rule evaluation offers the most benefit to improve fuzzy processing performance. The instruction set extensions are added to a RISC processor core based on the MIPS instruction set architecture. The core has been described in VHDL so that hardware implementations can be generated using logic synthesis.","","0-8186-8359","10.1109/DATE.1998.655961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655961","","Hardware;Reduced instruction set computing;Fuzzy sets;Instruction sets;Fuzzy logic;Clocks;Computer architecture;Costs;Performance analysis;Software metrics","fuzzy logic;reduced instruction set computing;instruction sets;high level synthesis;microprocessor chips;circuit CAD;CMOS digital integrated circuits","hardware/software co-design;fuzzy RISC processor;instruction set definition;subword parallelism;fuzzy rule evaluation;fuzzy processing performance;MIPS instruction set architecture;VHDL core description;logic synthesis;CMOS processor chip;CAD","","8","18","","","","","","IEEE","IEEE Conferences"
"Software documentation: how much is enough?","L. C. Briand","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","Seventh European Conference onSoftware Maintenance and Reengineering, 2003. Proceedings.","","2003","","","13","15","It is a well-known fact that software documentation is, in practice, poor and incomplete. Though specification, design, and test documents-among other things-are required by standards and capability maturity models (e.g., SEI CMM), such documentation does not exist in a complete and consistent form in most organizations. When documents are produced, they tend to follow no defined standard and lack information that is crucial to make them understandable and usable by developers and maintainers. Then a fundamental practical question, which motivated this keynote address, is to better understand what type of documentation is required, what is needed to support its completeness and consistency, and what is the level of precision required for each type of document. These questions cannot be investigated at that level of generality though. Answers are likely to be very context-dependent if they are to be precise. We focus our attention here on object-oriented development and the Unified Modeling Language (UML).","1534-5351","0-7695-1902","10.1109/CSMR.2003.1192406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192406","","Documentation;Software maintenance;Unified modeling language;Text analysis;Design optimization;Object oriented modeling;Software quality;Laboratories;Systems engineering and theory;Drives","system documentation;formal specification;object-oriented programming;specification languages","software documentation;formal specification;capability maturity models;standards;object-oriented development;Unified Modeling Language;UML","","13","8","","","","","","IEEE","IEEE Conferences"
"The challenges of hardware synthesis from C-like languages","S. A. Edwards","Dept. of Comput. Sci., Columbia Univ., New York, NY, USA","Design, Automation and Test in Europe","","2005","","","66","67 Vol. 1","Many techniques for synthesizing digital hardware from C-like languages have been proposed, but none have emerged as successful as Verilog or VHDL for register-transfer-level design. Familiarity is the main reason C-like languages have been proposed for hardware synthesis. Synthesize hardware from C, proponents claim, and a C programmer can be turned into a hardware designer. Another common motivation is hardware/software codesign: today's systems usually contain a mix of hardware and software, and it is often unclear initially which portions to implement in hardware. Here, using a single language should simplify the migration task. The paper surveys several C-like hardware synthesis languages and looks at two of the fundamental challenges, concurrency and timing control.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395531","","Concurrent computing;Hardware design languages;Circuit synthesis;Timing;Control system synthesis;Computer science;Programming profession;Software systems;Optimizing compilers;Performance analysis","C language;hardware description languages;hardware-software codesign;network synthesis","digital hardware synthesis;C-like languages;Verilog;VHDL;register-transfer-level design;concurrency;timing control;C programmer;hardware designer;hardware/software codesign","","22","26","","","","","","IEEE","IEEE Conferences"
"Research of novel electromagnetic catapults with many kinds of uses","Li Liyi; Hu Yusheng; Li Xiaopeng","Teaching & Res. Sect. of Electr. Motor, Harbin Inst. of Technol., China; Teaching & Res. Sect. of Electr. Motor, Harbin Inst. of Technol., China; Teaching & Res. Sect. of Electr. Motor, Harbin Inst. of Technol., China","2004 12th Symposium on Electromagnetic Launch Technology","","2004","","","473","476","In this paper, we firstly describe the wide application of the catapults, and develop a novel electromagnetic catapult that is made up of linear brushless DC motor, describing its basic design process, analyzing various kinds of parameters of the novel linear motor, and finally using finite element software to simulate and optimize the catapult. In addition, we also develop the test prototype machine whose length is one meter, and it can make velocity of 6.75 kilograms projectile reach 10 meters/second. By comparing the simulation with the experimental data, the result is satisfactory. Thus we verify that the novel catapult is feasible further. At last, we present the key technologies and the corresponding solution of practical application. Thus the applied foundation has been made through the above analysis.","","0-7803-8290","10.1109/ELT.2004.1398126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1398126","","Brushless DC motors;Application software;Process design;Electromagnetic analysis;DC motors;Brushless motors;Finite element methods;Analytical models;Design optimization;Testing","electromagnetic launchers;linear motors;brushless DC motors;finite element analysis;optimisation;projectiles","electromagnetic catapults;linear brushless DC motor;finite element software;optimization;projectile;electromagnetic launch","","3","5","","","","","","IEEE","IEEE Conferences"
"PRISC software acceleration techniques","R. Razdan; K. Brace; M. D. Smith","Digital Equipment Corp., Hudson, MA, USA; Digital Equipment Corp., Hudson, MA, USA; NA","Proceedings 1994 IEEE International Conference on Computer Design: VLSI in Computers and Processors","","1994","","","145","149","Programmable reduced instruction set computers (PRISC) are a new class of computers which can offer a programmable functional unit (PFU) in the context of a RISC datapath. PRISC create application-specific instructions to accelerate the performance for a particular application. Our previous work has demonstrated that peephole optimizations in a compiler can utilize PFU resources to accelerate the performance of general purpose programs. However these compiler optimizations are limited by the structure of the input source code. This work generalizes on our previous work, and demonstrates that the performance of general abstract data types such as short-set vectors, hash tables, and finite state machines is significantly accelerated (250%-500%) by using PFU resources. Thus, a wide variety of end-user applications can be specifically designed to use PFU resources to accelerate performance. Results from applications in the domain of computer-aided design (CAD) are presented to demonstrate the usefulness of our techniques.<<ETX>>","","0-8186-6565","10.1109/ICCD.1994.331875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=331875","","Acceleration;Automata;VLIW;Clocks;Prototypes;Reduced instruction set computing;Testing","reduced instruction set computing;instruction sets;performance evaluation;optimisation;program compilers;abstract data types;finite state machines;CAD","software acceleration techniques;PRISC;programmable reduced instruction set computers;programmable functional unit;RISC datapath;application-specific instructions;performance acceleration;peephole optimizations;compiler;general purpose programs;compiler optimizations;general abstract data types;short-set vectors;hash tables;finite state machines;end-user applications;computer-aided design;CAD","","16","21","","","","","","IEEE","IEEE Conferences"
"Neural engineering utility with adaptive algorithms","L. V. Kirkland; J. D. Wiederholt","TISA US Air Force Base, Hill AFB, UT, USA; TISA US Air Force Base, Hill AFB, UT, USA","Proceedings of AUTOTESTCON '94","","1994","","","709","716","The neural engineering utility with adaptive algorithms (NEUWAA) is a machine-based intelligence system for automatic test equipment, which integrates various technologies in an adaptive fault-detection environment. Computer enhancements and mathematical algorithms allow for the use of man-machine and intelligent applications. The human/machine interface optimizes the test environment by providing state-of-the-art adaptive algorithms to streamline test sequences and diagnostics. NEUWAA employs state-of-the-art diagnostics methodologies coupled with self-organizing evolution to provide an efficient test environment at any level. Highlighted visual images with dialogue of the unit under test provide interactive fault-isolation including guided-probe sequences to streamline fault/diagnosis. The system is completely interoperable with all other standard software packages. This paper outlines the neural engineering utility with adaptive algorithms system including its various characteristics and the techniques involved in its creation.<<ETX>>","","0-7803-1910","10.1109/AUTEST.1994.381547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381547","","Neural engineering;Adaptive algorithm;Testing;Machine intelligence;Streaming media;Intelligent systems;Automatic test equipment;Man machine systems;Application software;Humans","adaptive systems;automatic test equipment;neural nets;fault location;virtual reality;adaptive control;fault diagnosis;user interfaces;knowledge based systems","adaptive algorithms;neural engineering;machine-based intelligence system;machine-based intelligence;adaptive fault-detection environment;mathematical algorithms;intelligent applications;human/machine interface;state-of-the-art adaptive algorithms;test sequences;self-organizing evolution;visual images;unit under test;interactive fault-isolation;guided-probe sequences;fault/diagnosis;standard software packages","","","6","","","","","","IEEE","IEEE Conferences"
"Embedded systems verification with FGPA-enhanced in-circuit emulator","M. Meerwein; C. Baumgartner; T. Wieja; W. Glauert","Robert Bosch GmbH, Stuttgart, Germany; NA; NA; NA","Proceedings 13th International Symposium on System Synthesis","","2000","","","143","148","We present a novel coverification concept for embedded microcontrollers that satisfies industrial requirements. Based on a commercially available CPU in-circuit emulator coupled with FPGA boards, it verifies the correctness of an implementation in terms of function and timing within a real-world environment. Using our system, the software engineer can write, test and optimize programs for a chip that is not yet physically existent. In addition the system is used to obtain software module characterization data required for system partitioning. Its ability to integrate analog circuitry enables verification of the complete system-on-chip. Our methodology is fully integrated into the ASIC design flow providing ease of use and a high level of verification accuracy.","1080-1820","0-7695-0765","10.1109/ISSS.2000.874041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874041","","Embedded system;Microcontrollers;Field programmable gate arrays;Timing;Software systems;Systems engineering and theory;Circuit testing;Software testing;System testing;System-on-a-chip","embedded systems;formal verification;field programmable gate arrays;hardware-software codesign;microcontrollers;timing;application specific integrated circuits","embedded systems verification;FGPA;in-circuit emulator;coverification;embedded microcontrollers;CPU;timing;software engineer;program testing;program optimization;software module characterization;system partitioning;system-on-chip;ASIC design","","5","10","","","","","","IEEE","IEEE Conferences"
"Genetic algorithm optimisation of distributed database queries","M. Gregory","Central Queensland Univ., Qld., Australia","1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360)","","1998","","","271","276","Distributed relational database query optimisation is a combinatorial optimisation problem. This paper reports on an initial investigation into the potential for a genetic algorithm (GA) to optimise distributed queries. A genetic algorithm is developed and its performance compared with alternative stochastic optimisation techniques: random search, multistart and simulated annealing. The problem of fully reducing all tables in a tree query is used to compare the techniques. For this problem, evaluating the fitness function is an expensive operation. The proposed GA uses a tree-structured data model with tailored crossover and mutation operators that avoid the need to fully re-evaluate the fitness function for new solutions. Query optimisation is a task that must be performed in real-time. A technique is required that performs well at the start of a search, but avoids the problem of premature convergence. The proposed GA uses a local search phase to deliver the required real-time performance. Experiments show that the proposed GA can perform better than the alternative techniques tested. The potential for a GA to deliver valuable distributed query processing cost reductions is demonstrated.","","0-7803-4869","10.1109/ICEC.1998.699724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=699724","","Genetic algorithms;Distributed databases;Query processing;Relational databases;Stochastic processes;Simulated annealing;Data models;Genetic mutations;Performance evaluation;Testing","distributed databases;relational databases;query processing;simulated annealing;convergence;real-time systems;software performance evaluation;tree data structures;mathematical operators","distributed relational database query optimisation;combinatorial optimisation;genetic algorithm;algorithm performance;stochastic optimisation techniques;random search;multistart;simulated annealing;table reduction;tree query;fitness function evaluation;tree-structured data model;tailored crossover operator;tailored mutation operator;real-time query optimisation;premature convergence;local search phase;cost reduction","","2","9","","","","","","IEEE","IEEE Conferences"
"Using branch handling hardware to support profile-driven optimization","T. M. Conte; B. A. Patel; J. S. Cox","Dept. of Electr. & Comput. Eng., South Carolina Univ., Columbia, SC, USA; Dept. of Electr. & Comput. Eng., South Carolina Univ., Columbia, SC, USA; NA","Proceedings of MICRO-27. The 27th Annual IEEE/ACM International Symposium on Microarchitecture","","1994","","","12","21","Profile-based optimizations can be used for instruction scheduling, loop scheduling, data preloading, function in-lining, and instruction cache performance enhancement. However, these techniques have not been embraced by software vendors because programs instrumented for profiling run 2-30 times slower, an awkward compile-run-recompile sequence is required, and a test input suite must be collected and validated for each program. This paper proposes using existing branch handling hardware to generate profile information in real time. Techniques are presented for both one-level and two-level branch hardware organizations. The approach produces high accuracy with small slowdown in execution (0.4%-4.6%). This allows a program to be profiled while it is used, eliminating the need for a test input suite. This practically removes the inconvenience of profiling. With contemporary processors driven increasingly by compiler support, hardware-based profiling is important for high-performance systems.","1072-4451","0-89791-707","10.1145/192724.192726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=717402","","Hardware;Testing;Permission;Optimizing compilers;Processor scheduling;Optimization methods;Probes;Application software;Databases;Instruments","scheduling;program compilers;optimisation","branch handling hardware;profile-driven optimization;instruction scheduling;loop scheduling;data preloading;function in-lining;instruction cache performance enhancement;software vendors;compile-run-recompile sequence;high-performance systems","","14","19","","","","","","IEEE","IEEE Conferences"
"AutoFOCUS and the MoDe tool","J. Romberg; J. Jurjens; G. Wimmel; O. Slotosch; G. Hahn","Technische Univ. Munchen, Garching, Germany; Technische Univ. Munchen, Garching, Germany; Technische Univ. Munchen, Garching, Germany; NA; NA","Third International Conference on Application of Concurrency to System Design, 2003. Proceedings.","","2003","","","249","250","Software engineering for distributed automotive applications is shifting from a subsystem-level perspective, where the focus is on optimization of a single electronic control unit, towards a system-level view. However, optimization of distributed systems with respect to non-functional properties remains a challenging task. The goal of the MoDe (model based deployment) approach is to give early guidance for design decisions using architectural-level models of the system. In its current version, MoDe supports those architecture-level decisions that require a performance model of the overall system. The MoDe approach is based on a formal design notation, AutoFocus, which is used for specifying system models, functional models enriched with abstractions for communication and scheduling. The MoDe tool offers automated support for compiling platform abstractions into the system model, so MoDe allows a highly flexible evaluation of different architectural choices.","","0-7695-1887","10.1109/CSD.2003.1207727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207727","","Software engineering;Control systems;Yarn;Network synthesis;Automotive applications;Unified modeling language;Embedded system;Testing;Application software;Computer architecture","software tools;automotive engineering;distributed programming;systems analysis;formal specification;real-time systems","AutoFOCUS tool;MoDe tool;model based deployment approach;architectural level model;formal design notation;system design;software engineering;distributed automotive applications;subsystem-level perspecctive;distributed systems optimization","","","3","","","","","","IEEE","IEEE Conferences"
"Precise call graph construction in the presence of function pointers","A. Milanova; A. Rountev; B. G. Ryder","Dept. of Comput. Sci., Rutgers Univ., USA; Dept. of Comput. Sci., Rutgers Univ., USA; Dept. of Comput. Sci., Rutgers Univ., USA","Proceedings. Second IEEE International Workshop on Source Code Analysis and Manipulation","","2002","","","155","162","The use of pointers presents serious problems for software productivity tools for software understanding, restructuring, and testing. Pointers enable indirect memory accesses through pointer dereferences, as well as indirect procedure calls (e.g., through function pointers in C). Such indirect-accesses and calls can be disambiguated with pointer analysis. In this paper we evaluate the precision of a pointer analysis by Zhang et al. (1996, 1998) for the purposes of call graph construction for C programs with function pointers. The analysis is implemented in the context of a production-strength code-browsing tool from Siemens Corporate Research. The analysis uses an inexpensive, almost-linear flow- and context-insensitive algorithm. To measure analysis precision, we compare the call graph computed by the analysis with the most precise call graph obtainable by a large category of pointer analyses. Surprisingly, for all our data programs the analysis of Zhang et al. achieves the best possible precision. This result indicates that for the purposes of call graph construction, even inexpensive analyses can provide very good precision, and therefore the use of more expensive analyses may not be justified.","","0-7695-1793","10.1109/SCAM.2002.1134115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1134115","","Software tools;Productivity;Software testing;Information analysis;Computer science;Algorithm design and analysis;Data analysis;Optimizing compilers;Software engineering;Costs","reverse engineering;program testing;system monitoring","precise call graph construction;function pointers;software productivity tools;software understanding;software restructuring;software testing;indirect memory accesses;pointer dereferences;indirect procedure calls;C programs;production strength code browsing tool;context-insensitive algorithm;flow-insensitive algorithm;analysis precision","","3","20","","","","","","IEEE","IEEE Conferences"
"Optimized model embedding invisible robust watermark","Xiaoqiang Li; Xiangyang Xue","Dept. of Comput. Sci. & Eng., Fudan Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Fudan Univ., Shanghai, China","Proceedings 7th International Conference on Signal Processing, 2004. Proceedings. ICSP '04. 2004.","","2004","3","","2290","2293 vol.3","To set the proper strength of the embedded watermark signal is a critical problem for obtaining a robust and transparent watermark in image watermarking. In this paper, to solve this problem, a new simple scheme on how to optimally embed a watermark with fidelity and robustness constrains is proposed. Firstly, human visual system (HVS) model is used to get masking function by calculating the just noticeable distortion (JND) mask of image directly in the spatial domain. Then, based on adaptive iterative algorithm, the strength of the embedded watermark in discrete cosine transform (DCT) domain attains to maximum little by little under the rule of masking function. The new scheme has been tested with attack software StirMark 4.0. Compared with the conventional scheme, simulation results show that more robust watermark can be attained while retaining high image quality.","","0-7803-8406","10.1109/ICOSP.2004.1442237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1442237","","Robustness;Watermarking;Humans;Visual system;Discrete cosine transforms;Cryptography;Software testing;Protection;Signal processing;Computational modeling","watermarking;discrete cosine transforms;optimisation;distortion;iterative methods;image coding","invisible robust watermark;embedded watermark signal;image watermarking;transparent watermark;human visual system model;masking function;discrete cosine transform;StirMark 4.0 software","","","12","","","","","","IEEE","IEEE Conferences"
"Multi-objective and MGG evolutionary algorithm for constrained optimization","Yuren Zhou; Yuanxing Li; Jun He; Lishan Kang","Coll. of Comput. Sci. & Eng., South China Univ. of Technol., Guangzhou, China; NA; NA; NA","The 2003 Congress on Evolutionary Computation, 2003. CEC '03.","","2003","1","","1","5 Vol.1","This paper presents a new approach to handle constrained optimization using evolutionary algorithms. The new technique converts constrained optimization to a two-objective optimization: one is the original objective function, the other is the degree function violating the constraints. By using Pareto-dominance in the multi-objective optimization, individual's Pareto strength is defined. Based on Pareto strength and minimal generation gap (MGG) model, a new real-coded genetic algorithm is designed. The new approach is compared with some other evolutionary optimization techniques on several benchmark functions. The results show that the new approach outperforms those existing techniques in feasibility, effectiveness and generality. Especially for some complicated optimization problems with inequality and equality constraints, the proposed method provides better numerical accuracy.","","0-7803-7804","10.1109/CEC.2003.1299549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1299549","","Constraint optimization;Evolutionary computation;Pareto optimization;Genetic programming;Computer science;Genetic algorithms;Software engineering;Testing;Educational institutions;Helium","evolutionary computation;Pareto optimisation;nonlinear programming;constraint handling","MGG evolutionary algorithm;constrained optimization;two-objective optimization;objective function;degree function;Pareto-dominance;Pareto strength;minimal generation gap model;real-coded genetic algorithm;benchmark functions;inequality constraints;equality constraints;numerical accuracy;multiobjective evolutionary algorithm","","12","10","","","","","","IEEE","IEEE Conferences"
"Distributed reinforcement learning for multiple objective optimization problems","C. E. Mariano; E. F. Morales","Inst. Mexicano de Tecnologia del Agua, Morelos, Mexico; NA","Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)","","2000","1","","188","195 vol.1","This paper describes the application and performance evaluation of a new algorithm for multiple objective optimization problems (MOOP) based on reinforcement learning. The new algorithm, called MDQL, considers a family of agents for each objective function involved in a MOOP. Each agent proposes a solution for its corresponding objective function. Agents leave traces while they construct solutions considering traces made by other agents. The solutions proposed by the agents are evaluated using a non-domination criterion and solutions in the final Pareto set for each iteration are rewarded. A mechanism for the application of MDQL in continuous spaces which considers a fixed set of possible actions for the states (the number of actions depends on the dimensionality of the MOOP), is also proposed. Each action represents a path direction and its magnitude is changed dynamically depending on the evaluation of the state that the agent reached. Constraint handling, based on reinforcement comparison, considers reference values for constraints, penalizing agents violating any of them proportionally to the violation committed. MDQL performance was measured with ""error ratio"" and ""spacing"" metrics on four test bed problems suggested in the literature, showing competitive results with state-of-the-art algorithms.","","0-7803-6375","10.1109/CEC.2000.870294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870294","","Learning;Testing;Pareto optimization;Optimization methods;Autonomous agents;State estimation;Evolutionary computation","learning (artificial intelligence);optimisation;multi-agent systems;constraint handling;software performance evaluation","distributed reinforcement learning;multiple objective optimization problems;performance evaluation;MDQL;software agents;nondomination criterion;Pareto set;constraint handling;error ratio;spacing metrics","","2","16","","","","","","IEEE","IEEE Conferences"
"TEAMS: Testability Engineering and Maintenance System","K. R. Pattipati; V. Raghavan; M. Shakeri; S. Deb; R. Shrestha","Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; NA; NA","Proceedings of 1994 American Control Conference - ACC '94","","1994","2","","1989","1995 vol.2","TEAMS, Testability Engineering and Maintenance System, is a software package for testability analysis, automatic test sequencing, and design for testability of complex, hierarchically-described, modular systems. It consists of algorithms based on information theory, heuristic search, and graph theory to optimize design, diagnosis and maintenance of integrated systems and thereby reduce the life-cycle cost. A system is modeled as a hierarchical directed dependency graph with special nodes to denote modules, test points, modes of operation and redundancy. Links in the graph denote first order functional dependencies. TEAMS supports hierarchical testing in accordance with the field maintenance procedures; a failure source may be isolated to a component or a module at any level. Other practical features include options to integrate diagnosis with rectification, and to optimize diagnostic time and/or cost. An interactive menu-mouse graphical interface serves as a high-level front-end to these algorithms and enables the user to graphically enter, modify and integrate hierarchical functional models of systems. TEAMS presents concise testability reports consisting of important detection and isolation figures of merit, testability shortcomings and design for testability recommendations.","","0-7803-1783","10.1109/ACC.1994.752424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=752424","","System testing;Systems engineering and theory;Automatic testing;Software testing;Design for testability;Cost function;Design engineering;Software packages;Information theory;Graph theory","design for testability;CAD;maintenance engineering;software packages;information theory;heuristic programming;graph theory;graphical user interfaces","TEAMS;Testability Engineering and Maintenance System;software package;testability analysis;automatic test sequencing;design-for-testability;complex hierarchically-described modular systems;information theory;heuristic search;graph theory;life-cycle cost reduction;hierarchical directed dependency graph;CAD;test points;operation modes;redundancy;functional dependencies;hierarchical testing;field maintenance procedures;rectification","","26","10","","","","","","IEEE","IEEE Conferences"
"Computer generated transformer zones as part of township electrification design software","T. Rajakanthan; A. S. Meyer; B. Dwolatzky","Terrain Software Syst. Pty Ltd., Johannesburg, South Africa; NA; NA","IEEE Transactions on Power Delivery","","2000","15","3","1067","1072","To assist with the large scale rural electrification program in South Africa, special CAD based software is being developed for the design process. A new tool is presented to further reduce design time. Manually delineating suitable distribution transformer locations and their service areas (transformer zones) is still a time consuming activity. Delineation of optimum transformer zones, minimizes cable lengths and improves transformer utilization. The software routine described in this paper, delineates such transformer zones rapidly producing near optimum solutions. This module can be used to speedily investigate various alternative design strategies. Testing has been done on real maps and the final layouts were found to require only minor adjustment. Software cannot ever produce completely acceptable solutions due to the complexity in township layouts. A sample computer design is presented and discussed.","0885-8977;1937-4208","","10.1109/61.871376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=871376","","Software design;Africa;Voltage transformers;Power cables;Design optimization;Cost function;Geographic Information Systems;Artificial intelligence;Large-scale systems;Design automation","power distribution planning;power system CAD;power transformers;optimisation","township electrification design software;computer-generated transformer zones;rural electrification;South Africa;CAD-based software;distribution transformer locations;service areas;software routine","","1","19","","","","","","IEEE","IEEE Journals & Magazines"
"Test-based diagnosis: tree and matrix representations","A. Beygelzimer; M. Brodie; Sheng Ma; I. Rish","IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA; IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA; IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA; IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA","2005 9th IFIP/IEEE International Symposium on Integrated Network Management, 2005. IM 2005.","","2005","","","529","542","A common problem encountered in many application scenarios is how to represent some prior knowledge about a system in order to determine its true state as efficiently as possible. The information is typically in the form of tests, or questions about the system. Each test can potentially reduce our uncertainty about the system's state. The problem is to represent the information capturing the dependence between tests, their outcomes, and possible states in an efficiently navigable way to aid diagnosis. The most common such representation is a flowchart with leaf nodes corresponding to possible states, and non-leaf nodes corresponding to tests about the state. The problem with flowcharts is that they are notoriously difficult to maintain. Additional knowledge often has to be manually integrated as the system changes, making it impossible to keep track of all possible decision paths, let alone optimize the flow to maximize performance. We propose an efficient method for optimizing an existing flowchart based on a conversion to an auxiliary matrix representation. The main goal of the paper is show a synergy between the two representations in the hope that this will help practitioners choose a better strategy for their applications. We show that such a conversion suggests ways to improve both representations - ways that were not envisioned when using each representation alone. Finally, we show that the two representations are informationally equivalent in the sense that one can be transformed into the other so that if both are used as black-boxes, one would not be able to tell them apart, regardless of which state the system is in.","1573-0077","0-7803-9087","10.1109/INM.2005.1440825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1440825","","System testing;Flowcharts;Medical tests;Performance evaluation;Uncertainty;Optimization methods;Navigation;Information retrieval;Databases;Medical diagnosis","flowcharting;program diagnostics;program testing;tree data structures;software management","test-based diagnosis;tree representations;flowchart;nonleaf nodes;auxiliary matrix representation","","2","11","","","","","","IEEE","IEEE Conferences"
"Global optimization using linear lower bounds: one dimensional case","M. Bromberg; T. -. Chang","Dept. of Electr. Eng. & Comput. Sci., California Univ., Davis, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Davis, CA, USA","29th IEEE Conference on Decision and Control","","1990","","","2475","2476 vol.4","A new one-dimensional global optimization algorithm using linear lower bounds is presented. The algorithm is guaranteed to converge to a global minimum with given specified tolerance in a finite number of steps. Some numerical testing examples are used to illustrate the effectiveness of the algorithm.<<ETX>>","","","10.1109/CDC.1990.204069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=204069","","Computer aided software engineering;Testing;Computer science;Differential equations;Upper bound","convergence of numerical methods;optimisation","1D global optimisation;convergence;linear lower bounds","","3","4","","","","","","IEEE","IEEE Conferences"
"Compilation-based software performance estimation for system level design","M. T. Lazarescu; J. R. Bammi; E. Harcourt; L. Lavagno; M. Lajolo","Cadence Design Syst. Inc., San Jose, CA, USA; NA; NA; NA; NA","Proceedings IEEE International High-Level Design Validation and Test Workshop (Cat. No.PR00786)","","2000","","","167","172","The paper addresses embedded software performance estimation. Known approaches use either behavioral simulation with timing annotations, or a clock cycle-accurate model of instruction execution (e.g., an instruction set simulator). We propose a hybrid approach, that features both the high simulation speed and flexibility from the former approach and the awareness of compilation optimizations and processor features of the latter. The key idea is to translate the assembler generated by a target compiler to an ""assembler-level"", functionally equivalent, C code. This code, annotated with timing and other execution related informations, is used as a very precise, yet fast, software simulation model. The approach is used in Cadence VCC, a system-level design environment. We report a comparison of several known approaches, the description of the new methodology, and experimental results, that show the effectiveness of the proposed method. We also propose several improvements.","","0-7695-0786","10.1109/HLDVT.2000.889579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889579","","Software performance;System-level design;Timing;Computer architecture;Hardware;Clocks;Real time systems;Costs;Embedded system;Delay estimation","real-time systems;software performance evaluation;program compilers","software performance estimation;system level design;performance estimation;Cadence VCC;target compiler;compilation optimizations;real-time systems;timing analysis;compilation;architecture modeling","","15","14","","","","","","IEEE","IEEE Conferences"
"Towards an efficient assertion based verification of SystemC designs","A. Habibi; S. Tahar","Concordia Univ., Montreal, Que., Canada; Concordia Univ., Montreal, Que., Canada","Proceedings. Ninth IEEE International High-Level Design Validation and Test Workshop (IEEE Cat. No.04EX940)","","2004","","","19","22","In this paper, we present an approach to verify efficiently assertions added on top of the SystemC library and based on the property specification language (PSL). In order to improve the assertion coverage, we also propose an approach based on both static code analysis and genetic algorithms. Static code analysis will help generate a dependency relation between inputs and assertion parameters as well as define the ranges of inputs affecting the assertion. The genetic algorithm will optimize the test generation to get more efficient coverage of the assertion. Experimental results illustrate the efficiency of our approach compared to random simulation.","1552-6674","0-7803-8714","10.1109/HLDVT.2004.1431224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431224","","Object oriented modeling;Testing;Libraries;Genetic algorithms;DNA;Algorithm design and analysis;Specification languages;System-level design;Computer architecture;Hardware","formal verification;formal specification;specification languages;program diagnostics;genetic algorithms;software libraries;programming language semantics;C++ language;system-on-chip","SystemC design verification;SystemC library;property specification language;assertion coverage;static code analysis;genetic algorithm;dependency relation;test generation","","11","11","","","","","","IEEE","IEEE Conferences"
"Direct extraction of accurate DC bipolar parameters for the forward active region without using optimization","J. Kendall","Northern Telecom Electron., Nepean, Ont., Canada","Proceedings of the 1991 International Conference on Microelectronic Test Structures","","1990","","","197","202","Techniques are presented for directly extracting the DC bipolar SPICE parameters for the forward active region without optimization by consistent use of the model equations. The mathematical basis of the extraction technique is explained in detail, and examples of its application as an interactive software system interfaced to the TECAP parameter extractor are shown. Use of the interactive software system permits the forward DC parameters to be determined in less than 10 min. The fit to measured data obtained is good enough that optimization is not required.<<ETX>>","","0-87942-588","10.1109/ICMTS.1990.161740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=161740","","Data mining;Equations;SPICE;Software systems;Reactive power;Parameter extraction;Temperature;Voltage;Integrated circuit modeling;Telecommunications","bipolar integrated circuits;bipolar transistors;electronic engineering computing;interactive systems;semiconductor device models","DC bipolar parameters;forward active region;model equations;extraction technique;interactive software system;TECAP parameter extractor;forward DC parameters","","5","4","","","","","","IEEE","IEEE Conferences"
"OPMOR: optimization of motion control algorithms for mobile robots","J. R. Pimentel; E. A. Puente; D. Gachet; J. M. Pelaez","Dept. Ingenieria de Syst. y Autom., Univ. Politecnica de Madrid, Spain; Dept. Ingenieria de Syst. y Autom., Univ. Politecnica de Madrid, Spain; Dept. Ingenieria de Syst. y Autom., Univ. Politecnica de Madrid, Spain; Dept. Ingenieria de Syst. y Autom., Univ. Politecnica de Madrid, Spain","Proceedings of the 1992 International Conference on Industrial Electronics, Control, Instrumentation, and Automation","","1992","","","853","861 vol.2","OPMOR is a hierarchical, interactive, window-based, graphical oriented software environment for the specification, simulation, performance evaluation, and optimization of motion control algorithms for mobile robots. The software environment allows the definition of the robot operating environment, the robot sensor, the robot geometry, and the specification, development, and testing of sensor-based mobile robot control software. A feature of OPMOR is the simulation of real sensors, real operating environments, and actual robots. Robot control software developed under OPMOR runs with minor modifications on two mobile robots existing at the Madrid Polytechnic University. The software environment has its own graphical user interface implemented in X-Windows. Structured, unstructured, static, and dynamic environments can be modeled with OPMOR. Although OPMOR can be used to simulate any motion control paradigm, the authors successfully used it to study reactive systems using behavioral control strategies. Many configurations for the operating environment have been tried using a wide variety of algorithms and implementations, and some results are presented.<<ETX>>","","0-7803-0582","10.1109/IECON.1992.254519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=254519","","Motion control;Mobile robots;Robot sensing systems;Robot control;Software performance;Software algorithms;Computational geometry;Software testing;Graphical user interfaces;Control system synthesis","control engineering computing;digital simulation;graphical user interfaces;interactive systems;mobile robots;optimisation;position control","sensor-based control;interactive system;OPMOR;optimization;motion control algorithms;mobile robots;graphical oriented software environment;simulation;performance evaluation;robot operating environment;robot sensor;robot geometry;X-Windows","","5","16","","","","","","IEEE","IEEE Conferences"
"Optimal code placement of embedded software for instruction caches","H. Tomiyama; H. Yasuura","Dept. of Inf. Syst., Kyushu Univ., Fukuoka, Japan; Dept. of Inf. Syst., Kyushu Univ., Fukuoka, Japan","Proceedings ED&TC European Design and Test Conference","","1996","","","96","101","This paper presents a new code placement method for embedded software to maximize hit ratios of instruction caches. We formulate the code placement problem as an integer linear programming problem. One of the advantages of our method is that code can be moved beyond boundaries of functions, so that code placement is optimized globally. Experimental results show our method achieves 35% (max 45%) reduction of cache misses.","1066-1409","0-8186-7424","10.1109/EDTC.1996.494132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494132","","Embedded software;Costs;Clocks;Frequency;Energy consumption;Integer linear programming;Optimization methods;Distributed power generation;Hardware;Information systems","real-time systems;instruction sets;integer programming;linear programming;storage management;optimising compilers;cache storage","optimal code placement;embedded software;instruction caches;hit ratios;integer linear programming problem;global optimisation;cache misses","","21","11","","","","","","IEEE","IEEE Conferences"
"Jovian: a framework for optimizing parallel I/O","R. Bennett; K. Bryant; A. Sussman; R. Das; J. Saltz","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","Proceedings Scalable Parallel Libraries Conference","","1994","","","10","20","There has been a great deal of recent interest in parallel I/O. We discuss the design and implementation of the Jovian library, which is intended to optimize the I/O performance of multiprocessor architectures that include multiple disks or disk arrays. We also present preliminary performance measurements from benchmarking the Jovian I/O library on the IBM SP1 distributed memory parallel machine for two application templates.<<ETX>>","","0-8186-6895","10.1109/SPLC.1994.377009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=377009","","Libraries;Design optimization;Computer architecture;Kelvin;Computer science;Educational institutions;Parallel machines;Bandwidth;Computational modeling;Concurrent computing","parallel programming;distributed memory systems;software libraries;optimising compilers;software reusability;program testing;program compilers","parallel I/O optimisation;Jovian library;I/O performance;multiprocessor architectures;multiple disks;benchmarking;IBM SP1 distributed memory parallel machine;application templates","","17","17","","","","","","IEEE","IEEE Conferences"
"DSP Quant: design, validation, and applications of DSP hard real-time benchmark","Chunho Lee; D. Kirovski; I. Hong; M. Potkonjak","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; NA; NA; NA","1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","","1997","1","","679","682 vol.1","Although the undeniable importance of high quality, efficient and effective DSP synthesis benchmark has been firmly and widely established, until now the emphasis of benchmarking has been restricted on assembling individual examples. In this paper we introduce the ""ideal candidate benchmark methodology"" which poses the development of the benchmark as well as defines a statistical and optimization problem. We first outline the goals and requirements relevant for the benchmark development. After discussing the computational complexity of the benchmark selection problem, we present a simulated annealing-based algorithm for solving this computationally intractable optimization task. Using this approach from 150 examples we select 12 examples for the new DSP Quant benchmark for DSP hard Real-Time applications. The DSP benchmark is statistically validated, and its application to the analysis and development of system-level synthesis algorithms is demonstrated,.","1520-6149","0-8186-7919","10.1109/ICASSP.1997.599859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599859","","Digital signal processing;Benchmark testing;Application software;Computer science;Algorithm design and analysis;Circuit testing;Logic testing;Performance evaluation;Software performance;Software tools","computational complexity;software performance evaluation;logic testing;signal processing","DSP;hard real-time benchmark;DSP Quant;DSP synthesis benchmark;benchmarking;computational complexity","","4","9","","","","","","IEEE","IEEE Conferences"
"Optimization of revolver head SMT machines using adaptive simulated annealing (ASA)","T. M. Tirpak; P. C. Nelson; A. J. Asmani","Adv. Technol. Center, Motorola Inc., Schaumburg, IL, USA; NA; NA","Twenty Sixth IEEE/CPMT International Electronics Manufacturing Technology Symposium (Cat. No.00CH37146)","","2000","","","214","220","Among the various SMT assembly machines, revolver head machines are popular due to their small size, relatively high placement rate, wide range of parts they can place, and stationary feeder banks that can be replenished on-the-fly. These machines are capable of placing over 15,000 components per hour, thereby reducing PWB manufacturing cost. However, it is necessary to simultaneously optimize feeder set-up, nozzle set-up and placement sequence for the fastest placement rate to be achieved. This paper addresses development of optimization software for the Fuji NP-132, a dual station, dual revolver head, high-speed placement machine. Following a description of machine operation, a set of equations is derived for evaluation of total assembly time for a given board, based on the pick-place times of individual revolver head cycles. Feeder, nozzle and placement optimization problems are discussed in terms of the machine's degrees of freedom and physical constraints. An adaptive simulated annealing algorithm is proposed. Cheapest insertion and nearest neighbor path construction heuristics are used to generate placement sequences, while constraint satisfaction swapping heuristics are used to generate feeder and nozzle set-ups. This hybrid technique has the advantages of stochastic search for a global optimum and guided search which guarantees the feasibility of a solution, i.e. the feeder set-up fits in the machine's feeder bank and contains all parts required for the PWB. Experimental results are presented for the multi-dimensional combinatorial optimization problem for a set of Fuji NP-132 machine programs.","1089-8190","0-7803-6482","10.1109/IEMT.2000.910731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910731","","Magnetic heads;Surface-mount technology;Assembly;Manufacturing;Costs;Equations;Constraint optimization;Simulated annealing;Nearest neighbor searches;Stochastic processes","simulated annealing;surface mount technology;printed circuit manufacture;assembling;nozzles;soldering;printed circuit testing;constraint theory","optimization;revolver head SMT machines;adaptive simulated annealing;SMT assembly machines;revolver head machines;placement rate;stationary feeder banks;feeder bank replenishment;PWB manufacturing cost;feeder set-up optimization;nozzle set-up optimization;placement sequence optimization;optimization software;Fuji NP-132 dual-station dual revolver head high-speed placement machine;machine operation;total assembly time;pick-place times;revolver head cycles;placement optimization;nozzle optimization;feeder optimization;machine degrees of freedom;machine physical constraints;adaptive simulated annealing algorithm;nearest neighbor path construction heuristics;cheapest insertion heuristics;placement sequences;constraint satisfaction swapping heuristics;stochastic search;global optimum;guided search;solution feasibility;machine feeder bank;multi-dimensional combinatorial optimization problem;Fuji NP-132 machine programs","","8","10","","","","","","IEEE","IEEE Conferences"
"Test case generation of a protocol by a fault coverage analysis","Tae-Hyong Kim; Ik-Soon Hwang; Min-Seok Jang; Sung-Won Kang; Jai-Yong Lee; Sang-Bae Lee","Dept. of Electron. Eng., Yonsei Univ., Seoul, South Korea; NA; NA; NA; NA; NA","Proceedings Twelfth International Conference on Information Networking (ICOIN-12)","","1998","","","690","695","In this paper we generate conformance test cases for a communication protocol modeled in an EFSM(Extended Finite State Machine) by a fault coverage analysis. For the analysis model, we choose the expanded EFSM to resolve the inter-dependency problem between control and data flows within an EFSM. An expanded EFSM has several useful properties and makes it easy to generate test cases. For test case generation, at first we define data elements in the expanded EFSM. With the definition, we define some probable fault models in edges of the expanded EFSM and discuss what test cases to be needed for satisfying each fault model. The analysis shows that control flow test cases with full fault coverage and data flow test cases satisfying 'all-du-paths' criterion are needed to guarantee high fault coverage in the expanded EFSM. A mass of generated test cases by high fault coverage is optimized through some steps. The result of a simple protocol shows the efficacy of this method.","","0-8186-7225","10.1109/ICOIN.1998.648603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648603","","Computer aided software engineering;Protocols;Telecommunication control;Electronic equipment testing;Software testing;Research and development;Data analysis;Collaborative software;Explosions;Computer science","protocols","test case generation;protocol;fault coverage analysis;conformance test;communication protocol;extended finite state machine;inter-dependency problem;data elements;probable fault models;data flow test cases","","","15","","","","","","IEEE","IEEE Conferences"
"Algebraic techniques for the optimization of control flow checking","G. Noubir; B. Y. Choueiry","Dept. of Comput. Sci., Swiss Federal Inst. of Technol., Lausanne, Switzerland; Dept. of Comput. Sci., Swiss Federal Inst. of Technol., Lausanne, Switzerland","Proceedings of Annual Symposium on Fault Tolerant Computing","","1996","","","128","137","Leveugle (1990) addressed the problem of reducing the overhead of online testing in dedicated controllers. He introduced a low-overhead technique that allows the detection of illegal paths in finite state machines. Based on Leveugle's idea for detecting illegal paths, we introduce a new simple signature function. This signature function can be efficiently implemented in software. The assignment of values to the states is carried out algebraically by matrix inversion instead of using exhaustive search methods. We show that signatures computed using MISR or checksum are particular cases of our more general signature function. Thus, the state assignment problem can be solved more efficiently. Then, we address the problems of latency and checking from a formal perspective and show that finding the smallest set of checking states (i.e., states where the static signature as compared with the run-time signature) that induces a latency less than or equal to a given value L is NP-hard and there exists no polynomial time algorithm that solves this problem unless P=NP.","0731-3071","0-8186-7262","10.1109/FTCS.1996.534601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534601","","Delay;Control systems;Application software;Flow graphs;Hardware;Computer science;Testing;Matrices;Search methods;Runtime","optimisation;finite state machines;fault tolerant computing;reliability;matrix inversion;state assignment;computational complexity;flow graphs;formal verification;computer testing","algebraic techniques;control flow checking optimization;online testing;dedicated controllers;testing overhead reduction;illegal path detection;finite state machines;signature function;matrix inversion;exhaustive search methods;MISR;checksum;state assignment problem;latency;checking;static signature;run-time signature;NP-hard;polynomial time algorithm;flow graph","","1","20","","","","","","IEEE","IEEE Conferences"
"High-performance software emulation of 1750 A processor","K. Reinholtz","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA","16th DASC. AIAA/IEEE Digital Avionics Systems Conference. Reflections to the Future. Proceedings","","1997","2","","8.2","1","We describe a software emulator of the MIL-STD-1750 A architecture that executes 1750A code at a rate of about 4 MIPS on a Sun 200 MHz Ultra2 workstation, and effectively several times faster than that when application-specific optimizations are used. A number of optimization techniques were used, including binary translation and an unusual emulation of the 1750A timers and memory management unit. The performance technologies used within the emulator are for the most part applicable to the emulation of other processor architectures.","","0-7803-4150","10.1109/DASC.1997.637258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637258","","Emulation;Hardware;Space vehicles;Optimizing compilers;Software testing;Space technology;Cost function;History;Computer architecture;Propulsion","computer architecture;optimisation;aerospace computing;floating point arithmetic;timing circuits;virtual machines;storage management","1750 A processor;MIL-STD-1750 A architecture;Sun 200 MHz Ultra2 workstation;application-specific optimization;optimization;binary translation;timers;memory management unit;performance technologies;processor architectures;software emulation;4 MIPS","","1","12","","","","","","IEEE","IEEE Conferences"
"FFTW: an adaptive software architecture for the FFT","M. Frigo; S. G. Johnson","Lab. for Comput. Sci., MIT, Cambridge, MA, USA; NA","Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)","","1998","3","","1381","1384 vol.3","FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the processor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the computation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW's self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTW also compares favorably with machine-specific, vendor-optimized libraries.","1520-6149","0-7803-4428","10.1109/ICASSP.1998.681704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=681704","","Software architecture;Microprocessors;Pipelines;Computer architecture;Algorithm design and analysis;Hardware;Automatic testing;Software testing;Software performance;Software libraries","fast Fourier transforms;adaptive systems;mathematics computing;discrete Fourier transforms","FFTW;adaptive software architecture;FFT;floating-point operations;processor pipeline;memory hierarchy;performance;computer architecture;fast algorithm;adaptive FFT program;self-optimizing approach;DFT","","356","22","","","","","","IEEE","IEEE Conferences"
"Interpreting clustering results through cluster labeling","O. Maqbool; H. A. Babri","Lahore Univ. of Manage. Sci., Pakistan; NA","Proceedings of the IEEE Symposium on Emerging Technologies, 2005.","","2005","","","429","434","Software architecture refers to the overall structure of a software system, and is defined by the components (sub-systems) within a software system and their interactions with one another. Quite often, there is little documentation describing a software system's architecture, especially in the case of legacy software systems. Thus techniques must be employed for recovering the architecture from the software's source code. Given the size and complexity of legacy systems, researchers have started exploring the use of automated techniques for architecture recovery. A technique that has shown promising results is clustering. Clusters that are obtained as a result of the clustering process represent sub-systems within a software system, but are nor easy to interpret until they are given appropriate names. In this paper, we present a cluster labeling scheme based on identifiers. As the clustering process proceeds, keywords are ranked using the inverse document frequency ranking scheme. Results of experiments conducted on a test system demonstrate that our labeling approach is effective. We also compare the clustering results of the complete algorithm and the weighted combined algorithm based on labels of the clusters produced by them during clustering.","","0-7803-9247","10.1109/ICET.2005.1558920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1558920","","Labeling;Software systems;Computer architecture;Documentation;Software architecture;Technology management;Frequency;System testing;Costs;Manuals","software architecture;system documentation;software maintenance","cluster labeling;software architecture;system;documentation;source code;architecture recovery;clustering process;identifiers;inverse document frequency ranking scheme;weighted combined algorithm","","6","18","","","","","","IEEE","IEEE Conferences"
"The application of neural networks in smell analyzing system of grain quality","Zhao Dean; Zhu Jianyun; Pan Tianhong; Zhang Xiaochao","Sch. of Electr. & Inf. Eng., Jiangsu Univ., Zhenjiang, China; Sch. of Electr. & Inf. Eng., Jiangsu Univ., Zhenjiang, China; Sch. of Electr. & Inf. Eng., Jiangsu Univ., Zhenjiang, China; NA","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","","2004","3","","2622","2625 Vol.3","Moldy grain have mildew which are harmful to people and animals. To provide a simple and objective solution to identify whether grain are moldy, a smell analyzing system of grain quality is developed. The system consists of gas sensor array, gas pipe system, signal adjusting circuit, data collecting system and pattern recognition system. The software is developed on the basis of Visual Basic 6.0 and Matlab 6.5. In order to improve veracity of identification, three eigenvalues in the collecting data are used, one is the maximal response point, the other two are near the maximal response point. The sample eigenvalues are trained with the triplex-optimize BP NN. By testing, both training samples and test samples can be identified correctly, which indicates that the application of neural networks is valuable in the smell analyzing system of grain quality.","","0-7803-8273","10.1109/WCICA.2004.1342071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342071","","Neural networks;Sensor arrays;Eigenvalues and eigenfunctions;System testing;Animals;Gas detectors;Circuits;Pattern recognition;Software systems;Visual BASIC","neural nets;agricultural products;backpropagation;BASIC;visual languages;digital simulation;optimisation;eigenvalues and eigenfunctions;statistical testing","BP neural networks;smell analysing system;grain quality;moldy grain;gas sensor array;gas pipe system;signal adjusting circuit;data collecting system;pattern recognition system;Visual Basic 6.0;Matlab 6.5;triplex optimization;maximal response point;eigenvalues","","","","","","","","","IEEE","IEEE Conferences"
"Differential evolution: a fast and simple numerical optimizer","K. V. Price","836 Owl Circle, Vacaville, CA, USA","Proceedings of North American Fuzzy Information Processing","","1996","","","524","527","Differential evolution (DE) is a powerful yet simple evolutionary algorithm for optimizing real-valued multi-modal functions. Function parameters are encoded as floating-point variables and mutated with a simple arithmetic operation. During mutation, a variable-length, one-way crossover operation splices perturbed best-so-far parameter values into existing population vectors. A novel sampling technique adaptively scales the step-size of perturbations as the population evolves. DE's selection criterion demands that improved vectors always be accepted. The performance of DE on a testbed of 15 functions is compared with a variety of recently published results encompassing many different methods. DE converged for all 15 functions and was the fastest method for solving 11 of them. DE's performance on the remaining 4 functions was competitive.","","0-7803-3225","10.1109/NAFIPS.1996.534790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534790","","Genetic mutations;Design optimization;Evolutionary computation;Floating-point arithmetic;Constraint optimization;Algorithm design and analysis;Sampling methods;Testing;Hardware;Software performance","genetic algorithms;functional analysis;arithmetic;convergence of numerical methods","differential evolution;numerical optimizer;evolutionary algorithm;real-valued multi-modal functions;function parameter encoding;floating-point variables;arithmetic operation;variable-length one-way crossover operation;perturbed best-so-far parameter values;population vectors;sampling technique;adaptive step-size scaling;selection criterion;performance","","133","12","","","","","","IEEE","IEEE Conferences"
"CALMOS : A Portable Software System for the Automatic and Interactive Layout of MOS/LSI","H. Beke; W. Sansen","Leuven Industrial Software Company, Leuven, Belgium; NA","16th Design Automation Conference","","1979","","","102","108","CALMOS (Computer Aided Layout of MOS) is a computer system for the layout of custom MOS/LSI circuits. Starting with a standard cell library and a simple circuit connectivity description, the program performs various automatic and/or interactive procedures such as initial placement, assignment of equivalent and equipotential pins, optimization of the placement, prerouting, routing, routing compression, fan in fan out and crosstalk verification and circuit verification. The database has reentrant properties such that a designer can step through the system and try out different possibilities. After each step the results are immediately available and can be compared with previous outputs. This on-line optimization avoids multiple rerunning of the task and will also yield a better chip minimization. CALMOS is written in standard FORTRAN and only needs 32K (16 bit) of memory (300 cell version) together with simple overlay facilities. As a result it can easily be installed on almost any existing computersystem. Because of this portability, the algorithmic strength and the modularity of CALMOS is successfully combined with the on-line interactive facilities of existing graphic systems. Such an integration which is actually under development, will allow for the creation of a stand alone integrated minicomputer based LSI design system. It will be based on a common design language and include logic simulation, layout, circuit analysis, design rule checking, logic verification and test pattern generation.","","","10.1109/DAC.1979.1600095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1600095","","Software systems;Large scale integration;Logic design;Logic circuits;Logic testing;Software libraries;Pins;Crosstalk;Databases;Minimization","","","","8","15","","","","","","IEEE","IEEE Conferences"
"Beam loading and cavity compensation for the Ground Test Accelerator","S. P. Jachim; E. F. Natter","Los Alamos Nat. Lab., NM, USA; Los Alamos Nat. Lab., NM, USA","Proceedings of the 1989 IEEE Particle Accelerator Conference, . 'Accelerator Science and Technology","","1989","","","1870","1873 vol.3","The Ground Test Accelerator (GTA) will be a heavily beam-loaded H/sup -/ linac with tight tolerances on accelerating field parameters. The methods used in modeling the effects of beam loading in this machine are described. The response of the cavity to both beam and radio-frequency (RF) drive stimulus is derived, including the effects of cavity detuning. This derivation is not restricted to a small-signal approximation. An analytical method for synthesizing a predistortion network that decouples the amplitude and phase responses of the cavity is also outlined. Simulation of performance, including beam loading, is achieved by using a control system analysis software package. A straightforward method is presented for extrapolating this work to model large coupled structures with closely spaced parasitic modes. Results to date have enabled the RF control system designs for GTA to be optimized and have given insight into their operation.<<ETX>>","","","10.1109/PAC.1989.72952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=72952","","Testing;Radio frequency;Life estimation;Linear particle accelerator;Control system synthesis;Network synthesis;Predistortion;Analytical models;Control system analysis;Software packages","beam handling techniques;ion accelerators;linear accelerators","Ground Test Accelerator;beam loading;cavity detuning;small-signal approximation;predistortion network;control system analysis software package;closely spaced parasitic modes","","2","6","","","","","","IEEE","IEEE Conferences"
"Voting multi-dimensional data with deviations for Web services under group testing","W. -. Tsai; Y. Chen; D. Zhang; H. Huang","Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA","25th IEEE International Conference on Distributed Computing Systems Workshops","","2005","","","65","71","Web services (WS) need to be trustworthy to be used in critical applications. A technique called WS group testing has been proposed which can significantly reduce the cost of testing and ranking a large number of WS. A main feature of WS group testing is that it is able to establish the test oracles for the given test inputs from multiple WS and infer the oracles by plural voting. Efficient voting of complex and large number of data is critical to the success of group testing. Current voting techniques are not designed to deal with such a situation. This paper presents efficient voting algorithms that determine the plural value on multi-dimensional data and large number of data. The algorithm uses a clustering method to classify data into regions to identify the plural value. Experiments are designed and performed to concept-prove the algorithms and their applications with group testing.","1545-0678;2332-5666","0-7695-2328","10.1109/ICDCSW.2005.141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437158","Web Services Testing;Group Testing;Voting;Clustering","Voting;Web services;Testing;Application software;Clustering algorithms;Programming;Computer science;Data engineering;Costs;Clustering methods","Internet","Web service testing;multidimensional data;WS group testing;plural voting techniques;clustering method","","4","18","","","","","","IEEE","IEEE Conferences"
"The extended partitioning problem: hardware/software mapping and implementation-bin selection","A. Kalavade; E. A. Lee","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA","Proceedings Sixth IEEE International Workshop on Rapid System Prototyping. Shortening the Path from Specification to Prototype","","1995","","","12","18","The extended partitioning problem is the joint problem of mapping nodes in a precedence graph to hardware or software, and within each mapping, selecting an appropriate implementation for each node. The end-goal is to minimize the hardware area, subject to architectural and performance constraints. This is an NP-complete problem; we present an efficient heuristic called MIBS to solve it. The MIBS (Mapping and Implementation-Bin Selection) algorithm solves the extended partitioning problem by decomposing it into an iterative process consisting of two steps: mapping and implementation-bin selection (IBS). The GCLP (Global Criticality/Local Phase-driven) algorithm computes a mapping by using an adaptive optimization objective at each iteration. This objective is selected on the basis of a global time criticality measure and local optimality measures. The IBS algorithm solves the implementation-bin selection problem. It uses a bin sensitivity measure which correlates the implementation bin motion with the overall hardware area reduction, to determine the implementation bin of a node for a given mapping. Experimental results indicate that the added dimension of design flexibility (offered by implementation bins) can be used effectively in partitioning to reduce the overall area. The MIBS algorithm has O(|N|/sup 3/) complexity, with a solution quality comparable to that of ILP (integer linear programming).","1074-6005","0-8186-7100","10.1109/IWRSP.1995.518565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=518565","","Hardware;Partitioning algorithms;Iterative algorithms;Time measurement;Software algorithms;NP-complete problem;Area measurement;Motion measurement;Software quality;Design optimization","directed graphs;minimisation;computational complexity;logic CAD;software engineering;resource allocation;iterative methods;logic partitioning","extended partitioning problem;hardware/software mapping;implementation-bin selection;precedence graph node mapping;node implementation selection;hardware area reduction;architectural constraints;performance constraints;NP-complete problem;MIBS heuristic algorithm;iterative process;GCLP algorithm;global criticality;local phase-driven algorithm;adaptive optimization objective;global time criticality measure;local optimality measures;bin sensitivity measure;implementation bin motion;design flexibility;complexity;solution quality","","27","7","","","","","","IEEE","IEEE Conferences"
"An automated feedback system for computer organization projects","P. M. Chen","Dept. of Electr. Eng. & Comput. Sci., Univ. of Michigan, Ann Arbor, MI, USA","IEEE Transactions on Education","","2004","47","2","232","240","This paper describes a system, built and refined over the past five years, that automatically analyzes student programs assigned in a computer organization course. The system tests a student's program, then e-mails immediate feedback to the student to assist and encourage the student to continue testing, debugging, and optimizing his or her program. The automated feedback system improves the students' learning experience by allowing and encouraging them to improve their program iteratively until it is correct. The system has also made it possible to add challenging parts to each project, such as optimization and testing, and it has enabled students to meet these challenges. Finally, the system has reduced the grading load of University of Michigan's large classes significantly and helped the instructors handle the rapidly increasing enrollments of the 1990s. Initial experience with the feedback system showed that students depended too heavily on the feedback system as a substitute for their own testing. This problem was addressed by requiring students to submit a comprehensive test suite along with their program and by applying automated feedback techniques to help students learn how to write good test suites. Quantitative iterative feedback has proven to be extremely helpful in teaching students specific concepts about computer organization and general concepts on computer programming and testing.","0018-9359;1557-9638","","10.1109/TE.2004.825220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1296784","","Computer testing;Automatic programming;Software testing;Data processing;Computer science education","computer testing;automatic programming;program testing;educational administrative data processing;computer science education","automated feedback system;computer organization course;student programs;Michigan University;quantitative iterative feedback;computer programming;computer testing;software testing;automated grading system","","8","11","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability analysis of communicating recovery blocks","O. Berman; U. Dinesh Kumar","Toronto Univ., Ont., Canada; NA","IEEE Transactions on Reliability","","1998","47","3","245","254","This paper studies two models for communicating recovery blocks (RB) with statistically-independently failing software versions where versions are executed sequentially. Model one considers two RB: RB-1 & RB-2, where RB-2 receives some data from RB-1. Thus, if a version in RB-2 fails then RB-1 has to rollback to its initial state. Model two considers two RB in conversation: both blocks must satisfy their respective acceptance tests before any of the blocks are allowed to exit from the conversation. Simple expressions for the reliability of the system are derived for models 1 and 2, and it is proved for them that the reliability of a RB consisting of versions ordered from smallest to largest based on failure probability is as reliable as any other list of the versions. Optimization models are developed for models 1 and 2. The paper presents efficient branch and bound procedures to solve the optimization models.","0018-9529;1558-1721","","10.1109/24.740494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=740494","","Software testing;Hardware;Local area networks;Fault trees;Fault tolerance;Software reliability;Software systems;Sufficient conditions;Fault tolerant systems;Concurrent computing","software reliability;failure analysis;optimisation;tree searching;probability","software failures;communicating recovery blocks;reliability analysis;acceptance tests;conversation;failure probability;optimization models;branch and bound procedures","","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Code optimization across procedures","S. Richardson; M. Ganapathi","Center for Integrated Syst., Stanford Univ., CA, USA; Center for Integrated Syst., Stanford Univ., CA, USA","Computer","","1989","22","2","42","50","Procedure calls can be a major obstacle to the analysis of computer programs, preventing significant improvements in program speed. A broad range of techniques, each of which is in some sense interprocedural by nature, is considered to overcome this obstacle. Some techniques rely on interprocedural dataflow in their analysis. Others require interprocedural information in the form of detailed profile data or information concerning the scope of a given procedure in relation to other procedures. These include procedure integration, interprocedural register allocation, pointer and alias tracking, and dependency analysis.<<ETX>>","0018-9162;1558-0814","","10.1109/2.19831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=19831","","Data analysis;Optimizing compilers;Computer architecture;Software performance;Program processors;Processor scheduling;Pipeline processing;Statistical analysis;Hardware;Design optimization","optimisation;program compilers;program testing;storage allocation","code optimization;compilers;pointer tracking;program testing;program speed;interprocedural dataflow;interprocedural information;profile data;procedure integration;interprocedural register allocation;alias tracking;dependency analysis","","4","12","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of interior-point codes for medium-term hydro-thermal coordination","J. Medina; V. H. Quintana; A. J. Conejo; F. P. Thoden","Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; NA; NA; NA","Proceedings of the 20th International Conference on Power Industry Computer Applications","","1997","","","224","231","This paper studies the performance of newly developed and currently under development interior-point optimization codes as applied to the solution of medium-term hydro-thermal coordination (MTHTC) problems. The authors compare commercial and research codes, and their main advantages and drawbacks are pointed out. The codes that they study are: CPLEX 3.0-barrier; HOPDM; LOQO; PCx; LIPSOL; and IPA1 (a code currently being developed by the authors). All codes have been tested on the Spanish hydrothermal power system.","","0-7803-3713","10.1109/PICA.1997.599400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599400","","Job shop scheduling;Thermal decomposition;Lagrangian functions;Genetic algorithms;System testing;Production systems;Costs;Security;Telecommunication standards;Optimization methods","hydrothermal power systems;power system planning;power system analysis computing;optimisation;software packages;software packages;software packages;software packages;software packages","medium-term power system planning;hydrothermal power system coordination;interior-point optimization codes;computational performance;CPLEX 3.0-barrier;HOPDM;LOQO;computer simulation;PCx;LIPSOL;IPA1;Spain","","3","27","","","","","","IEEE","IEEE Conferences"
"A knowledge-based system for hardware-software partitioning","M. L. Lopez; C. A. Iglesias; J. C. Lopez","Dept. Ing. Electronica, Univ. Politecnica de Madrid, Spain; NA; NA","Proceedings Design, Automation and Test in Europe","","1998","","","914","915","This paper presents SHAPES, a tool for hardware-software partitioning. It is based on two main paradigms: the implementation of the partitioning tool by means of an expert system, and the use of fuzzy logic to model the parameters involved in the process.","","0-8186-8359","10.1109/DATE.1998.655967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655967","","Knowledge based systems;Shape;Expert systems;Hardware;Hybrid intelligent systems;Computer architecture;Fuzzy logic;Costs;Partitioning algorithms;Design optimization","fuzzy logic;high level synthesis;expert systems","knowledge-based system;hardware-software partitioning;SHAPES;partitioning tool;expert system;fuzzy logic;parameters modeling","","5","5","","","","","","IEEE","IEEE Conferences"
"A fuzzy adaptive neighborhood search for function optimization","D. A. Pelta; A. Blanco; J. -. Verdegay","Dept. de Ciencias de la Comput. e Inteligencia Artificial, Granada Univ., Spain; NA; NA","KES'2000. Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies. Proceedings (Cat. No.00TH8516)","","2000","2","","594","597 vol.2","Neighborhood-based search algorithms have been applied with success in the optimization field. In this paper, the FANS (Fuzzy Adaptive Neighborhood Search) algorithm, which uses fuzzy concepts and heuristic rules in its decision procedures, together with several schedulers to adapt its behavior, is proposed. The conceptual and practical design of FANS in components allows decision makers to test different schemes and alternatives.","","0-7803-6400","10.1109/KES.2000.884118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884118","","Fans;Decision support systems;Scheduling algorithm;Fuzzy systems;Processor scheduling;System testing;Genetic algorithms;Simulated annealing;Fuzzy sets;Statistics","optimisation;functional analysis;search problems;fuzzy logic;heuristic programming","fuzzy adaptive neighbourhood search algorithm;function optimization;FANS algorithm;heuristic rules;decision procedures;schedulers;behavioural adaptation;software components","","","7","","","","","","IEEE","IEEE Conferences"
"Holter system development for recording plantar pressures: software development","A. S. Vengsarkar; J. H. Abler; Z. U. Abu-Faraj; G. F. Harris; J. J. Wertsch","Dept. of Biomed. Eng., Marquette Univ., Milwaukee, WI, USA; Dept. of Biomed. Eng., Marquette Univ., Milwaukee, WI, USA; Dept. of Biomed. Eng., Marquette Univ., Milwaukee, WI, USA; Dept. of Biomed. Eng., Marquette Univ., Milwaukee, WI, USA; Dept. of Biomed. Eng., Marquette Univ., Milwaukee, WI, USA","Proceedings of 16th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","1994","2","","936","937 vol.2","We have developed software to control a Holter type microprocessor based, portable, in-shoe, plantar pressure data acquisition system. The system is used to monitor long term plantar pressures during the performance of daily living activities. The unit is able to record in continuous or intermittent modes. Real time data compression is utilized to optimize memory storage. Software routines have been developed to test the pseudo static RAM before recording, perform refresh and drive a liquid crystal display module. The unit was evaluated during several multi-step trials with adult male subjects. The unit is considered to be appropriate for clinical application and long term study of daily living activities.<<ETX>>","","0-7803-2050","10.1109/IEMBS.1994.415221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=415221","","Pressure control;Control systems;Microprocessors;Data acquisition;Monitoring;Data compression;Random access memory;Software performance;Software testing;Read-write memory","biomedical measurement;pressure measurement;patient monitoring;computerised monitoring;microcomputer applications;data acquisition;biomedical equipment;data compression;liquid crystal displays;biomechanics;biocontrol","Holter system development;plantar pressure recording;software development;microprocessor based portable in-shoe plantar pressure data acquisition system;long term plantar pressures;daily living activities;intermittent modes;continuous modes;real time data compression;memory storage;software routines;pseudo static RAM;liquid crystal display module;multi-step trials;adult male subjects;clinical application;long term study","","2","4","","","","","","IEEE","IEEE Conferences"
"Experimental validation of NEC analysis of software models of tape wound helical satellite antennas","C. W. Trueman; S. J. Kubina; T. Pellerin; N. Sultan","Space Syst. Directorate, Canadian Space Agency, St. Hubert, Ont., Canada; NA; NA; NA","IEEE Antennas and Propagation Society International Symposium. 1996 Digest","","1996","3","","1700","1703 vol.3","This paper is about the validation of modeling software that was developed to analyze and predict the theoretical performance of helical antennas in general and in particular those antennas using a metallic strip or tape as radiating elements. This was applied to uniform cylindrical helices as well as tapered helices with a single pitch angle and novel dual pitch antennas. Corresponding helices were built and tested. The experimental performances are in very good agreement with the theoretical results. This was investigated for helices working in the axial mode in near circular polarization with axial ratios between 1 and 2 dB and return loss better than 20 dB. This validation of the software, using Kraus' (1988) criteria for the equivalence of a tape element to a wire, against experimental results is providing a new, powerful, and accurate tool for reliably designing helical antennas, a much needed tool. This validation for tape helical antennas is particularly significant for space applications where deployability is required and optimization of the mass, site and gain of the antenna are essential.","","0-7803-3216","10.1109/APS.1996.549929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=549929","","National electric code;Helical antennas;Antenna theory;Antenna accessories;Predictive models;Software performance;Performance analysis;Testing;Polarization;Software tools","helical antennas;satellite antennas;program verification;antenna theory;telecommunication computing","NEC analysis;software models;tape wound helical satellite antennas;modeling software;metallic strip;radiating elements;uniform cylindrical helices;tapered helices;pitch angle;dual pitch antennas;axial mode;circular polarization;axial ratios;design;deployability;numerical electromagnetics code","","","10","","","","","","IEEE","IEEE Conferences"
"Optimization of one and two hidden layer neural network architectures for handwritten digit recognition","C. A. Perez; C. A. Holzmann; I. Morelli","Dept. of Electr. Eng., Univ. Catolica de Chile, Santiago, Chile; Dept. of Electr. Eng., Univ. Catolica de Chile, Santiago, Chile; Dept. of Electr. Eng., Univ. Catolica de Chile, Santiago, Chile","1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century","","1995","3","","2795","2799 vol.3","This paper presents extensive results of training architectures of one- and two- hidden-layer, fully connected, feedforward neural networks which are used for handwritten digit recognition. Both architectures include 1 to 400 units in each hidden layer to produce a series of 112 networks. These networks are trained by backpropagation up to 1000 iterations and tested while advancing in training. Results are presented as a function of the number of hidden units, the number of iterations, and the training mode (centered and position-shifted patterns). An overall maximum recognition rate of 92.4% is obtained for a two-hidden layer network with 100 hidden units and 144 iterations. No significative recognition rate improvement is achieved (within 2.1 percentage points) by increasing the number of hidden units and iterations above 50 and 30, respectively.","","0-7803-2559","10.1109/ICSMC.1995.538205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=538205","","Neural networks;Handwriting recognition;Character recognition;Feedforward systems;Feedforward neural networks;Optical character recognition software;Backpropagation;Testing;Optical network units;Speech coding","character recognition;feedforward neural nets;neural net architecture;optimisation;backpropagation;iterative methods","optimization;neural network architectures;handwritten digit recognition;feedforward neural networks;backpropagation;iterative method","","2","11","","","","","","IEEE","IEEE Conferences"
"Timing analysis of optimized code","R. Kirner; P. Puschner","Inst. fur Technische Informatik, Technische Univ. Wien, Austria; NA","Proceedings of the Eighth International Workshop on Object-Oriented Real-Time Dependable Systems, 2003. (WORDS 2003).","","2003","","","100","105","Timing analysis is a crucial test for dependable hard real-time systems (DHRTS). The calculation of the worst-case execution time (WCET) is mandatory. As modern compilers are able to produce small and efficient code, software development for DHRTS is mostly done in high-level languages instead of assembly code. Execution path information available at source code (flow facts) therefore has to be transformed correctly in accordance with code optimizations by the compiler to allow safe and precise WCET analysis. In this paper, we present a framework based on abstract interpretation to perform this mandatory transformation of flow facts. Conventional WCET analysis approaches use this information to analyze the object code.","","0-7695-1929","10.1109/WORDS.2003.1218071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218071","","Timing;Real time systems;Information analysis;High level languages;Design optimization;Optimizing compilers;Concrete;System testing;Programming;Assembly","program diagnostics;real-time systems;timing;optimising compilers","timing analysis;dependable hard real-time system;worst-case execution time;software development;code optimization;real-time language;compiler optimization;code transformation;high-level language;assembly code;execution path information;source code;flow facts;abstract interpretation","","","14","","","","","","IEEE","IEEE Conferences"
"Static identification of delinquent loads","V. -. Panait; Amit Sasturkar; W. -. Wong","Dept. of Comput. Sci., Politehnica Univ. of Bucharest, Romania; NA; NA","International Symposium on Code Generation and Optimization, 2004. CGO 2004.","","2004","","","303","314","The effective use of processor caches is crucial to the performance of applications. It has been shown that cache misses are not evenly distributed throughout a program. In applications running on RISC-style processors, a small number of delinquent load instructions are responsible for most of the cache misses. Identification of delinquent loads is the key to the success of many cache optimization and prefetching techniques. We propose a method for identifying delinquent loads that can be implemented at compile time. Our experiments over eighteen benchmarks from the SPEC suite shows that our proposed scheme is stable across benchmarks, inputs, and cache structures, identifying an average of 10% of the total number of loads in the benchmarks we tested that account for over 90% of all data cache misses. As far as we know, this is the first time a technique for static delinquent load identification with such a level of precision and coverage has been reported. While comparable techniques can also identify load instructions that cover 90% of all data cache misses, they do so by selecting over 50% of all load instructions in the code, resulting in a high number of false positives. If basic block profiling is used in conjunction with our heuristic, then our results show that it is possible to pin down just 1.3% of the load instructions that account for 82% of all data cache misses.","","0-7695-2102","10.1109/CGO.2004.1281683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281683","","Benchmark testing;Computer science;Prefetching;Application software;Optimizing compilers;Parallel processing;Hardware;Computer aided instruction;Runtime;Robustness","cache storage;optimising compilers;reduced instruction set computing;memory architecture;benchmark testing;resource allocation","static delinquent load identification;processor cache misses;RISC-style processors;delinquent load instructions;cache optimization;prefetching technique;SPEC suite benchmark;basic block profiling","","6","15","","","","","","IEEE","IEEE Conferences"
"SAMS: antenna radiation pattern acquisition","J. Korsakissok","NA","IEEE Antennas and Propagation Society International Symposium. 2001 Digest. Held in conjunction with: USNC/URSI National Radio Science Meeting (Cat. No.01CH37229)","","2001","2","","626","628 vol.2","SAMS is a product family in the area of antenna radiation pattern acquisition and processing. It has been developed and is commercialised since 1998, by SILICOM, in partnership with the CNES (French Spatial Agency). The CNES expertise in antenna measurements together with SILICOM's proficiency in software development and in hyper-frequency studies led to producing a modern, reliable and user adapted software system, both in its user modes and in its parameter setting, maintainability, speed and measurement accuracy. Beyond the innovation included in this system, we can firstly promote, the automatic measurement and calculation speed in order to optimise the velocity/precision compromise, but also a separated data analysis module that allows one to compare data issued from any simulation code and from experiment. The article presents the characteristics and the technical innovations of this system.","","0-7803-7070","10.1109/APS.2001.959802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959802","","Antenna radiation patterns;Velocity measurement;Antenna measurements;Software measurement;Technological innovation;Adaptive arrays;Programming;Software systems;Maintenance;Data analysis","antenna radiation patterns;electrical engineering computing;antenna testing;digital simulation;data acquisition;automatic test software","SAMS;antenna radiation pattern acquisition;antenna radiation pattern processing;SILICOM;CNES;antenna measurements;software development;software system;measurement accuracy;automatic measurement;velocity/precision compromise;data analysis module;simulation code","","","","","","","","","IEEE","IEEE Conferences"
"Simulation Bridge: a framework for multi-processor simulation","G. D. Nagendra; V. G. P. Kumar; B. S. Sheshadri","Software Dev. Syst., Texas Instruments India Ltd, Bangalore, India; Software Dev. Syst., Texas Instruments India Ltd, Bangalore, India; Software Dev. Syst., Texas Instruments India Ltd, Bangalore, India","Proceedings of the Tenth International Symposium on Hardware/Software Codesign. CODES 2002 (IEEE Cat. No.02TH8627)","","2002","","","49","54","Multi-processor solutions in the embedded world axe being designed to meet the ever increasing computational demands of the emerging applications. Such architectures comprise two or more processors (often a mix of general purpose and digital signal processors) together with a rich peripheral mix to provide a high performance computational platform. While there are many simulation solutions in the industry available to address the system partitioning issues and also the verification of HW-SW interactions in these complex systems, there are very few solutions targetted towards the SW application developers' needs. The primary concern of the SW application developers is to debug and optimize their code. Hence, cycle accuracy and performance of the simulation solution becomes the key enablers. Desired observability and controllability of the models are additional careabouts. Secondly, application developers are more comfortable at instruction level simulations than they are with RTL or gate level simulation. These specific requirements have a bearing on the choices in the simulation solutions. This paper describes the design of a generic, C based multi-processor instruction set simulator framework in the context of the above parameters. This framework, termed the ""simulation bridge"", facilitates highly accurate, yet efficient simulation. The SimBridge performs clock correct lock-step simulation of the models in the architecture using a global simulation engine that handles both intra-processor and inter-processor communication in a homogenous fashion. It addresses the multiple key issues of execution control, synchronization, connectivity and communication. The paper concludes with the performance analysis of the SimBridge in an experimental test setup as well as in the Texas Instruments (TI) TMS320C54x-based simulators.","","1-58113-542","10.1145/774789.774800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003600","","Bridges;Computational modeling;Embedded computing;Computer architecture;Digital signal processors;Computer peripherals;High performance computing;Observability;Controllability;Context modeling","multiprocessing systems;embedded systems;hardware-software codesign;performance evaluation;digital simulation;program debugging","Simulation Bridge;multi-processor simulation framework;embedded systems;system partitioning;HW-SW interactions verification;software debug;observability;controllability;instruction level simulations;gate level simulation;C based multi-processor instruction set simulator framework;SimBridge;performance analysis;TMS320C54x-based simulators","","6","12","","","","","","IEEE","IEEE Conferences"
"A hybrid approach to object library classification and retrieval","Chao-Tsun Chang; Chung-Shyan Liu","Dept. of General Sci., Air Commun. & Electron. Sch., Kaohsiung, Taiwan; NA","Proceedings Nineteenth Annual International Computer Software and Applications Conference (COMPSAC'95)","","1995","","","278","283","A hybrid approach, which combines a faceted scheme and free test analysis, is proposed for classifying and retrieving reusable software components. In our approach, the facet scheme is extended by associating a set of index words, instead of just one, with each facet. The index words are extracted using free test analysis and are ranked by their significance. We also conducted two simple experiments to evaluate the effectiveness of our approach and to determine the factors that contribute to the effectiveness of retrieval. We also found that, free test analysis may still introduce noise, even after two cutoffs are used to remove irrelevant index words. Thus, it may be necessary that, after free test analysis, a filtering phase should be performed for improving retrieval effectiveness.","0730-3157","0-8186-7119","10.1109/CMPSAC.1995.524791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=524791","","Libraries;Text analysis;Filtering;Software performance;Vocabulary;Frequency;Chaotic communication;Science - general;Software reusability;Productivity","software libraries;libraries;software reusability;noise;filtering theory;object-oriented programming","hybrid approach;object library classification;object library retrieval;faceted scheme;free test analysis;reusable software components;index words;index word significance;noise;cutoffs;irrelevant index word removal;filtering phase;retrieval effectiveness","","1","13","","","","","","IEEE","IEEE Conferences"
"A formal techniques environment for telecommunications software","R. Reed; J. De Man; B. Moller-Pedersen","NA; NA; NA","Seventh International Conference on Software Engineering for Telecommunication Switching Systems, 1989. SETSS 89.","","1989","","","6","11","The pre-competitive European Community RACE programme is designed to lead to Integrated Broadband Communications (IBC). The paper describes the work of the SPECS (Specification and Programming Environment for Communications Software) project which has the objective to provide maximum automation and optimisation of the whole software process from requirements through specification, design, implementation, test, execution and maintenance. The basis for the methodology is the application of formal methods. The emphasis of the paper is on the methodology and architecture of supporting tools which are under development within the SPECS project.<<ETX>>","","0-85296-381","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=41838","","Broadband communication;Software requirements and specifications","broadband networks;formal specification;project support environments","software tools;formal techniques environment;telecommunications software;European Community;RACE programme;Integrated Broadband Communications;IBC;SPECS;Specification and Programming Environment for Communications Software;automation;optimisation;requirements;specification;design;implementation;test;execution;maintenance;formal methods","","","","","","","","","IET","IET Conferences"
"Proceedings 16th International Conference on VLSI Design concurrently with the 2nd International Conference on Embedded Systems Design","","","16th International Conference on VLSI Design, 2003. Proceedings.","","2003","","","","","The following topics are dealt with: analog and RF devices; physical design; FPGA; MOS technology; ATPG and DFT; VLSI processors; memory technology; verification and synthesis; security; low power technologies; test optimization; system-on-a-chip; coupling effects; power estimation and control; high-level synthesis; device testing; reconfigurable system software.","1063-9667","0-7695-1868","10.1109/ICVD.2003.1183099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183099","","Very-large-scale integration;Analog integrated circuits;Integrated circuit design;Field programmable gate arrays;MOS integrated circuits;CMOS integrated circuits;Design for testability;Digital signal processors;Semiconductor memories;Integrated circuit testing;High-level synthesis","VLSI;analogue integrated circuits;radiofrequency integrated circuits;integrated circuit design;field programmable gate arrays;MOS integrated circuits;CMOS integrated circuits;automatic test pattern generation;design for testability;digital signal processing chips;integrated memory circuits;formal verification;low-power electronics;integrated circuit testing;system-on-chip;high level synthesis;embedded systems","VLSI design;analog devices;RF devices;physical design;FPGA;MOS technology;ATPG;DFT;VLSI processors;memory technology;verification;security;low power technologies;test optimization;system-on-a-chip;coupling effects;power estimation;power control;high-level synthesis;device testing;reconfigurable system software","","","","","","","","","IEEE","IEEE Conferences"
"A novel computerized multiharmonic active load-pull system for the optimization of high efficiency operating classes in power transistors","F. Blache; J. M. Nebus; P. Bouysse; J. P. Villotte","IRCOM, Limoges Univ., France; IRCOM, Limoges Univ., France; IRCOM, Limoges Univ., France; IRCOM, Limoges Univ., France","Proceedings of 1995 IEEE MTT-S International Microwave Symposium","","1995","","","1037","1040 vol.3","A fully automated multiharmonic load-pull system allowing accurate measurement and control of the first three harmonic load terminations of RF and microwave transistors is presented in this paper. The technical originality of the proposed system lies in that the first, second and third harmonic load terminations can be independently and automatically monitored and fixed while varying the input power driving the transistor at the fundamental frequency. Appropriate hardware and software allow fast and automatic plots of power/efficiency performances of DUTs versus input power for different harmonic loadings. To demonstrate an attractive application of the system, measurements of a 1800 /spl mu/m gate periphery MESFET at 1.8 GHz for mobile communication applications are presented. Both suitable harmonic load terminations and nonappropriate ones yielding respectively optimum and poor power added efficiency are given.<<ETX>>","0149-645X","0-7803-2581","10.1109/MWSYM.1995.406149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=406149","","Power system harmonics;Microwave transistors;Application software;Microwave measurements;Automatic control;Control systems;Radio frequency;Computerized monitoring;Hardware;Software performance","microwave power transistors;UHF transistors;semiconductor device testing;automatic test equipment;harmonics;UHF measurement;microwave measurement;power transistors","computerized multiharmonic active load-pull system;high efficiency operating classes;optimization;power transistors;harmonic load terminations;RF transistors;power/efficiency performances;MESFET;microwave transistors","","23","8","","","","","","IEEE","IEEE Conferences"
"Yield and cost estimation for a CAM based parallel processor","W. B. Noghani; I. P. Jalowiecki","Dept. of Electr. Eng., Brunel Univ., Uxbridge, UK; Dept. of Electr. Eng., Brunel Univ., Uxbridge, UK","Records of the 1995 IEEE International Workshop on Memory Technology, Design and Testing","","1995","","","110","116","A comprehensive model is developed to estimate yield values for an associative string processor (ASP) chip which is populated with content addressable memory (CAM). The yield model comprises analysis of row and column redundancy strategies for the CAM combined with floor planning of the processor architecture. At the end, a cost model is developed, based on some actual fabrication costs, in order to optimise the processor according to a suitable figure of merit.","","0-8186-7102","10.1109/MTDT.1995.518091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=518091","","Yield estimation;Computer aided manufacturing;CADCAM;Application specific processors;Associative memory;Cost function;Computer vision;Logic arrays;Computer architecture;Application software","content-addressable storage;costing;parallel machines;circuit optimisation;integrated circuit yield;microprocessor chips;redundancy;integrated circuit layout;parallel architectures;memory architecture","yield value estimation;associative string processor chip;content addressable memory;CAM based parallel processor;cost estimation;row redundancy strategies;column redundancy strategies;floor planning;processor architecture;fabrication costs;processor optimization;figure of merit","","1","5","","","","","","IEEE","IEEE Conferences"
"Structured Programming Applied to Equipment Testing","J. Towaley","Masschusettes Computer Associates","Computer","","1975","8","6","68","68","One of the loveliest uses of SP I have come across is in vehicle testing. The Army particularly makes use of computers and automatic test equipment to mechanically test out new vehicles and uncover malfunctions. The programs for automatic test equipment are such that significant economic gain can be achieved in their development, documentation, and maintenance, provided appropriate language and system features are available.","0018-9162;1558-0814","","10.1109/C-M.1975.218996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1649472","","System testing;Constraint optimization;Runtime;Systems engineering and theory;Programming;Automatic programming;Automatic testing;Distributed computing;Software design","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Cecil: a sequencing constraint language for automatic static analysis generation","K. M. Olender; L. J. Osterweil","Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA; Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA","IEEE Transactions on Software Engineering","","1990","16","3","268","280","A flexible and general mechanism for specifying problems relating to the sequencing of events and mechanically translating them into dataflow analysis algorithms capable of solving those problems is presented. Dataflow analysis has been used for quite some time in compiler code optimization. Most static analyzers have been custom-built to search for fixed and often quite limited classes of dataflow conditions. It is shown that the range of sequences for which it is interesting and worthwhile to search in actually quite broad and diverse. A formalism for specifying this diversity of conditions is created. It is shown that these conditions can be modeled essentially as dataflow analysis problems for which effective solutions are known. It is also shown how these solutions can be exploited to serve as the basis for mechanical creation of analyzers for these conditions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.48935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48935","","Software quality;Computer science;Data analysis;Software tools;Software testing;Security;Software engineering;Algorithm design and analysis;Optimizing compilers;Automata","automatic programming;parallel programming;program compilers;specification languages","Cecil;sequencing constraint language;automatic static analysis generation;general mechanism;dataflow analysis algorithms;compiler code optimization;custom-built;dataflow conditions;dataflow analysis problems","","38","34","","","","","","IEEE","IEEE Journals & Magazines"
"A new software tool for economical optimisation of low voltage networks","F. Provoost","NA","12th International Conference on Electricity Distribution, 1993. CIRED","","1993","6","","6.27/1","6.27/4 vol.6","The author describes a software tool giving support when planning the most economical low voltage distribution network. An easy-to-use software tool has been developed in order to simplify the design of the low voltage network. Based on certain pre-conditions like cost of cables, type of cables to be used, maximum voltage drop and cost of losses, the program calculates the right cables to obtain the most economical network, i.e. the total cost of construction and losses is minimised. Further, the program can test the network on several safety requirements (like contact voltage) and it enables the user to modify the network in order to meet these requirements. As the program is to be used by people who are not familiar with computers, attention is payed to user-friendliness. The program is menu driven and can be run by using only a few keys or a mouse, which reduces the risk of making errors and makes the program user-friendly.<<ETX>>","","0-85296-561","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=225796","","Power system planning","distribution networks;power engineering computing;power system planning","software tool;economical optimisation;low voltage networks;distribution network;safety;contact voltage;user-friendliness;menu driven","","1","","","","","","","IET","IET Conferences"
"Studying storage-recomputation tradeoffs in memory-constrained embedded processing","M. Kandemir; Feihul Li; Guilin Chen; Guangyu Chen; O. Ozturk","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","Design, Automation and Test in Europe","","2005","","","1026","1031 Vol. 2","Fueled by an unprecedented desire for convenience and self-service, consumers are embracing embedded technology solutions that enhance their mobile lifestyles. Consequently, we witness an unprecedented proliferation of embedded/mobile applications. Most of the environments that execute these applications have severe power, performance, and memory space constraints that need to be accounted for. In particular, memory limitations can present serious challenges to embedded software designers. The current solutions to this problem include sophisticated packaging techniques and code optimizations for effective memory utilization. While the first solution is not scalable, the second one is restricted by intrinsic data dependences in the code that prevent code restructuring. In this paper, we explore an alternate approach for reducing memory space requirements of embedded applications. The idea is to re-compute the result of a code block (potentially multiple times) instead of storing it in memory and performing a memory operation whenever needed. The main benefit of this approach is that it reduces memory space requirements, that is, no memory space is reserved for storing the result of the code block in question.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395725","","Application software;Space technology;Embedded software;Packaging;Data structures;Computer science;Mobile computing;Memory management;Software design;Embedded system","embedded systems;optimisation;storage management;software engineering","storage-recomputation tradeoffs;memory-constrained embedded processing;consumer convenience;embedded technology;mobile lifestyles;embedded mobile applications;power constraints;performance constraints;memory space constraints;embedded software design;packaging techniques;code optimization;memory utilization;intrinsic data dependences;code restructuring;code block result re-computation;memory operation","","6","8","","","","","","IEEE","IEEE Conferences"
"Switching time measurement and optimization issues in GNU Quagga routing software","V. Eramo; M. Listanti; A. Cianfrani","Dept. of INFOCOM, Rome Univ., Italy; Dept. of INFOCOM, Rome Univ., Italy; Dept. of INFOCOM, Rome Univ., Italy","GLOBECOM '05. IEEE Global Telecommunications Conference, 2005.","","2005","2","","6 pp.","732","OSPF (open shortest path first) is a widely used intra-domain routing protocol in IP networks. Processing delays in OSPF implementations impact the time needed for both intra-domain and inter-domain routing to reconverge after a topology change. In this paper we introduce a performance index, referred to as the switching time, allowing the router reconverge to be characterized when network topology changes occur. We propose a test methodology in order to measure the introduced performance index in router realized with the personal computer hardware and equipped with Quagga, the most used open routing software. An optimization of the Quagga routing software has been performed and better performance has been obtained in terms of switching time","1930-529X","0-7803-9414","10.1109/GLOCOM.2005.1577736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1577736","","Time measurement;Network topology;Performance analysis;Software performance;Routing protocols;IP networks;Delay effects;Software testing;Software measurement;Microcomputers","IP networks;routing protocols;telecommunication network topology","switching time measurement;GNU Quagga routing software;open shortest path first;intra-domain routing protocol;IP networks;personal computer hardware;network topology","","2","14","","","","","","IEEE","IEEE Conferences"
"Efficient strategies for integration and regression testing of OO systems","T. Jeron; J. -. Jezequel; Y. Le Traon; P. Morel","INISA, INRIA, Rennes, France; NA; NA; NA","Proceedings 10th International Symposium on Software Reliability Engineering (Cat. No.PR00443)","","1999","","","260","269","We present a model, a strategy and a methodology for planning integration and regression testing from an object oriented (OO) model. We show how to produce a model of structural system test dependencies which evolves with the refinement process of the OO design. The model, that is the test dependency graph, serves as a basis for ordering classes and methods to be tested for regression and integration purposes (minimization of test stubs). The mapping from UML to the defined model is detailed as well as the test methodology. While the complexity of optimal stub minimization is exponential with the size of the model, an algorithm which computes a strategy for integration testing with a quadratic complexity is detailed. This algorithm provides an efficient testing order for minimizing the number of stubs. A comparison is given of various integration strategies with the proposed optimized algorithm (a real-world case study illustrates this comparison). The results of the experiment seem to give nearly optimal stubs with a low cost despite the exponential complexity of getting optimal stubs.","1071-9458","0-7695-0443","10.1109/ISSRE.1999.809331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809331","","System testing;Unified modeling language;Strategic planning;Read only memory;Reactive power;Cost function;Standardization;Process planning;Refining;Minimization methods","program testing;planning;design for testability;object-oriented programming","OO systems;integration testing;object oriented model;structural system test dependencies;refinement process;test dependency graph;UML;optimal stub minimization;quadratic complexity;optimal stubs;design for testability","","4","13","","","","","","IEEE","IEEE Conferences"
"Study of the subjective performance of a range of MPEG-2 encoders","G. Keesman; A. Cotton; D. Kessler; J. De Lameillieure; J. -. Henot; A. Nicoulin; D. Kalivas","Philips Res. Lab., Eindhoven, Netherlands; NA; NA; NA; NA; NA; NA","Proceedings., International Conference on Image Processing","","1995","2","","543","546 vol.2","This contribution represents the results of subjective tests organized in November '94 by the HAMLET subgroup WP2. These tests, carried out in accordance with Rec. ITU-R 500-5, compare the performances of different types of MPEG-2 encoders: (1) the software encoder of the MPEG-2 technical documentation (reference); (2) an exact simulation of the hardware encoder that is being built in the same RACE project, (3) an optimized software encoder and (4) an SNR scalable encoder. The results reveal that the MPEG-2 standard leaves much room for optimization in the encoder. They also illustrate that optimizations carried out for the HAMLET hardware encoder lead to important improvements. Finally, they show the almost comparable quality of the MPEG-2 reference software encoder at 7 Mbit/s and the SNR scalable encoder at 3+4 Mbit/s for the base + enhancement layer.","","0-8186-7310","10.1109/ICIP.1995.537536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537536","","Transform coding;Video compression;Testing;IEC standards;ISO standards;MPEG standards;Bit rate;Hardware;Application software;Decoding","optimisation;telecommunication standards;video coding;data compression","subjective performance;MPEG-2 encoders;HAMLET subgroup WP2;Recommendation ITU-R 500-5;software encoder;hardware encoder;RACE project;optimized software encoder;SNR scalable encoder;base layer;enhancement layer;4 Mbit/s;3 Mbit/s;7 Mbit/s","","","8","","","","","","IEEE","IEEE Conferences"
"Methods for service software design","R. Reed; W. Bouma; M. M. Marques; J. Evans","NA; NA; NA; NA","Eighth International Conference on Software Engineering for Telecommunication Systems and Services, 1992.","","1992","","","127","134","The need to define an appropriate software support was recognised within the European RACE I programme, and a group of four projects have contributed to a programming infrastructure (PI). Within the PI the Specification and Programming Environment for Communications Systems (SPECS) project has a primary aim to specify a methodology to provide maximum automation and optimization of software engineering of IBC (integrated broadband communication) software from requirements and specification through design, implementation, test, execution, maintenance and adaptation. The project demonstrates how the objective can be achieved by methods using both formal techniques, informal descriptions and engineering judgement. The architecture and tools are outlined in this paper to present a review of SPECS. This paper gives more detail of PEGS methods for elaborating requirements (and existing designs) towards design oriented definitions.<<ETX>>","","0-85296-542","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145611","","Software requirements and specifications;Programming environments;Communication system software","formal specification;programming environments;telecommunications computing","service software design;European RACE I programme;Specification and Programming Environment for Communications Systems;SPECS;optimization;software engineering;integrated broadband communication;requirements","","1","","","","","","","IET","IET Conferences"
"Statistical criteria to rationalize the choice of run-time observation points in embedded software","P. Bourret; A. Fernandez; C. Seguin","ONERA-DTIM, Toulouse, France; ONERA-DTIM, Toulouse, France; ONERA-DTIM, Toulouse, France","First International Workshop onTestability Assessment, 2004. IWoTA 2004. Proceedings.","","2004","","","41","49","Runtime observation points are fixed once for all in embedded software in aircraft. Thus the selection should optimize the detection, explanation and isolation of all failures or errors that can occur Currently, this selection is based on expert practice. We propose to use statistical criteria to support this choice. In this paper, we present some criteria inspired by information theory and some experiments conducted to evaluate the criteria relevance.","","0-7803-8851","10.1109/IWOTA.2004.1428413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428413","","Runtime;Embedded software;Computer errors;Information theory;Software testing;Condition monitoring;Aircraft;Aerospace safety;Accelerometers;Flow graphs","design for testability;statistical analysis;embedded systems;program testing;aircraft testing;aerospace computing","design-for-testability;statistical criteria;run-time observation points;embedded software;aircraft;information theory","","2","5","","","","","","IEEE","IEEE Conferences"
"Stochastic Bayes measures to compare forecast accuracy of software-reliability models","M. Sahinoglu; J. J. Deely; S. Capar","Dept. of Comput. & Inf. Sci., Troy State Univ., Montgomery, AL, USA; NA; NA","IEEE Transactions on Reliability","","2001","50","1","92","97","ARE (absolute relative error) and SqRE (squared relative error), are random variables that are suggested as measurements of forecast accuracy of the total number of estimated software failures at the end of a mission time. The purpose is to compare the predictive merit of competing software reliability models, an important concern to software reliability analysts. This technique calculates the Bayes probability of how much better the prediction accuracy is for one method relative to a competitor. This novel approach is more realistic, in the assessment of predictive merit, than (a) comparing merely the average values of ARE and SqRE as conventionally done; and (b) conducting statistical hypothesis tests of pair-wise means of ARE and SqRE, an approach somewhat more sensible than (a), because (b) incorporates variability of predicted values, which (a) does not. To implement this technique, first noninformative (across the border) are used and then informative (specified) priors. For the informative case, half-normal priors are placed on the mean of the ARE or SqRE random variables, because these means are hypothesized to remain peaked around zero relative-error (ideal error percentage). This problem is related to the general problem of ranking usual means discussed in the literature by Berger and Deely (1988), and is a follow-up to an invited research paper presented at ISI-97 by Sahinoglu and Capar (1997).","0018-9529;1558-1721","","10.1109/24.935022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935022","","Stochastic processes;Predictive models;Software reliability;Random variables;Software measurement;Maximum likelihood estimation;Testing;Statistics;Digital arithmetic;Particle measurements","software reliability;Bayes methods;probability;stochastic processes","stochastic Bayes measures;forecast accuracy;software-reliability models;absolute relative error;squared relative error;random variables;estimated software failures;predictive merit;Bayes probability;noninformative priors;informative priors;zero relative-error;ideal error percentage","","11","19","","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis of the ATLAS Second-Level Trigger software","J. A. C. Bogaerts; D. R. Botterill; Weidong Li; R. P. Middleton; P. Werner; R. J. Wickens; H. Zobernig","CERN, Geneva, Switzerland; NA; NA; NA; NA; NA; NA","IEEE Transactions on Nuclear Science","","2002","49","2","383","388","In this paper, we analyze the performance of the prototype software developed for the ATLAS Second-Level Trigger. The OO framework written in C++ has been used to implement a distributed system which collects (simulated) detector data on which it executes event selection algorithms. The software has been used on testbeds of up to 100 nodes with various interconnect technologies. The final system will have to sustain traffic of /spl sim/40 Gb/s and require an estimated number of /spl sim/750 processors. Timing measurements are crucial for issues such as trigger decision latency, assessment of required CPU and network capacity, scalability, and load-balancing. In addition, final architectural and technological choices, code optimization, and system tuning require a detailed understanding of both CPU utilization and trigger decision latency. In this paper, we describe the instrumentation used to disentangle effects due to such factors as OS system intervention, blocking on interlocks (applications are multithreaded), multiple CPUs, and I/O. This is followed by an analysis of the measurements and concluding with suggestions for improvements to the ATLAS Trigger/DAQ dataflow components in the next phase of the project.","0018-9499;1558-1578","","10.1109/TNS.2002.1003742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003742","","Performance analysis;Software performance;Delay;Software prototyping;Discrete event simulation;Event detection;Detectors;Software testing;Telecommunication traffic;Traffic control","timing circuits;input-output programs;position sensitive particle detectors;trigger circuits;high energy physics instrumentation computing;nuclear electronics;readout electronics;data acquisition;interconnected systems;optimisation;particle calorimetry","performance analysis;distributed system;detector data;interconnect technology;timing measurements;trigger decision latency;network capacity;scalability;load-balancing;code optimization;system tuning;I/O;data communication;DAQ;data acquisition;ATLAS second-level trigger software","","","2","","","","","","IEEE","IEEE Journals & Magazines"
"Graph modeling of parallelism in superscalar architecture-a case study of HP PA-RISC microprocessor","M. Malek; P. Chillakanti; S. A. Shaikh; A. Sinha","Ins. fur Inf., Humboldt-Univ., Berlin, Germany; NA; NA; NA","Proceedings of SOUTHEASTCON '96","","1996","","","641","648","Extracting instruction level parallelism is a key issue in superscalar architecture. We propose a graph theoretic model to identify parallelism in a sequence of instructions. The system graph model (SGM), presented, is a fundamental source of information regarding the resource dependencies, intra-instruction and inter-instruction parallelism, and the cost-performance of the architecture. Additionally, we propose a new technique called hierarchical identification of parallelism (HIP), which is a systematic approach to identify parallelism. Using this technique, an optimizing compiler can obtain a better static schedule of the assembly level instructions. For better understanding of the techniques developed, we have presented a case study of Hewlett-Packard's PA-RISC microprocessor. Finally, we discuss potential application of the proposed graph model in architectural level testing.","","0-7803-3088","10.1109/SECON.1996.510149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=510149","","Computer aided software engineering;Parallel processing;Reduced instruction set computing;Testing;Computer architecture;Microprocessors;Optimizing compilers;Hip;Hardware;Data mining","parallel architectures;reduced instruction set computing;instruction sets;graph theory;microprocessor chips;optimising compilers;computer testing;integrated circuit testing","superscalar architecture;HP PA-RISC microprocessor;instruction level parallelism;graph theoretic model;system graph model;resource dependencies;intrainstruction parallelism;inter-instruction parallelism;interinstruction parallelism;cost-performance;hierarchical identification of parallelism;optimizing compiler;static schedule;assembly level instructions;Hewlett-Packard;architectural level testing","","","13","","","","","","IEEE","IEEE Conferences"
"FPGA-based conformance testing and system prototyping of an MPEG-4 SA-DCT hardware accelerator","A. Kinane; A. Casey; V. Muresan; N. O'Connor","Centre for Digital Video Process., Dublin City Univ., Ireland; Centre for Digital Video Process., Dublin City Univ., Ireland; Centre for Digital Video Process., Dublin City Univ., Ireland; Centre for Digital Video Process., Dublin City Univ., Ireland","Proceedings. 2005 IEEE International Conference on Field-Programmable Technology, 2005.","","2005","","","317","318","Two FPGA implementations of a shape adaptive discrete cosine transform (SA-DCT) accelerator are presented in this paper: one PCI-based and the other AMBA-based. The former is used for conformance testing with the MPEG-4 standard requirements. The latter is an alternative platform for system prototyping and has an architecture more representative of a mobile device. The proposed accelerator meets real time constraints on both platforms with a gate count of approximately 40k, and outperforms the optimised reference software implementation by 20times. It is estimated that the accelerator consumes 250mW on a Virtex-E FPGA and 79mW on a Virtex-II FPGA in the worst case scenario","","0-7803-9407","10.1109/FPT.2005.1568579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1568579","","System testing;Prototypes;MPEG 4 Standard;Hardware;Life estimation;Field programmable gate arrays;Shape;Discrete cosine transforms;Software prototyping;Computer architecture","conformance testing;discrete cosine transforms;field programmable gate arrays;logic design;transform coding;video coding","FPGA implementations;shape adaptive discrete cosine transform;MPEG-4 standard;PCI-based implementation;AMBA-based implementation;gate count;Virtex-E FPGA;Virtex-II FPGA;SA-DCT hardware accelerator;79 mW;250 mW","","3","3","","","","","","IEEE","IEEE Conferences"
"An integrated diagnostics virtual test bench for life cycle support","K. Cavanaugh","Qualtech Syst. Inc., Wethersfield, CT, USA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","7","","7","3246 vol.7","Qualtech Systems, Inc. (QSI) has developed an architecture that utilizes the existing TEAMS (Testability Engineering and Maintenance Systems) integrated tool set as the foundation to a computing environment for modeling and rigorous design analysis. This architecture is called a Virtual Test Bench (VTB) for Integrated Diagnostics. The VTB approach addresses design for testability, safety, and risk reduction because it provides an engineering environment to develop/provide: 1. Accurate, comprehensive, and graphical model based failure mode, effects and diagnostic analysis to understand failure modes, their propagation, effects, and ability of diagnostics to address these failure modes. 2. Optimization of diagnostic methods and test sequencing supporting the development of an effective mix of diagnostic methods. 3. Seamless integration from analysis, to run-time implementation, to maintenance process and life cycle support. undetected fault lists, ambiguity group lists, and optimized diagnostic trees. 4. A collaborative, widely distributed engineering environment to ""ring-out"" the design before it is built and flown. The VTB architecture offers an innovative solution in a COTS package for system/component modeling, design for safety, failure mode/effect analysis, testability engineering, and rigorous integration/testing of the IVHM (Integrated Vehicle Health Management) function with the rest of the vehicle. The VTB approach described in this paper will use the TEAMS software tool to generate detailed, accurate ""failure"" models of the design, assess the propagation of the failure mode effects, and determine the impact on safety, mission and support costs. It will generate FMECA, mission reliability assessments, incorporate the diagnostic and prognostic test designs, and perform testability analysis. Diagnostic functions of the VTB include fault detection and isolation metrics undetected fault lists, ambiguity group lists, and optimized diagnostic trees.","","0-7803-6599","10.1109/AERO.2001.931400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931400","","Life testing;Design engineering;System testing;Vehicle safety;Computer architecture;Failure analysis;Automotive engineering;Systems engineering and theory;Design for testability;Risk management","design for testability;failure analysis;fault diagnosis;aerospace testing;maintenance engineering;condition monitoring","integrated diagnostics virtual test bench;Qualtech Systems;TEAMS;design for testability;risk reduction;failure mode;diagnostic analysis;test sequencing;run-time implementation;maintenance process;life cycle support;COTS package;IVHM;integrated vehicle health management;support costs;mission reliability assessments;optimized diagnostic trees","","1","17","","","","","","IEEE","IEEE Conferences"
"Evaluation of students' knowledge in theoretical basis of optoelectronics","T. Pencheva; T. Yankova","Phys. Dept., Univ. of Russe, Bulgaria; Phys. Dept., Univ. of Russe, Bulgaria","27th International Spring Seminar on Electronics Technology: Meeting the Challenges of Electronics Technology Progress, 2004.","","2004","2","","361","365 vol.2","The investigation deals with some important problems connected with evaluation and rating of student's knowledge using computer-aided tests which may be used both for regular education and for e-learning. Two alternative approaches for test organization are applied. Their potentialities are compared by means of tests in the 'theoretical basis of optoelectronics', with alternative arrangements. A test rating of students' knowledge is carried out using a specialized software application. The experiments are carried out with 5 control groups of students from the electrical engineering faculties of the University of Rousse. The presented results are summarised and discussed. The results may be used for optimisation and improvement of such tests. They are necessary for further development of the student's logical thinking, for their future professional realisation in the area of electronics.","","0-7803-8422","10.1109/ISSE.2004.1490452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1490452","","Electronic equipment testing;Electronic learning;Automatic testing;Materials testing;Educational institutions;Computer science education;Software testing;Application software;Logic testing;Electronic switching systems","electronic engineering education;optoelectronic devices;computer aided instruction;distance learning","student knowledge evaluation;optoelectronics theory;computer-aided tests;e-learning;test organization;student knowledge test rating;electronics education;test control","","","6","","","","","","IEEE","IEEE Conferences"
"Program slicing with dynamic points-to sets","M. Mock; D. C. Atkinson; C. Chambers; S. J. Eggers","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","8","657","678","Program slicing is a potentially useful analysis for aiding program understanding. However, in reality even slices of small programs are often too large to be useful. Imprecise pointer analyses have been suggested as one cause of this problem. In this paper, we use dynamic points-to data, which represents optimistic pointer information, to obtain a bound on the best case slice size improvement that can be achieved with improved pointer precision. Our experiments show that slice size can be reduced significantly for programs that make frequent use of calls through function pointers because for them the dynamic pointer data results in a considerably smaller call graph, which leads to fewer data dependences. Programs without or with only few calls through function pointers, however, show considerably less improvement. We discovered that C programs appear to have a significant fraction of direct and nonspurious pointer data dependences so that reducing spurious dependences via pointers is only of limited benefit. Consequently, to make slicing useful in general for such programs, improvements beyond better pointer analyses are necessary. On the other hand, since we show that collecting dynamic function pointer information can be performed with little overhead (average slowdown of 10 percent for our benchmarks), dynamic pointer information may be a practical approach to making slicing of programs with frequent function pointer use more successful in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498771","Index Terms- Dynamic analysis;points-to analysis;program slicing.","Data analysis;Computer Society;Application software;Programming profession;Debugging;Software maintenance;Software testing;Reverse engineering;Computer languages;Information analysis","program slicing;reverse engineering;software cost estimation","program slicing;optimistic pointer information;function pointers;call graph;data dependences;C programs;dynamic function pointer information;dynamic points-to set analysis;program understanding","","20","61","","","","","","IEEE","IEEE Journals & Magazines"
"Syntax-directed construction of Value Dependence Graphs","D. Byers; M. Kamkar; T. Palsson","Linkoping Univ., Sweden; NA; NA","Proceedings IEEE International Conference on Software Maintenance. ICSM 2001","","2001","","","692","703","Most software analysis and assessment techniques operate on graph representations of the target software. One of the most common representations is the Program Dependence Graph in one of its many variations. The Value Dependence Graph is an alternative that is more suitable for many types of analyses, including static slicing and many code transformations, such as instruction scheduling and invariant analysis. The authors present a new algorithm that builds Value Dependence Graphs from the parse tree of a program. By generating the Value Dependence Graph from the parse tree, our algorithm is significantly simpler and clearer than previously published algorithms.","1063-6773","0-7695-1189","10.1109/ICSM.2001.972788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972788","","Shape;Tree graphs;Software algorithms;Flow graphs;Algorithm design and analysis;Software metrics;Software testing;Program processors;Optimizing compilers;Calculus","scheduling;trees (mathematics);computational linguistics;program compilers;program slicing","syntax-directed construction;Value Dependence Graphs;software analysis;graph representations;target software;Program Dependence Graph;static slicing;code transformations;instruction scheduling;invariant analysis;parse tree","","1","15","","","","","","IEEE","IEEE Conferences"
"Cost-benefit analysis of electric power system reliability","K. N. Tinnium; P. Rastgoufard; P. F. Duvoisin","Electr. Power Res. Lab., Tulane Univ., New Orleans, LA, USA; Electr. Power Res. Lab., Tulane Univ., New Orleans, LA, USA; Electr. Power Res. Lab., Tulane Univ., New Orleans, LA, USA","Proceedings of 26th Southeastern Symposium on System Theory","","1994","","","468","472","The purpose of this investigation is to determine an appropriate cost-benefit formula that will help the power system planners in prioritizing transmission system projects. This paper deals with describing the value of increased reliability and security in bulk power systems. Three different approaches used for prioritization of transmission system projects by the electric utilities are discussed and analyzed for two different transmission system alternatives. Utilizing the best approach, transmission alternatives are prioritized and the best alternative is placed on top of the prioritized table. An analysis of the three approaches and a relative comparison is performed on the IEEE 25 bus Reliability Test System. TRELSS (Transmission Reliability Evaluation of Large Scale Systems), a software package developed by EPRI, is utilized in determining the probabilistic indices that are used in the proposed approach.<<ETX>>","0094-2898","0-8186-5320","10.1109/SSST.1994.287831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=287831","","Cost benefit analysis;Power system reliability;Power system security;Power system analysis computing;Power industry;Performance analysis;Performance evaluation;System testing;Large-scale systems;Software packages","power system reliability;power system analysis computing;costing;economics","electric power system reliability;cost-benefit analysis;power system planners;security;bulk power systems;transmission system projects prioritisation;electric utilities;IEEE 25 bus Reliability Test System;TRELSS;Transmission Reliability Evaluation of Large Scale Systems;software package;probabilistic indices","","2","9","","","","","","IEEE","IEEE Conferences"
"HW/SW partitioned optimization and VLSI-FPGA implementation of the MPEG-2 video decoder","M. Verderber; A. Zemva; D. Lampret","Fac. of Electr. Eng., Univ. of Ljubljana, Slovenia; Fac. of Electr. Eng., Univ. of Ljubljana, Slovenia; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","238","243 suppl.","In this paper, we propose an optimized real-time MPPEG-2 video decoder. The decoder has been implemented in one FPGA device as a HW/SW partitioned system. We carried out time/power-consumption analysis and optimization of the MPEG-2 decoder. On the basis of the achieved results, we decided for HW implementation of the IDCT and VTD algorithms. Remaining parts were realized in SIV with 32-bit RISC processor. MPEG-2 decoder (RISC processor, IDCT core, VLD core) has been described in Verilog/VHDL and implemented in Virtex 1600E FPGA. Finally, the decoder has been tested on the Flextronics prototyping board.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253835","","Decoding;Transform coding;Field programmable gate arrays;ISO standards;IEC standards;Partitioning algorithms;Reduced instruction set computing;Testing;Video compression;Hardware","hardware-software codesign;VLSI;decoding;video coding;data compression;discrete cosine transforms;field programmable gate arrays;reduced instruction set computing;code standards;digital signal processing chips;timing;circuit optimisation;integrated circuit design;circuit CAD","HW/SW partitioned optimization;VLSI-FPGA implementation;MPEG-2 video decoder;real-time video decoder;time/power-consumption analysis;IDCT algorithm;inverse DCT;VTD algorithm;RISC processor;IDCT core;VLD core;Verilog/VHDL;Virtex 1600E FPGA;Flextronics prototyping board;32 bit","","7","10","","","","","","IEEE","IEEE Conferences"
"Global multiprocessor scheduling of aperiodic tasks using time-independent priorities","L. Lundberg; H. Lennerstad","Dept. of Software Eng. & Comput. Sci., Blekinge Inst. of Technol., Ronneby, Sweden; Dept. of Software Eng. & Comput. Sci., Blekinge Inst. of Technol., Ronneby, Sweden","The 9th IEEE Real-Time and Embedded Technology and Applications Symposium, 2003. Proceedings.","","2003","","","170","180","We provide a constant time schedulability test for a multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing the tasks in two priority classes based on task utilization: heavy and light. We prove that if the load on the multiprocessor server stays below U/sub threshold/ = 3 - /spl radic/7 = 35.425%, the server can accept incoming aperiodic tasks and guarantee that the deadlines of all accepted tasks will be met. 35.425% utilization is also a threshold for a task to be characterized as heavy. The bound U/sub threshold/ = 3 - /spl radic/7 = 35.425% is easy-to-use, but not sharp if we know the number of processors in the multiprocessor. For a server with m processors, we calculate a formula for the sharp bound U/sub threshold/(m), which converges to Uthreshold from above as m - -. The results are based on a utilization function u/sub m/(x) = 2(1 - x)/(2 + /spl radic/(2 + 2x)) + x/m. By using this function, the performance of the multiprocessor can in some cases be improved beyond U/sub threshold/ (m) by paying the extra overhead of monitoring the individual utilization of the current tasks.","1545-3421","0-7695-1956","10.1109/RTTAS.2003.1203049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203049","","Processor scheduling;Delay;Admission control;Software engineering;Computer science;Software testing;Monitoring;Real time systems","processor scheduling;computational complexity;real-time systems","multiprocessor scheduling;time-independent priority;multiprocessor server","","5","7","","","","","","IEEE","IEEE Conferences"
"Dynamic recompilation and profile-guided optimisations for a .NET JIT compiler","K. Vaswani; Y. N. Srikant","Dept. of Comput. Sci. Autom., Indian Inst. of Sci., Bangalore, India; Dept. of Comput. Sci. Autom., Indian Inst. of Sci., Bangalore, India","IEE Proceedings - Software","","2003","150","5","296","302","The paper describes the design and implementation of an adaptive recompilation framework for Rotor, a shared source implementation of the common language infrastructure (CLI) that can increase program performance through intelligent recompilation decisions and optimisations based on the program's past behaviour. Our extensions to Rotor include a low overhead run-time-stack based sampling profiler that identifies program hotspots. At the first level of optimisation, the compiler uses a fast yet effective linear scan algorithm for register allocation. Hot methods can be instrumented to collect basic-block, edge and call-graph profile information. Profile-guided optimisations driven by online profile information are used to further optimise heavily executed methods at the second level of recompilation. An evaluation of the framework using a set of test programs shows that performance can improve by a maximum of 42.3% and by 9% on average. Our results also show that the overheads of collecting accurate profile information through instrumentation to an extent outweigh the benefits of profile-guided optimisations in our implementation, suggesting the need for implementing techniques that can reduce such overheads. A flexible and extensible framework design implies that additional profiling and optimisation techniques can easily be incorporated to further improve performance.","1462-5970","","10.1049/ip-sen:20030986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249340","","","optimising compilers;software performance evaluation;program control structures","dynamic recompilation;profile-guided optimisations;NET JIT compiler;adaptive recompilation framework;Rotor;shared source implementation;common language infrastructure;program performance;CLI;low overhead sampling profiler;run-time-stack based sampling profiler;program hotspots;optimisation;compiler;linear scan algorithm;register allocation;basic-block information;edge information;call-graph profile information;accurate profile information","","1","","","","","","","IET","IET Journals & Magazines"
"Search-based execution-time verification in object-oriented and component-based real-time system development","H. G. Gross; N. Mayer","Software Eng., Fraunhofer Inst. for Exp., Kaiserslautern, Germany; Software Eng., Fraunhofer Inst. for Exp., Kaiserslautern, Germany","Proceedings of the Eighth International Workshop on Object-Oriented Real-Time Dependable Systems, 2003. (WORDS 2003).","","2003","","","113","120","Execution time analysis is an essential verification activity during real-time system construction. This activity can be performed dynamically through search-based analysis techniques such as evolutionary algorithms. Evolutionary algorithms have already been successfully used for execution-time analysis under the traditional procedural development paradigm. This paper describes a first attempt in making search-based execution-time analysis techniques also applicable under the more recent object-oriented and component-based software development paradigms. Here, their application is more difficult compared with the traditional procedural development approaches because object-based architectures are inherently encapsulated, and they often represent state machines. This work proposes a solution that makes search techniques applicable in real-time system development with object technology. It is based on built-in testing artefacts and on the execution and optimization of an object's invocation history through a genetic algorithm. We demonstrate the applicability of the technique through a simple object-oriented real-time system.","","0-7695-1929","10.1109/WORDS.2003.1218073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218073","","Real time systems;Algorithm design and analysis;Evolutionary computation;Performance analysis;Programming;Application software;Computer architecture;Testing;History;Genetic algorithms","object-oriented programming;real-time systems;program diagnostics;software engineering;genetic algorithms","search-based verification;execution-time verification;object-oriented system development;component-based system development;real-time system;execution time analysis;search-based analysis;evolutionary algorithm;search-based execution-time analysis;object-oriented software development;component-based software development;genetic algorithm;state machine;optimization","","1","11","","","","","","IEEE","IEEE Conferences"
"Shortening matching time in OPS5 production systems","J. A. Kang; A. M. K. Cheng","Humax Co. Ltd., South Korea; NA","IEEE Transactions on Software Engineering","","2004","30","7","448","457","A rule-based system must satisfy stringent timing constraints when applied to a real-time environment. As the scale of rule-based expert systems increases, the efficiency of systems becomes a pressing concern. The most critical performance factor in the implementation of a production system is the condition-testing algorithm. We propose a new method based on the widely used RETE match algorithm. We show an approach designed to reduce the response time of rule-based expert systems by reducing the matching time. There are two steps in the method we propose: The first makes an index structure of the tokens to reduce the /spl alpha/-node-level join candidates. The second chooses the highest time tag for certain /spl beta/-nodes to reduce the amount of combinatorial match that is problematical in a real-time production system application. For this purpose, a simple compiler is implemented in C and the response time of test programs is measured.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318606","Matching;knowledge-based systems;expert systems;rule-based systems;OPS5;Rete;response time.","Production systems;Real time systems;Expert systems;Delay;Knowledge based systems;Timing;Pressing;Program processors;Testing;Time measurement","expert systems;real-time systems;program testing;optimising compilers","real-time environment;rule-based expert systems;condition-testing algorithm;RETE match algorithm;/spl alpha/-node-level join candidates;OPS5 production system;program compiler;test programs;knowledge-based systems","","13","35","","","","","","IEEE","IEEE Journals & Magazines"
"Distribution feeder reconfiguration for operation cost reduction","Qin Zhou; D. Shirmohammadi; W. -. E. Liu","San Francisco, CA, USA; NA; NA","IEEE Transactions on Power Systems","","1997","12","2","730","735","This paper describes a new feeder reconfiguration algorithm for the purpose of reducing the distribution network operating cost in a real-time operation environment. The methodology developed is a heuristic-based approach which emphasizes minimizing the cost of operation over a specified time period rather than a fixed operating point. The practical operating concerns of feeder reconfiguration and the coordination with other distribution automation applications are also addressed. The developed algorithm has been implemented as a production grade software. Test results on PG&E distribution feeders show that the performance of this software is efficient and robust.","0885-8950;1558-0679","","10.1109/59.589665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589665","","Automation;Cost function;Application software;Software algorithms;Production;Software testing;Software performance;Robustness;Switches;Real time systems","distribution networks;power system planning;economics;optimisation;heuristic programming","distribution feeder reconfiguration planning;operation cost reduction;reconfiguration algorithm;real-time operation environment;heuristic-based approach;time period;distribution automation;production grade software","","42","10","","","","","","IEEE","IEEE Journals & Magazines"
"A new RSA encryption architecture and hardware implementation based on optimized Montgomery multiplication","A. P. Fournaris; O. Koufopavlou","Electr. & Comput. Eng. Dept., Patras Univ., Greece; Electr. & Comput. Eng. Dept., Patras Univ., Greece","2005 IEEE International Symposium on Circuits and Systems","","2005","","","4645","4648 Vol. 5","RSA is a widely acceptable and well used algorithm in many security applications. Its main mathematical function is the demanding, in terms of speed, operation of modular exponentiation. In this paper a systolic, scalable, redundant carry-save modular multiplier and an RSA encryption architecture are proposed using the Montgomery modular multiplication algorithm. By completely avoiding the transformations from redundant to non-redundant numbers at the intermediate stages of the architectures, the need for addition is eliminated and very interesting results, in terms of clock frequency, throughput and chip covered area, are achieved.","0271-4302;2158-1525","0-7803-8834","10.1109/ISCAS.2005.1465668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465668","","Hardware;Public key cryptography;Clocks;Frequency;Throughput;Computer architecture;Computer security;Application software;Privacy;Testing","public key cryptography;optimisation;carry logic","RSA encryption architecture;hardware implementation;optimized Montgomery multiplication;security;modular exponentiation;systolic multiplier;scalable redundant multiplier;Montgomery modular multiplication algorithm;carry-save modular multiplier;clock frequency;throughput;chip covered area","","21","10","","","","","","IEEE","IEEE Conferences"
"Identification of the parameters of a.c. machines","W. Michalik","Dept. of Electr. Eng., Tech. Univ. Dresden, Germany","Proceedings of First International Caracas Conference on Devices, Circuits and Systems","","1995","","","122","126","Modern developments in electrical drives require methods for exact determination of the parameters of electrical machines. Users are mostly interested by methods for parameter identification with a little additional expenditure in device components, but a higher expenditure in software components is usually accepted. The paper describes a parameter identification method by a.c. induction machines, based on a transient run-up test. No additional load or additional speed measurement are required. The supply voltage and the transient behaviour of the stator current during run-up test have to be measured only. For the parameter identification a nonlinear optimization procedure is used. Practical results with small single-phase capacitor motors and three-phase squirrel-cage induction motors are presented.","","0-7803-2672","10.1109/ICCDCS.1995.499128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=499128","","Capacitors;Testing;Induction motors;Rotors;Parameter estimation;Stators;Torque;Iron;Large Hadron Collider;Intersymbol interference","parameter estimation;machine testing;machine theory;transient analysis;capacitor motors;squirrel cage motors;induction motors;optimisation","electrical machine;parameter identification;AC induction motor;transient run-up test;supply voltage;stator current;nonlinear optimization;single-phase capacitor motor;three-phase squirrel-cage motor","","","5","","","","","","IEEE","IEEE Conferences"
"0RC2DSP: compiler infrastructure supports for VLIW DSP processors","Cheng-Wei Chen; Chung-Lin Tang; Young-Chia Lin; Jenq-Kuen Lee","Dept. of Comput. Sci., National Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., National Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., National Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., National Tsing Hua Univ., Hsinchu, Taiwan","2005 IEEE VLSI-TSA International Symposium on VLSI Design, Automation and Test, 2005. (VLSI-TSA-DAT).","","2005","","","229","232","In this paper, we describe our experiences in deploying ORC infrastructures for a novel 32-bit VLIW DSP processor (known as PAC core), which equips with new architectural features, such as distributed and 'ping-pong' register files. We also present methods in retargeting ORC compilers for PAC VLIW DSP processors. In addition, mechanisms arc proposed to incorporate register allocation policies in the compiler framework for distributed register files in PAC architectures. In the early design stage, several iterations of tuning are needed between architecture and software designs. Our work gives an early estimation of architecture performance so that refinements of architectures are possible with the software feedbacks.","","0-7803-9060","10.1109/VDAT.2005.1500062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1500062","","VLIW;Digital signal processing;Computer architecture;Registers;Open source software;Optimizing compilers;Software design;Software performance;Feedback;Linux","digital signal processing chips;instruction sets;program compilers;integrated circuit design;hardware-software codesign;electronic design automation","0RC2DSP;compiler infrastructure;VLIW DSP processors;ORC infrastructures;PAC core;distributed files;ping-pong register files;register allocation policies;PAC architectures;software design;software feedback;32 bit","","1","13","","","","","","IEEE","IEEE Conferences"
"A multi-application FFT analyzer based on a DSP architecture","G. Betta; C. Liguori; A. Pietrosanto","Dept. of Autom. Electromagn. Inf. Eng. & Ind. Math., Cassino Univ., Italy; NA; NA","IEEE Transactions on Instrumentation and Measurement","","2001","50","3","825","832","A digital signal processor (DSP)-based FFT analyzer capable of adapting its behavior according to specific application requirements was set up. A suitably designed user-interface allows both the specific application to be chosen and its main parameters to be defined. An automatic optimization procedure then reorganizes the instrument configuration in order to meet user requirements. Multi-DSP architecture implementation ensures real-time behavior. The experimental test results indicate that the instrument is capable of optimally operating in a large number of typical applications.","0018-9456;1557-9662","","10.1109/19.930461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=930461","","Digital signal processing;Application software;Signal analysis;Instruments;Frequency;Hardware;Software performance;Digital signal processors;Computer architecture;Software testing","virtual instrumentation;automatic test software;discrete Fourier transforms;digital signal processing chips;frequency-domain analysis;spectral analysis;user interfaces","multi-application FFT analyzer;DSP architecture;user-interface;automatic optimization procedure;user requirements;real-time behavior;automatic test software;DFT;frequency domain analysis;intelligent systems;spectral analysis;tone monitoring","","16","23","","","","","","IEEE","IEEE Journals & Magazines"
"A model for optimizing the assembly and disassembly of electronic systems","P. A. Sandborn; C. F. Murphy","CALCE Electron. Products & Syst. Center, Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Electronics Packaging Manufacturing","","1999","22","2","105","117","This paper presents a methodology that incorporates simultaneous consideration of economic and environmental merit during the virtual prototyping phase of electronic product design. A model that allows optimization of a product life cycle, which includes primary assembly, disassembly, and secondary assembly using a mix of new and salvaged components, is described. Optimizing this particular life cycle scenario is important for products that are leased to customers or subject to product take-back laws. Monte Carlo simulation is used to account for uncertainty in the data, and demonstrates that high-level design and process decisions may be made with a few basic metrics and without highly specific data sets for every material and component used in a product. A web-based software tool has been developed that implements this methodology.","1521-334X;1558-0822","","10.1109/6104.778170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=778170","","Assembly systems;Costs;Manufacturing;Waste materials;Virtual prototyping;Product design;Materials testing;Joining processes;Electronic equipment testing;Design optimization","design for environment;recycling;product development;Monte Carlo methods;rapid prototyping (industrial)","electronic systems;disassembly;life cycle scenario;environmental merit;economic merit;virtual prototyping phase;electronic product design;product life cycle;secondary assembly;primary assembly;Monte Carlo simulation;high-level design;process decisions;web-based software tool","","16","18","","","","","","IEEE","IEEE Journals & Magazines"
"A rapid prototyping framework for audio signal processing algorithms","N. VoB; T. Eisenbach; B. Mertsching","Fac. of Comput. Sci., Electr. Eng. & Math., Paderborn Univ., Germany; Fac. of Comput. Sci., Electr. Eng. & Math., Paderborn Univ., Germany; Fac. of Comput. Sci., Electr. Eng. & Math., Paderborn Univ., Germany","Proceedings. 2004 IEEE International Conference on Field- Programmable Technology (IEEE Cat. No.04EX921)","","2004","","","375","378","We present a rapid-prototyping environment for functional verification and test of digital signal processing algorithms. The environment consists of a Virtex-ll device on a PCI-card and an appropriate generic software backend which is used to pre- and post-process the data and to transfer it to the FPGA and pull the results from it. It is designed to meet real-time requirements by means of interleaving block-transfers to and from a large on-board memory. We use the system for the development and test of audio signal processing applications. The implementation and test of the gammatone-resynthesis algorithm is described as an exemplary algorithm that has been tested within the environment. The presented system is part of a software framework for rapid development of power optimized audio signal processing applications on behavioral level using library elements.","","0-7803-8651","10.1109/FPT.2004.1393303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1393303","","Prototypes;Signal processing algorithms;Testing;Field programmable gate arrays;Hardware;Psychoacoustic models;Software libraries;Signal design;Object oriented modeling;Digital signal processing","audio signal processing;software prototyping;field programmable gate arrays;software libraries;formal verification;hardware-software codesign","audio signal processing algorithms;rapid-prototyping environment;functional verification;Virtex-ll device;PCI-card;FPGA;real-time requirements;block-transfer interleaving;gammatone-resynthesis algorithm;power optimized audio signal processing applications;library elements","","","6","","","","","","IEEE","IEEE Conferences"
"Symmetrical N-port waveguide junction loaded with dielectric sleeve and metallic post","S. P. Yeo; L. Qiao; M. Cheng","Dept. of Electr. Eng., Nat. Univ. of Singapore, Singapore; Dept. of Electr. Eng., Nat. Univ. of Singapore, Singapore; Dept. of Electr. Eng., Nat. Univ. of Singapore, Singapore","IEEE Transactions on Microwave Theory and Techniques","","1995","43","6","1298","1302","The present paper describes the development of a computer model that is able to predict the characteristics of the symmetrical N-port waveguide junction, which has a dielectric sleeve and metallic post inserted concentrically into its central cavity. Computational and experimental test results demonstrate that the resultant software package (which is compact enough to be run on an IBM 486 PC) can yield accuracies of /spl plusmn/0.5% for the scattering parameters of the junction.<<ETX>>","0018-9480;1557-9670","","10.1109/22.390186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=390186","","Waveguide junctions;Loaded waveguides;Dielectrics;Arm;Predictive models;Software testing;Software packages;Scattering parameters;Availability;Optimized production technology","multiport networks;waveguide couplers;cavity resonators;S-parameters;rectangular waveguides;CAD;software packages","symmetrical N-port waveguide junction;dielectric sleeve;metallic post;computer model;central cavity;software package;scattering parameters;rectangular waveguides","","10","17","","","","","","IEEE","IEEE Journals & Magazines"
"Global optimisation methods for choosing the connectivity pattern of N-tuple classifiers","","","2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)","","2004","3","","2263","2266 vol.3","An experimental study on the use of global optimisation methods, such as Genetic Algorithms, Simulated Annealing and Tabu Search, applied to the problem of choosing the connectivity pattern of the N-tuple classifiers is presented. For example, in an experiment, the use of Tabu Search decreased in 17.27% the mean of the classification errors of the networks. In other experiment, the application of Genetic Algorithms not only decreased in 61% the use of memory, but also the mean of the classification errors obtained were lower than the ones initially achieved without this method.","1098-7576","0-7803-8359","10.1109/IJCNN.2004.1380975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380975","","Optimization methods;Genetic algorithms;Informatics;Simulated annealing;Sampling methods;Testing;Application software;Hardware;Computational efficiency;Joining processes","pattern classification;genetic algorithms;simulated annealing;search problems","global optimisation methods;connectivity pattern;N-tuple classifiers;genetic algorithms;simulated annealing;tabu search;classification errors","","3","8","","","","","","IEEE","IEEE Conferences"
"Bounding rollback-recovery of large distributed computation in WAN environment","Jin-Min Yang; Da-Fang Zhang","Coll. of Software, Hunan Univ., ChangSha, China; Coll. of Software, Hunan Univ., ChangSha, China","13th Asian Test Symposium","","2004","","","394","399","In the existing optimistic message logging protocols, the dependency must be tracked in whole system, and all processes are involved in rollback recovery in the event of failure. For large distributed computation in WAN environment with the low available bandwidth and high transmission latency, its fault-free overhead and recovery overhead are outstanding, recovery efficiency decreasing with the scale of system. This paper introduces a three-layer model of large distributed system in WAN environment, and presents a protocol of message dependency tracking based on proxy. Utilizing private proxy to log messages and dependencies, the protocol limits rollback-recovery to a scope called block rather than the entire system, achieving relative low fault-free overhead and fast output commit, as well as improved recovery efficiency and low recovery overhead.","1081-7735","0-7695-2235","10.1109/ATS.2004.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376590","","Distributed computing;Wide area networks;Bandwidth;Protocols;Delay;Internet;Large-scale systems;Local area networks;Message passing;Testing","wide area networks;protocols;system recovery;message passing;software fault tolerance","rollback-recovery;large distributed computation;WAN environment;optimistic message logging protocols;fault-free overhead;recovery overhead;message dependency tracking protocol;private proxy","","1","17","","","","","","IEEE","IEEE Conferences"
"Monitoring programs using rewriting","K. Havelund; G. Rosu","Kestrel Technol., NASA Ames Res. Center, Moffett Field, CA, USA; NA","Proceedings 16th Annual International Conference on Automated Software Engineering (ASE 2001)","","2001","","","135","143","We present a rewriting algorithm for efficiently testing future time Linear Temporal Logic (LTL) formulae on finite execution traces. The standard models of LTL are infinite traces, reflecting the behavior of reactive and concurrent systems which conceptually may be continuously alive. In most past applications of LTL, theorem provers and model checkers have been used to formally prove that down-scaled models satisfy such LTL specifications. Our goal is instead to use LTL for up-scaled testing of real software applications, corresponding to analyzing the conformance of finite traces against LTL formulae. We first describe what it means for a finite trace to satisfy an LTL formula and then suggest an optimized algorithm based on transforming LTL formulae. We use the Maude rewriting logic, which turns out to be a good notation and being supported by an efficient rewriting engine for performing these experiments. The work constitutes part of the Java PathExplorer (JPAX) project, the purpose of which is to develop a flexible tool for monitoring Java program executions.","1938-4300","0-7695-1426","10.1109/ASE.2001.989799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989799","","Logic testing;Java;NASA;Data structures;Heuristic algorithms;Computerized monitoring;Computer science;Application software;Software testing;Engines","temporal logic;system monitoring;rewriting systems;theorem proving","programs monitoring;rewriting;linear temporal logic formulae;theorem provers;model checkers;down-scaled models;software applications;optimized algorithm;Maude rewriting logic;Java PathExplorer project;Java program executions;finite execution traces;standard models;infinite traces","","64","28","","","","","","IEEE","IEEE Conferences"
"Time optimisation of soft real-time virtual instrument design","P. Bilski; W. Winiecki","Inst. of Radioelectronics, Warsaw Univ. of Technol., Poland; Inst. of Radioelectronics, Warsaw Univ. of Technol., Poland","Proceedings of the 21st IEEE Instrumentation and Measurement Technology Conference (IEEE Cat. No.04CH37510)","","2004","3","","2223","2228 Vol.3","The paper presents a detailed description of the Real-Time conditions for the virtual instrumentation. The aspects of the computer measurements system are explained and the soft Real-Time mode is found to be the most likely to achieve. The requirements for it are shown, then the experiments verify the approach, resulting in certain conditions for the designers. The premises for the future research arc also included.","1091-5281","0-7803-8248","10.1109/IMTC.2004.1351533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1351533","","Design optimization;Instruments;Data acquisition;Signal processing;Time measurement;Spectral analysis;Testing;Process design;Application software;Software safety","virtual instrumentation;synchronisation;data acquisition;optimisation;real-time systems","time optimisation;soft real-time virtual instrument design;computer measurements system;soft real-time mode;time limits;multi-buffer measurement configuration;software platform;synchronization conditions","","2","8","","","","","","IEEE","IEEE Conferences"
"A new symbolic method for analog circuit testability evaluation","G. Fedi; A. Luchetta; S. Manetti; M. C. Piccirilli","Dept. of Electron. Eng., Florence Univ., Italy; NA; NA; NA","IEEE Transactions on Instrumentation and Measurement","","1998","47","2","554","565","Testability is a very useful concept in the field of circuit testing and fault diagnosis and can be defined as a measure of the effectiveness of a selected test point set. A very efficient approach for automated testability evaluation of analog circuits is based on the use of symbolic techniques. Different algorithms relying on the symbolic approach have been presented in the past by the authors and in this work noteworthy improvements on these algorithms are proposed. The new theoretical approach and the description of the subsequent algorithm that optimizes the testability evaluation from a computational point of view are presented. As a result, in the computer implementation the roundoff errors are completely eliminated and the computing speed is increased. The program which implements this new algorithm is also presented.","0018-9456;1557-9662","","10.1109/19.744205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744205","","Circuit testing;Analog circuits;Fault diagnosis;Automatic testing;System testing;Roundoff errors;Design engineering;Nonlinear equations;Fault location;Circuit faults","circuit testing;analogue circuits;roundoff errors;automatic test software;symbol manipulation;fault location;Jacobian matrices;polynomial matrices;circuit analysis computing","analog circuit testability evaluation;symbolic method;fault diagnosis;selected test point set effectiveness;automated testability evaluation;computer implementation;roundoff errors elimination;increased computing speed;network functions;Jacobian matrix;polynomial functions;auxiliary matrix;TEST program","","21","19","","","","","","IEEE","IEEE Journals & Magazines"
"A comprehensive study of the backpropagation algorithm and modifications","A. Sidani; T. Sidani","Dept. of Electr. & Comput. Eng., Univ. of Central Florida, Orlando, FL, USA; Dept. of Electr. & Comput. Eng., Univ. of Central Florida, Orlando, FL, USA","Conference Record Southcon","","1994","","","80","84","Many connectionist/neural network learning systems use some derivative of the popular backpropagation (BP) algorithm. BP learning, however, is too slow for many applications. In addition, it scales poorly as tasks become larger and more complex. As a result, researchers in the field have come up with variations and modifications to the original BP learning technique that address the aforementioned issues. This research was conducted to collect a representative sample of BP modifications and compare them against one another. The benchmarks utilized are certain ""toy-problems"" that have been extensively used in the literature. A software package that allows one to experiment with a multitude of BP variations was developed to achieve the desired goal. The modifications are evaluated and cross examined for each task tested. The package provides the means for parameter optimization and allows a user to build hybrid algorithms based on the different functionalities and features of the various modifications.","","0-7803-9988","10.1109/SOUTHC.1994.498919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=498919","","Benchmark testing;Convergence;Supervised learning;Multilayer perceptrons;Neural networks;Learning systems;Application software;Software packages;Packaging;Jacobian matrices","neural nets;backpropagation;performance evaluation;software packages","backpropagation algorithm;neural network;learning technique;benchmarks;software package;parameter optimization;functionalities;Quickprop;Delta-Bar-Delta","","2","6","","","","","","IEEE","IEEE Conferences"
"Investigation of impacts of different three-phase transformer connections and load models on unbalance in power systems by optimization","Hong Ying-Yi; Wang Fu-Ming","Dept. of Electr. Eng., Chung Yuan Univ., Chung Li, Taiwan; NA","IEEE Transactions on Power Systems","","1997","12","2","689","697","This paper investigates the impacts of different three-phase transformer connections and load models on unbalance in power systems. New three-phase transformer models are presented. These models are derived from a new primitive impedance matrix incorporated with positive, negative and zero sequence elements. The constant power and constant impedance load models are discussed. The three-phase models are implemented in a general-purpose optimization software package. Two power systems are used to illustrate the test results. The test results provide assistance to engineers in understanding the effect of different connections on the transformers. This paper also emphasizes that a proper load model should be used; otherwise, an incorrect solution will be obtained.","0885-8950;1558-0679","","10.1109/59.589653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589653","","Phase transformers;Load modeling;Power system modeling;Impedance;Symmetric matrices;System testing;Power engineering and energy;Power transmission lines;Load flow;Gaussian processes","power system stability;power transformers;power system analysis computing;optimisation;electric impedance","power system unbalance;three-phase transformer connections;load models;primitive impedance matrix;positive sequence elements;negative sequence elements;zero sequence elements;computer simulation;software package","","12","14","","","","","","IEEE","IEEE Journals & Magazines"
"Yield methodology-three phases approach","T. Pouedras; Mira Ben-Tzur","Cypress Semicond., San Jose, CA, USA; NA","10th Annual IEEE/SEMI. Advanced Semiconductor Manufacturing Conference and Workshop. ASMC 99 Proceedings (Cat. No.99CH36295)","","1999","","","14","17","A novel yield methodology approach is developed. This methodology is applicable to each of the three phases from technology development to manufacturing. This methodology highlights the goals for each phase and describes all the procedures needed for optimizing the learning rate and reducing the cycle time between technology development and manufacturing.","1078-8743","0-7803-5217","10.1109/ASMC.1999.798171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=798171","","Silicon;Qualifications;Semiconductor device manufacture;Manufacturing processes;Continuous production;Software testing;Optimized production technology;Scanning electron microscopy;Failure analysis;Pareto analysis","integrated circuit yield;production testing;integrated circuit testing;quality control","yield methodology;technology development;semiconductor manufacturing;learning rate optimisation;cycle time reduction;three phases approach;IC manufacture","","1","2","","","","","","IEEE","IEEE Conferences"
"System on silicon, Where are we?","J. Borel","Central Res. & Dev., SGS-Thomson, France","Proceedings of 14th VLSI Test Symposium","","1996","","","XXVI","","Since the early 70's, chip complexity has been continuously growing at a rate that has never slowed down, targeting the gigabit complexities for DRAM around the year 2000. The consequences of such an evolution in the year 1996, is the capability to put several hundred kilobytes of memory on a single microprocessor core with complexities ranging in the 30 million transistors per chip. What is behind such an evolution is clearly two challenges that still have not been met: (a) Designing for such complexities, using system level design methodologies going from the behavioral level down to silicon (that means, software-hardware co-design, advanced floor planning and complex validation approaches). (b) Power conscious design (that means designing with data throughput constraints within the chip and minimizing power consumption with low-voltage, high-speed optimization of the device behavior). Capitalizing on intellectual property in a company will also be of major importance to address the new, emerging markets like multimedia, where experiences from various market segments should be reused in a single system on a chip. The reusability of functions previously designed in heterogeneous technologies will be needed (abstraction from netlist). The economy of system on chip remains to be proven in most of the applications where limited quantities are needed and where programmability may be the only solution to go to.","1093-0167","0-8186-7304","10.1109/VTEST.1996.510826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=510826","","Silicon;Microprocessors;Transistors;System-level design;Power system planning;Throughput;Energy consumption;Constraint optimization;Design optimization;Intellectual property","ULSI;integrated circuit design;circuit CAD;industrial property","system on silicon;chip complexity;system level design methodologies;behavioral level;software-hardware co-design;floor planning;validation approaches;power conscious design;data throughput constraints;power consumption;intellectual property;market segments;programmability","","","","","","","","","IEEE","IEEE Conferences"
"Model-based design and verification of automotive electronics compliant with OSEK/VDX","Guoqing Yang; Minde Zhao; Lei Wang; Zhaohui Wu","Coll. of Comput. Sci., Zhejiang Univ., China; Coll. of Comput. Sci., Zhejiang Univ., China; Coll. of Comput. Sci., Zhejiang Univ., China; Coll. of Comput. Sci., Zhejiang Univ., China","Second International Conference on Embedded Software and Systems (ICESS'05)","","2005","","","7 pp.","","Model-based approaches are gradually applied in embedded system design with Unified Modeling Language (UML) and its profiles, but in terms of automotive electronics domain, few developers adopt UML to design system models because of inadequate tools that support the domain-specific modeling. This paper puts forward a model-based approach for automobile electronics software design and verification with a dependable platform compliant with OSEK/VDX standard. In addition, a case study is presented to demonstrate the application of the approach. The contribution of the approach is threefold. First, the approach applies the theory of model-based design with OSEK/VDX standard in automotive electronics domain. Second, the approach solves the transformation between UML models and OSEK/VDX models through an efficient method. Third, the approach simulates the system models and provides the designer with the results to optimize the design at design-level.","","0-7695-2512","10.1109/ICESS.2005.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609882","","Automotive electronics;Unified modeling language;Embedded system;Embedded software;Software standards;Standards development;Software design;Application software;Design optimization;Electronic equipment testing","automotive electronics;formal verification;Unified Modeling Language;embedded systems;electronic engineering computing;open systems;vehicles;object-oriented programming;object detection;standards","automotive electronics;OSEK;VDX;embedded system design;Unified Modeling Language;automobile electronics software design;automobile electronics software verification","","4","18","","","","","","IEEE","IEEE Conferences"
"An integrated approach for improving cache behavior","G. Memik; M. Kandemir; A. Choudhary; I. Kadayif","Dept. of Electr. Eng., UCLA, Los Angeles, CA, USA; NA; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","796","801","The widening gap between processor and memory speeds renders data locality optimization a very important issue in data-intensive embedded applications. Throughout the years hardware designers and compiler writers focused on optimizing data cache locality using intelligent cache management mechanisms and program-level transformations, respectively. Until now, there has not been significant research investigating the interaction between these optimizations. In this work, we investigate this interaction and propose a selective hardware/compiler strategy to optimize cache locality for integer numerical (array-intensive), and mixed codes. In our framework, the role of the compiler is to identify program regions that can be optimized at compile time using loop and data transformations and to mark (at compile-time) the unoptimizable regions with special instructions that activate/deactivate a hardware optimization mechanism selectively at run-time. Our results show that our technique can improve program performance by as much as 60% with respect to the base configuration and 17% with respect to a non-selective hardware/compiler approach.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253704","","Hardware;Optimizing compilers;Program processors;Runtime;Software performance;Design optimization;Detection algorithms;Algorithm design and analysis;Computer architecture;Prefetching","cache storage;embedded systems;hardware-software codesign;memory architecture","cache behavior;data locality optimization;data-intensive embedded applications;cache locality;loop transformations;data transformations;optimization mechanism;base configuration","","2","13","","","","","","IEEE","IEEE Conferences"
"An optimized fast ambiguity search method for ambiguity resolution on the fly","Yang Gao; J. F. McLellan; J. B. Schleppe","Pulsearch Navigation Syst. Inc., Calgary, Alta., Canada; Pulsearch Navigation Syst. Inc., Calgary, Alta., Canada; Pulsearch Navigation Syst. Inc., Calgary, Alta., Canada","Proceedings of Position, Location and Navigation Symposium - PLANS '96","","1996","","","246","253","Numerous ambiguity search algorithms have been proposed over the last several years and the required computational time for the search has been significantly reduced. In this paper, an optimized ambiguity search method is described. The method has incorporated several existing fast ambiguity search concepts and therefore the ambiguity search speed can be further improved. In addition to the improvement of the ambiguity search speed, special attention has been paid to the improvement of the reliability of the ambiguity search process. High reliability of ambiguity resolution is essential for robust ambiguity fixed GPS positioning and navigation. The search method developed has been implemented into a commercial GPS data processing software package JUPITER and tested with respect to various data sets. Numerical analysis are conducted to demonstrate its efficiency in terms of ambiguity search speed and reliability.","","0-7803-3085","10.1109/PLANS.1996.509085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=509085","","Optimization methods;Search methods;Global Positioning System;Robustness;Navigation;Data processing;Software packages;Jupiter;Software testing;Numerical analysis","Global Positioning System;telecommunication computing;search problems;optimisation;matrix algebra","fast ambiguity search method;ambiguity resolution;GPS;navigation;JUPITER;software package","","1","7","","","","","","IEEE","IEEE Conferences"
"An online signature verification system using hidden Markov model in polar space","H. S. Yoon; J. Y. Lee; H. S. Yang","Comput. Software Tech Lab., ETRI, Taejon, South Korea; Comput. Software Tech Lab., ETRI, Taejon, South Korea; Comput. Software Tech Lab., ETRI, Taejon, South Korea","Proceedings Eighth International Workshop on Frontiers in Handwriting Recognition","","2002","","","329","333","In this paper, we propose a method of online signature verification system using HMM models in a polar coordinate system. In the previous works, the signature verification was performed in terms of a standard X-Y coordinate system. This coordinate system always needs the normalization of signature stroke size and angle variance. To reduce normalization error and computing time, we propose the polar coordinate system. Under the polar coordinate system, the feature sets are robust and independent of size and angle variation according to the user characteristics. The use of HMMs instead of a traditional Euclidian distance metric for the determination of the degree of match between a test signature and a reference signature model brings about a significant improvement in performance as it permits the incorporation of heuristics. The experiments shows that the proposed method gives lower equal error rate 2.2% from 50,000 testing cases.","","0-7695-1692","10.1109/IWFHR.2002.1030931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030931","","Handwriting recognition;Hidden Markov models;Writing;Pattern recognition;Space technology;Testing;Speech recognition;Databases;Sampling methods;Software","handwriting recognition;hidden Markov models;real-time systems;feature extraction;probability;optimisation","online signature verification system;hidden Markov model;polar space;HMM models;polar coordinate system;feature sets;Euclidian distance metric;reference signature model;heuristics;signature database;feature extraction;probability","","9","15","","","","","","IEEE","IEEE Conferences"
"A method for solving trade-off among cost for owned/borrowed resource and loss of business chances","T. Masuishi; K. Mori","Software Div., Hitachi Ltd., Japan; NA","Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.","","2004","","","156","163","This paper proposes a trade-off new problem for resource consumers who join resource-sharing communities to calculate how much resource to be owned from the view point of cost for owing and borrowing resource and loss of business chances. The consumers can borrow resource from the communities when their resource is insufficient. to plan how much resource to be owned and how much resource to be borrowed from the communities. A community model and some resource usage models are assumed to figure out the optimization problem. A cost function is introduced. The function consists of the three kinds of cost/loss for the depreciation time of owned resource. The function value grows when owned resource is too much, and also grows when owned resource is too little. This means that the function value indicates the least point when owned resource is optimal. This paper introduces some assumptions about the character of resource sharing community and the shape of resource usage curve, and shows that the optimization problem can be solved analytically.","1530-2059","0-7695-2094","10.1109/HASE.2004.1281740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281740","","Resource management;Grid computing;Application software;Delay;Cost function;Shape;Internet;TV broadcasting;Displays;Portals","resource allocation;optimisation","resource-sharing;optimization problem;cost function;resource quantity optimisation;resource sharing community model;system architecture;business loss;owned resource;borrowed resource;business chance loss;trade-off solving","","1","14","","","","","","IEEE","IEEE Conferences"
"Compile-Time Analysis of Data List-Format List Correspondences","P. W. Abrahams; L. A. Clarke","Department of Computer Science, New York University; NA","IEEE Transactions on Software Engineering","","1979","SE-5","6","612","617","Formatted input-output is available in a number of programming languages. In the most general case, the correspondence between data items and format items cannot be determined during compilation, and so it is determined dynamically during execution. However, in most pairs of data and format lists that occur in practice, determination of the correspondence is in fact possible during compilation. Although some commercial compilers make this determination, there is little published literature on the subject. In this paper, we briefly examine three areas in which compile-time determination of the data-format correspondence is useful: optimization, program validation, and automatic test data generation. A formalism for stating the problem is given, and a solution is discussed in terms of formal language theory. Using this formalism, an algorithm for determining the correspondence is given, and its application is illustrated by examples in both PL/I and Fortran.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1979.230197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702678","Compilers;formats;input-output;progam optimization;program validation;static program analysis;test data generation","Data analysis;Automatic testing;Computer languages;Formal languages;Programming profession;Couplings;Writing;Computer science;Information science;Program processors","","Compilers;formats;input-output;progam optimization;program validation;static program analysis;test data generation","","1","14","","","","","","IEEE","IEEE Journals & Magazines"
"A software based approach to achieving optimal performance for signature control flow checking","N. J. Warter; W. -. W. Hwu","Center for Reliable High-Performance Comput., Illinois Univ., Urbana, IL, USA; Center for Reliable High-Performance Comput., Illinois Univ., Urbana, IL, USA","[1990] Digest of Papers. Fault-Tolerant Computing: 20th International Symposium","","1990","","","442","449","The authors present a software-based approach that uses run-time program behavior to minimize the performance overhead in signature control flow checking. In general, for both RISC (reduced-instruction-set-computer) and CISC (complex-instruction-set-computer) architectures, it is found that using run-time information can reduce the performance overhead by 50%. For the MC68000, the performance overhead for adding justifying and reference signatures to the program code is approximately 2.8%. In addition to optimizing the performance, the authors' approach does not increase the hardware complexity of the monitor. Furthermore, an O(N/sup 2/) algorithm which inserts justifying signatures on the arcs of the program control flow graph with N nodes is presented. It is shown that the algorithm complexity of previous schemes which insert justifying signatures in the program nodes is exponential.<<ETX>>","","0-8186-2051","10.1109/FTCS.1990.89399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89399","","Software performance;Optimal control;Monitoring;Error correction;Runtime;Flow graphs;Hardware;Algorithm design and analysis;Process design;Delay","computational complexity;fault tolerant computing;logic testing;performance evaluation;reduced instruction set computing","software based approach;optimal performance;signature control flow checking;run-time program behavior;RISC;reduced-instruction-set-computer;CISC;complex-instruction-set-computer;MC68000;program code;program control flow graph;algorithm complexity","","15","20","","","","","","IEEE","IEEE Conferences"
"Empirical performance evaluation methodology and its application to page segmentation algorithms","Song Mao; T. Kanungo","Center for Autom. Res., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2001","23","3","242","256","While numerous page segmentation algorithms have been proposed in the literature, there is lack of comparative evaluation of these algorithms. In the existing performance evaluation methods, two crucial components are usually missing: 1) automatic training of algorithms with free parameters and 2) statistical and error analysis of experimental results. We use the following five-step methodology to quantitatively compare the performance of page segmentation algorithms: 1) first, we create mutually exclusive training and test data sets with groundtruth, 2) we then select a meaningful and computable performance metric, 3) an optimization procedure is then used to search automatically for the optimal parameter values of the segmentation algorithms on the training data set, 4) the segmentation algorithms are then evaluated on the test data set, and, finally, 5) a statistical and error analysis is performed to give the statistical significance of the experimental results. In particular, instead of the ad hoc and manual approach typically used in the literature for training algorithms, we pose the automatic training of algorithms as an optimization problem and use the simplex algorithm to search for the optimal parameter value. A paired-model statistical analysis and an error analysis are then conducted to provide confidence intervals for the experimental results of the algorithms. This methodology is applied to the evaluation of live page segmentation algorithms of which, three are representative research algorithms and the other two are well-known commercial products, on 978 images from the University of Washington III data set. It is found that the performance indices of the Voronoi, Docstrum, and Caere segmentation algorithms are not significantly different from each other, but they are significantly better than that of ScanSoft's segmentation algorithm, which, in turn, is significantly better than that of X-Y cut.","0162-8828;2160-9292;1939-3539","","10.1109/34.910877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910877","","Image segmentation;Optical character recognition software;Error analysis;Measurement;Automatic testing;Statistical analysis;Character recognition;Optimization methods;Training data;Performance evaluation","optical character recognition;statistical analysis;optimisation;image segmentation;document image processing;search problems","empirical performance evaluation methodology;page segmentation algorithms;automatic training;error analysis;statistical analysis;performance metric;optimization procedure;simplex algorithm;paired-model statistical analysis;confidence intervals;Voronoi algorithms;Caere segmentation algorithms;Docstrum segmentation algorithms","","40","38","","","","","","IEEE","IEEE Journals & Magazines"
"Analytical techniques for diagnostic functional allocation","L. A. Stratton; D. C. Doskocil","General Electric Co., Burlington, MA, USA; General Electric Co., Burlington, MA, USA","IEEE Conference on Systems Readiness Technology, 'Advancing Mission Accomplishment'.","","1990","","","465","471","The allocation of maintenance system elements to accomplish the diagnostic mission must be optimized to balance performance, size, weight, and cost. The authors discuss techniques used to create a functional dependency model of a subject system and derive an effectiveness and cost model for the diagnostic system. It is concluded that the diagnostic elements of a weapon system may be designed properly through the use of dependency models and adherence to a structured system engineering process. The critical process of functional allocation is the trade-off of diagnostic performance against operational system efficiencies. Models which have been created for this purpose assist the system engineer in the allocation and design processes, document the results for updates and changes, and help to achieve an optimized system design.<<ETX>>","","","10.1109/AUTEST.1990.111549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=111549","","Fault detection;Cost function;System testing;Systems engineering and theory;Application software;Procurement;Humans;Manuals;System analysis and design;Resource management","automatic test equipment;failure analysis;maintenance engineering;military computing;optimisation;weapons","effectiveness model;multiple failure model;diagnostic functional allocation;maintenance;diagnostic mission;cost model;weapon;dependency models;optimized system design","","","","","","","","","IEEE","IEEE Conferences"
"Automatic test sequences generation for SSCOP protocol","Boo Ho Lee; Byoung Moon Chin","Electron. & Telecommun. Res. Inst., Daejeon, South Korea; NA","Proceedings Twelfth International Conference on Information Networking (ICOIN-12)","","1998","","","684","689","This paper presents some considerations for the simulation of real protocol, SSCOP(Service Specific Connection Oriented Protocol), in the viewpoint of test sequence generation, not of implementation. For the automatic generation of test sequences, we need information for the state transitions of the system to be tested with as high coverage as possible. The information can be obtained during the simulation of the protocol. In the case, the final goal of such a simulation is to achieve 100 percent of coverage for all the possible transitions of the protocol. To obtain a good coverage during the simulation, we modified the SDL model of the protocol. By applying the test sequence generation algorithm for the state transition graph obtained after the simulation of the system, we have generated optimized test sequences for the SSCOP.","","0-8186-7225","10.1109/ICOIN.1998.648601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648601","","Automatic testing;Protocols;System testing;Performance evaluation;Automata;Costs;Moon;Communication industry;Computer industry;Software testing","protocols","automatic test sequences generation;SSCOP protocol;real protocol simulation;service specific connection oriented protocol;state transitions;SDL model;optimized test sequences","","","12","","","","","","IEEE","IEEE Conferences"
"Design method for antenna arrays on cars with electrically short elements under incorporation of the radiation properties of the car body","R. Kronberger; H. K. Lindenmeier; J. F. Hopf; L. M. Reiter","Inst. for High Frequency Tech., Univ. der Bundeswehr Munchen, Neubiberg, Germany; NA; NA; NA","IEEE Antennas and Propagation Society International Symposium 1997. Digest","","1997","1","","418","421 vol.1","A new type of car phone antenna at the upper rim of the rear window is presented being realized as array with narrowly spaced short antenna elements. For this a combined method of measurement and computer optimization was developed. Thus the disadvantage of a reduced radiation in the front direction going along with a single small antenna at the same location is overcome and an almost invisible and highly efficient phone antenna is realized.","","0-7803-4178","10.1109/APS.1997.630184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630184","","Design methodology;Antenna arrays;Phased arrays;Mutual coupling;Mobile antennas;Antenna measurements;Optimization methods;Frequency;Mobile handsets;Application software","antenna arrays;antenna radiation patterns;antenna testing;optimisation;electrical engineering computing;telephony;radiotelephony;automobiles","antenna arrays;design method;electrically short elements;radiation properties;car body;car phone antenna;measurement;computer optimization;reduced radiation;front direction","","11","4","","","","","","IEEE","IEEE Conferences"
"Perils and pitfalls of Weibull life-data analysis","L. Warrington; J. A. Jones","Dept. of Eng., Warwick Univ., Coventry, UK; Dept. of Eng., Warwick Univ., Coventry, UK","Annual Reliability and Maintainability Symposium, 2005. Proceedings.","","2005","","","121","125","Weibull analysis offers considerable insight into the lifetime reliability of products. This paper will describe and illustrate common pitfalls, together with suggestions for their avoidance and aims to provide practical guidance above and beyond standard texts, illustrated with key data and comparisons. This approach will offer significantly better results than either ignoring unfailed items, or automatically assuming high-life for each, beyond the longest failure time. Weibull analysis is a life-based approach to reliability, also useful for optimizing preventive maintenance and replacement intervals.","0149-144X","0-7803-8824","10.1109/RAMS.2005.1408349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408349","","Failure analysis;Data analysis;Application software;Suspensions;Warranties;Computer errors;Performance evaluation;Performance analysis;Optimization methods;Maintenance","reliability;failure analysis;life testing;optimisation;Weibull distribution;preventive maintenance","pitfall analysis;Weibull analysis;product lifetime reliability;failure time;life-based approach;optimization;preventive maintenance;replacement interval","","","2","","","","","","IEEE","IEEE Conferences"
"Stochastic approximation with simulated annealing as an approach to global discrete-event simulation optimization","M. H. Jones; K. P. White","Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA; Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA","Proceedings of the 2004 Winter Simulation Conference, 2004.","","2004","1","","","507","This paper explores an approach to global, stochastic, simulation optimization which combines stochastic approximation (SA) with simulated annealing (SAN). SA directs a search of the response surface efficiently, using a conservative number of simulation replications to approximate the local gradient of a probabilistic loss function. SAN adds a random component to the SA search, needed to escape local optima and forestall premature termination. Using a limited set of simple test problems, we compare the performance of SA/SAN with the commercial package OptQuest. Results demonstrate that SA/SAN can outperform OptQuest when properly tuned. The practical difficulty lies in specifying an appropriate set of SA/SAN gain coefficients for a given application. Further results demonstrate that a multistart approach greatly improves the coverage and robustness of SA/SAN, while also providing insights useful in directing iterative improvement of the gain coefficients before each new start. This preliminary study is sufficiently encouraging to invite further research on SA/SAN.","","0-7803-8786","10.1109/WSC.2004.1371354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371354","","Stochastic processes;Simulated annealing;Discrete event simulation;Storage area networks;Response surface methodology;Testing;Design optimization;Probes;Packaging;Robustness","discrete event simulation;stochastic processes;simulated annealing;approximation theory;probability;search problems;public domain software;software packages","stochastic approximation;simulated annealing;global discrete-event simulation optimization;probabilistic loss function;OptQuest package;open source software","","1","18","","","","","","IEEE","IEEE Conferences"
"Going global with user testing","J. Nielsen","SunSoft, Mountain View, CA, USA","IEEE Software","","1996","13","4","129","130","In Sweden, the automatic teller machines have very large buttons. The author hadn't noticed this particular design element on previous visits, which have usually been in warmer months. One year he was in Stockholm in February and immediately realized why the ATM buttons are so big: you can press them wearing thick gloves. Clearly, the ATM vendor had manufactured a localized version of the product with cold-climate users in mind. Unfortunately, although products are commonly used in countries other than the one they were designed for, designers often forget to consider different usage circumstances. Various tools, techniques and concepts to optimize user interfaces are discussed.","0740-7459;1937-4194","","10.1109/52.526840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526840","","Usability;Tin;User interfaces;Manufacturing;Books;Automatic testing;Permission;Cameras;Employee welfare","human factors;user interfaces;interactive systems;social aspects of automation","user testing;automatic teller machine;Sweden;localized version;cold-climate users;user interfaces","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Robotect: serial-link manipulator design software for modeling, visualization and performance analysis","H. D. Nayar","OphirTech, Altadena, CA, USA","7th International Conference on Control, Automation, Robotics and Vision, 2002. ICARCV 2002.","","2002","3","","1359","1364 vol.3","We have developed Robotect-a software package for modeling, visualization and performance analysis of serial-link manipulator arms. The package allows interactive creation of a wide variety of serial-link manipulators using three alternative modeling formats. Manipulator models created in Robotect are easily configured, visualized and animated. These features support Robotect's main objective-to provide an integrated environment for analysis of serial-link manipulator performance. A suite of analyses including dexterity, repeatability, accuracy, static and dynamic force-torque and load deflection are, available within Robotect to help designers interactively optimize their manipulator designs. This PC-Windows-based package can be used: a) as a tool to interactively synthesize and optimize serial-link manipulator designs, b) as an inverse kinematics algorithm test and verification tool and c) as an educational tool to instruct and learn principles in robot manipulator design. In this paper, we describe Robotect and its applications.","","981-04-8364","10.1109/ICARCV.2002.1234971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1234971","","Robots;Software design;Software performance;Visualization;Performance analysis;Manipulator dynamics;Packaging;Design optimization;Algorithm design and analysis;Software packages","manipulators;program visualisation;optimisation","Robotect;serial link manipulator arms;manipulator models;manipulator designs;visualization;performance analysis;dexterity;load deflection;repeatability;accuracy;dynamic force torque;static force torque;optimisation","","2","15","","","","","","IEEE","IEEE Conferences"
"Emulation of a material delivery system","T. LeBaron; K. Thompson","AutoSimulations Inc., Bountiful, UT, USA; NA","1998 Winter Simulation Conference. Proceedings (Cat. No.98CH36274)","","1998","2","","1055","1060 vol.2","Emulation is the process of exactly imitating a real system. Recent advances in simulation technology make it possible to emulate real world control systems by using a system's control logic to interact with a simulation model. Routing logic, PLC or PC control software, sequencing algorithms, and more can be integrated, tested, and debugged within a simulation environment. Simulation models communicate with control software and provide animation and statistical-output for evaluating control logic and material handling systems. Traditionally, simulation models have been a good tool to test and refine system algorithms and control logic. However, once refined, the algorithms and logic must be re-implemented in the real system. One of the main benefits of emulation is that it eliminates the need to re-implement code. Because the actual control system is used to develop, test, and refine algorithms and logic, it exists as developed in the real system. This eliminates re-implementation errors and provides greater confidence in the emulation results. Emulation has been used for a Rapistan Systems project to test, debug, and optimize complex algorithms and control logic. Emulation of the complex pick and pack conveyor system is presented. The emulation approach and benefits that have been achieved are discussed.","","0-7803-5133","10.1109/WSC.1998.745853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=745853","","Emulation;Logic testing;Control system synthesis;System testing;Control systems;Routing;Programmable control;Software debugging;Software algorithms;Software testing","materials handling;digital simulation;control system analysis computing;conveyors;computer integrated manufacturing","material delivery system emulation;simulation;control system emulation;control logic;routing logic;PLC;PC control software;sequencing algorithms;debugging;testing;animation;statistical-output;material handling systems;errors;Rapistan Systems project;pick and pack conveyor system","","9","4","","","","","","IEEE","IEEE Conferences"
"An empirical investigation of the user-parameters and performance of continuous PBIL algorithms [population-based incremental learning]","M. Gallagher","Dept. of Comput. Sci. & Electr. Eng., Univ. of queensland, Qld., Australia","Neural Networks for Signal Processing X. Proceedings of the 2000 IEEE Signal Processing Society Workshop (Cat. No.00TH8501)","","2000","2","","702","710 vol.2","Evolutionary algorithms (EAs) are powerful methods for solving optimization problems, inspired by natural systems and incorporating population-based searching. Although the implementation of EAs is in many cases quite straightforward, it almost always involves making choices which can be viewed as assumptions regarding the nature of the problem to be solved. In this paper, one such choice is examined: the setting of user-defined parameters in three simple algorithms for solving unconstrained continuous optimization problems. Thre results agree with the notion that these algorithms are often robust to parameter settings, but also reveal interesting relationships between the parameters.","1089-3555","0-7803-6278","10.1109/NNSP.2000.890149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=890149","","Evolutionary computation;Cost function;Sampling methods;Computer science;Optimization methods;Robustness;Adaptive systems;Stochastic processes;Genetic mutations;Testing","evolutionary computation;learning (artificial intelligence);software performance evaluation;search problems;problem solving;optimisation","user-defined parameters;performance;continuous population-based incremental learning algorithms;evolutionary algorithms;parameter settings;population-based search;assumptions;problem solving;unconstrained continuous optimization problems;robustness;parameter relationships","","2","14","","","","","","IEEE","IEEE Conferences"
"EvA: a tool for optimization with evolutionary algorithms","J. Wakunda; A. Zell","Wilhelm-Schickard-Inst. fur Inf., Tubingen Univ., Germany; NA","EUROMICRO 97. Proceedings of the 23rd EUROMICRO Conference: New Frontiers of Information Technology (Cat. No.97TB100167)","","1997","","","644","651","We describe the EvA software package which consists of parallel (and sequential) implementations of genetic algorithms (GAs) and evolution strategies (ESs) and a common graphical user interface. We concentrate on the descriptions of the two distributed implementations of GAs and ESs which are of most interest for the future. We present comparisons of different kinds of genetic algorithms and evolution strategies that include implementations of distributed algorithms on the Intel Paragon, a large MIMD computer and massively parallel algorithms on a 16384 processor MasPar MP-1, a large SIMD computer. The results show that parallelization of evolution strategies not only achieves a speedup in execution time of the algorithm, but also a higher probability of convergence and an increase of quality of the achieved solutions. In the benchmark functions we tested, the distributed ESs have a better performance than the distributed GAs.","1089-6503","0-8186-8129","10.1109/EURMIC.1997.617395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617395","","Evolutionary computation;Electronic switching systems;Genetic algorithms;Concurrent computing;Distributed computing;Software packages;Graphical user interfaces;Distributed algorithms;Parallel algorithms;Benchmark testing","genetic algorithms;mathematics computing;software packages;parallel algorithms;graphical user interfaces;parallel machines;software performance evaluation;convergence","EvA tool;optimization;evolutionary algorithms;software package;parallel implementations;sequential implementations;genetic algorithms;graphical user interface;distributed algorithms;Intel Paragon;MIMD computer;massively parallel algorithms;16384 processor;MasPar MP-1;SIMD computer;execution time;convergence;probability;quality","","","14","","","","","","IEEE","IEEE Conferences"
"Metadiagnosis","O. Yue; Y. Lirov","AT&T Bell Lab., Holmdel, NJ, USA; AT&T Bell Lab., Holmdel, NJ, USA","Proceedings IEEE International Symposium on Intelligent Control 1988","","1988","","","303","306","The need for a computerized diagnostic strategy choice aiding tool is identified, and a methodology for its implementation is proposed. In troubleshooting practice, a diagnostic strategy may be described by the test sequencing optimization objective and the algorithm to produce the optimal test sequence. It is possible to pose a constrained optimization problem in the space of the diagnostic strategies. Solving such optimization problems requires a methodology for representing and manipulating the strategies in conjunction with the given problem in the troubleshooting domain. It is emphasized that an expert diagnostic system should be able to choose and implement a diagnostic strategy depending on the current situation. To facilitate such an ability, it is necessary to create a methodology for representing different diagnostic strategies, to provide tools for evaluating the current situation (e.g. specifics of the failure rates, costs, system topology, etc.), and to provide a mechanism for matching and implementing a diagnostic strategy in the given situation. Such an evaluation of the system under diagnosis is called metadiagnosis. A semantic control approach for a computerized strategy choice aiding tool architecture is proposed, and an example of its application is provided.<<ETX>>","2158-9860","0-8186-2012","10.1109/ISIC.1988.65448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65448","","System testing;Constraint optimization;Cost function;Design engineering;Knowledge engineering;Process control;Optimization methods;Computer architecture;Application software;Operating systems","engineering computing;failure analysis;knowledge based systems","strategy matching;computerized diagnostic strategy choice aiding tool;diagnostic strategy;test sequencing optimization;constrained optimization problem;failure rates;costs;system topology;metadiagnosis;semantic control;computerized strategy choice aiding tool architecture","","","24","","","","","","IEEE","IEEE Conferences"
"Operation behaviour of roof installed photovoltaic modules","W. Knaupp","Zentrum fur Sonnenenergie- und Wasserstoff-Forschung Baden Wuerttemberg, Stuttgart, Germany","Conference Record of the Twenty Fifth IEEE Photovoltaic Specialists Conference - 1996","","1996","","","1445","1448","Photovoltaic energy in the building environment is a very interesting application. On existing building roofs, the installation of photovoltaic generators is a common mounting configuration. It is important to assess and predict the operational behaviour regarding energy output, power rating and critical operation limits of such modules. This contribution summarizes some detailed experimental and theoretical examinations regarding the operational behaviour of roof-installed PV power system modules. Reverse ventilation was analyzed on the basis of buoyancy forces and pressure loss mechanisms. The correlations were transferred to a computer program PVROOF and verified in the ZSW test site, Germany. Experimental and simulation results regarding the influence of roof cover and the distance between the roof and the PV module are shown. With such results, roof-mounted PV power system installations can be optimized with respect to electrical and thermal energy output.","0160-8371","0-7803-3166","10.1109/PVSC.1996.564407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=564407","","Photovoltaic systems;Solar power generation;Temperature measurement;Ventilation;Insulation;Testing;Tiles;Computational modeling;Solar heating;Resistance heating","photovoltaic power systems;solar cell arrays;solar cells;semiconductor device testing;power system measurement;semiconductor device models;software packages;power system analysis computing;electronic engineering computing","roof-mounted PV power systems;photovoltaic generators;solar cell modules;energy output;power rating;performance testing;critical operation limits;reverse ventilation;buoyancy forces;computer simulation;pressure loss mechanisms;PVROOF software","","3","3","","","","","","IEEE","IEEE Conferences"
"A simulation environment for dimensioning telecommunication management systems","G. Damm; S. Giorcelli; G. Fouquet","Alcatel Corp. Res. Center, Marcoussis, France; NA; NA","Proceedings 1999 IEEE Symposium on Application-Specific Systems and Software Engineering and Technology. ASSET'99 (Cat. No.PR00122)","","1999","","","290","293","This paper presents an integrated simulation environment to help dimensioning distributed systems in the field of telecommunication network management, and to be a guideline for software design of such systems. Dimensioning concerns sales persons, who have to quickly optimise configurations in terms of cost and efficiency in order to meet customer requirements. Software designers can use this tool to make and validate architectural choices, as well as to exhaustively test the performance and limits of the system. The proposed tool focuses on management traffic. To improve the assessment efficiency, simulation studies should be performed by the same teams who design or market the products; available simulation tools require specific skills. As a consequence, we developed a tool which provides an intuitive, user-friendly graphical interface, and gives easy access to the underlying models of the objects composing the network.","","0-7695-0122","10.1109/ASSET.1999.756783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756783","","Software design;Telecommunication network management;Guidelines;Marketing and sales;Cost function;Software performance;Software tools;Software testing;System testing;Traffic control","telecommunication network management;telecommunication computing;digital simulation;graphical user interfaces","simulation environment;dimensioning;telecommunication network management;distributed systems;graphical user interface;performance evaluation;GUESS tools","","1","5","","","","","","IEEE","IEEE Conferences"
"Upgrading engine test cells for improved troubleshooting and diagnostics","M. J. Roemer; R. F. Orsagh; M. Schoeller; J. Scheid; R. Friend; W. Sotomayer","Impact Technol., LLC, Rochester, NY, USA; Impact Technol., LLC, Rochester, NY, USA; Impact Technol., LLC, Rochester, NY, USA; NA; NA; NA","Proceedings, IEEE Aerospace Conference","","2002","6","","6","6","Upgrading military engine test cells with advanced diagnostic and troubleshooting capabilities will play a critical role in increasing aircraft availability and test cell effectiveness while simultaneously reducing engine operating and maintenance costs. Sophisticated performance and mechanical anomaly detection and fault classification algorithms utilizing thermodynamic, statistical, and empirical engine models are now being implemented as part of a United States Air Force Advanced Test Cell Upgrade Initiative. Under this program, a comprehensive set of realtime and post-test diagnostic software modules, including sensor validation algorithms, performance fault classification techniques and vibration feature analysis are being developed. An automated troubleshooting guide is also being implemented to streamline the troubleshooting process for both inexperienced and experienced technicians. This artificial intelligence based tool enhances the conventional troubleshooting tree architecture by incorporating probability of occurrence statistics to optimize the troubleshooting path. This paper describes the development and implementation of the F404 engine test cell upgrade at the Jacksonville Naval Air Station.","","0-7803-7231","10.1109/AERO.2002.1036142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1036142","","Engines;Testing;Military aircraft;Classification algorithms;Aircraft propulsion;Availability;Costs;Fault detection;Thermodynamics;Force sensors","aircraft testing;aerospace engines;military aircraft;military computing;artificial intelligence;automatic test equipment;aerospace computing","automated diagnostics;military engine test cell upgrading;aircraft availability;sensor validation algorithm;software module;F404 engine;performance fault classification;vibration feature analysis;automated troubleshooting;artificial intelligence","","4","13","","","","","","IEEE","IEEE Conferences"
"Supervisor-student model in particle swarm optimization","Yu Liu; Zheng Qin; Xingshi He","Dept. of Comput. Sci., Xian Jiaotong Univ., China; Dept. of Comput. Sci., Xian Jiaotong Univ., China; NA","Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)","","2004","1","","542","547 Vol.1","Particle swarm optimization (PSO) algorithms have exhibited good performance on well-known numerical test problems. In this paper, we propose a supervisor-student model in particle swarm optimization (SSM-PSO) that may further reduce computational cost in two aspects. On the one hand, it introduces a new parameter, called momentum factor, into the position update equation, which can restrict the particles inside the defined search space without checking the boundary at every iteration. On the other hand, relaxation-velocity-update strategy that is to update the velocities of the particles as few times as possible during the run, is employed to reduce the computational cost for evaluating the velocity. Comparisons with the linear decreasing weight PSO on three benchmark functions indicate that SSM-PSO not only greatly reduces the computational cost for updating the velocity, but also exhibit good performance.","","0-7803-8515","10.1109/CEC.2004.1330904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1330904","","Particle swarm optimization;Equations;Computational efficiency;Velocity control;Computer science;Software performance;Mathematics;Software algorithms;Software testing;Evolutionary computation","evolutionary computation;search problems;computational complexity","supervisor-student model;particle swarm optimization;PSO algorithm;numerical test problems;momentum factor;position update equation;search space;relaxation-velocity-update strategy;SSM-PSO;evolutionary computation;computational cost","","13","15","","","","","","IEEE","IEEE Conferences"
"Analysis of high-level address code transformations for programmable processors","S. Gupta; M. Miranda; F. Catthoor; R. Gupta","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; NA; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","9","13","Memory intensive applications require considerable arithmetic for the computation and selection of the different memory access pointers. These memory address calculations often involve complex (non) linear arithmetic expressions which have to be calculated during program execution under tight timing constraints, this becoming a critical bottleneck in the overall system performance. This paper explores applicability and effectiveness of source-level optimisations (as opposed to instruction-level) for address computations in the context of multimedia. We propose and evaluate two processor-target independent source-level optimisation techniques, namely, global scope operation cost minimisation complemented with loop-invariant code hoisting, and nonlinear operator strength reduction. The transformations attempt to achieve minimal code execution within loops and reduced operator strengths. The effectiveness of the transformations is demonstrated with two real-life multimedia application kernels by comparing the improvements in the number of execution cycles, before and after applying the systematic source-level optimisations. Using state-of-the-art C compilers on several popular RISC platforms.","","0-7695-0537","10.1109/DATE.2000.840008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840008","","Hardware;Arithmetic;Electronic switching systems;Timing;Costs;Optimizing compilers;Information analysis;Computer science;Application software;Ores","reduced instruction set computing;C language;program compilers;multimedia computing;digital signal processing chips","high-level address code transformations;programmable processors;memory intensive applications;memory access pointers;program execution;timing constraints;source-level optimisations;processor-target independent source-level optimisation;global scope operation cost minimisation;loop-invariant code hoisting;nonlinear operator strength reduction;real-life multimedia application kernels;state-of-the-art C compilers;RISC platforms","","5","16","","","","","","IEEE","IEEE Conferences"
"Reliable estimation of execution time of embedded software","P. Giusto; G. Martin; E. Harcourt","Cadence Design Syst. Inc., San Jose, CA, USA; NA; NA","Proceedings Design, Automation and Test in Europe. Conference and Exhibition 2001","","2001","","","580","588","Estimates of execution time of embedded software play an important role in function-architecture co-design. This paper describes a technique based upon a statistical approach that improves existing estimation techniques. Our approach provides a degree of reliability in the error of the estimated execution time. We illustrate the technique using both control-oriented and computational-dominated benchmark programs.","1530-1591","0-7695-0993","10.1109/DATE.2001.915082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915082","","Embedded software;Program processors;Microprocessors;Space exploration;Embedded system;Virtual prototyping;Hardware design languages;Buildings;Timing;Optimizing compilers","embedded systems;hardware-software codesign;formal verification;instruction sets","execution time;embedded software;function-architecture co-design;statistical approach;estimation techniques;reliability;computational-dominated benchmark programs;control-oriented benchmark programs","","34","22","","","","","","IEEE","IEEE Conferences"
"A case study for optimal dynamic simulation allocation in ordinal optimization","Chun-Hung Chen; Donghai He; Michael Fu","Dept. of Syst. Eng. & Oper. Res., George Mason Univ., Fairfax, VA, USA; Dept. of Syst. Eng. & Oper. Res., George Mason Univ., Fairfax, VA, USA; NA","Proceedings of the 2004 American Control Conference","","2004","6","","5754","5759 vol.6","Ordinal optimization has emerged as an efficient technique for simulation and optimization. Exponential convergence rates can be achieved in many cases. A good allocation of simulation samples across designs can further dramatically improve the efficiency of ordinal optimization by orders of magnitude. However, the allocation problem itself is a big challenge. Most existing methods offer approximations. Assuming the availability of perfect information, we investigate theoretically optimal allocation schemes for some special cases. We compare our theoretically optimal solutions with existing approximation methods using a series of numerical examples. While perfect information is not available in real life, such an optimal solution provides an upper bound for the simulation efficiency we can achieve. The results indicate that the simulation efficiency can still be further improved beyond the existing methods. The numerical testing shows that dynamic allocation is also much more efficient than the static allocation.","0743-1619","0-7803-8335","10.23919/ACC.2004.1384774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1384774","","Computer aided software engineering;Computational modeling;Design optimization;Analytical models;Systems engineering and theory;Operations research;Approximation methods;Testing;Costs;NASA","resource allocation;discrete event simulation;approximation theory;convergence;optimisation","optimal dynamic simulation allocation problem;ordinal optimization;exponential convergence rate;approximation methods;numerical testing;discrete event simulation","","","11","","","","","","IEEE","IEEE Conferences"
"Prioritized token-based mutual exclusion for distributed systems","F. Mueller","Inst. fur Inf., Humboldt-Univ., Berlin, Germany","Proceedings of the First Merged International Parallel Processing Symposium and Symposium on Parallel and Distributed Processing","","1998","","","791","795","A number of solutions have been proposed for the problem of mutual exclusion in distributed systems. Some of these approaches have since been extended to a prioritized environment suitable for real-time applications but impose a higher message passing overhead than our approach. We present a new protocol for prioritized mutual exclusion in a distributed environment. Our approach uses a token-based model working on a logical tree structure, which is dynamically modified. In addition, we utilize a set of local queues whose union would resemble a single global queue. Furthermore, our algorithm is designed for out-of-order message delivery, handles messages asynchronously and supports multiple requests from one node for multi-threaded nodes. The prioritized algorithm has an average overhead of O(log(n)) messages per request for mutual exclusion with a worst-case overhead of O(n), where n represents the number of nodes in the system. Thus, our prioritized algorithm matches the message complexity of the best non-prioritized algorithms while previous prioritized algorithms have a higher message complexity, to our knowledge. Our concept of local queues can be incorporated into arbitrary token-based protocols with or without priority support to reduce the amount of messages. Performance results indicate that the additional functionality of our algorithm comes at the cost of 30% longer response times within our test environment for distributed execution when compared with an unprioritized algorithm. This result suggests that the algorithm should be used when strict priority ordering is required.","1063-7133","0-8186-8404","10.1109/IPPS.1998.670018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670018","","Protocols;Delay;Testing;Message passing;Tree data structures;Algorithm design and analysis;Cost function;Memory architecture;Hardware;Broadcasting","message passing;real-time systems;communication complexity;protocols;distributed algorithms;software performance evaluation","prioritized token-based mutual exclusion;distributed systems;real-time applications;message passing overhead;protocol;logical tree structure;local queues;global queue;out-of-order message delivery;multiple requests;multithreaded nodes;overhead;message complexity;performance;response times;strict priority ordering","","25","20","","","","","","IEEE","IEEE Conferences"
"Tolerance Assignment for IC Selection Tests","W. Maly; Z. Pizlo","Department of Electrical and Computer Engineering, Carnegie-Mellon University, Pittsburgh, PA, USA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1985","4","2","156","162","IC chips or manufactured wafers run through selection processes at various stages of the fabrication process. Typically, the most important is the selection performed on IC dice which are tested directly on manufacturing wafers. This paper deals with the problem of the optimal assignment of the upper and lower selection thresholds applied for selecting dice during the wafer measurements. The tolerance assignment is defined as a statistical optimization problem, where the optimization objective function is a measure of the manufacturing profit. In the paper a method for computing a solution of this optimization problem is proposed, and an example of the industrial application of this method in the Computer-Aided Manufacturing (CAM) area is given.","0278-0070;1937-4151","","10.1109/TCAD.1985.1270109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270109","","Integrated circuit testing;Computer aided manufacturing;Manufacturing processes;Fabrication;Performance evaluation;Optimization methods;Computer industry;Manufacturing industries;Application software;CADCAM","","","","14","6","","","","","","IEEE","IEEE Journals & Magazines"
"Translating Java to C without inserting class initialization tests","Y. Chiba","NA","Proceedings 16th International Parallel and Distributed Processing Symposium","","2002","","","7 pp","","","","0-7695-1573","10.1109/IPDPS.2002.1016502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1016502","","Java;Application software;Software performance;Productivity;Operating systems;Optimizing compilers;Laboratories;System testing;Benchmark testing;Object oriented programming","","","","1","11","","","","","","IEEE","IEEE Conferences"
"SPECS methods for FDTs","R. Reed","Telecommun. Software Eng. Ltd., Lutterworth, UK","IEE Tutorial Colloquium on Formal Methods and Notations Applicable to Telecommunications","","1992","","","1/1","1/4","The Research and development in Advanced Communications technologies in Europe (RACE) programme recognised that an improved programming infrastructure (PI) would be needed for integrated broadband communications (IBC). Within this context the specification and programming environment for communication software (SPECS) project defines advanced methods and tools to provide maximum automation and optimization of the software engineering of IBC software from requirements and specification through design, implementation, test, execution, maintenance and adaptation. The author outlines SPECS methods for telecommunications software which utilise formal description techniques (FDTs) as a key element of the engineering process.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=170077","","Software requirements and specifications;Communication system software","formal specification;research initiatives;telecommunications computing","FDT;RACE;programming infrastructure;integrated broadband communications;specification and programming environment for communication software;software engineering;design;implementation;test;execution;maintenance;adaptation;SPECS methods;telecommunications software;formal description techniques","","","","","","","","","IET","IET Conferences"
"A fast algorithm for finding common multiple-vertex dominators in circuit graphs","R. Krenz; E. Dubrova","IMIT-KTH, R. Inst. of Technol., Stockholm, Sweden; IMIT-KTH, R. Inst. of Technol., Stockholm, Sweden","Proceedings of the ASP-DAC 2005. Asia and South Pacific Design Automation Conference, 2005.","","2005","1","","529","532 Vol. 1","In this paper we present a fast algorithm for computing common multiple-vertex dominators in circuit graphs. Dominators are widely used in CAD applications such as satisfiability checking, equivalence checking, ATPG, technology mapping, decomposition of Boolean functions and power optimization. State of the art algorithms compute single-vertex dominators in linear time. However, the rare appearance of single-vertex dominators in circuit graphs requires the investigation of a broader type of dominators and the development of algorithms to compute them. We show that our new technique is faster and computes more common multiple-vertex dominators than existing techniques.","2153-6961;2153-697X","0-7803-8736","10.1109/ASPDAC.2005.1466220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466220","","Circuits;Automatic test pattern generation;Space technology;Flow graphs;Boolean functions;Design automation;Electronic design automation and methodology;Application software;Scalability;Information analysis","circuit CAD;circuit optimisation;automatic test pattern generation;computability;equivalent circuits","common multiple-vertex dominators;circuit graphs;circuit CAD;satisfiability checking;equivalence checking;ATPG;technology mapping;Boolean function decomposition;power optimization","","2","19","","","","","","IEEE","IEEE Conferences"
"Software Development for Distributed Systems","D. J. Faber","University of California","Computer","","1975","8","6","68","69","My prime interest in participating in this session is to project the applicability of SP to the development of software for Distributed Computer Systems (such as the UC Irvine Distributed Computer System and the CMU C.MMP). The difficulties common to all software design and development efforts are made even more acute when writing software for a DCS. Setting up experiments that involve widely separated, loosely coupled computers is not an easy or straightforward task. Error situations may develop fortuitously and then evade most efforts to cause them to occur again. In this predicament, structured modular top-down programming techniques appear likely to play an important role since they offer an opportunity to bring significant software reliability improvements.","0018-9162;1558-0814","","10.1109/C-M.1975.218997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1649473","","Programming;Distributed control;System testing;Software tools;Communication industry;Computer industry;Constraint optimization;Runtime;Systems engineering and theory","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Transforming the search space with Gray coding","K. E. Mathias; L. D. Whitley","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence","","1994","","","513","518 vol.1","Genetic algorithm test functions have typically been designed with properties in numeric space that make it difficult to locate the optimal solution using traditional optimization techniques. The use of Gray coding has been found to enhance the performance of genetic search in some cases. However, Gray coding produces a different function mapping that may have fewer local optima and different relative hyperplane relationships. Therefore, inferences about a function will not necessarily hold when transformed to another search space. In fact, empirical results indicate that some genetic algorithm test functions are significantly altered by Gray coding such that local optimization methods often perform better than genetic algorithms.<<ETX>>","","0-7803-1899","10.1109/ICEC.1994.349897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=349897","","Genetic algorithms;Testing;Reflective binary codes;Optimization methods;Performance evaluation;Matrix converters;Algorithm design and analysis;Design optimization;Search methods;Software algorithms","genetic algorithms;optimisation;encoding;search problems","search space;Gray coding;genetic algorithm test functions;optimal solution;optimization techniques;genetic search;performance;function mapping;hyperplane relationships;inference;local optimization methods","","9","12","","","","","","IEEE","IEEE Conferences"
"Simulation optimization using tabu search: an empirical study","A. Konak; S. Kulturel-Konak","Inf. Sci. & Technol., Penn State-Berks, Reading, PA, USA; NA","Proceedings of the Winter Simulation Conference, 2005.","","2005","","","7 pp.","","This paper proposes alternative strategies to perform simulation within a simulation optimization algorithm based on tabu search. These strategies are tested empirically on a stochastic knapsack problem. Results have shown that the way simulation is implemented and the number of simulation replications, have a profound effect on the performance of tabu search","0891-7736;1558-4305","0-7803-9519","10.1109/WSC.2005.1574571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1574571","","Input variables;Computational modeling;Ant colony optimization;Noise level;Stochastic processes;Testing;Software packages;Design optimization;Analytical models;Processor scheduling","knapsack problems;optimisation;search problems;simulation;stochastic processes","simulation optimization;tabu search;stochastic knapsack problem","","2","21","","","","","","IEEE","IEEE Conferences"
"Functional equivalence checking for verification of algebraic transformations on array-intensive source code","K. C. Shashidhar; M. Bruynooghe; F. Catthoor; G. Janssens","Faculteit Toegepaste Wetenschappen, Katholieke Univ., Leuven, Belgium; Faculteit Toegepaste Wetenschappen, Katholieke Univ., Leuven, Belgium; Faculteit Toegepaste Wetenschappen, Katholieke Univ., Leuven, Belgium; Faculteit Toegepaste Wetenschappen, Katholieke Univ., Leuven, Belgium","Design, Automation and Test in Europe","","2005","","","1310","1315 Vol. 2","The development of energy and performance-efficient embedded software increasingly relies on the application of complex transformations on critical parts of the source code. Designers applying such nontrivial source code transformations are often faced with the problem of ensuring functional equivalence of the original and transformed programs. Currently, they have to rely on incomplete and time-consuming simulation. Formal automatic verification of the transformed program against the original is desirable instead. This calls for equivalence checking tools similar to the ones available for comparing digital circuits. We present such a tool to compare array-intensive programs related through a combination of important global transformations like expression propagations, loop and algebraic transformations. When the transformed program fails to pass the equivalence check, the tool provides specific feedback on the possible locations of errors.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395774","","Signal processing algorithms;Embedded software;Application software;Circuit simulation;Digital circuits;Feedback;Design optimization;Optimizing compilers;Program processors;Mobile computing","formal verification;embedded systems;software tools","functional equivalence checking;algebraic transformation verification;array-intensive source code;embedded software development;complex transformations;source code transformations;formal automatic verification;formal verification;expression propagations;loop transformations;equivalence checking tools;compiler","","13","13","","","","","","IEEE","IEEE Conferences"
"Design of a high level SOC platform for the conversion of network protocols","J. P. Bissou; Y. Savaria","Dept. de Genie Electr., Ecole Polytech. de Montreal, Que., Canada; Dept. de Genie Electr., Ecole Polytech. de Montreal, Que., Canada","CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)","","2003","2","","1271","1274 vol.2","This article presents a SOC (system on a single chip) platform for the conversion of protocols. The method of design structure and high level which is shown, models the behaviour and architecture of the target system. This modelling allows realisation of separation of software/hardware optimisation in order to validate our specifications.","0840-7789","0-7803-7781","10.1109/CCECE.2003.1226131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226131","","Firewire;Ethernet networks;Testing;Context modeling","system-on-chip;hardware-software codesign;protocols;optimisation","high level SOC platform;network protocols;system on single chip;target system;software-hardware optimization","","","7","","","","","","IEEE","IEEE Conferences"
"Optimized generation of data-path from C codes for FPGAs","Z. Guo; B. Buyukkurt; W. Najjar; K. Vissers","California Univ., Riverside, CA, USA; California Univ., Riverside, CA, USA; California Univ., Riverside, CA, USA; NA","Design, Automation and Test in Europe","","2005","","","112","117 Vol. 1","FPGAs, as computing devices, offer significant speedup over microprocessors. Furthermore, their configurability offers an advantage over traditional ASICs. However, they do not yet enjoy high-level language programmability, as microprocessors do. This has become the main obstacle for their wider acceptance by application designers. ROCCC is a compiler designed to generate circuits from C source code to execute on FPGAs, more specifically on CSoCs. It generates RTL level HDLs from frequently executing kernels in an application. In this paper, we describe the ROCCC's system overview and focus on its data path generation. We compare the performance of ROCCC-generated VHDL code with that of Xilinx IPs. The synthesis result shows that the ROCCC-generated circuit takes around 2/spl times//spl sim/3/spl times/ the area and runs at a comparable clock rate.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395540","","Field programmable gate arrays;Hardware;Microprocessors;High level languages;Parallel processing;Optimizing compilers;Electronic design automation and methodology;Kernel;Integrated circuit synthesis;Clocks","high level synthesis;hardware-software codesign;system-on-chip;reconfigurable architectures;C language;program compilers;hardware description languages;field programmable gate arrays;circuit optimisation","configurable system-on-a-chip;data-path generation optimization;C source code compiler;CSoC;FPGA;high-level language programmability;ROCCC;RTL level HDL;frequently executing kernels;VHDL code","","15","26","","","","","","IEEE","IEEE Conferences"
"COTS and high assurance: an oxymoron?","J. Voas","Reliable Software Technologies","Proceedings 4th IEEE International Symposium on High-Assurance Systems Engineering","","1999","","","119","119","","","0-7695-0418","10.1109/HASE.1999.809486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809486","","Software standards;Operating systems;Testing;Embedded system;Software quality;Optimized production technology;Code standards;Embedded software;Software reusability;Costs","","","","1","4","","","","","","IEEE","IEEE Conferences"
"Tuning system-dependent applications with alternative MPI calls: a case study","T. T. Le","Dept. of Electr. Eng., San Jose State Univ., CA, USA","Third ACIS Int'l Conference on Software Engineering Research, Management and Applications (SERA'05)","","2005","","","137","143","This paper shows the effectiveness of using optimized MPI calls for MPI based applications on different architectures. Using optimized MPI calls can result in reasonable performance gain for most of MPI based applications running on most of high-performance distributed systems. Since relative performance of different MPI function calls and system architectures can be uncorrelated, tuning system-dependent MPI applications by exploring the alternatives of using different MPI calls is the simplest but most effective optimization method. The paper first shows that for a particular system, there are noticeable performance differences between using various MPI calls that result in the same communication pattern. These performance differences are in fact not similar across different systems. The paper then shows that good performance optimization for an MPI application on different systems can be obtained by using different MPI calls for different systems. The communication patterns that were experimented in this paper include the point-to-point and collective communications. The MPI based application used for this study is the general-purpose transient dynamic finite element application and the benchmark problems are the public domain 3D car crash problems. The experiment results show that for the same communication purpose, using alternative MPI calls can result in quite different communication performance on the Fujitsu HPC2500 system and the 8-node AMD Athlon cluster, but very much the same performance on the other systems such as the Intel Itanium2 and the AMD Opteron clusters.","","0-7695-2297","10.1109/SERA.2005.67","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563154","","Computer aided software engineering;Concurrent computing;Hardware;Application software;Finite element methods;Message passing;Delay;Performance gain;Optimization methods;Vehicle crash testing","message passing;optimisation;remote procedure calls;performance evaluation","system-dependent application tuning;MPI call;high-performance distributed system;optimization method;point-to-point communication;collective communication;general-purpose transient dynamic finite element application;3D car crash problem;Fujitsu HPC2500 system;8-node AMD Athlon cluster;Intel Itanium2;AMD Opteron cluster","","","17","","","","","","IEEE","IEEE Conferences"
"A multiobjective genetic algorithm for radio network optimization","H. Meunier; E. -. Talbi; P. Reininger","Lille I Univ., Villeneuve d'Ascq, France; NA; NA","Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)","","2000","1","","317","324 vol.1","Engineering of mobile telecommunication networks endures two major problems: the design of the network and the frequency assignment. We address the first problem in this paper, which has been formulated as a multiobjective constrained combinatorial optimisation problem. We propose a genetic algorithm (GA) that aims to approximate the Pareto frontier of the problem. Advanced techniques have been used, such as Pareto ranking, sharing and elitism. The GA has been implemented in parallel on a network of workstations to speed up the search. To evaluate the performance of the GA, we have introduced two new quantitative indicators: the entropy and the contribution. Encouraging results are obtained on real-life problems.","","0-7803-6375","10.1109/CEC.2000.870312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870312","","Genetic algorithms;Radio network;Testing;Frequency;Design engineering;Performance evaluation;Base stations;Constraint optimization;Workstations;Entropy","radio networks;genetic algorithms;operations research;workstation clusters;parallel algorithms;Pareto distribution;combinatorial mathematics;constraint theory;entropy;telecommunication computing;software performance evaluation","multiobjective genetic algorithm;radio network optimization;mobile telecommunication networks;network design;frequency assignment;multiobjective constrained combinatorial optimisation problem;Pareto frontier approximation;Pareto ranking;sharing;elitism;parallel algorithm;workstation network;search speed;performance evaluation;quantitative indicators;entropy;contribution","","35","16","","","","","","IEEE","IEEE Conferences"
"Synthesizing neural network applications using computer algebra","C. P. Tsang","Dept. of Comput. Sci., Western Australia Univ., Nedlands, WA, Australia","Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)","","1993","1","","919","922 vol.1","The software engineering of neural network (NN) applications faces many different problems from those of traditional programming. Due to the stochastic nature of NNs, it is extremely hard to verify the correctness of a NN application by exhaustive testing. It is also very hard to tell whether a NN application is generating the intended results, because there are many parameters and variables. Lastly, NN software is usually time intensive and highly optimised code is required. This paper explores the use of computer algebra to synthesize neural network applications. The author can generate highly reliable and efficient codes (probably the most efficient possible) from a high level NN algebraic specification. The computer algebra package not only handles most of the algebraic manipulation in the derivation of the computation formulas, but also performs simplification and verification. Due to the non-existence of control codes, synthesized NN codes can fully exploit computer architecture such as high-speed cache pipelines.","","0-7803-1421","10.1109/IJCNN.1993.714061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=714061","","Network synthesis;Neural networks;Application software;Computer applications;Computer networks;Algebra;Software engineering;Stochastic processes;Testing;Computer network reliability","symbol manipulation;algebraic specification;software packages;neural nets;computer aided software engineering","neural network applications;software engineering;high level NN algebraic specification;computer algebra package;algebraic manipulation","","3","10","","","","","","IEEE","IEEE Conferences"
"Partial discharge diagnosis using statistical optimization on a PC-based system","H. -. Kranz; R. Krump","Bergische Univ., Wuppertal, Germany; Bergische Univ., Wuppertal, Germany","IEEE Transactions on Electrical Insulation","","1992","27","1","93","98","Personal computer (PC)-aided partial discharge (PD) evaluation needs high-speed electronic devices for on-line measurement and digital conversion of PD signals. The authors include quantities to be measured to evaluate PD signals, which are characterized by a statistical scatter of magnitude and duration and are additionally influenced by noise and a complex time behavior. Using the correct algorithms and parameters, the PC gains some intelligence to discriminate unknown defects. Artificial sample defects, representing sources of PD, have been implanted into a GIS system. By performing a statistical analysis of charge, energy and phase angle on the measured signals, it is possible to solve the diagnosis problem by a noise resistant software solution. The final diagnosis is carried out by pattern recognition, using a specially calculated identification data set, which is compared to reference patterns measured earlier. The strategy takes into account the fundamental differences in the physics of discernible defects.<<ETX>>","0018-9367;1557-962X","","10.1109/14.123444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=123444","","Partial discharges;Partial discharge measurement;Noise measurement;Microcomputers;High-speed electronics;Time measurement;Scattering;Artificial intelligence;Geographic Information Systems;Performance evaluation","charge measurement;computerised instrumentation;computerised pattern recognition;fault location;insulation testing;microcomputer applications;partial discharges;statistical analysis;switchgear testing","partial discharge diagnosis;defect discrimination;artificial sample defects;signal charge;signal energy;statistical optimization;PC-based system;high-speed electronic devices;digital conversion;GIS system;statistical analysis;phase angle;noise resistant software solution;pattern recognition;identification data set","","42","8","","","","","","IEEE","IEEE Journals & Magazines"
"Reverse software engineering of concurrent programs","X. Ge; N. Prywes","Computer Command & Control Co., Philadelphia, PA, USA; Computer Command & Control Co., Philadelphia, PA, USA","Proceedings of the 5th Jerusalem Conference on Information Technology, 1990. 'Next Decade in Information Technology'","","1990","","","731","742","The development of a system for reverse software engineering of real-time programs is described. It is addressed specifically to the US Navy's modernization of tactical and strategic systems. The approach is based on completely automatic translation from CMS-2 into a user-oriented nonprocedural specification language called MODEL. The user can better understand, maintain, and modernize the specification of programs in the MODEL language. There is an existing system for analysis, translation, and optimization in the conversion of MODEL to Ada. This completes the translation from the real-time CMS-2 programs to Ada programs. The overall system will use a powerful workstation with graphics.<<ETX>>","","0-8186-2078","10.1109/JCIT.1990.128357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=128357","","Software engineering;Military computing;Computer architecture;Automation;Hardware;Investments;Automatic testing;Documentation;Power system modeling;Computer aided software engineering","parallel programming;real-time systems;software engineering;specification languages","concurrent programs;reverse software engineering;real-time programs;tactical and strategic systems;completely automatic translation;CMS-2;user-oriented nonprocedural specification language;MODEL;optimization;Ada","","","17","","","","","","IEEE","IEEE Conferences"
"A hardware/software co-design method of ITS image processing development","Y. Endo; H. Koizumi","Dept. of Comput. & Syst. Eng., Tokyo Denki Univ., Japan; NA","Proceedings Seventh International Conference on Parallel and Distributed Systems: Workshops","","2000","","","415","420","A hardware and software co-design method for the image processing system of intelligent transport systems (ITS) is proposed. The objective of this method is to optimize the division of functions between hardware and software by the optimum tradeoff. In this method, the targeted overall system is first modeled by software, and then functions and structures are adjusted. The performance of each function of software and hardware in the model is evaluated by simulation, and the optimized division of functions between hardware and software is obtained. Each functionally divided process of hardware and software is re-designed and fine-tuned and then operations are verified by the co-design test board in order to quickly design and construct a system which satisfies the design objectives. The ITS image processing development area is chosen as an application example for this method and its effectiveness is verified by evaluating the construction of a safe driving support system.","","0-7695-0571","10.1109/PADSW.2000.884661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884661","","Hardware;Image processing;Software performance;Design methodology;Embedded system;Embedded software;Application software;Costs;Systems engineering and theory;Intelligent systems","image processing;hardware-software codesign;automated highways;knowledge based systems","hardware/software co-design method;image processing system;intelligent transport systems;optimum tradeoff;simulation;driving support system","","1","6","","","","","","IEEE","IEEE Conferences"
"Bestfit, distribution fitting software by palisade corporation","L. Jankauskas; S. McLafferty","Palisade Corporation; NA","Proceedings Winter Simulation Conference","","1996","","","551","555","BestFit is distribution-fitting software for Microsoft Windows that finds the statistical distribution function that best fits a data set. BestFit provides a flexible, easy to-use environment for analysis that allows users to focus on their data, not their software. Users of simulation software can use distributions produced by BestFit to define uncertainty in their simulation models. BestFit helps you find the best representation of randomness in your model, making your simulation results more accurate. Users of @RISK, ProModel or any other simulation software will find BestFit an invaluable tool for defining uncertainty.","","0-7803-3383","10.1109/WSC.1996.873332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873332","","Statistical distributions;Uncertainty;Software tools;Distribution functions;Optimization methods;Impedance matching;Tail;Displays;Testing;Probability distribution","","","","","7","","","","","","IEEE","IEEE Conferences"
"Best practices for development and deployment of man-portable ATE","D. L. Williams","Syst. & Electron. Inc., St. Louis, MO, USA","Proceedings AUTOTESTCON 2004.","","2004","","","23","29","Man-portable test systems are under severe size, weight, environmental and power consumption constraints. Yet, these systems generally must provide all test capabilities required by the supported units-under-test (UUT). The successful development of man-portable systems requires a paradigm shift in the test system architectural design. General purpose ATE architectures by and large still do not address the means to satisfy the UUT's expected operational environment during test. Instrument packaging has evolved from bench-top and rack-mount configurations to card/modular level products. However, most card/modular level instruments still need the support of card cages and backplanes for intercommunication, control, physical constraint, power and cooling and the legacy system architecture remains. It is a significant challenge to produce a rugged, man-portable automated test system by selecting only those hardware and software components available from standard commercial test equipment catalogs. Yet, the RFP requirements in conjunction with cost and schedule constraints mandate the building of a target system from as many existing commercial building blocks as possible. This paper presents a systems approach to the design, development and deployment of man-portable test systems that meet or exceed the user's real UUT test requirements. The paper discusses in detail the best practices SEI developed in order to optimize resulting performance, size, weight, environmental protection, power requirements and reusability of man-portable systems.","1088-7725","0-7803-8449","10.1109/AUTEST.2004.1436751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436751","","Best practices;System testing;Energy consumption;Instruments;Packaging;Level control;Backplanes;Automatic control;Temperature control;Control systems","automatic test equipment;weapons;user interfaces;power consumption","man-portable automated test system;power consumption constraints;units-under-test;ATE architectures;instrument package;hardware components;software components;standard commercial test equipment catalogs;environmental protection;power requirements;bench-top configurations;rack-mount configurations;card level product;modular level instrument","","","","","","","","","IEEE","IEEE Conferences"
"Hardware/software fault tolerance with multiple task modular redundancy","C. P. Fuhrman; S. Chutani; H. J. Nussbaumer","Dept. of Comput. Sci., Swiss Federal Inst. of Technol., Lausanne, Switzerland; Dept. of Comput. Sci., Swiss Federal Inst. of Technol., Lausanne, Switzerland; Dept. of Comput. Sci., Swiss Federal Inst. of Technol., Lausanne, Switzerland","Proceedings IEEE Symposium on Computers and Communications","","1995","","","171","177","N-modular redundancy (NMR) and N-version programming (NVP) are two popular fault tolerance techniques in which hardware and software redundancy is exploited to mask faults. Redundant hardware is used to improve fault tolerance rather than throughput. We introduce a scheme for combined hardware-software fault tolerance derived from NMR and NVP that shows how redundancy can also be used to improve throughput by grouping the execution of several tasks. Our scheme uses a dynamic task allocation algorithm with an optimistic execution policy where the number of task executions is kept close to the minimum required to produce fault-free results. For equivalent hardware and software resources, the proposed method is 50% to 100% more efficient in terms of throughput and latency.","","0-8186-7075","10.1109/SCAC.1995.523663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=523663","","Hardware;Fault tolerance;Redundancy;Testing;Voting;Fault detection;Nuclear magnetic resonance;Throughput;Delay;Fault tolerant systems","redundancy;programming;fault tolerant computing;software fault tolerance;modules","hardware/software fault tolerance;multiple task modular redundancy;N-modular redundancy;N-version programming;software redundancy;hardware redundancy;redundant hardware;throughput;hardware-software fault tolerance;dynamic task allocation algorithm;optimistic execution policy;task executions;fault-free results;software resources;hardware resources;latency","","2","13","","","","","","IEEE","IEEE Conferences"
"SoRDS: platform for voice/video/network radio","S. Reichhart; D. Benfey; B. Youmans","AFRL/IFGC, Rome, NY, USA; NA; NA","2001 MILCOM Proceedings Communications for Network-Centric Operations: Creating the Information Force (Cat. No.01CH37277)","","2001","1","","202","207 vol.1","PAR Rome Research Corporation (RRC), in conjunction with AFRL (Rome Research Site, RRS), have developed a software radio development system (SoRDS) as a testbed for the development, evaluation and verification of communications waveforms. SoRDS is based on a heterogeneous multiprocessor hardware configuration running a real-time Linux operating system. The integration of multiple conventional processors with adaptive computing systems, i.e., field programmable gate arrays (FPGA), provides a platform for the rapid implementation and optimization of computationally intense waveform algorithms on a real-time system. We describe the SoRDS hardware/software configuration and how this is well suited for a real-time communications testbed. A number of different waveforms and applications have been implemented on SoRDS and are also presented These include narrowband voice, wideband video, error correction to assure data transfer and a networked radio configuration. Finally, plans for future utilization and extensions of SoRDS capabilities are discussed.","","0-7803-7225","10.1109/MILCOM.2001.985790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=985790","","Software testing;Hardware;Real time systems;Field programmable gate arrays;Software radio;System testing;Linux;Operating systems;Adaptive systems;Adaptive arrays","operating systems (computers);test facilities;radio equipment;waveform generators;telecommunication computing;military communication;military computing","PAR Rome Research Corporation;AFRL;software radio development system;communications waveforms;heterogeneous multiprocessor hardware configuration;real-time Linux operating system;adaptive computing systems;field programmable gate arrays;FPGA;waveform algorithms;narrowband voice communication;wideband video communication;error correction;radio network;military software radios;joint tactical radio system","","2","","","","","","","IEEE","IEEE Conferences"
"Net level aggregation using nonlinear optimization for the solution of hierarchical GSPN in performance evaluation","G. Klas","Siemens Corporate Res. & Dev., Munich, Germany","CompEuro 1992 Proceedings Computer Systems and Software Engineering","","1992","","","604","611","An approach for the hierarchical solution of large generalized stochastic Petri net models is presented. The method is based on the aggregation of submodels to substitute networks. The stochastic equivalence between these models is achieved by matching the flow time distributions of tokens in the submodel and in the aggregate net. This leads to a nonlinear optimization problem for finding the best aggregate net. As the main result, some insight is provided into the crucial point of estimating the parameters of a suitable aggregate net from a flow time distribution of the original net. The approach is demonstrated by means of an example.<<ETX>>","","0-8186-2760","10.1109/CMPEUR.1992.218465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=218465","","Aggregates;State-space methods;Stochastic processes;Parameter estimation;Testing;Network servers;Research and development;Tin;Petri nets;Performance evaluation","nonlinear programming;performance evaluation;Petri nets;stochastic processes","parameter estimation;net level aggregation;nonlinear optimization;hierarchical GSPN;performance evaluation;generalized stochastic Petri net models;submodels;stochastic equivalence;flow time distribution","","","22","","","","","","IEEE","IEEE Conferences"
"Significant microelectronics systems design experience for a heterogeneous class of CS, CE, and EE students","C. N. Purdy","Electron. Design Autom. Res. Center, Cincinnati Univ., OH, USA","Proceedings of International Conference on Microelectronic Systems Education","","1997","","","143","144","Using structured design techniques borrowed from software programming, beginning circuit designers at the University of Cincinnati consistently create correct, working chips containing several thousand gates. Careful choice of topics and easy-to-use tools allow consideration of physical behaviour and optimization techniques and also prepare students for more advanced study of circuit design. Chip testing is accomplished with user-friendly software and hardware created locally by a group of M.S. Students. Initially targeting CMOS tiny chip designs, course materials have recently been modified for programmable logic devices (PLDs). Immediate testability of PLDs provides better feedback, and the associated design tools support high-level hardware description languages (HDLs). This also makes multi-chip student projects feasible.","","0-8186-7996","10.1109/MSE.1997.612586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=612586","","Circuit testing;Very large scale integration;Computational modeling;Circuit simulation;Design optimization;Circuit synthesis;Software testing;Hardware design languages;Microelectronics;Computer science","integrated circuit design;electronic engineering education","microelectronics systems design;CS students;CE students;EE students;University of Cincinnati;optimization;testing;programmable logic device;CMOS chip;high-level hardware description language","","1","16","","","","","","IEEE","IEEE Conferences"
"A Data Base Driven Automated System for MOS Device Characterization, Parameter Optimization and Modeling","O. Melstrand; E. O'Neill; G. E. Sobelman; D. Dokos","Sperry Semiconductor Operations, Eagan, MN, USA; NA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1984","3","1","47","51","An automated system has been developed for use in the characterization and modeling of MOS transistors. The system, consisting of automatic testing, a dynamic data base, and device parameter extraction, has been applied to process characterization and device modeling. The data base handles storage and retrieval of the data. Parameter extraction is based on optimization with constraints.","0278-0070;1937-4151","","10.1109/TCAD.1984.1270056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270056","","MOS devices;System testing;Circuit testing;Parameter extraction;Design automation;Monitoring;Microcomputers;Software testing;Automatic testing;Information retrieval","","","","5","13","","","","","","IEEE","IEEE Journals & Magazines"
"Improved generalization through learning a similarity metric and kernel size","D. G. Lowe","Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada","Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)","","1993","1","","501","504 vol.1","Nearest-neighbour interpolation algorithms have many useful properties for applications to learning, but they often exhibit poor generalization. In this paper, it is shown that much better generalization can be obtained by using a variable interpolation kernel in combination with conjugate gradient optimization of the similarity metric and kernel size. The resulting method is called variable-kernel similarity metric (VSM) learning. It has been tested on a number of standard classification data sets, and on these problems it shows better generalization than backpropagation and most other learning methods. An important advantage is that the system can operate as a black box in which no model minimization parameters need to be experimentally set by the user. The number of parameters that must be determined through optimization are orders of magnitude less than for backpropagation or RBF networks, which may indicate that the method better captures the essential degrees of variation in learning.","","0-7803-1421","10.1109/IJCNN.1993.713963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713963","","Kernel;Learning systems;Interpolation;Neural networks;Testing;Optimization methods;Training data;Computer science;Application software;Radial basis function networks","neural nets;learning (artificial intelligence);generalisation (artificial intelligence);interpolation;optimisation","generalization;nearest-neighbour interpolation;learning;variable interpolation kernel;conjugate gradient optimization;variable-kernel similarity metric learning;neural network","","","11","","","","","","IEEE","IEEE Conferences"
"Information filtering for the newspaper","T. Nakashima; R. Nakamura","Kumamoto Univ., Japan; NA","1997 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing, PACRIM. 10 Years Networking the Pacific Rim, 1987-1997","","1997","1","","142","145 vol.1","A large quantity of information is spread out to our life, especially to our cyberspace. Not only are major newspaper companies providing a Web service, so are regional companies with their characteristic Web pages. We built an information filtering system using multi-agent for the article which a user wanted to read in the regional newspaper. Our system has two main agents. One is the ranking agent for the newspaper, the other is the feedback agent. The ranking agent ranks articles based on user keywords. The feedback agent feeds back to the user profile data based on the access frequency. We propose the ranking and feedback algorithms, and estimate these algorithms by experiments. Experiments show there is relation between filtered and read article by a chi-square test of 0.05 significance level. Also they show our system is efficient for the passage of time.","","0-7803-3905","10.1109/PACRIM.1997.619921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619921","","Information filtering;Feedback;Frequency;Information filters;Data models;Feeds;Relational databases;Web services;Testing;Computer networks","Internet;online front-ends;information retrieval systems;electronic publishing;software agents;knowledge based systems","information filtering;Web service;regional newspaper;multiagent;ranking agent;feedback agent;user keywords;feedback algorithm;ranking algorithm;experiments;chi-square test;significance level;computer networks;Web browser","","","2","","","","","","IEEE","IEEE Conferences"
"Fast prototyping of memory models in VHDL for hardware emulation","K. O'Brien; S. Maginot","LEDA S.A., Meylan, France; LEDA S.A., Meylan, France","Proceedings Seventh IEEE International Workshop on Rapid System Prototyping. Shortening the Path from Specification to Prototype","","1996","","","108","113","In this paper, we present a methodology whereby the whole synthesis and prototyping cycle can be speeded up simply by extending the acceptable VHDL subset to include hitherto unsynthesisable constructs. VHDL elaboration transformations as well as some compiler optimisation techniques can be performed to ensure that the VHDL model is still acceptable by commercial synthesis tools. The advantages of this methodology are shown using a real industrial application: the development of a generic VHDL memory model for fast system reconfiguration in a hardware emulation environment.","1074-6005","0-8186-7603","10.1109/IWRSP.1996.506736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506736","","Prototypes;Hardware;Emulation;Power system modeling;Control system synthesis;Optimizing compilers;Power generation;Design automation;Debugging;Turning","software prototyping;hardware description languages;optimising compilers;memory architecture","fast prototyping;memory models;VHDL;hardware emulation;prototyping cycle;unsynthesisable constructs;compiler optimisation techniques;commercial synthesis tools;real industrial application;generic VHDL memory model;system reconfiguration;hardware emulation environment","","","8","","","","","","IEEE","IEEE Conferences"
"Lifetime multiple response optimization of metal extrusion die","D. Lepadatu; A. Kobi; R. Hambli; A. Barreau","Quality & Reliability Dept., I.S.T.I.A., Angers, France; Quality & Reliability Dept., I.S.T.I.A., Angers, France; Quality & Reliability Dept., I.S.T.I.A., Angers, France; Quality & Reliability Dept., I.S.T.I.A., Angers, France","Annual Reliability and Maintainability Symposium, 2005. Proceedings.","","2005","","","37","42","The aim of this paper is to optimize the lifetime of dies for the multiple response in metal extrusion process. The main objective of the multiple response optimization is to improve the quality of a product or process by minimizing the effects of variation without eliminating the causes. Numerical simulations with ABAQUS software obtain the results of the tests.","0149-144X","0-7803-8824","10.1109/RAMS.2005.1408335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408335","","Fatigue;Costs;Optimization methods;Manufacturing processes;Temperature;Finite element methods;Software testing;Numerical simulation;Application software;Productivity","dies (machine tools);extrusion;finite element analysis;mechanical engineering computing","die lifetime;multiple response optimization;metal extrusion process;product quality;numerical simulation;ABAQUS software;finite element method","","4","17","","","","","","IEEE","IEEE Conferences"
"Benchmarking attribute cardinality maps for database systems using the TPC-D specifications","B. J. Oommen; M. Thiyagarajah","Sch. of Comput. Sci., Carleton Univ., Ottawa, Ont., Canada; Sch. of Comput. Sci., Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2003","33","6","913","924","Benchmarking is an important phase in developing any new software technique because it helps to validate the underlying theory in the specific problem domain. But benchmarking of new software strategies is a very complex problem, because it is difficult (if not impossible) to test, validate and verify the results of the various schemes in completely different settings. This is even more true in the case of database systems because the benchmarking also depends on the types of queries presented to the databases used in the benchmarking experiments. Query optimization strategies in relational database systems rely on approximately estimating the query result sizes to minimize the response time for user-queries. Among the many query result size estimation techniques, the histogram-based techniques are by far the most commonly used ones in modern-day database systems. These techniques estimate the query result sizes by approximating the underlying data distributions, and, thus, are prone to estimation errors. In two recent works , we proposed (and thoroughly analyzed) two new forms of histogram-like techniques called the rectangular and trapezoidal attribute cardinality maps (ACM), respectively, that give much smaller estimation errors than the traditional equi-width and equi-depth histograms currently being used by many commercial database systems. This paper reports how the benchmarking of the Rectangular-ACM (R-ACM) and the Trapezoidal-ACM (T-ACM) for query optimization can be achieved. By conducting an extensive set of experiments using the acclaimed TPC-D benchmark queries and database , we demonstrate that these new ACM schemes are much more accurate than the traditional histograms for query result size estimation. Apart from demonstrating the power of the ACMs, this paper also shows how the TPC-D benchmarking can be achieved using a large synthetic database with many different patterns of synthetic queries, which are representative of a real-world business environment.","1083-4419;1941-0492","","10.1109/TSMCB.2003.810909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245267","","Database systems;Benchmark testing;Query processing;Delay;Relational databases;Estimation error;Histograms;Business;Councils;Software testing","benchmark testing;query processing;relational databases;software engineering","attribute cardinality maps benchmarking;software technique;benchmarking;query optimization;relational database systems;histogram-like techniques;TPC-D benchmark queries","","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling complex systems by a set of interacting finite-state models","S. Kundu","Comput. Sci. Dept., Louisiana State Univ., Baton Rouge, LA, USA","Tenth Asia-Pacific Software Engineering Conference, 2003.","","2003","","","380","389","We present here a new way of modelling a complex system by a number of finite-state components which work together by transferring control among them in a fashion similar to the usual function-calls, including recursive calls. This gives us a simpler modelling technique than statecharts, which are often too complex for general users. The new technique also helps to keep the number of states small as in statecharts. We define the semantics of an interacting family of finite-state models in terms of their behavior-trees. As an elegant application of finite-state modelling, we present a maximally efficient controller design for a tree-structured set of tasks in a reactive system by taking advantage of the common computations among the tasks.","","0-7695-2011","10.1109/APSEC.2003.1254392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254392","","Buildings;Software testing;Computer science;Communication system control;Design optimization;Application software;Software design;Automatic control","finite state machines;program control structures;tree data structures;optimising compilers","complex system modeling;finite-state models;function-calls;recursive calls;statecharts;behavior-trees;tree-structured task","","2","16","","","","","","IEEE","IEEE Conferences"
"Design simulation tool to improve product reliability","W. D. Yates; D. M. Beaman","McDonnell Douglas Aerospace, St. Louis, MO, USA; McDonnell Douglas Aerospace, St. Louis, MO, USA","Annual Reliability and Maintainability Symposium 1995 Proceedings","","1995","","","193","199","The design community has historically been challenged to design-in reliability and maintainability (R&M) during the front-end of the design process. This has been difficult due to the budget profiles on most programs: R&M activities have not traditionally been funded up-front, and the designers are usually just too busy trying to make their designs work to worry about R&M. This paper describes a McDonnell Douglas Aerospace (MDA) independent research and development (IRAD) supportability sponsored project entitled ""Integrated Crew Chiefs Associate"". This IRAD has developed a new design simulation software tool that permits designers to assess and optimize their design for reliability up-front, during the early conceptual design phases of product development. The result has been a tool that allows the designer to reduce cycle time in developing a new prototype hydromechanical subsystem and its associated onboard embedded control logic software, the ability to optimize the configuration and subsystem reliability very early-on, and the ability to capture early design information for use throughout the acquisition process for performing reliability, maintainability, testability, and advanced logistics analysis.","0149-144X","0-7803-2470","10.1109/RAMS.1995.513245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=513245","","Maintenance;Software tools;Design optimization;Logic testing;Process design;Research and development;Software design;Product design;Product development;Software prototyping","product development;reliability;maintenance engineering;CAD;software tools;engineering computing;aerospace computing;aircraft instrumentation","design simulation tool;product reliability;maintainability;research and development;software tool;conceptual design;cycle time;hydromechanical subsystem;onboard embedded control logic software;acquisition process;testability;advanced logistics analysis;computer simulation","","2","","","","","","","IEEE","IEEE Conferences"
"Optimization in a hierarchical distributed performance monitoring system","Ling Shi; O. de Vel; Jiannong Cao; M. Cosnard","Dept. of Comput. Sci., James Cook Univ. of North Queensland, Townsville, Qld., Australia; Dept. of Comput. Sci., James Cook Univ. of North Queensland, Townsville, Qld., Australia; Dept. of Comput. Sci., James Cook Univ. of North Queensland, Townsville, Qld., Australia; Dept. of Comput. Sci., James Cook Univ. of North Queensland, Townsville, Qld., Australia","Proceedings 1st International Conference on Algorithms and Architectures for Parallel Processing","","1995","2","","537","544 vol.2","Monitoring program execution in a distributed system can generate large quantities of data, and the collection and processing of the monitoring data is one of the primary factors that contribute to the complexity of distributed monitoring. In order to reduce such complexity, a hierarchical distributed performance monitoring system has been developed. In this paper we describe an optimization method to improve the efficiency of the monitoring system. By considering the topology used by the application program and the distribution of monitoring records, an optimized grouping can be determined to obtain an improved performance for the monitoring system. The experiments presented in this paper have demonstrated such an improvement in performance.<<ETX>>","","0-7803-2018","10.1109/ICAPP.1995.472238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472238","","Computerized monitoring;Computer science;Prototypes;Binary search trees;Delay;Concurrent computing;Distributed computing;System testing;Centralized control;Communication system control","software performance evaluation;optimisation;distributed processing;program diagnostics","hierarchical distributed performance monitoring system;program execution monitoring;distributed monitoring;optimization method;optimized grouping","","8","10","","","","","","IEEE","IEEE Conferences"
"Component object modeling for beam physics problems","S. N. Andrianov","St. Petersburg State Univ., Russia","Proceedings of the 1999 Particle Accelerator Conference (Cat. No.99CH36366)","","1999","4","","2701","2703 vol.4","In recent years sufficient success was achieved in the modeling and optimization of beamlines on the base of high-order maps. In this paper a new approach based on symbolic representation of high-order aberrations in matrix forms is described. We discuss all the pros and cons of such an approach.","","0-7803-5573","10.1109/PAC.1999.792909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792909","","Physics;Object oriented modeling;Algebra;Software tools;Particle beams;Computer languages;Software engineering;Application software;Software testing;System testing","optimisation;aberrations;beam handling equipment;symbol manipulation;matrix algebra;beam handling techniques;particle accelerators;high energy physics instrumentation computing","component object modeling;beam physics problems;optimization;beamlines;high-order maps;symbolic representation;high-order aberrations;matrix forms;particle accelerators","","","12","","","","","","IEEE","IEEE Conferences"
"A comparison of reverse engineering tools based on design pattern decomposition","F. Arcelli; S. Masiero; C. Raibulet; F. Tisato","Dipt. di Informatica Sistemistica e Comunicazione, Univ. degli Studi di Milano, Milan, Italy; Dipt. di Informatica Sistemistica e Comunicazione, Univ. degli Studi di Milano, Milan, Italy; Dipt. di Informatica Sistemistica e Comunicazione, Univ. degli Studi di Milano, Milan, Italy; Dipt. di Informatica Sistemistica e Comunicazione, Univ. degli Studi di Milano, Milan, Italy","2005 Australian Software Engineering Conference","","2005","","","262","269","The usefulness of design patterns in forward engineering is already well-known and several tools provide support for their application in the development of software systems. While the role of design patterns in reverse engineering is still argued primarily due to their informal definition which leads to various possible implementations of each pattern. One of the most discussed aspects related to design patterns is about the need of their formalization according to the drawbacks this can represent. Formalization leads to the identification of the so-called sub-patterns, which are the recurring fundamental elements design patterns are composed of. In this paper we analyze the role sub-patterns play in two reverse engineering tools: FUJABA and SPQR. Attention is focused on how sub-patterns are exploited to define and to detect design patterns. To emphasize the similarities and differences between the two approaches, the composite design pattern is considered as example.","1530-0803;2377-5408","0-7695-2257","10.1109/ASWEC.2005.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402021","","Reverse engineering;Forward contracts;Design engineering;Application software;Software systems;Design optimization;Software design;Software architecture;Testing;Acceleration","reverse engineering;software tools;object-oriented programming;object-oriented methods;formal specification","reverse engineering tool;design pattern decomposition;FUJABA tool;SPQR tool;composite design pattern;formal specification","","9","24","","","","","","IEEE","IEEE Conferences"
"Compact binaries with code compression in a software dynamic translator","S. Shogan; B. R. Childers","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; Dept. of Comput. Sci., Pittsburgh Univ., PA, USA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","2","","1052","1057 Vol.2","Embedded software is becoming more flexible and adaptable, which presents new challenges for management of highly constrained system resources. Software dynamic translation (SDT) has been used to enable software malleability at the instruction level for dynamic code optimizers, security checkers, and binary translators. This paper studies the feasibility of using SDT to manage program code storage in embedded systems. We explore to what extent code compression can be incorporated in a software infrastructure to reduce program storage requirements, while minimally impacting run-time performance and memory resources. We describe two approaches for code compression, called full and partial image compression, and evaluate their compression ratios and performance in a software dynamic translation system. We demonstrate that code decompression is indeed feasible in a SDT.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269032","","Image coding;System-on-a-chip;Virtual manufacturing;Memory management;Image storage;Shadow mapping;Decoding;Computer science;USA Councils;Design automation","data compression;program interpreters;embedded systems;system-on-chip;virtual machines;cache storage","code compression;software dynamic translation system;embedded software systems;software malleability;dynamic code optimizers;binary translators;program code storage;software infrastructure;memory resources;full image compression;partial image compression;code decompression","","4","12","","","","","","IEEE","IEEE Conferences"
"Optimizing synchronous systems for multi-dimensional applications","N. L. Passos; E. H. -. Sha; Liang-Fang Chao","Dept. of Comput. Sci. & Eng., Notre Dame Univ., IN, USA; Dept. of Comput. Sci. & Eng., Notre Dame Univ., IN, USA; NA","Proceedings the European Design and Test Conference. ED&TC 1995","","1995","","","54","58","Time-critical sections of multi-dimensional problems, such as image processing applications, are in general iterative or recursive. In this paper these sections are modeled us cyclic multi-dimensional data flow graphs (MDFGs), which are also used to represent the digital circuit designed to compute such problems. Each node in the MDFG is associated with a set of functional elements in the circuit. Memory elements and circuit paths are associated with graph edges representing data dependencies. This new optimization technique consists of a multi-dimensional re-timing being applied to the MDFG to reduce its cycle time while considering memory requirements. This technique guarantees that all functional elements of a circuit, designed to be applied to problems involving more than one dimension, can be executed simultaneously. The algorithm runs in O.<<ETX>>","","0-8186-7039","10.1109/EDTC.1995.470420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470420","","Delay;Application software;Design optimization;Data flow computing;Digital circuits;Circuit synthesis;Registers;Concurrent computing;Computer science;Time factors","two-dimensional digital filters;timing;circuit optimisation;data flow graphs","multi-dimensional applications;time-critical sections;image processing applications;cyclic multi-dimensional data flow graphs;digital circuit;functional elements;memory elements;graph edges;data dependencies;optimization technique;multi-dimensional re-timing;memory requirements;2D digital filters","","1","8","","","","","","IEEE","IEEE Conferences"
"Failproof team projects in software engineering courses","A. T. Berztiss","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA","Proceedings Frontiers in Education 1997 27th Annual Conference. Teaching and Learning in an Era of Change","","1997","2","","1015","1019 vol.2","The computer science department of the University of Pittsburgh offers two undergraduate and two graduate courses in software engineering in which we emphasize the importance of general engineering principles for software development. For the last ten years the undergraduate courses have been based on team projects. This structure has advantages: students see immediately the relevance of what they learn, and the team setting leads to a better understanding of what they learn. The projects in the two courses are of different types. In one course the result is the formal specification and design of a software system. In the other, the teams implement such a system, but emphasis is on testing rather than on the implementation itself. The success of each project is guaranteed by making it open-ended. A team establishes a list of priorities that is to ensure that a useful product will have been built by the time the term ends. We discuss the nature of team projects, and our evaluation scheme.","0190-5848","0-7803-4086","10.1109/FIE.1997.636027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=636027","","Software engineering;Computer science;Programming;Design engineering;Computer science education;Teamwork;Formal specifications;Software design;Software systems;System testing","computer science education;formal specification;educational courses","failproof team projects;software engineering courses;Pittsburgh University;graduate courses;undergraduate courses;general engineering principles;course projects;formal specification;software system design;open-ended project","","2","9","","","","","","IEEE","IEEE Conferences"
"Application of Decision Theory to the Testing of Large Systems","P. J. Wong","Stanford Research Institute Menlo Park, Calif. 94025","IEEE Transactions on Aerospace and Electronic Systems","","1971","AES-7","2","379","384","A methodology for determining priorities in allocating test resources among the various subsystems within a large system is described. The methodology is based on concepts from applied decision theory. Two versions of the methodology are presented: a complete version, called the extensive form, and an approximate version, called the diminutive form.","0018-9251;1557-9603;2371-9877","","10.1109/TAES.1971.310378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4103708","","Decision theory;System testing;Aerospace testing;Hardware;Resource management;Software testing;Uncertainty;Software performance;Performance evaluation;Performance analysis","","","","2","7","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability analysis of the rank transform for stereo matching","J. Banks; M. Bennamoun","Med. Imaging & Cognitive Comput. Dept., Fraunhofer Inst. for Comput. Graphics, Darmstadt, Germany; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2001","31","6","870","880","The rank transform is a nonparametric technique which has been recently proposed for the stereo matching problem. The motivation behind its application to this problem is its invariance to certain types of image distortion and noise, as well as its amenability to real-time implementation. This paper derives one constraint which must be satisfied for a correct match. This has been termed the rank constraint. Experimental work has shown that this constraint is capable of resolving ambiguous matches, thereby improving matching reliability. A novel matching algorithm incorporating the rank constraint has also been proposed. This modified algorithm consistently resulted in an increased percentage of correct matches, for all test imagery used. Furthermore, the rank constraint has been used to devise a method of identifying regions of an image where the rank transform, and hence matching outcome, is more susceptible to noise. Experimental results have shown that the errors predicted using this technique are consistent with the actual errors which result when images are corrupted with noise. Such a method could be used to identify matches which are likely to be incorrect and/or provide a measure of confidence in a match.","1083-4419;1941-0492","","10.1109/3477.969491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969491","","Testing;Stereo vision;Constraint theory;Image matching;Face detection;Area measurement;Application software;Biomedical imaging;Computer graphics;Biomedical engineering","image matching;stereo image processing;transforms;reliability","reliability analysis;rank transform;nonparametric technique;stereo matching;real-time implementation;rank constraint;ambiguous match resolution;errors","","28","14","","","","","","IEEE","IEEE Journals & Magazines"
"A method for PWM rectifier line side filter optimization in transient and steady states","V. A. Katic; D. Graovac","Inst. for Power, Electron., Telecommun. Eng., Novi Sad Univ., Serbia; Inst. for Power, Electron., Telecommun. Eng., Novi Sad Univ., Serbia","IEEE Transactions on Power Electronics","","2002","17","3","342","352","The purpose of this paper is to develop, design and test an overall approach to line side filter optimization for the current type PWM controlled active rectifier and to investigate influence of control system to converters performance and filter size (TkVA). For the instantly predominating offline current rectifier control methods, a line side filter optimization algorithm and corresponding computer program was proposed and developed. It uses both complete transient and steady state, characteristics of system, ensuring minimized individual and total harmonic distortion (according to standards) of line current and a good power factor together with minimum costs. Suggested method was numerically verified through the extensive simulations using SIMULINK toolbox of MATLAB software. Final verification came from the implementation on a prototype. Detailed testing was performed and the proposed algorithm showed satisfying performance under different operating conditions. As far as the authors are aware, the novel contribution coming from this paper is in completing the line side filter optimization algorithm taking into account both transient and steady state of system, power supply demands and the power factor restriction for the offline controlled current type AC/DC converter.","0885-8993;1941-0107","","10.1109/TPEL.2002.1004242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004242","","Pulse width modulation;Rectifiers;Filters;Optimization methods;Control systems;Size control;Steady-state;Reactive power;System testing;Design optimization","PWM power convertors;AC-DC power convertors;rectifying circuits;power conversion harmonics;power harmonic filters;harmonic distortion;circuit optimisation;circuit simulation;power engineering computing","PWM rectifier line side filter optimization;active rectifier;transient characteristics;steady state characteristics;line current total harmonic distortion;power factor;SIMULINK;MATLAB;computer simulation;power supply demands;offline controlled current type AC/DC converter","","8","18","","","","","","IEEE","IEEE Journals & Magazines"
"Results from a large-scale study of performance optimization techniques for source code analyses based on graph reachability algorithms","D. Binkley; M. Harman","Loyola Coll., Baltimore, MD, USA; NA","Proceedings Third IEEE International Workshop on Source Code Analysis and Manipulation","","2003","","","203","212","Internally, many source code analysis tools make use of graphs. For example, one of the oldest and most widely used internal graphs is the control-flow graph developed for use within a compiler. Work on compilation has also led to the development of the call graph, the procedure dependence graph (PDG), and the static-single assignment (SSA) graph. Compilers are not the only source-code analysis tools to use internal graphs. A variety of software engineering tools incorporate a variety of different graphs. A study of techniques that improve graph-based program analysis is presented. Several different techniques are considered, including forming strongly-connected components, topological sorting, and removing transitive edges. Graph reachability, a pervasive graph analysis operation, is used as a representative graph analysis operation in the study. Data collected from a test bed of just over 1000000 lines of code is presented. This data illustrates the impact on computation time of the improvement techniques. Overall, the combination of all techniques produces a 71% reduction in run-time (and 64% reduction in memory usage). In other words, they increase the size of the problem that can be effectively handled by factor of over three times.","","0-7695-2005","10.1109/SCAM.2003.1238046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238046","","Large-scale systems;Optimization;Performance analysis;Algorithm design and analysis;Testing;Runtime;Educational institutions;Software engineering;Sorting;Tree graphs","program diagnostics;reachability analysis;graph theory;software performance evaluation;data structures","source code analysis;graph reachability algorithms;control flow graph;call graph;procedure dependence graph;SSA graph;static single assignment graph;graph based program analysis;strongly connected components;topological sorting;transitive edge removing;performance optimization","","9","20","","","","","","IEEE","IEEE Conferences"
"SAR processing using PVM","S. Wuyts; M. R. Inggs","Grintek, Centurion, South Africa; NA","Proceedings of the 1998 South African Symposium on Communications and Signal Processing-COMSIG '98 (Cat. No. 98EX214)","","1998","","","105","108","This paper presents a study of the use of PVM (parallel virtual machine) to improve the speed of synthetic aperture radar (SAR) image processing. PVM is a software system that enables a user-configurable pool of heterogeneous computers to be used as a virtual machine. The SAR processing test software performed simplified range and azimuth compression on simulated SAR images of a point target. Results show that PVM improves the time taken to process a SAR image once system factors are optimised.","","0-7803-5054","10.1109/COMSIG.1998.736931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=736931","","Virtual machining;Synthetic aperture radar;Image processing;Software systems;Software testing;Software performance;Performance evaluation;Azimuth;Image coding;Computational modeling","synthetic aperture radar;radar imaging;virtual machines;radar computing;parallel processing;multiprocessing systems","SAR processing;PVM;parallel virtual machine;synthetic aperture radar;SAR image processing;software system;user-configurable pool;heterogeneous computers;SAR processing test software;azimuth compression;range compression;point target","","","2","","","","","","IEEE","IEEE Conferences"
"Optimisation of the gas-exchange system of combustion engines by Genetic Algorithm","C. D. Rose; S. R. Marsland; D. Law","School of Engineering and Advanced Technology, Massey University, Palmerston North, New Zealand; School of Engineering and Advanced Technology, Massey University, Palmerston North, New Zealand; School of Engineering and Advanced Technology, Massey University, Palmerston North, New Zealand","2009 4th International Conference on Autonomous Robots and Agents","","2000","","","555","560","Current techniques for the optimisation of combustion engine gas-exchange systems still predominantly use trial and error. This paper proposes a new method for the optimisation of these systems through the use of modified Genetic Algorithm techniques, principally a variable length chromosome encoding. Promising initial results are presented and discussed.","","978-1-4244-2712-3978-1-4244-2713","10.1109/ICARA.2000.4804021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804021","genetic algorithm;variable-length input encoding;combustion engine;optimisation","Genetic algorithms;Internal combustion engines;Optimization methods;Encoding;Humans;Computer errors;Automotive engineering;Process design;Software testing;System testing","genetic algorithms;internal combustion engines","gas-exchange system optimisation;combustion engines;modified genetic algorithm techniques;variable length chromosome encoding","","","10","","","","","","IEEE","IEEE Conferences"
"Arithmetic transformations to maximise the use of compressor trees","P. Ienne; A. K. Verma","Processor Archit. Lab., Fed. Inst. of Technol., Lausanne, Switzerland; Processor Archit. Lab., Fed. Inst. of Technol., Lausanne, Switzerland","Proceedings. DELTA 2004. Second IEEE International Workshop on Electronic Design, Test and Applications","","2004","","","219","224","Complex arithmetic computations, especially if derived from bit-level software descriptions, can be very inefficient if implemented directly in hardware (e.g., by translation of the relevant C section in VHDL or Verilog). In this paper we show that known arithmetic optimisation techniques are in some cases insufficient to achieve the high-performance implementation that a designer could produce through an attentive study of the computation. We therefore introduce an algorithm to restructure dataflow graphs so that they can be synthesized in high-quality arithmetic circuits, especially when arithmetic operations are interspersed with logic operations. On typical software benchmarks, the new technique reduces the critical path by around 20-40% and generally achieves the quality of manual implementations. In many cases, our algorithm also manages to reduce the cell area by 10-20%.","","0-7695-2081","10.1109/DELTA.2004.10054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409843","","Arithmetic;Adders;Tree graphs;Hardware design languages;Design optimization;Circuit synthesis;Libraries;Kernel;Computer architecture;Laboratories","digital arithmetic;data flow graphs;trees (mathematics);hardware description languages;integrated logic circuits;optimisation","arithmetic transformations;compressor trees;complex arithmetic computations;bit-level software descriptions;VHDL;very high-speed integrated circuit hardware description language;Verilog;arithmetic optimisation;data flow graphs;high quality arithmetic circuits;arithmetic operations;logic operations;software benchmarks","","4","9","","","","","","IEEE","IEEE Conferences"
"Development and implementation of a rail current optimization program","T. L. King; R. Dharamshi; K. Kim; J. Zhang; M. W. Tompkins; M. A. Anderson; Q. Feng","Dept. of Electr. & Comput. Eng., Houston Univ., TX, USA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Magnetics","","1997","33","1","571","575","Efforts are underway to automate the operation of a railgun hydrogen pellet injector for fusion reactor refueling. A plasma armature is employed to avoid the friction produced by a sliding metal armature and, in particular, to prevent high-Z impurities from entering the tokamak. High currents are used to achieve high accelerations, resulting in high plasma temperatures. Consequently, the plasma armature ablates and accumulates material from the pellet and gun barrel. This increases inertial and viscous drag, lowering acceleration. A railgun model has been developed to compute the acceleration in the presence of these losses. The model suggests that, depending on the rail and insulator materials used, there is a point of diminishing returns. Namely, for a given current, there is an acceleration time beyond which little or no increase in pellet speed is produced. The optimal pulse width was determined by identifying the time at which the acceleration decreased to zero. In order to quantify these losses, the ablation coefficient, /spl alpha/, and drag coefficient, C/sub d/, must be determined. These coefficients are estimated based on the pellet acceleration. The sensitivity of acceleration to /spl alpha/ and C/sub d/ has been calculated using the model. Once /spl alpha/ and C/sub d/ have been determined, their values are applied to the model to compute the appropriate current pulse width. An optimization program was written in LabVIEW software to carry out this procedure. This program was then integrated into the existing code used to operate the railgun system. Preliminary results obtained after test firing the gun indicate that the program computes reasonable values for /spl alpha/ and C/sub d/ and calculates realistic pulse widths.","0018-9464;1941-0069","","10.1109/20.560076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=560076","","Rails;Acceleration;Railguns;Plasma temperature;Space vector pulse width modulation;Plasma accelerators;Plasma materials processing;Hydrogen;Fusion reactors;Friction","particle beam fusion accelerators;fusion reactors;railguns;optimisation","rail current optimization;railgun hydrogen pellet injector;fusion reactor refueling;plasma armature;tokamak;acceleration;ablation coefficient;drag coefficient;inertial drag;viscous drag;LabVIEW software;pulse width;automation;H","","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Optimised diabetes therapy knowledge-based systems: safety critical issues","J. L. Nealon; A. D. Jackson-Smale; R. R. Holman","Knowledge Eng. Res. Group, Oxford Brookes Univ., UK; NA; NA","IEE Colloquium on Knowledge-Based Systems for Safety Critical Applications","","1994","","","1/1","1/3","A recurring theme in physicians' criticism of medical knowledge-based systems, particularly where the determination of treatment is concerned, has been one of doubts on reliability. Acceptance criteria for reliability are related to risk and safety considerations. The development of two related safety critical knowledge-based systems for optimised diabetes therapy illustrates approaches to these problems. The project shows the importance of thorough testing and clinical trials in overcoming physicians' very reasonable fears concerning the use of safety critical medical computer systems.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=369622","","Medical expert systems;Medical treatment;Safety;Software reliability","medical expert systems;patient treatment;safety;software reliability","acceptance criteria;safety critical issues;medical knowledge-based systems;reliability;safety considerations;optimised diabetes therapy;clinical trials","","","","","","","","","IET","IET Conferences"
"Exploiting don't cares during data sequencing using genetic algorithms","N. Drechsler; R. Drechsler","Inst. of Comput. Sci., Albert-Ludwigs-Univ., Freiburg, Germany; NA","Proceedings of the ASP-DAC '99 Asia and South Pacific Design Automation Conference 1999 (Cat. No.99EX198)","","1999","","","303","306 vol.1","In this paper we present a Genetic Algorithm (GA) for the Data Ordering Problem (DOP) where Don't Cares (DCs) are assigned during optimization. The DOP has large application in the area of low power design and circuit testing. We implemented a GA to solve this problem and discuss several applications. We carried out a large set of experiments. A comparison of our results to previously published demonstrates the efficiency of our approach.","","0-7803-5012","10.1109/ASPDAC.1999.760019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=760019","","Genetic algorithms;Distributed control;Greedy algorithms;Hamming distance;Computer science;Application software;Circuit testing;Optimization methods;Very large scale integration;Runtime","genetic algorithms;integrated circuit design;circuit optimisation;circuit CAD","data sequencing;genetic algorithms;optimization;Data Ordering Problem;low power design;VLSI;CAD;IC design","","3","20","","","","","","IEEE","IEEE Conferences"
"A New Method for Constructing the Search Tree in Branch and Bound Algorithm","A. Jalilvand; S. Khanmohammadi","Department of Electrical Engineering, Zanjan University, Zanjan, Iran. ajalilvand@mail.znu.ac.ir; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran. khan@tabrizu.ac.ir","2005 Pakistan Section Multitopic Conference","","2005","","","1","5","The branch and bound (B&amp;B) algorithm is one of the common-used methods for solving the discrete optimization problems. In this method the optimal solution will be found by searching the space of solutions. The search space of most B&amp;B algorithms is inherently large and computationally complex. Hence constructing whole of the search space in applying the B&amp;B algorithm needs a large memory size. This paper presents a new method to construct search tree in B&amp;B algorithm gradually. In this method the search tree is formed step by step. Each node is constructed when it must be tested and there isn't need to construct the whole search tree at once. This method needs a minimum size of memory. Two lemmas are proposed and proved related to this new method","","0-7803-9429-10-7803-9430","10.1109/INMIC.2005.334438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4133453","","Optimization methods;Cost function;Optical computing;Testing;Computer applications;Application software","computational complexity;matrix algebra;optimisation;tree searching","search tree;branch and bound algorithm;discrete optimization problems;search space","","","8","","","","","","IEEE","IEEE Conferences"
"Dynamic feature traces: finding features in unfamiliar code","A. D. Eisenberg; K. De Volder","Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada; Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada","21st IEEE International Conference on Software Maintenance (ICSM'05)","","2005","","","337","346","This paper introduces an automated technique for feature location: helping developers map features to relevant source code. Like several other automated feature location techniques, ours is based on execution-trace analysis. We hypothesize that these techniques, which rely on making binary judgments about a code element's relevance to a feature, are overly sensitive to the quality of the input. The main contribution of this paper is to provide a more robust alternative, whose most distinguishing characteristic is that it employs ranking heuristics to determine a code element's relevance to a feature. We believe that our technique is less sensitive with respect to the quality of the input and we claim that it is more effective when used by developers unfamiliar with the target system. We validate our claim by applying our technique to three systems with comprehensive test suites. A developer unfamiliar with the target system spent a limited amount of effort preparing the test suite for analysis. Our results show that under these circumstances our ranking-based technique compares favorably to a technique based on binary judgements.","1063-6773","0-7695-2368","10.1109/ICSM.2005.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510129","","System testing;Computer science;Software maintenance;Robustness;Software engineering;Feedback loop;Performance analysis;Design for testability;Visualization;Reconnaissance","program diagnostics;software maintenance","dynamic feature traces;source code;automated feature location technique;execution-trace analysis;ranking-based technique;unfamiliar code feature determination","","51","20","","","","","","IEEE","IEEE Conferences"
"A power optimized display memory organization for handheld user terminals","L. Hollevoet; A. Dewilde; K. Denolf; F. Catthoor; F. Louagie","IMEC, Heverlee, Belgium; IMEC, Heverlee, Belgium; IMEC, Heverlee, Belgium; IMEC, Heverlee, Belgium; IMEC, Heverlee, Belgium","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","3","","294","299 Vol.3","Today's handheld devices become more and more multimedia capable. One subsystem of a multimedia terminal that accounts for a considerable amount of the total power consumption is the display unit. The backlight is the major culprit there. As new display units without backlights emerge, the data transfers required to put data on the screen start using up an increasingly important part of the platform's power. We have examined a novel system view that allows for power savings by decreasing the required number of memory accesses to put a frame on the screen. A two-step optimization method for existing platforms is presented. Measurements on a multimedia application show that, on average, power savings of 72% can be obtained on the display related memory accesses. For the proposed optimizations methods to work, it is important that both hardware and software designers become aware of the impact their design-time decisions have on the final power consumption of a system.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269253","","Liquid crystal displays;Energy consumption;Optimization methods;Hardware;Flat panel displays;Large screen displays;Handheld computers;Batteries;Design optimization;Power measurement","liquid crystal displays;portable instruments;multimedia communication;power consumption","power optimized display memory organization;handheld user terminals;handheld devices;display units;power savings;memory access;two step optimization method;multimedia terminals;power consumption","","3","9","","","","","","IEEE","IEEE Conferences"
"Multiple fault diagnostics for communicating nondeterministic finite state machines","R. Belhassine-Cherif; A. Ghedamsi","SYSCOM, Tunis, Tunisia; NA","Proceedings. Sixth IEEE Symposium on Computers and Communications","","2001","","","661","666","During the last decade, different methods were developed to produce optimized test sequences for detecting faults in, communication protocol implementations. However, the application of these methods gives only limited information about the location of detected faults. We propose a complementary step, which localizes the faults, once detected. It consists of a generalized diagnostic algorithm for the case where more than one fault may be present in the transitions of a system represented by communicating nondeterministic finite state machines, if existing faults are detected, this algorithm permits the generation of a minimal set of diagnoses, each of which is formed by a set of transitions suspected of being faulty. A simple example is used to demonstrate the functioning of the proposed diagnostic algorithm. The complexity of each step in the algorithm are calculated.","1530-1346","0-7695-1177","10.1109/ISCC.2001.935446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935446","","Automata;Fault detection;Electronic mail;Protocols;Software testing;System testing;Optimization methods;Communication system software;Hardware","finite state machines;sequences;optimisation;protocols;fault diagnosis;computational complexity","multiple fault diagnostics;communicating nondeterministic finite state machines;optimized test sequences;fault detection;communication protocol implementation;fault localization;generalized diagnostic algorithm;algorithm complexity","","","9","","","","","","IEEE","IEEE Conferences"
"A compiler-based approach for improving intra-iteration data reuse","M. Kandemir","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","984","990","Intra-iteration data reuse occurs when multiple array references exhibit data reuse in a single loop iteration. An optimizing compiler can exploit this reuse by clustering (in the loop body) array references with data reuse as much as possible. This reduces the number of intervening references between references to the same array and improves overall execution time and energy consumption. In this paper, we present a strategy where inter-statement and intra-statement optimizations are used in concert for optimizing intra-iteration data reuse. The objective is to cluster (within the loop body) the array references with spatial or temporal reuse. Using four array-intensive applications from image processing domain, we show that our approach improves the cache behavior of programs by 13.8% on the average.","1530-1591","0-7695-1471","10.1109/DATE.2002.998419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998419","","Optimizing compilers;Computer science;Energy consumption;Image processing","optimising compilers;iterative methods;parallel processing;cache storage;software performance evaluation;embedded systems","compiler-based approach;intra-iteration data reuse;multiple array references;optimizing compiler;clustering;overall execution time;energy consumption;inter-statement optimizations;intrastatement optimizations;array references;spatial reuse;temporal reuse;array-intensive applications;image processing domain;cache behavior;single loop iteration","","4","9","","","","","","IEEE","IEEE Conferences"
"Software channel approach to data banding","P. C. Arnett; D. Lam","IBM Almaden Res. Center, San Jose, CA, USA; IBM Almaden Res. Center, San Jose, CA, USA","IEEE Transactions on Magnetics","","1990","26","5","2324","2326","A software channel is used to study the capacity gains made possible by data banding. Data banding increases the data rate in steps as the head moves from ID (inner diameter) to OD (outer diameter) to keep the linear bit density more nearly constant. The software channel is used for this experiment because of the ease in changing data rates and codes and optimizing other channel parameters for each band. The components studied are an MIG (metal-in-gap) head of 9.1- mu m track width on a thin-film disk of 900-Oe coercivity. The projected gains in disk storage capacity using five data bands, an error-rate criterion for maximum data rate, and optimization of channel parameters for each band vary from 15% to 27% depending on code and the use of equalization.<<ETX>>","0018-9464;1941-0069","","10.1109/20.104713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=104713","","Hardware;Magnetic heads;Error analysis;Disk recording;Sampling methods;Software testing;Transistors;Coercive force;Magnetic recording;Density measurement","hard discs;magnetic recording","software channel;capacity gains;data banding;linear bit density;thin-film disk;coercivity;disk storage capacity;equalization","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Swarm intelligence based design of intelligent house embedded system","Hui Xiao; Yunshi Xiao; Jiguang Yue; Cong Cao; Chen Ye; Lei Jiang","Sch. of Electron. & Information Eng., Tongji Univ., Shanghai, China; Sch. of Electron. & Information Eng., Tongji Univ., Shanghai, China; Sch. of Electron. & Information Eng., Tongji Univ., Shanghai, China; Sch. of Electron. & Information Eng., Tongji Univ., Shanghai, China; Sch. of Electron. & Information Eng., Tongji Univ., Shanghai, China; Sch. of Electron. & Information Eng., Tongji Univ., Shanghai, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","","2004","6","","5370","5374 Vol.6","A swarm intelligence is introduced into the analysis and design of intelligent house in this paper. The system performance, software and hardware framework, and the module function of the intelligent house system are discussed thoroughly. The general framework of the realized intelligent house embedded system is derived. Software framework of the system is based on Web and XML technology, which can be highly modularized and easily developed, tested and maintained. Based on the network technology, the control of the electric equipment is focused upon its independent intelligent control characteristics. The core of the hardware framework is a 32-bit embedded processor. The circumstance control of the intelligent house is implemented with the technology of communication, sensors and computer networks.","","0-7803-8273","10.1109/WCICA.2004.1343752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1343752","","Particle swarm optimization;Intelligent systems;Embedded system;Hardware;Communication system control;Intelligent control;Intelligent sensors;System performance;Software performance;Software systems","home automation;embedded systems;systems software;Internet;XML;intelligent control","intelligent house embedded system;swarm intelligence;system software framework;Web technology;XML technology;intelligent control characteristics;hardware framework core;32-bit embedded processor","","","","","","","","","IEEE","IEEE Conferences"
"Memory access optimization of dynamic binary translation for reconfigurable architectures","Se Jong Oh; Tag Gon Kim","Dept. of Electr. Eng. & Comput. Sci., Korea Adv. Inst. of Sci. & Technol., South Korea; Dept. of Electr. Eng. & Comput. Sci., Korea Adv. Inst. of Sci. & Technol., South Korea","ICCAD-2005. IEEE/ACM International Conference on Computer-Aided Design, 2005.","","2005","","","1014","1020","Recently, reconfigurable architectures, which outperform DSP processors, have become important. Although many compilers have been developed on a source-level, there are several practical benefits to translating the binary targeted to popular processors onto reconfigurable architectures. However, the translated code could be less optimized. In particular, it is difficult to optimize memory accesses on a binary to exploit pipeline parallelism. This paper introduces dynamic binary translation and memory access optimization to overcome the limitations of static binary translation for reconfigurable architecture. The experimental results show a promising speedup up to 3.02 compared with the code whose memory accesses is not optimized in the pipeline fashion.","1092-3152;1558-2434","0-7803-9254","10.1109/ICCAD.2005.1560210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1560210","","Reconfigurable architectures;Hardware;Pipelines;Virtual machining;Runtime;Application software;Embedded software;Logic testing;Software testing;System testing","reconfigurable architectures;program interpreters;storage management;binary codes;memory architecture","memory access optimization;dynamic binary translation;reconfigurable architecture","","1","14","","","","","","IEEE","IEEE Conferences"
"Scalable parallel list ranking of image edges on fine-grained machines","J. N. Patel; A. A. Khokhar; L. H. Jamieson","Sch. of Electr. Eng., Purdue Univ., West Lafayette, IN, USA; Sch. of Electr. Eng., Purdue Univ., West Lafayette, IN, USA; Sch. of Electr. Eng., Purdue Univ., West Lafayette, IN, USA","Proceedings of 9th International Parallel Processing Symposium","","1995","","","717","721","We present analytical and experimental results for fine-grained list ranking algorithms, with the objective of examining how the locality properties of image edge lists can be used to improve the performance of this highly data-dependent operation. Starting with Wyllie's (1979) algorithm and Anderson and Miller's (1990) randomized algorithm as bases, we use the spatial locality of edge links to derive scalable algorithms designed to exploit the characteristics of image edges. Tested on actual and synthetic edge data, this approach achieves significant speedup on the MasPar MP-1 and MP-2, compared to the standard list ranking algorithms. The modified algorithms exhibit good scalability and are robust across a wide variety of images.<<ETX>>","","0-8186-7074","10.1109/IPPS.1995.395869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395869","","Scalability;Algorithm design and analysis;Computer vision;High performance computing;Artificial intelligence;Image analysis;Performance analysis;Testing;Graph theory;Phase change random access memory","list processing;parallel algorithms;randomised algorithms;edge detection;parallel machines;software performance evaluation","fine-grained machines;scalable parallel list ranking;image edges;fine-grained list ranking algorithms;locality properties;image edge lists;performance;data-dependent operation;randomized algorithm;edge link spatial locality;actual edge data;synthetic edge data;speedup;MasPar MP-1;MasPar MP-2","","","10","","","","","","IEEE","IEEE Conferences"
"Ground data system risk mitigation techniques for faster, better, cheaper missions","J. Catena; R. Casasanta; R. Saylor; C. Weikel","NASA Goddard Space Flight Center, Greenbelt, MD, USA; NA; NA; NA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","1","","1/513","1/521 vol.1","The Small Explorer (SMEX) program's ground data system (GDS) team developed risk mitigation techniques. These approaches have developed through the SMEX series of missions and are practised today under the Triana program. These techniques are: (1) Mission team organization-empowerment of a close-knit ground data system team comprising system engineering, software engineering, testing, and flight operations personnel. (2) Common spacecraft test and operational control system-utilization of the prelaunch spacecraft integration system as the post-launch ground data system on-orbit command and control system. (3) Utilization of operations personnel in prelaunch testing-making the flight operations team an integrated member of the spacecraft testing activities at the beginning of the spacecraft fabrication phase. (4) Consolidated test team-combining system, mission readiness and operations testing to optimize test opportunities with the ground system and spacecraft. (5) Reuse of spacecraft, systems, standards, and people obtaining greater efficiencies through reuse of common spacecraft components, flight and ground-based subsystems, standardized interfaces, and project team personnel. The SMEX ground system development approach for faster, better, cheaper missions has been very successful. This paper discusses these risk management techniques in the areas of ground data system design, implementation, test and operational readiness.","","0-7803-6599","10.1109/AERO.2001.931742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931742","","Data systems;System testing;Software testing;Space vehicles;Personnel;Aerospace engineering;Data engineering;Systems engineering and theory;Software engineering;Control systems","ground support systems;space vehicles;risk management;aerospace testing;aerospace computing","ground data system;Small Explorer program;risk mitigation techniques;data system design;implementation;operational readiness;Triana program;mission team organization;system engineering;software engineering;prelaunch testing;operational control system;mission readiness","","","3","","","","","","IEEE","IEEE Conferences"
"Multiaccess in a Nonqueueing Mailbox Environment","M. J. Ferguson","INRS Tlcommunications, Montreal, P.Q., Canada.","IEEE Transactions on Software Engineering","","1984","SE-10","3","237","243","A new and flexible solution to the problem of multiple users accessing a single resource, such as communication bandwidth or composite object in memory, is derived. The means of communication consists of sending and receiving messages in known locations (or equivalently, mailboxes without queueing). Any particular user is able to deposit, and hence destroy, previous messages in a mailbox. It is assumed that exclusive access to a mailbox is supplied by an underlying system. The major results of this paper are: 1) a simple tree-based algorithm that guarantees  no user or group of users can conspire to prevent access by some other user to the resource;  only one user accesses the resource at a time;  if there are N users, an individual user is guaranteed access, when requested, to the resource in no more than N-1 turns; Knuth's solution [6] can delay a user up to 2** (N-1)-1 turns; 2) an extension of Dekker's algorithm (2 users) [2] that allows the relative rates of reservations for access to the resource to be proportional to a set of N integers. When a reservation is not being used by its ``owner,'' it will be assigned to another contending request. The assignment is optimal for periodic requests.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1984.5010232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010232","Access reservation priorities;extension of Dekker's algorithm;mailbox communication environment;many user exclusive access;single resource multiaccess;single resource mutually exclusive access;tree-based multiaccess","Bandwidth;Broadcasting;Multiprocessing systems;System testing;Timing;Delay;Time of arrival estimation;Software testing","","","","2","6","","","","","","IEEE","IEEE Journals & Magazines"
"Experimental application of digital PD measurements to the diagnostics of railway system drives insulating systems","L. Centurioni; G. Coletti; F. Guastavino; A. Colombo","Dept. of Electr. Eng., Genoa Univ., Italy; NA; NA; NA","ICSD'98. Proceedings of the 1998 IEEE 6th International Conference on Conduction and Breakdown in Solid Dielectrics (Cat. No.98CH36132)","","1998","","","131","134","The present work explores the possibility of introducing, in the service monitoring of electrical drives for railway systems (or for similar transportation systems), a diagnostic tool of the relevant insulating system, based on the digital measurement of partial discharges (PD) by means of PRPDA (Phase Resolved Partial Discharge Analyser) devices. The relevant investigation regarded both new and ""aged"" (that is having a record of 3500 service hours) asynchronous rotating machines, which, after a specific optimisation of the testing procedure and of the testing circuit, have been subjected to several PD measuring campaigns, run at four different AC voltages. A suitable analysis of the relevant PD data, performed through an ""ad hoc"" software, allowed to identify several indicators (derived quantities and/or statistical parameters related to the acquired PD patterns) which have a potential for a clear discrimination between new and ""aged"" insulation. Therefore the results of this preliminary work evidence that a correct application of digital PD measuring techniques can offer a sound basis to develop a tool to monitor the conditions of the insulating system of electrical drives for railways systems.","","0-7803-4237","10.1109/ICSD.1998.709243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=709243","","Partial discharges;Circuit testing;Partial discharge measurement;Rail transportation;Dielectrics and electrical insulation;Electric variables measurement;Monitoring;Drives;Phase measurement;Rotating machines","railways;induction motor drives;machine insulation;partial discharge measurement;insulation testing;condition monitoring;ageing;digital instrumentation","railway system drive insulation diagnostics;digital PD measurements;service monitoring;electrical drives;phase resolved partial discharge analyser devices;condition monitoring;asynchronous rotating machines;testing procedure optimisation;AC voltage;ad hoc software;statistical parameters;aged insulation;700 kVA;1770 V;2.5 to 4 kV","","","9","","","","","","IEEE","IEEE Conferences"
"Data memory organization and optimizations in application-specific systems","P. Ranjan Panda; N. D. Dutt; A. Nicolau; F. Catthoor; A. Vandecappelle; E. Brockmeyer; C. Kulkarni; E. De Greef","Synopsys Inc., Mountain View, CA, USA; NA; NA; NA; NA; NA; NA; NA","IEEE Design & Test of Computers","","2001","18","3","56","68","In application-specific designs, customized memory organization expands the search space for cost-optimized solutions. Several optimization strategies can be applied to embedded systems with several different memory architectures: data cache, scratch-pad memory, custom memory architectures, and dynamic random-access memory (DRAM).","0740-7475;1558-1918","","10.1109/54.922803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922803","","Design optimization;Memory architecture;Pattern analysis;Neural networks;Embedded system;Random access memory;Testing;Application software;Intelligent structures","memory architecture;embedded systems","application-specific designs;customized memory organization;optimization strategies;memory architectures;embedded systems;data cache;scratch-pad memory;custom memory architectures;dynamic random-access memory;DRAM","","19","18","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic functional verification of memory oriented global source code transformations","K. C. Shashidhar; M. Bruynooghe; F. Catthoor; G. Janssens","IMEC, Heverlee, Belgium; NA; NA; NA","Eighth IEEE International High-Level Design Validation and Test Workshop","","2003","","","31","36","In this paper, we present a fully automatic technique to verify an important class of optimizing program transformations applied to reduce accesses to the data memory. These are prevalent while developing software for power and performance-efficient embedded multimedia systems. The verification of the transformations relies on an automatic proof of functional equivalence of the initial and the transformed program functions. It is based on extracting and reasoning on the polyhedral models representing the dependencies between the elements of the output and the input variables, which are preserved under the transformations considered. If the verification reports failure, the technique also identifies the errors and their location in the function, hence providing an effective means to debug the transformed program function.","","0-7803-8236","10.1109/HLDVT.2003.1252471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252471","","Multimedia systems;Signal processing algorithms;Memory management;Embedded software;Software performance;Data mining;Power system modeling;Algorithm design and analysis;Embedded system","optimising compilers;compiler generators;storage management;multimedia computing;embedded systems","automatic functional verification;global source code transformations;memory oriented transformations;optimizing program transformations;data memory;embedded multimedia;polyhedral models;custom memory management;loop transformations;input-output functional equivalence;sequential imperative program functions;optimizing compiler","","1","11","","","","","","IEEE","IEEE Conferences"
"JPEG2000 software implementation","D. Novosel; M. Kovac","RIZ transmitters Ltd., Zagreb, Croatia; NA","Proceedings. Elmar-2004. 46th International Symposium on Electronics in Marine","","2004","","","573","578","JPEG2000 is latest ISO/IEC standard for still image coding. Its core part has already been published as an international standard (IS), and work on its subsequent parts is under way. In this paper, main features of JPEG2000 still image coding standards will be briefly presented, as well as an encoder implementation compliant with core part of JPEG2000. Compliance and end user testing of this implementation will be presented, as well as some propositions regarding its further testing and optimization.","1334-2630","953-7044-02","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1356445","","Transform coding;Image coding;Testing;Standards publication;Code standards;Streaming media;Transmitters;Electrical engineering;ISO;IEC","image coding;code standards","JPEG2000;ISO-IEC standard;still image coding;international standard;compliance testing;end user testing;software implementation","","","8","","","","","","IEEE","IEEE Conferences"
"Neighborhood based Levenberg-Marquardt algorithm for neural network training","G. Lera; M. Pinzolas","Dept. Automatica y Computacion, Univ. Publica de Navarra, Pamplona, Spain; NA","IEEE Transactions on Neural Networks","","2002","13","5","1200","1203","Although the Levenberg-Marquardt (LM) algorithm has been extensively applied as a neural-network training method, it suffers from being very expensive, both in memory and number of operations required, when the network to be trained has a significant number of adaptive weights. In this paper, the behavior of a recently proposed variation of this algorithm is studied. This new method is based on the application of the concept of neural neighborhoods to the LM algorithm. It is shown that, by performing an LM step on a single neighborhood at each training iteration, not only significant savings in memory occupation and computing effort are obtained, but also, the overall performance of the LM method can be increased.","1045-9227;1941-0093","","10.1109/TNN.2002.1031951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1031951","","Neural networks;Neurons;Testing;Backpropagation algorithms;Multilayer perceptrons;Optimization methods;Software performance;Software tools;Computer languages","neural nets;learning (artificial intelligence);optimisation","neighborhood based Levenberg-Marquardt algorithm;neural network training;adaptive weights;LM algorithm;memory occupation;performance;learning;optimization","","117","12","","","","","","IEEE","IEEE Journals & Magazines"
"Computer morphometry for liver fibrosis using an automatic image analysis system","Xi-Zhang Lin; Yung-Nien Sun; Ming-Huwi Horng; Xiao-Zhen Guo","Med. Coll., Nat. Cheng Kung Univ., Tainan, Taiwan; NA; NA; NA","Proceedings of 18th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","1996","2","","682","683 vol.2","Liver biopsy is the gold standard for evaluation of liver diseases. The severity of fibrosis is considered the stage of chronic liver disease. However, it is evaluated by subjective description or semiquantitation. The authors developed an automatic image analysis system, which including a microscope, computer-driven slide-driver, and a software system for image acquisition, processing and data analysis. The automatic image analysis system deals mainly with color image segmentation. The severity of liver fibrosis is reported as the percentage of the entire fibrous area to the whole liver tissue area. Thirty-one liver needle biopsy specimens are used for this study. The results from computer morphometry were compared with that from colorimetric method and Knodell's score. Pearson correlation and Spearman rank test revealed that the Kondell's score and colorimetric method are significantly correlated with the result of computer morphometry. The authors found that the system is a reliable tool for evaluating the severity of liver fibrosis.","","0-7803-3811","10.1109/IEMBS.1996.651926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=651926","","Biopsy;Liver diseases;Image color analysis;Gold;Microscopy;Software systems;Data analysis;Image segmentation;Needles;Testing","liver;medical image processing;optical microscopy;image segmentation;colorimetry;cellular biophysics","computer morphometry;liver fibrosis;automatic image analysis system;chronic liver disease;computer-driven slide-driver;software system;color image segmentation;liver needle biopsy specimens;colorimetric method;Knodell's score;Pearson correlation;Spearman rank test","","","4","","","","","","IEEE","IEEE Conferences"
"The rapid development of hybrid ISG control system by on-line debugging system","Wang Baohua; Luo Yongge; Zhang Jianwu","Sch. of Mech. Eng., Shanghai Jiao Tong Univ., China; Sch. of Mech. Eng., Shanghai Jiao Tong Univ., China; Sch. of Mech. Eng., Shanghai Jiao Tong Univ., China","IEEE International Conference on Vehicular Electronics and Safety, 2005.","","2005","","","305","310","An integrated starter/generator (ISG) hybrid propulsion system is being developed that has a parallel configuration. A small engine which is used to supply power approximately equal to the average load power is coaxially configured with an induction motor with an external rotor which is used to supply the peaking power required by the required peaking load of vehicle. The motor can also absorb the excess power of the engine while the load power is less than the peak power. The excess power will be used to charge the vehicular battery pack to keep state of charge (SOC) of the battery pack as the regenerative braking power. With the electrically assist principle, a controller for hybrid ISG assemble has been developed, and a fuzzy logic control strategy used to control the ISG system and motor was discussed. The control system for hybrid propulsion system is responsible for collecting information relative to vehicle and judges the load rate of engine and motor, and finally decides the operating state and rate of power distribution between engine and motor. By optimizing the system efficiency and minimizing fuel consumption and emission, the optimal performance will be reached. Electronics control unit (ECU) of hybrid propulsion system controls ECUs of engine, motor and automatic mechanics transmission (AMT), and monitors the state of battery management system (BMS), they communicate with each other and exchange information by control area network (CAN) bus interface. In order to accelerate developing process of hybrid propulsion system, a rapid developing system called on-line debugging and programming software has been developed. This system runs in PC, and communicates with the ECU of hybrid propulsion system by serial communication interface (SCI) RS232, but the ECU of hybrid propulsion system communicates with the ECUs of engine, AMT, BMS and accessory by CAN bus. This system mainly includes the following functions: (1) field test and bench test, (2) parameter calibration, (3) on-ling debugging for HEV, AMT and CAN bus, (4) vehicle dynamics simulation, (5) on-line programming, (6) troubleshooting etc. Those functions may achieve the rapid development on hardware and software of control system for hybrid propulsion system. In addition, the on-line debugging and programming system can monitor state information of overall system and modify parametric maps or update control program onboard by the on-line debugging or programming function. Based on the on-line debugging software package developed by our group at HUBEI automotive industries institute, the hybrid ISG assemble and its controller have been designed. Finally, experiments verify that the system is feasible and reliable, it can save 30%-50% developing time for the ISG hybrid propulsion system.","","0-7803-9435","10.1109/ICVES.2005.1563662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563662","","Control systems;Debugging;Propulsion;Engines;Automatic control;Functional programming;Power supplies;Vehicles;Batteries;System testing","hybrid electric vehicles;program debugging;starting;automotive components;DC motors;fuzzy control;control system synthesis;controller area networks;control engineering computing;software packages;software engineering;electric propulsion;power engineering computing","on-line debugging system;integrated starter/generator;hybrid propulsion;state of charge;regenerative braking power;fuzzy logic control strategy;power distribution;electronics control unit;automatic mechanics transmission;battery management system;control area network;CAN bus interface;programming software;serial communication interface;field test;bench test;parameter calibration;HEV;vehicle dynamics simulation;on-line programming;software package","","1","7","","","","","","IEEE","IEEE Conferences"
"Generation of interconnect topologies for communication synthesis","M. Gasteier; M. Munch; M. Glesner","Darmstadt Univ. of Technol., Germany; NA; NA","Proceedings Design, Automation and Test in Europe","","1998","","","36","42","One of the key problems in hardware/software co-design is communication synthesis which determines the amount and type of interconnect between the hardware components of a digital system. To do so, communication synthesis derives a communication topology to determine which components are to be connected to a common communication channel in the final hardware implementation. In this paper, we present a novel approach to cluster processes to share a communication channel. An iterative graph-based clustering algorithm is driven by a heterogeneous cost function which takes into account bit widths, the probability of access collisions on the channels, cost for arbitration logic as well as the availability of interface resources on the hardware components to trade-off cost against performance in a most optimum fashion. The key aspects of the approach are demonstrated on a small example.","","0-8186-8359","10.1109/DATE.1998.655834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655834","","Topology;Hardware;Cost function;Communication channels;Communication system software;Digital systems;Clustering algorithms;Iterative algorithms;Logic;Availability","circuit layout CAD;network topology;integrated circuit layout;graph theory;circuit optimisation;iterative methods;integrated circuit interconnections","interconnect topologies generation;communication synthesis;hardware/software codesign;communication topology;communication channel;iterative graph-based clustering algorithm;heterogeneous cost function;bit widths;access collisions probability;arbitration logic cost;interface resources availability","","10","11","","","","","","IEEE","IEEE Conferences"
"When the project absolutely must get done: marrying the organization chart with the precedence diagram","S. Rifkin","Master Syst. Inc., McLean, VA, USA","Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium","","2000","","","588","596","Presents a new project planning technique to marry the organization chart with a project's task precedence diagram. This permits one to simulate the project at a micro-scale, project-specific level. One can perform ""what-if"" scenarios related to organizational structures, the deployment of specific individuals and skills, and the structure of information flow and exception handling in a project. The tool used, ViteProject, was developed in a Stanford University laboratory, where substantial results have been achieved when it was applied to design activities other than software. The author presents his real-world experience with several software projects, where ViteProject has improved project visibility and allowed projects to be rationally optimized in a way that has hitherto been impossible.","0270-5257","1-58113-206","10.1145/337180.337475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870451","","Organization Charts;Software tools;Life estimation;Costs;Buildings;Laboratories;Project management;Permission;Testing;Electronic mail","project management;planning;diagrams;task analysis;software development management;corporate modeling","project planning technique;organization chart;task precedence diagram;micro-scale project-specific simulation;what-if scenarios;organizational structures;individual deployment;skills deployment;information flow;exception handling;ViteProject tool;design activities;software projects;project visibility;rational optimization;project management;micro-estimation","","","8","","","","","","IEEE","IEEE Conferences"
"Current trends in the design of automotive electronic systems","T. Beck","ETAS, Stuttgart, Germany","Proceedings Design, Automation and Test in Europe. Conference and Exhibition 2001","","2001","","","38","","Today's situation in this field is characterized by three distinct development phases: First, the analysis and design of functionality. This type of work is typically performed in the laboratory, i.e. on the desk. Second, the implementation of a prototype system, realized by (semi)automatic code generation and followed by a test with a ""Lab-car"" or in a real vehicle. The third and final step comprises the calibration and fine-tuning of algorithms and their parameters, commonly done in a real car. However, there are some flaws associated with this approach. There is no support for multiple interconnected electronic control units. Automatic generation of code of production quality is still a challenging task. And there is a large gap between the properties of a virtual car and the behavior of the real vehicle. The latter is one reason why nowadays the adjustment of calibration parameters still needs to be done manually. In the future, the picture outlined above will change remarkably. Function development tools will be able to generate efficient and reliable software code automatically. Vehicle models will mimic the characteristics of the real object to an extent we cannot imagine today. And automated test without manual interference will unprecedented degree of optimization and quality throughout a complex network of electronic control units. Almost the entire development process will be shifted to the desk with no need for costly, risky, and error-prone experiments with prototype engines or vehicles.","1530-1591","0-7695-0993","10.1109/DATE.2001.914998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914998","","Automotive electronics;Vehicles;Prototypes;Calibration;Automatic control;Laboratories;System testing;Production;Software tools;Electronic equipment testing","automotive electronics;design engineering;calibration;modelling","automotive electronic systems;functionality;prototype system;automatic code generation;calibration;production quality;vehicle models;electronic control units;design engineering","","4","","","","","","","IEEE","IEEE Conferences"
"The use of online diagnostic systems for real-time quality assurance, control (and future continuous online optimization) in roll-to-roll amorphous silicon PV manufacturing","T. Ellison; J. Call; D. Dodge; J. Karn; R. Kopf; Rujiang Liu; M. Lycette","Energy Conversion Devices Inc., Troy, MI, USA; NA; NA; NA; NA; NA; NA","Conference Record of the Thirty-first IEEE Photovoltaic Specialists Conference, 2005.","","2005","","","1472","1475","In partnership with the NREL/DOE PV Manufacturing R&D Program, Energy Conversion Devices, Inc. (ECD) has developed a comprehensive set of online diagnostic systems that allow real-time measurement of PV device characteristics in-situ during amorphous silicon (a-Si) deposition, prior to deposition of ITO, PV device characteristics measured include open-circuit voltage (V/sub oc/), charging rate (CR), and thickness (t) of each of the three cells in the triple-junction device. Measurements are made with an rms precision of about 0.05%, and at periods of 1 s to 1 min (1 cm to 1 m). The information from these systems is displayed real-time in the control room for online quality assurance (QA) and trouble-shooting. The diagnostics systems have been incorporated into software feedback loops to control film thicknesses. We are now beginning the development of programs for continuous online optimization.","0160-8371","0-7803-8707","10.1109/PVSC.2005.1488420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488420","","Real time systems;Quality assurance;Control systems;Amorphous silicon;Manufacturing;Energy measurement;Thickness measurement;US Department of Energy;Research and development;Energy conversion","silicon;elemental semiconductors;amorphous semiconductors;solar cells;semiconductor device manufacture;quality assurance;semiconductor device testing;closed loop systems","online diagnostic systems;real-time quality assurance;online optimization;roll-to-roll amorphous silicon PV manufacturing;amorphous silicon deposition;open-circuit voltage;charging rate;triple-junction device;trouble-shooting;software feedback loops;1 s to 1 min;1 cm to 1 m;Si","","1","4","","","","","","IEEE","IEEE Conferences"
"A fast neural network learning algorithm and its application","P. S. Chang; H. S. Hou","Tennessee Valley Authority, Chattanooga, TN, USA; NA","Proceedings The Twenty-Ninth Southeastern Symposium on System Theory","","1997","","","206","210","The neural network can be used to solve constrained optimization problems for multiple input and output variables. In the constrained system optimization, ordinary methods, such as linear and nonlinear programming and statistical regression, have encountered many difficulties. In contrast, the artificial neural network (ANN) has shown success in performing such tasks. ANN technology offers many opportunities in the performance optimization of fossil power plant systems. ANN can learn the performance characteristics of those systems from the regular monitoring or testing data. Plant performance tradeoffs can be predicted based on the ANN simulation. A PC-based computer code with a fast-learning algorithm application was developed to assist the system tuning. A combustion optimization example is presented to demonstrate the effectiveness of using this software to achieve the NO/sub x/ reduction and preserve the other performance parameters.","0094-2898","0-8186-7873","10.1109/SSST.1997.581608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581608","","Neural networks;Artificial neural networks;Constraint optimization;Linear programming;Power generation;Computerized monitoring;System testing;Computational modeling;Predictive models;Application software","nonlinear programming;linear programming;neural nets;digital simulation","neural network learning algorithm;constrained optimization problems;constrained system optimization;nonlinear programming;linear programming;statistical regression;performance optimization;fossil power plant systems;ANN simulation;PC-based computer code;fast-learning algorithm;combustion optimization;performance parameters","","2","7","","","","","","IEEE","IEEE Conferences"
"Search-based amorphous slicing","D. Fatiregun; M. Harman; R. M. Hierons","Kings Coll. London, UK; Kings Coll. London, UK; NA","12th Working Conference on Reverse Engineering (WCRE'05)","","2005","","","10 pp.","12","Amorphous slicing is an automated source code extraction technique with applications in many areas of software engineering, including comprehension, reuse, testing and reverse engineering. Algorithms for syntax-preserving slicing are well established, but amorphous slicing is harder because it requires arbitrary transformation; finding good general purpose amorphous slicing algorithms therefore remains as hard as general program transformation. In this paper we show how amorphous slices can be computed using search techniques. The paper presents results from a set of experiments designed to explore the application of genetic algorithms, hill climbing, random search and systematic search to a set of six subject programs. As a benchmark, the results are compared to those from an existing analytical algorithm for amorphous slicing, which was written specifically to perform well with the sorts of program under consideration. The results, while tentative at this stage, do give grounds for optimism. The search techniques proved able to reduce the size of the programs under consideration in all cases, sometimes equaling the performance of the specifically-tailored analytic algorithm. In one case, the search techniques performed better, highlighting a fault in the existing algorithm","1095-1350;2375-5369","0-7695-2474","10.1109/WCRE.2005.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566141","slicing;transformation;search based software engineering","Amorphous materials;Algorithm design and analysis;Reverse engineering;Educational institutions;Software engineering;Genetic algorithms;Performance analysis;Space exploration;Application software;Automatic testing","genetic algorithms;graph theory;program slicing;reverse engineering;search problems","search-based amorphous slicing;source code extraction;software engineering;software comprehension;software reusability;software testing;reverse engineering;syntax-preserving slicing;general program transformation;genetic algorithms;hill climbing;random search;systematic search;analytical algorithm","","11","34","","","","","","IEEE","IEEE Conferences"
"Solution Of The Load Flow Problem By A Parallel Optimization Method","E. C. Housos; Omar Wing","Columbia University; NA","IEEE Conference Proceedings Power Industry Computer Applications Conference, 1979. PICA-79.","","1979","","","332","336","The Load Flow problem is treated as a minimization problem and is solved using a parallel nangradient optimization procedure similar to the one suggested by Chazan and Miranker. The algorithm is described and test case results are presented. A speed-up nearly equal to q is possible if a parallel computer with q processors is used for the solution of the problem.","","","10.1109/PICA.1979.720085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=720085","","Load flow;Optimization methods;Concurrent computing;Minimization methods;Equations;Parallel processing;Testing;Ear;Problem-solving;Software","","","","2","10","","","","","","IEEE","IEEE Conferences"
"Image quality optimization for automatic warping registration in X-ray DSA","A. S. Talukda; D. L. Wilson","Dept. of Biomed. Eng., Case Western Reserve Univ., Cleveland, OH, USA; NA","Proceedings of the 19th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 'Magnificent Milestones and Emerging Opportunities in Medical Engineering' (Cat. No.97CH36136)","","1997","2","","549","552 vol.2","X-ray digital subtraction angiography images frequently suffer from misregistration artifacts. Current systems use whole-image registration techniques that can require extensive operator interaction and can be tedious to use. Frequently, patient motion leads to complex artifacts that whole-image registration cannot remove. Available technology makes warping registration feasible. We optimize and test different warping registration algorithms. We also perform an image quality based evaluation of automatic warping registration to investigate its usefulness and feasibility in future DSA software.","1094-687X","0-7803-4262","10.1109/IEMBS.1997.757668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=757668","","Image quality;X-ray imaging;Angiography;Testing;Hospitals;Image registration;Optimization methods;Robustness;Biomedical engineering;Radiology","diagnostic radiography;medical image processing;image registration;image motion analysis","X-ray DSA;automatic warping registration;image quality optimization;digital subtraction angiography images;misregistration artifacts;patient motion;whole-image registration;complex artifacts","","1","15","","","","","","IEEE","IEEE Conferences"
"Applying the decorator pattern for profiling object-oriented software","E. B. Duffy; J. P. Gibson; B. A. Malloy","Dept. of Comput. Sci., Clemson Univ., SC, USA; NA; NA","11th IEEE International Workshop on Program Comprehension, 2003.","","2003","","","84","93","A profiler can provide valuable information to a developer to facilitate program optimization, debugging or testing. In this paper, we describe the use of the decorator pattern for non-intrusive profiling of object-oriented applications. We provide a formal specification of the decorator pattern, and show that the pattern can be used as a program transformation without altering the external, observable behavior of the system. We refer to such a transformation as a correctness preserving transformation, or CPT. As a CPT, the decorator pattern can be used to non-intrusively profile object-oriented applications and we illustrate this application with an invariant validator for enforcement of design by contract, and for profiling memory. We provide a case study to compare the cost trade-offs of validating invariants at different points in a program.","1092-8138","0-7695-1883","10.1109/WPC.2003.1199192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199192","","Computer science;Debugging;Testing;Frequency;Instruments;Formal specifications;Costs;Runtime;Counting circuits;Contracts","program testing;program debugging;object-oriented programming;formal specification;reverse engineering","object-oriented software profiling;program optimization;program debugging;program testing;decorator pattern;formal specification;program transformation;correctness preserving transformation","","2","33","","","","","","IEEE","IEEE Conferences"
"Computer aided design and modeling for ultrasonic applications: CADMUS-on-PC","M. Spies; F. Walte; H. Rieder; H. Wustner","Fraunhofer-Inst. fur Zerstorungsfreie Prufverfahren, Saarbrucken, Germany; NA; NA; NA","1996 IEEE Ultrasonics Symposium. Proceedings","","1996","1","","677","680 vol.1","The CADMUS-system for computer aided design and modeling for ultrasonic applications is presented. The Windows-supported menu-system is implemented on PC-basis thus retaining user-friendliness to a high degree. At the present stage, it incorporates analytical modeling as applied for transducer optimization. As a new feature, a flaw scattering module for backwall-breaking cracks is presented. In view of verification and comparison with experimental data, a signal processing subsystem-based on a multi-processing DSP-unit-to be incorporated for detection, analysis and synthesis of ultrasonic data is also shown. The modular arrangement of the software allows for an easy up-dating with respect to customer-directed applications.","1051-0117","0-7803-3615","10.1109/ULTSYM.1996.584065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584065","","Acoustic applications;Analytical models;Ultrasonic transducers;Inspection;Scattering;Signal processing;Signal analysis;Signal synthesis;Application software;Testing","ultrasonic materials testing;CAD;physics computing;crack detection;acoustic signal processing;computer aided analysis","computer aided design;ultrasonic applications;CADMUS-on-PC software;Windows-supported menu-system;transducer optimization;analytical modeling;flaw scattering module;backwall-breaking cracks;signal processing subsystem;multi-processing DSP-unit","","","5","","","","","","IEEE","IEEE Conferences"
"On the predictability of program behavior using different input data sets","Wei Chung Hsu; Howard Chen; Pen Chung Yew; Dong-Yuan Chen","Dept. of Comput. Sci., Univ. of Minnesota, MN, USA; NA; NA; NA","Proceedings Sixth Annual Workshop on Interaction between Compilers and Computer Architectures","","2002","","","45","53","Smaller input data sets such as the test and the train input sets are commonly used in simulation to estimate the impact of architecture/micro-architecture features on the performance of SPEC benchmarks. They are also used for profile feedback compiler optimizations. In this paper, we examine the reliability of reduced input sets for performance simulation and profile feedback optimizations. We study the high level metrics such as IPC and procedure level profiles as well as lower level measurements such as execution paths exercised by various input sets on the SPEC2000int benchmark. Our study indicates that the test input sets are not suitable to be used for simulation because they do not have an execution profile similar to the reference input runs. The train data set is better than the test data sets at maintaining similar profiles to the reference input set. However, the observed execution paths leading to cache misses are very different between using the smaller input sets and the reference input sets. For current profile based optimizations, the differences in quality of profiles may not have a significant impact on performance, as tested on the Itanium processor with an Intel compiler. However, we believe the impact of profile quality will be greater for more aggressive profile guided optimizations, such as cache prefetching.","","0-7695-1534","10.1109/INTERA.2002.995842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995842","","Optimizing compilers;Benchmark testing;Computer science;Microprocessors;Feedback;Design optimization;Program processors;Runtime","virtual machines;cache storage;software performance evaluation;program compilers","program behavior predictability;reduced input data sets;SPEC benchmarks;profile feedback compiler optimizations;performance simulation;high level metrics;procedure level profiles;IPC profiles;execution paths;SPEC2000int benchmark;test input sets;train input sets;cache misses;reference input sets;Itanium processor;Intel compiler;cache prefetching;profile guided optimizations","","4","13","","","","","","IEEE","IEEE Conferences"
"A new educational software tool for robust control design using the QFT method","R. Nandakumar; G. D. Halikias; A. C. Zolotas","Sch. of Eng. & Math. Sci., City Univ., London, UK; Sch. of Eng. & Math. Sci., City Univ., London, UK; NA","42nd IEEE International Conference on Decision and Control (IEEE Cat. No.03CH37475)","","2003","1","","803","808 Vol.1","We present a new educational software tool for robust control design based on the Quantitative Feedback Design (QFT) method. This is a graphical design methodology for systems with large parametric uncertainty, which has been successfully applied to many complex practical problems. The software tool is implemented in Matlab and may be used to introduce students to robust control methods via small and medium-size design applications. The software is a library of programmable M-files with open access to users and is intended as a test-bed for developing new techniques in this area and for automating parts of the design procedure, such as loop-shaping. A simple design problem is used to illustrate the main features of the software.","0191-2216","0-7803-7924","10.1109/CDC.2003.1272664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272664","","Software tools;Robust control;Uncertainty;Frequency;Robustness;Design methodology;Design engineering;Feedback loop;Control systems;Software design","courseware;control engineering education;engineering graphics;feedback;robust control;control system CAD;optimisation;mathematics computing","educational software tool;robust control design;quantitative feedback design method;graphical design methodology;parametric uncertainty;Matlab;small size design applications;medium size design applications;programmable M-files;loop shaping;test bed","","1","12","","","","","","IEEE","IEEE Conferences"
"Providing data adhering to IHO standards for hydrographic surveys (SP44 and SP52) for navigational chart products with ELAC's Bottom Chart Compact System","M. Solvsten","R. Danish Adm. of Navigation & Hydrography, Copenhagen, Denmark","OCEANS 96 MTS/IEEE Conference Proceedings. The Coastal Ocean - Prospects for the 21st Century","","1996","3","","1114","1118 vol.3","The Royal Danish Administration of Navigation and Hydrography (RDANH) is using two software packages from AlliedSignal ELAC-Nautik. Both packages are still undergoing development to achieve the maximum from the Bottom Chart Compact (BCC) system. As everyone involved with data from any multibeam system are aware of the time saved during data analysis and processing can be spent acquiring new data. Therefore, it is necessary to optimize all analytic and processing tools in order to leave the surveyor with a minimum time attended during these processes. Last year RDANH used a proportion of time between analysing/processing and data collection in the order of one to one. The time used has now for a skilled surveyor come down to half the time processing spent during data collection.","","0-7803-3519","10.1109/OCEANS.1996.569058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=569058","","Navigation;Transducers;System testing;Performance analysis;Algorithm design and analysis;Software packages;Packaging;Data analysis;Permission;Standards organizations","geophysical techniques;oceanographic techniques;bathymetry;seafloor phenomena;sonar;geophysics computing;software packages","geophysical measurement technique;sonar imaging;seafloor topography;bathymetry;IHO standard;hydrographic survey;SP44;SP52;navigational chart production;ELAC;Bottom Chart Compact System;Royal Danish Administration of Navigation and Hydrography;RDANH;software packages;AlliedSignal ELAC-Nautik;Bottom Chart Compact;data analysis;data processing;survey optimization","","","6","","","","","","IEEE","IEEE Conferences"
"Combined full-wave/ANN based modeling of MEMS switches for RF and microwave applications","Yongjae Lee; D. S. Filipovic","Dept. of Electr. & Comput. Eng., Colorado Univ., Boulder, CO, USA; Dept. of Electr. & Comput. Eng., Colorado Univ., Boulder, CO, USA","2005 IEEE Antennas and Propagation Society International Symposium","","2005","1A","","85","88 Vol. 1A","An efficient approach for the design and analysis of RF MEMS switches and related components is presented. It is shown that the developed ANN model preserves the accuracy of the full-wave FEM model while having large savings in the running time. The proposed methodology is suitable for CAD and optimization for MEMS switches embedded in RF/microwave circuits.","1522-3965;1947-1491","0-7803-8883","10.1109/APS.2005.1551249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1551249","","Microswitches;Radio frequency;Switches;Radiofrequency microelectromechanical systems;Computational modeling;Circuit simulation;Design optimization;Coplanar waveguides;Application software;Circuit testing","microswitches;microwave switches;finite element analysis;CAD;optimisation","full-wave/ANN based modeling;RF applications;full-wave FEM model;CAD;optimization;microwave applications;RF MEMS switches","","4","9","","","","","","IEEE","IEEE Conferences"
"Compiling ATR probing codes for execution on FPGA hardware","W. Bohm; R. Beveridge; B. Draper; C. Ross; M. Chawathe; W. Najjar","NA; NA; NA; NA; NA; NA","Proceedings. 10th Annual IEEE Symposium on Field-Programmable Custom Computing Machines","","2002","","","301","302","This paper describes the implementation of an automatic target recognition (ATR) Probing algorithm on a reconfigurable system, using the SA-C programming language and optimizing compiler. The reconfigurable system is 800 times faster than a comparable Pentium running a C implementation of the same probing task. The reasons for this are analyzed.","","0-7695-1801","10.1109/FPGA.2002.1106693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106693","","Field programmable gate arrays;Hardware;Probes;Optimizing compilers;Application software;Target recognition;Table lookup;Circuit testing;Computer languages;Image recognition","optimising compilers;C language;field programmable gate arrays;reconfigurable architectures","ATR;reconfigurable system;optimizing compiler;SA-C programming language;highlevel language;automatic target recognition;FPGA hardware","","2","","","","","","","IEEE","IEEE Conferences"
"Incremental compilation in the VCS environment","V. Kripa Sundar; A. V. Naik; D. R. Chowdhury","Synopsys Inc., USA; NA; NA","Proceedings International Verilog HDL Conference and VHDL International Users Forum","","1998","","","14","19","Viewlogic's Chronologic VCS/sup TM/ is an industry standard simulator for Verilog HDL. This paper describes incremental compilation in VCS. Incremental compilation is a general compiler optimization technique that improves turnaround time of the typical develop-test-debug-edit cycle of software development. It provides the user with the ability to make small changes to a design, while guaranteeing that the re-compilation time will be proportional to the change in the design. The compiled code simulation environment in Verilog poses unique challenges and opportunities for incremental compilation. The paper describes the issues in determining whether a design unit is unchanged since the preview compilation, or whether it needs to be re-compiled. It also addresses the specifics of incremental compilation in single-user and multiple-user environments. Finally, results are presented that demonstrate the benefits of incremental compilation in VCS.","1085-9403","0-8186-8415","10.1109/IVC.1998.660674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=660674","","Libraries;Testing;Application specific integrated circuits;History;Monitoring;Wire","hardware description languages;logic CAD;digital simulation;optimising compilers;incremental compilers","incremental compilation;Viewlogic;Chronologic VCS;industry standard simulator;Verilog;hardware description language;compiler optimization;develop-test-debug-edit cycle;software development;single-user environment;multiple-user environment","","","2","","","","","","IEEE","IEEE Conferences"
"An automatic hardware-software partitioner based on the possibilistic programming","I. Karkowski; R. H. J. M. Otten","Fac. of Electr. Eng., Delft Univ. of Technol., Netherlands; Fac. of Electr. Eng., Delft Univ. of Technol., Netherlands","Proceedings ED&TC European Design and Test Conference","","1996","","","467","472","The problem of hardware-software partitioning in the design of embedded systems is addressed. Uncertainties about the performance of the options for realization are expressed in triangular possibilistic numbers. To handle such numbers an integer programming formulation of the partitioning problem is derived. This formulation can be converted into a possibilistic program without changing the asymptotic computational complexity. The approach is illustrated with results obtained with the receiver part of a transceiver of a wireless indoor spread spectrum system. This example and several other experiments have shown that these optimizations can reach solutions within seconds for designs of that complexity and above.","1066-1409","0-8186-7424","10.1109/EDTC.1996.494342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494342","","Automatic programming;Hardware;Embedded system;Computational complexity;Software performance;Partitioning algorithms;Iterative algorithms;Uncertainty;Linear programming;Transceivers","real-time systems;integer programming;computational complexity;linear programming;logic CAD;logic partitioning","automatic hardware-software partitioner;possibilistic programming;embedded system design;triangular possibilistic numbers;integer programming formulation;asymptotic computational complexity;receiver part;transceiver;wireless indoor spread spectrum system;optimizations;hardware-software cosynthesis environment","","4","10","","","","","","IEEE","IEEE Conferences"
"The Arena Product Family: enterprise modeling solutions","Bapat; Sturrock","Rockwell Software, Sewickley, PA, USA; Rockwell Software, Sewickley, PA, USA","Proceedings of the 2003 Winter Simulation Conference, 2003.","","2003","1","","210","217 Vol.1","This paper introduces the Arena suite of products for modeling, simulation, and optimization, highlighting product architecture and technology features that are targeted toward successful deployment of simulation and Arena throughout an enterprise.","","0-7803-8131","10.1109/WSC.2003.1261426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261426","","Business process re-engineering;Analytical models;Capacity planning;Logic testing;Production planning;Strategic planning;System testing;Virtual manufacturing;Animation;Application software","digital simulation;business data processing;corporate modelling;software packages","Arena product family;enterprise modeling solutions;simulation;optimization;product architecture;product technology features;business management;process improvement","","15","1","","","","","","IEEE","IEEE Conferences"
"Aviation application over IPv6: performance issues","V. Srivastava; C. Wargo; S. Lai","Comput. Networks & Software, Springfield, VA, USA; Comput. Networks & Software, Springfield, VA, USA; Comput. Networks & Software, Springfield, VA, USA","2004 IEEE Aerospace Conference Proceedings (IEEE Cat. No.04TH8720)","","2004","3","","","1670 Vol.3","Aviation industries in United States and in Europe are undergoing a major paradigm shift in the introduction of new network technologies. In the US, NASA is also actively investigating the feasibility of IPv6 based networks for the aviation needs of the United States. In Europe, the Eurocontrol lead, Internet protocol for aviation exchange (iPAX) Working Group is actively investigating the various ways of migrating the aviation authorities backbone infrastructure from X.25 based networks to an IPv6 based network. For the last 15 years, the global aviation community has pursued the development and implementation of an industry-specific set of communications standards known as the aeronautical telecommunications network (ATN). These standards are now beginning to affect the emerging military global air traffic management (GATM) community as well as the commercial air transport community. Efforts are continuing to gain a full understanding of the differences and similarities between ATN and Internet architectures as related to communications, navigation, and surveillance (CNS) infrastructure choices. This research paper describes the implementation of the IPv6 testbed at Computer Networks & Software, Inc. and it's interface connection mechanism to Eurocontrol and NASA's (Cleveland) testbed in the first phase of the project. In the second phase this research work investigates the performance issues of aviation applications such as controller to pilot data link communication (CPDLC), on an IPv6 based backbone network. Aviation applications are grouped into different priority levels. Desired quality of service (QoS) to each priority level is implemented via Diffserv implementation. This research work looks into the possibility of providing similar QoS performance for aviation application in an IPv6 network as is provided in an ATN based network. The testbed consists of three autonomous systems. The autonomous system represents CNS domain, NASA domain and a EUROCONTROL domain. The primary mode of connection between CNS IPv6 testbed and NASA and EUROCONTROL IPv6 testbed is initially a set of IPv6 over IPv4 tunnels. The aviation application under test (CPDLC) consists of two processes running on different IPv6 enabled machines. These processes communicate with each other over the IPv6 network. One machine resides on the CNS portion of the testbed and other may reside in NASA (Cleveland) and/or in Eurocontrol. The IPv6 packets between Eurocontrol, NASA and CNS testbeds would be carried on IPv6 over IPv4 tunnels. We present some results, which suggest that IPv6 QoS has matured enough, so as to provide the QoS service, which is similar in capability to die ATN architecture. We implemented three basic priorities of flow: (1) command & control; (2) surveillance; and (3) general traffic. Various parameters like throughput, packet loss and delay are investigated. The results are analyzed to get a conceptual view of the effect of IPv6 based network on the aviation applications.","1095-323X","0-7803-8155","10.1109/AERO.2004.1367941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1367941","","NASA;Quality of service;Europe;Spine;Military standards;Surveillance;Software testing;Application software;Space technology;IP networks","aircraft communication;data communication;IP networks;Internet;aerospace simulation;protocols;air traffic control;telecommunication links;quality of service;telecommunication standards","aviation application;aviation industries;IPv6 based networks;United States;Europe;Eurocontrol;Internet protocol for aviation exchange;iPAX Working Group;aviation authorities backbone infrastructure;X.25 based networks;communications standards;aeronautical telecommunications network;military global air traffic management;air transport community;Internet architectures;CNS infrastructure;IPv6 testbed;Computer Networks & Software Inc;controller to pilot data link communication;quality of service;Diffserv implementation;autonomous systems;CNS domain;NASA domain;EUROCONTROL domain;IPv6 packets;IPv4 tunnels","","3","14","","","","","","IEEE","IEEE Conferences"
"Assembly to high-level language translation","C. Cifuentes; D. Simon; A. Fraboulet","Dept. of Comput. Sci. & Electr. Eng., Queensland Univ., Brisbane, Qld., Australia; NA; NA","Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)","","1998","","","228","237","Translation of assembly code to high-level language code is of importance in the maintenance of legacy code, as well as in the areas of program understanding, porting, and recovery of code. We present techniques used in the asm2c translator, a SPARC assembly to C translator. The techniques involve data and control flow analyses. The data flow analysis eliminates machine dependencies from the assembly code and recovers high-level language expressions. The control flow analysis recovers control structure statements. Simple data type recovery is also done. The presented techniques are extensions and improvements on previously developed CISC techniques. The choice of intermediate representation allows for both RISC and CISC assembly code to be supported by the analyses. We tested asm2c against SPEC95 SPARC assembly programs generated by a C compiler. Results using both unoptimized and optimized assembly code are presented.","1063-6773","0-8186-8779","10.1109/ICSM.1998.738514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738514","","Assembly;High level languages;Information analysis;Data analysis;Reduced instruction set computing;Testing;Portfolios;Hardware;Computer science;Information systems","reverse engineering;data flow analysis;program assemblers;software maintenance","assembly code;high-level language code;legacy code;maintenance;asm2c translator;data flow analysis;machine dependencies","","19","17","","","","","","IEEE","IEEE Conferences"
"Analysis and modeling of energy reducing source code transformations","C. Brandolese; W. Fornaciari; F. Salice; D. Sciuto","Politecnico di Milano, Italy; Politecnico di Milano, Italy; Politecnico di Milano, Italy; Politecnico di Milano, Italy","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","3","","306","311 Vol.3","This paper presents a methodology and a set of models supporting energy-driven source-to-source transformations. The most promising code transformation techniques have been isolated and studied leading to accurate analytical and/or statistical models. Experimental results, obtained for some common embedded-system processors over a set of typical benchmarks, are presented, showing the viability of the proposed approach as a support tool for embedded software design.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269255","","Registers;Embedded software;Embedded system;Assembly;Optimizing compilers;Space technology;Software design;Software reusability;Computer architecture;Hardware","source coding;power consumption;optimising compilers;program control structures","energy reducing source code transformations;energy driven source-to-source transformations;code transformation techniques;embedded system processors;embedded software design","","2","8","","","","","","IEEE","IEEE Conferences"
"Fast simulated diffusion: an optimization algorithm for multiminimum problems and its application to MOSFET model parameter extraction","T. Sakurai; B. Lin; A. R. Newton","Semicond. Divice Eng. Lab., Toshiba Corp., Kawasaki, Japan; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1992","11","2","228","234","An optimization method, called fast simulated diffusion (FSD), is proposed to solve a multiminimial optimization problem on multidimensional continuous space. The algorithm performs a greedy search and a random search alternately and can find the global minimum with a practical success rate. An efficient hill-descending method employed as the greedy search in the FSD is proposed. When the FSD is applied to a set of standard test functions, it shows an order of magnitude faster speed than the conventional simulated diffusion. Some of the optimization problems encountered in system and VLSI designs are classified into multioptimal problems. The proposed FSD is successfully applied to a MOSFET parameter extraction problem with a deep submicron MOSFET.<<ETX>>","0278-0070;1937-4151","","10.1109/43.124401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=124401","","MOSFET circuits;Parameter extraction;Very large scale integration;Circuit simulation;Computational modeling;Design optimization;Application software;Multidimensional systems;Optimization methods;Temperature","insulated gate field effect transistors;optimisation;search problems;semiconductor device models","deep submicron device;optimization algorithm;multiminimum problems;MOSFET;model parameter extraction;fast simulated diffusion;multidimensional continuous space;greedy search;random search;global minimum;hill-descending method;VLSI designs;multioptimal problems","","33","14","","","","","","IEEE","IEEE Journals & Magazines"
"SATIRE: A new incremental satisfiability engine","J. Whittemore; J. Kim; K. Sakallah","Michigan Univ., MI, USA; NA; NA","Proceedings of the 38th Design Automation Conference (IEEE Cat. No.01CH37232)","","2001","","","542","545","We introduce SATIRE, a new satisfiability solver that is particularly suited to verification and optimization problems in electronic design automation. SATIRE builds on the most recent advances in satisfiability research, and includes two new features to achieve even higher performance: a facility for incrementally solving sets of related problems, and the ability to handle non-CNF constraints. We provide experimental evidence showing the effectiveness of these additions to classical satisfiability solvers.","0738-100X","1-58113-297","10.1145/378239.379019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935567","","Engines;Electronic design automation and methodology;Algorithm design and analysis;Permission;Large-scale systems;Application software;Test pattern generators;Timing;Pattern analysis;Field programmable gate arrays","computability;electronic design automation;logic CAD;logic testing;automatic test pattern generation;design for testability;formal verification;circuit optimisation;Boolean functions","SATIRE;incremental satisfiability engine;verification;optimization problems;electronic design automation;incremental solution;nonCNF constraints;logic testing;logic design;ATPG","","44","20","","","","","","IEEE","IEEE Conferences"
"I/P/B frame type decision by collinearity of displacements","A. Dumitras; B. G. Haskell","Apple Comput., Cupertino, CA, USA; Apple Comput., Cupertino, CA, USA","2004 International Conference on Image Processing, 2004. ICIP '04.","","2004","4","","2769","2772 Vol. 4","State-of-art encoders in real life applications typically select the number of B frames to be coded between each I or P frame to be equal to one or two by default. The few research works that have addressed the problem of encoder optimization by frame type decision rely on measures of motion magnitude and amount to select the number of B frames. In contrast to such solutions, in this paper we advocate the idea of motion similarity in terms of speed and direction as the basis for deciding how many B frames to encode between any two stored frames. Such similarity is evaluated by the collinearity of the displacements in successive frames. Experimental results using the proposed decision method integrated in our H.264 compliant codec show that the bit rates achieved in the compressed bitstreams are optimal and near-optimal for the tested sequences, and are lower by up to 26% than those obtained using the default number of B frames of typical encoders. Furthermore, such bit rate savings are obtained with no subjective loss in the visual quality of the decoded sequences. Last but not least, our method provides a simple form of temporal scalability and can be employed in any coding system that makes use of I, P and B frames.","1522-4880","0-7803-8554","10.1109/ICIP.2004.1421678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421678","","Bit rate;Decoding;Application software;Motion measurement;Codecs;Testing;Scalability;Art;Video sequences;Cameras","video coding;optimisation;data compression;image motion analysis;decoding;code standards;video codecs;image sequences","state-of-art encoder;real life application;B frame;I frame;P frame;encoder optimization;motion magnitude measurement;collinearity displacement;integrated decision method;H.264 compliant codec;bit rate;bitstream compression;sequence testing;visual quality;decoded sequence;temporal scalability","","5","6","","","","","","IEEE","IEEE Conferences"
"Genetic algorithms for optimal reactive power compensation on the national grid system","Furong Li; J. D. Pilgrim; C. Dabeedin; A. Chebbo; R. K. Aggarwal","Dept. of Electron. & Electr. Eng., Univ. of Bath, UK; NA; NA; NA; NA","IEEE Transactions on Power Systems","","2005","20","1","493","500","This work proposes an Integer-coded, multiobjective Genetic Algorithm (IGA) applied to the full Reactive-power Compensation Planning (RCP) problem considering both intact and contingent operating states. The IGA is used to simultaneously solve both the siting problem-optimization of the installation of new devices-and the operational problem-optimization of preventive transformer taps and the controller characteristics of dynamic compensation devices. The aim is to produce an optimal siting plan that does not violate any system or operational constraint and is optimal in terms of the voltage deviation from the ideal and the cost incurred through the installation and use of reactive power compensation devices. This multiobjective problem is solved through the use of Pareto optimality. The developed algorithm is tested on the IEEE 30-bus system and on a reduced practical system that was developed with the cooperation of the National Grid. The algorithm is validated via the comparison with the SCORPION software package, which is a Linear Programming-based (LP) planning tool developed and used by the National Grid for the England and Wales transmission system. This work demonstrates that the IGA is superior to the LP-based method, both in terms of system conditions and installation and utilization cost when fixed and dynamic compensation devices are being sited; the system performance is optimized via the adjustment of tap settings and controller characteristic across multiple operating states.","0885-8950;1558-0679","","10.1109/TPWRS.2004.841236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388544","Genetic algorithms;intact and contingent operating states;multiobjectives;reactive compensation planning","Genetic algorithms;Reactive power;Cost function;Power system planning;System testing;Software algorithms;Software packages;System performance;Optimization methods;Control systems","genetic algorithms;reactive power;power system interconnection;power transmission planning;Pareto optimisation;linear programming;power engineering computing","genetic algorithm;optimal reactive power compensation planning;national grid system;optimization;Pareto optimality;IEEE 30-bus system;SCORPION software package;linear programming tool;LP-based method","","46","23","","","","","","IEEE","IEEE Journals & Magazines"
"Group decision support and multiple criteria optimization","P. H. Iz","Dept. of Inf. & Quantitative Sci., Baltimore Univ., MD, USA","Proceedings of the Twenty-Fourth Annual Hawaii International Conference on System Sciences","","1991","iii","","678","686 vol.3","The paper proposes a structured group decision aid based on multiple criteria optimization. The procedure is designed to solve optimization problems which involve conflicting objectives and multiple decision-makers with different priorities. Most of the empirical findings regarding the performance of multi criteria techniques involve a single decision-maker. The focus in these algorithms is on determining a compromise solution to a multicriteria problem which best coincides with the preference structure of a decision-maker. The approach taken is to imbed the task of finding a compromise solution in a more general and flexible framework. The underlying concept in this framework is the analytic hierarchy process and the Tchebycheff algorithm is used to solve the multiple criteria problem. The objectives and the alternative solutions to the multiple criteria problem are evaluated through the analytic hierarchy process.<<ETX>>","","","10.1109/HICSS.1991.184201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=184201","","Decision making;Design optimization;Algorithm design and analysis;Delta modulation;Testing;Aggregates;Spine;Mathematical programming;Application software;Linear programming","decision support systems;groupware;management science;optimisation","Chebychev algorithm;multiple criteria optimization;structured group decision aid;conflicting objectives;multiple decision-makers;compromise solution;preference structure;analytic hierarchy process;Tchebycheff algorithm","","2","33","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of a perceptual ringing distortion metric for digital video","Zhizhong Zhe; Hong Ren Wu; Zhenghua Yu; T. Ferguson; D. Tan","Sch. of Comput. Sci. & Software Eng., Monash Univ., Clayton, Vic., Australia; Sch. of Comput. Sci. & Software Eng., Monash Univ., Clayton, Vic., Australia; NA; NA; NA","2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)","","2003","1","","I","825","This paper evaluates a perceptual impairment measure for ringing artifacts, which are common in hybrid MC/DPCM/DCT coded video, as a predictor of the mean opinion score (MOS) obtained in the standard subjective assessment. The perceptual ringing artifacts measure is based on a vision model and a ringing distortion region segmentation algorithm, which is converted into a new perceptual ringing distortion metric (PRDM) on a scale of 0 to 5. This scale corresponds to a modified double-stimulus impairment scale variant II (DSIS-II) method. The Pearson correlation, the Spearman rank order correlation and the average absolute error are used to evaluate the performance of the PRDM compared with the subjective test data. The results show a strong correlation between the PRDM and the MOS with respect to ringing artifacts.","","0-7803-7965","10.1109/ICME.2003.1221045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221045","","Distortion measurement;Acoustic distortion;Video coding;Quality assessment;Pulse modulation;Humans;Signal processing algorithms;Frequency;Computer science;Software engineering","video coding;motion compensation;differential pulse code modulation;discrete cosine transforms;correlation methods;transform coding","perceptual ringing distortion metric;digital video;mean opinion score;subjective assessment;ringing distortion region segmentation algorithm;double-stimulus impairment scale variant II;Pearson correlation;Spearman rank order correlation;average absolute;subjective test data","","1","16","","","","","","IEEE","IEEE Conferences"
"Optimization of boiling water reactor loading pattern using an improved genetic algorithm","Y. Kobayashi; E. Aiyoshi","Toden Software Inc., Tokyo, Japan; NA","Proceeding of the 2001 IEEE International Symposium on Intelligent Control (ISIC '01) (Cat. No.01CH37206)","","2001","","","383","390","When a nuclear power reactor is shut down between successive operation cycles, refueling or reloading is needed. Developing a good refueling or reloading pattern is called ""loading pattern optimization"". It is a large, combinatorial optimization problem with a nonlinear objective function and nonlinear constraints. An algorithm based on the genetic algorithm was developed to generate optimized boiling water reactor (BWR) reloading patterns. The proposed algorithms are demonstrated in an actual BWR plant. In test calculations, candidates that shuffled fresh and burned fuel assemblies within a reasonable computation time were obtained.","2158-9860","0-7803-6722","10.1109/ISIC.2001.971540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971540","","Inductors;Genetic algorithms;Assembly;Constraint optimization;Design optimization;Safety;Testing;Power generation economics;Fuel economy;Nuclear fuels","nuclear power stations;fission reactor fuel reprocessing;genetic algorithms","BWR plant;boiling water reactor;nuclear power reactor;reactor loading pattern;loading pattern;optimization;genetic algorithm;reloading;refueling","","5","12","","","","","","IEEE","IEEE Conferences"
"Package cooling designs for a dual-chip electronic package with one high power chip","A. Arvelo; H. Toy; K. Sikka; A. Tai; H. Longworth; Wei Zou; J. Coffin","Thermal & Mech. Eng., IBM Microelectron., Hopewell Junction, NY, USA; Thermal & Mech. Eng., IBM Microelectron., Hopewell Junction, NY, USA; Thermal & Mech. Eng., IBM Microelectron., Hopewell Junction, NY, USA; NA; NA; NA; NA","The Ninth Intersociety Conference on Thermal and Thermomechanical Phenomena In Electronic Systems (IEEE Cat. No.04CH37543)","","2004","2","","23","33 Vol.2","Dual-chip microelectronic packages (DCP) with one high power chip are being increasingly encountered in computer and other electronic systems where a common chip carrier, whether a ceramic or an organic laminate, has a central processing unit (CPU) accompanied by a memory chip. In this study, package cooling designs are developed and presented for cooling two product applications of the DCP, with one application having larger power dissipation on the CPU compared to the other. Thermal analysis was conducted to identify the encapsulation solutions for the DCP. Mechanical analysis was then conducted to identify any structural integrity concerns and include appropriate verification tests during reliability assurance testing. The encapsulation processes were optimized to ensure the reliability of the package under field operation. The reliability of the packaging structures was assured using thermal measurements, acoustic sonography, and shear and tensile strength measurements of using test vehicles and actual product DCPs.","","0-7803-8357","10.1109/ITHERM.2004.1318248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318248","","Electronics packaging;Electronics cooling;Central Processing Unit;Electronic packaging thermal management;Acoustic testing;Application software;Encapsulation;Acoustic measurements;Microelectronics;Ceramics","laminates;ceramics;cooling;adhesives;tensile testing;shear strength;tensile strength;thermal analysis;encapsulation;flip-chip devices;ceramic packaging;integrated circuit testing;integrated circuit reliability;integrated circuit modelling;chip scale packaging","package cooling design;dual chip electronic package;power chip;computer;electronic systems;organic laminates;chip carriers;ceramics;central processing unit;CPU;memory chip;power dissipation;thermal analysis;encapsulation solutions;mechanical analysis;structural integration;reliability assurance testing;optimization;thermal measurements;acoustic sonography;tensile strength measurement;shear strength measurement;test vehicles","","3","14","","","","","","IEEE","IEEE Conferences"
"Research of novel electromagnetic catapults with many kinds of uses","Li Liyi; Hu Yusheng; Li Xiaopeng","Teaching & Res. Sect. of Electr. Motor, Harbin Inst. of Technol., China; Teaching & Res. Sect. of Electr. Motor, Harbin Inst. of Technol., China; Teaching & Res. Sect. of Electr. Motor, Harbin Inst. of Technol., China","IEEE Transactions on Magnetics","","2005","41","1","474","477","We first describe the wide application of catapults, and then develop a novel electromagnetic catapult that is made up of linear brushless dc motor, describing its basic design process, analyzing various kinds of parameters of the novel linear motor, and finally using finite-element software to simulate and optimize the catapult. In addition, we also develop the test prototype machine whose length is 1 m, and it can make velocity of 6.75 kg projectile reach 10 m/s. By comparing the simulation with the experimental data, the result is satisfactory. Thus, we verify that the novel catapult is feasible further. Finally, we present the key technologies and the corresponding solution of practical application. Thus, the applied foundation has been made through the above analysis.","0018-9464;1941-0069","","10.1109/TMAG.2004.838922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1381593","Catapult;electromagnetic launch;finite element;linear motor","Brushless DC motors;Application software;Process design;Electromagnetic analysis;DC motors;Brushless motors;Finite element methods;Analytical models;Design optimization;Testing","linear induction motors;electromagnetic launchers;finite element analysis","electromagnetic catapults;linear brushless dc motor;linear motor;finite-element software;test prototype machine;electromagnetic launch;1 m","","1","5","","","","","","IEEE","IEEE Journals & Magazines"
"An effective approach to smartly allocate computing budget for discrete event simulation","Chun-Hung Chen","Dept. of Syst. Eng., Pennsylvania Univ., Philadelphia, PA, USA","Proceedings of 1995 34th IEEE Conference on Decision and Control","","1995","3","","2598","2603 vol.3","Ordinal Optimization concentrates on isolating a subset of good designs with high probability and reduces the required simulation time dramatically for discrete event simulation. To obtain the same probability level, we may smartly allocate our computing budget among different designs, instead of equally simulating all designs. In this paper we present an effective approach to smartly allocate computing budget for DES simulation. While Ordinal Optimization can dramatically reduce computation cost, our approach can further reduce the already-low cost. Numerical testing shows an additional factor of ten speed-up.","0191-2216","0-7803-2685","10.1109/CDC.1995.478499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=478499","","Discrete event simulation;Computational modeling;Design optimization;Testing;Cost function;Computational efficiency;Design engineering;Systems engineering and theory;Telecommunication computing;Discrete event systems","discrete event simulation;resource allocation;optimisation;CAD;costing;software cost estimation","computing budget allocation;discrete event simulation;simulation time;computation cost","","16","10","","","","","","IEEE","IEEE Conferences"
"Monitoring of mobile agents in large cluster systems","M. Gonne; C. Grewe; H. Pals","Inst. fur Tech. Inf., Medizinische Univ. zu Lubeck, Germany; NA; NA","Proceedings IEEE International Symposium on Network Computing and Applications. NCA 2001","","2001","","","340","343","Mobile agents represent a modern approach for distributed programming in heterogenous clusters. To deal with massive dynamic behaviour, suitable tools for coding, testing, and observation of agents are needed. In this work, a universal concept and an implementation for a portable agent-oriented monitoring system is explained. The focal point of the monitor environment is the analysis and visualization of dynamic agent behaviour and node utilization for performance optimization as well as for basic debugging purposes.","","0-7695-1432","10.1109/NCA.2001.962551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962551","","Monitoring;Mobile agents;Dynamic programming;Fault tolerance;Load management;Control systems;Electronic mail;Modems;Testing;Visualization","distributed programming;software agents;system monitoring;workstation clusters;software portability;program visualisation;software performance evaluation;program debugging","mobile agent monitoring;large cluster systems;distributed programming;heterogenous clusters;massive dynamic behaviour;coding tools;testing tools;agent observation tools;portable agent-oriented monitoring system;dynamic agent behaviour visualization;node utilization;performance optimization;debugging","","","7","","","","","","IEEE","IEEE Conferences"
"Data dependence analysis for complex loop regions","K. Kyriakopoulos; K. Psarris","Dept. of Comput. Sci., Texas Univ., San Antonio, TX, USA; Dept. of Comput. Sci., Texas Univ., San Antonio, TX, USA","International Conference on Parallel Processing, 2001.","","2001","","","195","204","Parallelizing compilers rely on data dependence information in order to produce valid parallel code. Polynomial data dependence analysis techniques, such as the Banerjee test and the I-Test, can efficiently compute data dependence information for simple instances of the data dependence problem. In more complicated cases such as triangular or trapezoidal loop regions with direction vector constraints these tests, including the triangular Banerjee test, ignore or simplify many of the constraints and thus introduce further approximations. The I-Test and the Omega test are two data dependence tests that can provide exact data dependence information. In addition the Omega test can accurately handle complex loop regions but at a higher computation cost. We extend the ideas behind the I-Test to handle such complex regions which are frequently found in actual source code. In particular, we provide a polynomial-time algorithm, the VI-Test that can detect data dependences in loops with triangular bounds and symbolic variables subject to any direction vector. We also perform an extensive experimental evaluation of the various dependence tests, including the I-Test, the VI-Test and the Omega test. We run several experiments using the Perfect Club Benchmarks and the scientific libraries Eispack, Linpack and Lapack. We present accuracy results, reasons for inconclusive answers, and comparative efficiency metrics.","0190-3918","0-7695-1257","10.1109/ICPP.2001.952063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952063","","Data analysis;Benchmark testing;Polynomials;Program processors;Computer science;Information analysis;Computational efficiency;Performance evaluation;Libraries;Optimizing compilers","parallelising compilers;parallel programming;program control structures;software performance evaluation;program testing","complex loop regions;data dependence analysis;parallelizing compilers;triangular loop regions;trapezoidal loop regions;direction vector constraints;I-Test;Omega test;polynomial-time algorithm;VI-test;symbolic variables;triangular bounds;Perfect Club Benchmarks;scientific libraries;Eispack;Linpack;Lapack;accuracy;efficiency metrics","","2","17","","","","","","IEEE","IEEE Conferences"
"Starvation and critical race analyzers for Ada","G. M. Karam; R. J. A. Buhr","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","1990","16","8","829","843","Starvation and critical race analysis tools for Ada designs are described. These tools are part of a temporal analysis toolset that includes an operational specification language, a language interpreter, and a deadlock analyzer for Ada. The starvation analyzer is based on a set-theoretic model of starvation. It uses a proof tree produced by the deadlock analyzer to define the possible computation space of the design. A preprocessing phase of the starvation tool optimizes the analysis so that the resulting analysis is efficient. Unlike livelock analysis in state machines, the starvation analyzer does not require a priori specification of home states to discern liveness. The critical race analysis tool provides semiautomatic proof of critical races by identifying nondeterministic rendezvous (races) from the proof tree generated by the deadlock analyzer, and then assisting the human operator in identifying which of these constitute critical races. Several design examples are used to demonstrate the capabilities of the two analysis methods.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=57622","","System recovery;Algorithm design and analysis;Information analysis;Specification languages;Automatic testing;Delay;Software testing;Humans;System analysis and design","Ada;program interpreters;programming;software tools;specification languages;system recovery","race analyzers;critical race analysis tools;Ada designs;temporal analysis toolset;operational specification language;language interpreter;deadlock analyzer;starvation analyzer;set-theoretic model;deadlock analyzer;computation space;preprocessing phase;starvation tool;liveness;semiautomatic proof;nondeterministic rendezvous;human operator;design examples","","17","25","","","","","","IEEE","IEEE Journals & Magazines"
"Conceptual Modeling in the Context of Development","C. H. Kung","Department of Computer Science, University of Iowa. Iowa city, IA 52242.","IEEE Transactions on Software Engineering","","1989","15","10","1176","1187","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559766","","Context modeling;Prototypes;Software prototyping;Programming;Design optimization;System testing;Information systems;Software maintenance;Software quality;Costs","","Conceptual modeling;consistency checking;executable specification;formal analysis of model;requirements specification","","24","36","","","","","","IEEE","IEEE Journals & Magazines"
"Analyses of flip chip attach reliability","R. Dudek; A. Schubert; B. Michel","Fraunhofer-Inst. fur Zuverlassigkeit und Mikriintegration, Berlin, Germany; NA; NA","4th International Conference on Adhesive Joining and Coating Technology in Electronics Manufacturing. Proceedings. Presented at Adhesives in Electronics 2000 (Cat. No.00EX431)","","2000","","","77","85","Computer based thermo-mechanical design and performance optimization are in widespread use. They are mainly based on finite element (FE-) analyses. Applications of the method in performing parametric studies on the thermomechanical behavior of different flip chip assemblies are given. Both die size and underfill material are varied. It is shown that for flip-chip on board (FCOB) assemblies the die size is no key reliability parameter. Fatigue failure limits are estimated for both not underfilled and underfilled assemblies with small dies. Underfill materials with different characteristics, ranging from stiff to soft, are treated. Polymer materials are of special importance for this packaging technology. The theoretical analysis of stresses within polymeric material compounds induced by environmental conditions, especially temperature changes, requires the characterization of material properties, especially for the underfill and solder mask materials. Measurement results on typical commercially available electronic polymers are reported, which have been investigated by DMA and TMA measurements as well as tensile tests. Isothermal relaxation tests were performed on tensile specimens to study the viscoelastic material behavior at different temperatures.","","0-7803-6460","10.1109/ADHES.2000.860577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860577","","Flip chip;Assembly;Polymers;Thermomechanical processes;Temperature;Optimization;Finite element methods;Application software;Parametric study;Fatigue","flip-chip devices;integrated circuit reliability;integrated circuit design;circuit optimisation;encapsulation;chip-on-board packaging;fatigue;tensile testing;viscoelasticity","flip chip attach reliability;thermo-mechanical design;performance optimization;parametric studies;die size;underfill material;FCOB;fatigue failure limits;packaging technology;polymeric material compounds;environmental conditions;temperature changes;solder mask materials;DMA;TMA;tensile tests;isothermal relaxation tests;tensile specimens;viscoelastic material behavior","","6","8","","","","","","IEEE","IEEE Conferences"
"Memory hierarchy optimization of multimedia applications on programmable embedded cores","K. Tatas; A. Argyriou; M. Dasigenis; D. Soudris; N. Zervas","Dept. of Electr. & Comput. Eng., Democritus Univ. of Thrace, Xanthi, Greece; NA; NA; NA; NA","Proceedings of the IEEE 2001. 2nd International Symposium on Quality Electronic Design","","2001","","","456","461","Data memory hierarchy optimization and partitioning for a widely used multimedia application kernel known as the hierarchical motion estimation algorithm is undertaken, with the use of global loop and data-reuse transformations for three different embedded processor architecture models. Exhaustive exploration of the obtained results clarifies the effect of the transformations on power, area, and performance and also indicates a relation between the complexity of the application and the power savings obtained by this strategy. Furthermore, the significant contribution of the instruction memory even after the application of performance optimizations to the total power budget becomes evident and a methodology is introduced in order to reduce this component.","","0-7695-1025","10.1109/ISQED.2001.915271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915271","","Partitioning algorithms;Application software;Energy consumption;Very large scale integration;Motion estimation;Computer architecture;Memory;Variable speed drives;Testing;Embedded computing","memory architecture;multimedia computing;motion estimation;embedded systems","memory hierarchy optimization;multimedia applications;programmable embedded cores;multimedia application kernel;hierarchical motion estimation algorithm;global loop;data-reuse transformations;embedded processor architecture models;power savings;performance optimizations;total power budget","","2","12","","","","","","IEEE","IEEE Conferences"
"Partitioning and analysis of static digital CMOS circuits","U. Hubner; H. T. Vierhaus; R. Camposano","NA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1997","16","11","1292","1310","Performance optimization of automatic test pattern generation (ATPG) algorithms has received considerable attention. While the application of high-performance algorithms is often limited to simple gates such as AND's, OR's, and XOR, the cell libraries of silicon vendors usually contain more sophisticated structures. To deal with this problem, we present a library independent algorithm for the partitioning and analysis of static digital CMOS circuits described at the switch level. The algorithm recognizes inverters, NANDs, and NORs. It also checks whether a partition can set its output to a high impedance state, thus being capable of partitioning large bus structures with tristate gates. Our approach supports existing ATPG algorithms at the gate level. Moreover, it allows a mixed-level approach for ATPG using detailed fault models at the switch level for whatever partitions it is necessary. Our implementation processes in the order of 2000 transistors per second. This is for circuits containing combinational and sequential logic on a state-of-the-art workstation. We processed complete chips with up to 72000 transistors, which is clearly adequate for practical purposes.","0278-0070;1937-4151","","10.1109/43.663819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=663819","","CMOS digital integrated circuits;Partitioning algorithms;Automatic test pattern generation;Switches;Software libraries;Optimization;Silicon;Algorithm design and analysis;Switching circuits;Inverters","CMOS logic circuits;automatic testing;integrated circuit testing;logic testing;fault diagnosis","static digital CMOS circuits;automatic test pattern generation;switch level;library independent algorithm;impedance state;bus structures;tristate gates;ATPG algorithms;mixed-level approach;detailed fault models","","2","38","","","","","","IEEE","IEEE Journals & Magazines"
"A graph based processor model for retargetable code generation","J. Van Praet; D. Lanneer; G. Goossens; W. Geurts; H. De Man","IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium","Proceedings ED&TC European Design and Test Conference","","1996","","","102","107","Embedded processors in electronic systems typically are tuned to a few applications. Development of processor specific compilers is prohibitively expensive and as a result such compilers, if existing, yield code of an unacceptable quality. To improve this code quality, we developed a retargetable and optimising code generator. It uses a graph based processor model that captures the connectivity the parallelism and all architectural peculiarities of an embedded processor In this paper; the processor model is presented and we formally define the code generation task, including code selection, register allocation and scheduling, in terms of this model.","1066-1409","0-8186-7424","10.1109/EDTC.1996.494133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494133","","Registers;Application specific processors;Digital signal processing;Hardware;Application software;Design optimization;Processor scheduling;Consumer electronics;Multimedia systems;Multimedia communication","software reusability;real-time systems;compiler generators;optimising compilers;scheduling;instruction sets","graph based processor model;retargetable code generation;embedded processors;processor specific compilers;code quality;optimising code generator;connectivity;parallelism;code generation task;code selection;register allocation;scheduling","","19","11","","","","","","IEEE","IEEE Conferences"
"Path-based error coverage prediction","J. Aidemark; P. Folkesson; J. Karlsson","Chalmers Univ. of Technol., Goteborg, Sweden; NA; NA","Proceedings Seventh International On-Line Testing Workshop","","2001","","","14","20","Previous studies have shown that error detection coverage and other dependability measures estimated by fault injection experiments are affected by the workload. The workload is determined by the program executed during the experiments, and the input sequence to the program. In this paper, we present a promising analytical post-injection prediction technique, called path-based error coverage prediction, which reduces the effort of estimating error coverage for different input sequences. It predicts the error coverage for one input sequence based on fault injection results obtained for another input sequence. Although the accuracy of the prediction is low, path based error coverage prediction manages to correctly rank the input sequences with, respect to error detection coverage, provided that the difference in the actually coverage is significant. This technique may, drastically decrease the number of fault injection experiments, and thereby the time, needed to find the input sequence with the worst-case error coverage among a set of input sequences.","","0-7695-1290","10.1109/OLT.2001.937811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937811","","Error correction;Accuracy;Computer errors;Hardware;Microprocessors;Fault detection;Application software;Fault diagnosis;Analytical models;Availability","error detection;fault simulation;computer testing;microprocessor chips;logic testing;automatic testing","path-based error coverage prediction;error classification;simulation-based fault injection;error detection coverage;post-injection prediction technique;input sequences;fault injection results","","","12","","","","","","IEEE","IEEE Conferences"
"A system level approach to a structured MCM design methodology","E. Ringroot; C. Truzzi; E. Beyne","IMEC, Leuven, Belgium; NA; NA","Proceedings. 1998 International Conference on Multichip Modules and High Density Packaging (Cat. No.98EX154)","","1998","","","184","189","The implementation of complex electronic systems using high density packaging (HDP) technologies requires an IC-like ""right first-time"" design approach, as rework and prototyping for such technologies can be extremely expensive. Highly integrated and versatile MCM design environments, able to provide an automated design data flow and to cope with the required flexibility, are therefore a necessity. Available MCM CAD tools are an adaptation of PCB tools, and cannot provide the required features. This paper points out these limitations and outlines a minimum set of features for an optimized MCM design environment. The paper also describes the development of one such environment, tailored for MCM-D technologies.","","0-7803-4850","10.1109/ICMCM.1998.670777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670777","","Design methodology;Design automation;Electronics packaging;Design optimization;Multichip modules;Costs;Optimized production technology;Software packages;Substrates;System testing","multichip modules;circuit CAD;integrated circuit packaging;integrated circuit design;software tools;optimisation","system level approach;structured MCM design methodology;electronic systems;high density packaging;IC-like right-first-time design;rework;prototyping;MCM design environment;automated design data flow;design flexibility;MCM CAD tools;PCB CAD tools;optimized MCM design environment;MCM-D technology","","1","10","","","","","","IEEE","IEEE Conferences"
"Robustness optimization for vehicular crash simulations","Ren-Jye Yang; A. Akkerman; D. F. Anderson; O. M. Faruque; Lei Gu","NA; NA; NA; NA; NA","Computing in Science & Engineering","","2000","2","6","8","13","Over the past 10 years (1990-2000), the computer analysis of vehicle crashworthiness has become a powerful and effective tool, reducing the cost and time to market of new vehicles that meet corporate and government crash safety requirements. Crash simulation is fundamentally computation-intensive, and it requires fast and powerful supercomputers to ensure reasonable turnaround time for the analyses. Because the explicit FE method is computationally efficient, researchers generally prefer it for crash simulation. Explicit FE codes are available from several third-party mechanical computer aided engineering (CAE) software vendors, and the major automotive manufacturers, including Ford Motor Company, use them for crash simulation. The article focuses on how this technology has been extended to perform design optimization and robustness assessment.","1521-9615;1558-366X","","10.1109/5992.881701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881701","","Robustness;Vehicle crash testing;Computational modeling;Vehicle safety;Power engineering computing;Iron;Computer aided engineering;Costs;Time to market;Government","mechanical engineering computing;computer aided engineering;automobile industry;automobiles;digital simulation;safety;accidents;finite element analysis","robustness optimization;vehicular crash simulations;computer analysis;vehicle crashworthiness;government crash safety requirements;computation-intensive;supercomputers;turnaround time;explicit FE method;explicit FE codes;third-party mechanical computer aided engineering software vendors;CAE;automotive manufacturers;Ford Motor Company;design optimization;robustness assessment","","26","5","","","","","","IEEE","IEEE Journals & Magazines"
"Optimized design for insulator pollution performance, an easy to use method free of conventional limitations","R. Axelsson; T. Johansson","Ifo Cermics AB, Sweden; NA","2000 Annual Report Conference on Electrical Insulation and Dielectric Phenomena (Cat. No.00CH37132)","","2000","1","","230","233 vol.1","Designing insulators for high or modest contamination is complicated and object for simplification in order to make design and production easier. The limitations of creepage distance, CD, as design base has become recognized. Exaggerated with excesses in material and production, or insufficient with risk of failure in service, the end result is unavoidably increased total cost. The obvious restriction is that CD considers distance. The conductivity of a surface is, apart from the specific surface conductivity, dependent only on surface shape. Referred to as form factor when measuring test results and K-value, Kv, when used for design, this parameter describes the shape exactly and is the true design base. The problem is that Kv is mathematically complicated. Software development has enhanced CAE programs and solved this problem. But these programs are custom made and not readily accessible. Herein we present a method to determine insulators Kv in a manual but convenient way with acceptable results, valid for all insulator materials and processes. Examples are presented and evaluated for CD and Kv comparison. General conclusion is that replacing CD with Kv, while maintaining all other design parameters, will result in simultaneous cost and performance improvements.","","0-7803-6413","10.1109/CEIDP.2000.885269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885269","","Design optimization;Insulation;Production;Costs;Conductivity;Shape measurement;Contamination;Conducting materials;Pollution measurement;Testing","insulator contamination","design optimization;insulator pollution;contamination;creepage distance;surface conductivity;form factor;K-value;shape parameter","","","8","","","","","","IEEE","IEEE Conferences"
"TEST: a Tracer for Extracting Speculative Threads","M. Chen; K. Olukotun","Stanford Univ., CA, USA; Stanford Univ., CA, USA","International Symposium on Code Generation and Optimization, 2003. CGO 2003.","","2003","","","301","312","Thread-level speculation (TLS) allows sequential programs to be arbitrarily decomposed into threads that can be safely executed in parallel. A key challenge for TLS processors is choosing thread decompositions that speedup the program. Current techniques for identifying decompositions have practical limitations in real systems. Traditional parallelizing compilers do not work effectively on most integer programs, and software profiling slows down program execution too much for real-time analysis. Tracer for Extracting Speculative Threads (TEST) is hardware support that analyzes sequential program execution to estimate performance of possible thread decompositions. This hardware is used in a dynamic parallelization system that automatically transforms unmodified, sequential Java programs to run on TLS processors. In this system, the best thread decompositions found by TEST are dynamically recompiled to run speculatively. The paper describes the analysis performed by TEST and presents simulation results demonstrating its effectiveness on real programs. Estimates are also provided that show the tracer requires minimal hardware additions to our speculative chip-multiprocessor (< 1% of the total transistor count) and causes only minor slowdowns to programs during analysis (3-25%).","","0-7695-1913","10.1109/CGO.2003.1191554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191554","","Yarn;Hardware;Performance analysis;Sequential analysis;Program processors;Hazards;Java;System testing;Performance evaluation;Analytical models","multiprocessing systems;multi-threading;program diagnostics","TEST;Tracer for Extracting Speculative Threads;thread-level speculation;sequential programs;TLS processors;thread decompositions;parallelizing compilers;integer programs;software profiling;program execution;real-time analysis;hardware support;sequential program execution;dynamic parallelization system;unmodified sequential Java programs;minimal hardware additions;speculative chip multiprocessor;total transistor count","","17","34","","","","","","IEEE","IEEE Conferences"
"Leveraging an avionics support environment for shared application to multiple platforms","K. Smith; G. Miyahara","WR-ALC/LYPCA, Robins AFB, GA, USA; NA","16th DASC. AIAA/IEEE Digital Avionics Systems Conference. Reflections to the Future. Proceedings","","1997","1","","1.3","9","The Advanced Avionics Multi-Radar Software Support Study (AAMRSSS) sponsored by the Warner Robins Air Logistics Center Special Operations Forces Engineering, has evolved an avionics support strategy to leverage support facility resources currently used to support the F-15 Eagle, to also support the AC-130U Gunship. In support of the recommendations of the AAMRSS Study, a second set of studies (AAMRSSS2) have been undertaken to develop specific technologies and tools to improve support of the air-to-ground capabilities of the radar avionics. Primarily focused on the system-in-the-loop test and validation capabilities of the F-15 APG-70 Software Development Facility, technology improvements have been identified for the analog and digital target generators, as well as the incorporation of a playback system for instrumented flight data. These technology improvements have been integrated into the plan for the shared support environment to optimize effective support for the unique capabilities of the AC-130U Gunship. As the second study nears completion in the evaluation of prototype technology improvements and the establishment of design specifications, this paper reviews the capabilities of the current Warner Robins F-15 APG-78 Software Development Facility, describes the support needs and strategies for a shared multi-platform facility, and summarizes the support environment improvements targeted.","","0-7803-4150","10.1109/DASC.1997.635020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=635020","","Aerospace electronics;Programming;Application software;Radar;Aircraft;Embedded software;Logistics;System testing;Weapons;Costs","radar computing;military avionics;military computing;weapons;software engineering;real-time systems","avionics support environment;multiple platforms;F-15 Eagle;AC-130U Gunship;air-to-ground capabilities;radar avionics;system-in-the-loop test;playback system;instrumented flight data;shared support environment;Warner Robins F-15 APG-78 software development facility;weapon systems;dedicated software development","","","4","","","","","","IEEE","IEEE Conferences"
"Validating functional system requirements with scenarios","A. Sutcliffe; A. Gregoriades","Dept. of Comput., Univ. of Manchester Inst. of Sci. & Technol., UK; Dept. of Comput., Univ. of Manchester Inst. of Sci. & Technol., UK","Proceedings IEEE Joint International Conference on Requirements Engineering","","2002","","","181","188","This paper addresses the problem of requirements engineering for complex socio-technical systems where an optimal set of technology components and human operators have to be selected to achieve system goals. Goals are achieved by tasks that are expressed as operational scenarios with variations in environmental conditions. The approach taken is to develop a probabilistic model of system reliability as a Bayesian Belief Network (BBN). The BBN model predicts human and machine reliabilities, given input variables representing the scenario and ranges of environmental conditions (i.e. weather, climate, etc.). A software tool, the System Reliability Analyser (SRA) is described that runs a set of scenarios against the BBN models while systematically varying the ranges of 12 input variables specifying properties of human operators such as training, technical equipment specification and environmental conditions. The tool reports human and technical equipment specifications that ""survive"" the scenario testing at a reliability level higher than a user-defined level. A case study evaluation of the tool is reported using a naval command and control domain, in which different combinations of human roles and equipment requirements specifications are automatically validated against a set of scenarios describing missile attacks on a navy frigate. The implication of using BBN technology and the SRA tool for automating socio-technical system requirements validation and optimising requirements selection in component-based systems is discussed.","1090-705X","0-7695-1465","10.1109/ICRE.2002.1048521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048521","","Humans;Sociotechnical systems;Input variables;Reliability engineering;Bayesian methods;Predictive models;Weather forecasting;Software tools;Testing;Command and control systems","formal specification;systems analysis;belief networks;command and control systems;naval engineering computing;formal verification;software reusability;software reliability","functional system requirements validation;scenarios;requirements engineering;complex socio-technical systems;probabilistic model;system reliability;Bayesian Belief Network;software tool;System Reliability Analyser;human operators;training;case study evaluation;naval command and control;requirements specifications;missile attacks;component-based systems","","13","17","","","","","","IEEE","IEEE Conferences"
"Research of a generic design model based on evolutionary algorithms","Hao Pan; Jing-Ling Yuan; Luo Zhong","Sch. of Comput. Sci. & Technol., Wuhan Univ. of Technol., China; Sch. of Comput. Sci. & Technol., Wuhan Univ. of Technol., China; Sch. of Comput. Sci. & Technol., Wuhan Univ. of Technol., China","Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826)","","2004","5","","3135","3139 vol.5","A generic design model for evolutionary algorithms is proposed in this paper. The model, which was described by UML in details, focuses on the key concepts and mechanisms in evolutionary algorithms. The mode not only achieves separation of concerns and encapsulation of implementations by classification and abstraction of those concepts, but also has a flexible architecture due to tile application of design patterns. As a result, the model is reusable, extendible, easy to understand, easy to use, and easy to test. A large number of experiments applying the model to solve many different problems adequately illustrate the generality and effectiveness of the model.","","0-7803-8403","10.1109/ICMLC.2004.1378573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378573","","Algorithm design and analysis;Evolutionary computation;Tiles;Software design;Design optimization;Genetic mutations;Stochastic processes;Electronic mail;Unified modeling language;Encapsulation","evolutionary computation;Unified Modeling Language;software prototyping;software reusability","generic design model;evolutionary algorithms;tile model;UML;flexible architecture;software design;software reusability","","1","5","","","","","","IEEE","IEEE Conferences"
"An O(bn/sup 2/) time algorithm for optimal buffer insertion with b buffer types","Zhuo Li; Weiping Shi","Dept. of Electr. Eng., Texas A&M Univ., College Station, TX, USA; Dept. of Electr. Eng., Texas A&M Univ., College Station, TX, USA","Design, Automation and Test in Europe","","2005","","","1324","1329 Vol. 2","Buffer insertion is a popular technique to reduce interconnect delay. The classic buffer insertion algorithm of L.P.P.P. van Ginneken (see ISCAS, p.865-8, 1990) has time complexity O(n/sup 2/), where n is the number of buffer positions. J. Lillis et al. (see IEEE Trans. Solid-Slate Circuits, vol.31, no.3, p.437-47, 1996) extended van Ginneken's algorithm to allow b buffer types in time O(b/sup 2/n/sup 2/). For modern design libraries that contain hundreds of buffers, it is a serious challenge to balance the speed and performance of the buffer insertion algorithm. We present a new algorithm that computes the optimal buffer insertion in O(bn/sup 2/) time. The reduction is achieved by the observation that the (Q, C) pairs of the candidates that generate the new candidates must form a convex hull. On industrial test cases, the new algorithm is faster than the previous best buffer insertion algorithms by orders of magnitude.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395776","","Delay;Capacitance;Libraries;Clustering algorithms;Algorithm design and analysis;Design automation;Testing;Design optimization;Timing;Repeaters","computational complexity;buffer circuits;delays;interconnections;software libraries;electronic design automation;optimisation","buffer insertion algorithm;interconnect delay optimization;time complexity;design libraries;convex hull;intra-block repeaters;design automation tools","","3","15","","","","","","IEEE","IEEE Conferences"
"Determination of wheelchair dynamic load data for use with finite element analysis","D. P. VanSickle; R. A. Cooper; R. N. Robertson; M. L. Boninger","Dept. of Rehabilitation Sci. & Technol., Pittsburgh Univ., PA, USA; Dept. of Rehabilitation Sci. & Technol., Pittsburgh Univ., PA, USA; Dept. of Rehabilitation Sci. & Technol., Pittsburgh Univ., PA, USA; Dept. of Rehabilitation Sci. & Technol., Pittsburgh Univ., PA, USA","IEEE Transactions on Rehabilitation Engineering","","1996","4","3","161","170","A methodology is introduced for the experimental determination of the dynamic loads which act on a wheelchair. A box frame wheelchair and a cantilever frame wheelchair were tested on an ANSI/RESNA curb-drop tester. The accelerations of an ANSI/RESNA test dummy were recorded with an array of 12 accelerometers mounted as four three-axis groups. Signal averaging was used to produce a composite dynamic load history. The dynamic loads were calculated from the acceleration data and the inertia of the test dummy using software written by the authors. These loads were imported into a finite element program (ALGOR) as load cases. A prototype carbon fiber design was then optimized through design and analysis iterations. The results of the acceleration data indicate that the curb-drop test produces an asymmetric loading scheme. One of the rear wheels hits the ground before the other, placing most of the dynamic load on one side of the wheelchair. The favored side appears to be fixed at the time of setup, preliminary results are given for the design of a modular carbon fiber wheelchair using the finite element (FE) method. These results indicate, however, that the use of a static factor of safety is, in most cases, inadequate for the dynamic loads present in the curb-drop test.","1063-6528;1558-0024","","10.1109/86.536771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=536771","","Wheelchairs;Life estimation;Finite element methods;Optical fiber testing;Accelerometers;History;Software testing;Software prototyping;Prototypes;Design optimization","finite element analysis;acceleration;biomedical measurement;biomechanics;handicapped aids;mechanical variables measurement","wheelchair dynamic load data determination;box frame wheelchair;cantilever frame wheelchair;ANSI/RESNA curb-drop tester;ANSI/RESNA test dummy;accelerometers array;inertia;prototype carbon fiber design;ALGOR finite element program;static safety factor;curb-drop test;asymmetric loading scheme;rear wheels","","6","28","","","","","","IEEE","IEEE Journals & Magazines"
"Household hints for embedded systems designers","W. Wolf","Princeton Univ., NJ, USA","Computer","","2002","35","5","106","108","I spend a lot of time working on tools for embedded system design-hardware-software codesign, system scheduling, software optimization. I've worked on them because I think that good tools are important. They help make design tasks possible that would otherwise be impossible; they also help provide the discipline that produces working systems and minimizes working nights. But tools don't necessarily have to be complex or specially designed for embedded systems. just as you can use a screwdriver to open a can, you can adapt existing, everyday programs to new uses for embedded computing.","0018-9162;1558-0814","","10.1109/MC.2002.1009509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1009509","","Embedded system;Clocks;Process design;Costs;Embedded computing;Testing;Writing;Frequency;Hardware;Semiconductor device measurement","embedded systems;hardware-software codesign;software engineering","embedded system design;hardware-software codesign;system scheduling;software optimization","","1","","","","","","","IEEE","IEEE Journals & Magazines"
"Optimization routing and security features for transparent mobile IP","A. Giovanardi; G. Mazzini","Ferrara Univ., Italy; NA","IEEE GLOBECOM 1998 (Cat. NO. 98CH36250)","","1998","2","","880","885 vol.2","By considering the problem of transparently linked mobile hosts (MH), i.e., without reconfigurations and host software modifications, this paper gives a possible solution and the relative details on the implementation. Even if the communications between MH to fixed hosts (FH) have just been investigated, the main topic of this work concerns those of MH versus MH (MH-to-MH). The transparent implementation is based on the introduction of network agents with proxy, tunneling and signaling functions that have been changing and improving in order to optimize MH-to-MH links by means of suitable notification procedures. In order to avoid intruders and unauthorized users, or those who should acquire privileges or perform dangerous procedures, security features based on an authentication strategy and a secure hash algorithm have been implemented. The system has been designed, realized and tested in an actual environment, by verifying that the average performance of MH-to-MH optimized links are closer to those of FH-to-FH.","","0-7803-4984","10.1109/GLOCOM.1998.776858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=776858","","Routing;Mobile agents;Mobile computing;Telecommunication traffic;Authentication;System testing;Performance evaluation;Design optimization;Web and internet services;Costs","mobile computing;telecommunication network routing;telecommunication security;transport protocols;optimisation;land mobile radio;cryptography;Internet","transparent mobile IP;fixed hosts;optimization routing;security features;network agents;proxy;tunneling function;signaling function;notification procedures;intruders;unauthorized users;authentication strategy;secure hash algorithm;average performance;optimized links;mobile computing;Internet","","2","12","","","","","","IEEE","IEEE Conferences"
"HGA: A Hardware-Based Genetic Algorithm","S. D. Scott; A. Samal; S. Seth","Washington University, St. Louis, MO; NA; NA","Third International ACM Symposium on Field-Programmable Gate Arrays","","1995","","","53","59","A genetic algorithm (GA) is a robust problem-solving method based on natural selection. Hardware's speed advantage and its ability to parallelize offer great rewards to genetic algorithms. Speedups of 1-3 orders of magnitude have been observed when frequently used software routines were implemented in hardware by way of reprogrammable field-programmable gate arrays (FPGAs). Reprogrammability is essential in a general-purpose GA engine because certain GA modules require changeability (e.g. the function to be optimized by the GA). Thus a hardware-based GA is both feasible and desirable. A fully functional hardware-based genetic algorithm (the HGA) is presented here as a proof-of-concept system. It was designed using VHDL to allow for easy scalability. It is designed to act as a coprocessor with the CPU of a PC. The user programs the FPGAs which implement the function to be optimized. Other GA parameters may also be specified by the user. Simulation results and performance analyses of the HGA are presented. A prototype HGA is described and compared to a similar GA implemented in software. In the simple tests, the prototype took about 6% as many clock cycles to run as the software-based GA. Further suggested improvements could realistically make the HGA 2-3 orders of magnitude faster than the software-based GA.","","0-7695-2550","10.1109/FPGA.1995.241945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1377261","Parallel Genetic Algorithms;Function Optimization;Field Programmable Gate Arrays (FPGAs);Performance Acceleration;Performance Evaluation","Genetic algorithms;Field programmable gate arrays;Hardware;Software prototyping;Robustness;Problem-solving;Engines;Scalability;Coprocessors;Design optimization","","Parallel Genetic Algorithms;Function Optimization;Field Programmable Gate Arrays (FPGAs);Performance Acceleration;Performance Evaluation","","27","15","","","","","","IEEE","IEEE Conferences"
"ICCAD-2003. International Conference on Computer Aided Design (IEEE Cat. No.03CH37486)","","","ICCAD-2003. International Conference on Computer Aided Design (IEEE Cat. No.03CH37486)","","2003","","","i","","The following topics are dealt with: interconnect-centric SoC design; energy optimization using dynamic voltage scaling for embedded systems; high-level synthesis; placement and floorplanning; SoC testing; dynamic verification; delay and signal modeling for timing analysis; software techniques for energy and performance optimization; optimization of global interconnects; numerical methods for analog optimization and analysis; CAD algorithms; design techniques for customized processors; verification engines; analog design and methodology; automatic abstraction for formal verification; nonlinear modelling of analog and optical systems; routing; nanometer scale simulation; constraint driven high-level synthesis; optimal interconnect synthesis and analysis; memory testing; statistical static timing; power-aware design; logic synthesis; graph algorithmic approaches to EDA problems; power grid and substrate analysis; interconnect modeling; test data reduction techniques.","","1-58113-762","10.1109/ICCAD.2003.159660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1257548","","Routing;Integrated circuit layout;Circuit optimization;Integrated circuit interconnections;Analog integrated circuits;Design automation;Analog circuits;High-level synthesis","network routing;electronic design automation;integrated circuit layout;system-on-chip;circuit optimisation;integrated circuit interconnections;analogue integrated circuits;formal verification;nanoelectronics;logic CAD;embedded systems;analogue circuits;high level synthesis;circuit CAD;circuit layout CAD","computer aided design;SoC design;system-on-chip;energy optimization;dynamic voltage scaling;embedded systems;floorplanning;dynamic verification;signal modeling;software techniques;global interconnects;CAD algorithms;verification engines;formal verification;nonlinear modelling;nanometer scale simulation;EDA;graph algorithmic approaches;test data reduction techniques;timing analysis;numerical methods;analog optimization;customized processors;routing;high-level synthesis;memory testing;logic synthesis;power grid;substrate analysis;interconnect modeling;statistical static timing","","","","","","","","","IEEE","IEEE Conferences"
"Perceptual coding of digital monochrome images","D. M. Tan; Hong Ren Wu; ZhengHua Yu","Sch. of Comput. Sci. & Software Eng., Monash Univ., Clayton, Vic., Australia; Sch. of Comput. Sci. & Software Eng., Monash Univ., Clayton, Vic., Australia; NA","IEEE Signal Processing Letters","","2004","11","2","239","242","A novel perceptual image coder of gray level images is presented. This coder adopts the EBCOT coding structure employed in the JPEG2000 image coding standard , coupled with an advanced vision model based perceptual distortion measure for rate-distortion optimization to produce coded images with improved visual quality. The proposed coder maintains the EBCOT coding features such as scalability and is fully bitstream compliant with the JPEG2000 standard. Results of subjective evaluations, using comparative forced-choice subjective tests, demonstrated the superior performance of the proposed perceptual coder over the the EBCOT/JPEG2000 benchmark coders.","1070-9908;1558-2361","","10.1109/LSP.2003.821730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261989","","Image coding;Frequency;Brain modeling;Humans;Optical distortion;Optical sensors;Australia;Code standards;Distortion measurement;Rate-distortion","image coding;code standards;optimisation;rate distortion theory","perceptual coding;digital monochrome images;gray level images;EBCOT coding structure;JPEG2000 image coding standard;perceptual distortion metric;rate-distortion optimization;visual quality;bitstream;forced-choice subjective tests;EBCOT/JPEG2000 benchmark coders;human vision system;vision modeling","","21","17","","","","","","IEEE","IEEE Journals & Magazines"
"4-dimensional design analysis and optimization of system-in-package","Tong Yan Tee; Hun Shen Ng; Jing-en Luan; Xueren Zhang; Kim Yong Goh; A. M. Grech; R. Duca","STMicroelectronics, Singapore, Singapore; STMicroelectronics, Singapore, Singapore; STMicroelectronics, Singapore, Singapore; STMicroelectronics, Singapore, Singapore; STMicroelectronics, Singapore, Singapore; STMicroelectronics, Singapore, Singapore; STMicroelectronics, Singapore, Singapore","2005 7th Electronic Packaging Technology Conference","","2005","1","","7 pp.","","This paper is an overview of applications of CAE (computer-aided-engineering) in design for package and board level reliability of system-in-package (SiP). CAE is an efficient tool for virtual prototyping of complex SiP to save the development time and cost with understanding on the physics of failures. The paper highlights on five major reliability issues frequently encountered in the development of SiP, such as thermomechanical related package failures, matrix package warpage, moisture-induced package failures, board level solder joint reliability under thermal cycling test and drop test. For each individual topic, introduction, brief theory, and an example of application with correlation to experiment are given","","0-7803-9578","10.1109/EPTC.2005.1614415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1614415","","Design optimization;Packaging;Computer aided engineering;Testing;Application software;Computer applications;Virtual prototyping;Costs;Physics;Thermomechanical processes","computer aided engineering;failure analysis;rapid prototyping (industrial);reliability;system-in-package;virtual prototyping","4D design analysis;design optimization;system-in-package;computer-aided-engineering;virtual prototyping;thermomechanical failure;package failures;package warpage;solder joint reliability;thermal cycling test;drop test","","1","74","","","","","","IEEE","IEEE Conferences"
"Modeling with the Micro Saint simulation package","D. Schunk","Micro Anal. & Design Inc., Boulder, CO, USA","2000 Winter Simulation Conference Proceedings (Cat. No.00CH37165)","","2000","1","","274","279 vol.1","Micro Saint is a discrete-event simulation software package for building models that simulate real-life processes. With Micro Saint models, users can gain useful information about processes that might be too expensive or time-consuming to test in the real world. Some common application areas for simulation modeling include the following: (a) Modeling manufacturing processes, such as production lines, to examine resource utilization, efficiency, and cost. (b) Modeling transportation systems to examine issues such as scheduling and resource requirements. (c) Modeling service systems to optimize procedures, staffing and other logistical considerations. (d) Modeling training systems and their effectiveness over time. (e) Modeling human operator performance and interaction under changing conditions. Simulation is a cost-effective way to help show decision-makers the most cost-efficient alternatives to any problem.","","0-7803-6579","10.1109/WSC.2000.899729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899729","","Packaging;Discrete event simulation;Software packages;Testing;Application software;Manufacturing processes;Production systems;Resource management;Cost function;Rail transportation","software packages;discrete event simulation;modelling;production control;manufacturing processes","modelling;Micro Saint simulation package;discrete-event simulation software package;real-life processes;manufacturing processes;production lines;resource utilization;transportation systems;resource requirements;scheduling;service systems;staffing;training systems;human operator performance","","","","","","","","","IEEE","IEEE Conferences"
"On smoothed rank profile tests in eigenstructure methods for directions-of-arrival estimation","Tie-Jun Shan; A. Paulraj; T. Kailath","Stanford University, Stanford, CA; NA; NA","IEEE Transactions on Acoustics, Speech, and Signal Processing","","1987","35","10","1377","1385","We propose a statistical procedure known as the smoothed rank profile (SRP) test that can be applied to an array covariance matrix to determine the source coherency structure and solvability of the directions-of-arrival estimation problem in the presence of coherent sources. The SRP is the rank profile of a telescoping series of matrices extracted from the observed array covariance matrix.","0096-3518","","10.1109/TASSP.1987.1165048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1165048","","Testing;Covariance matrix;Direction of arrival estimation;Multiple signal classification;Smoothing methods;Additive noise;Contracts;Management information systems;Application software;Computer simulation","","","","58","14","","","","","","IEEE","IEEE Journals & Magazines"
"A programmable parallel processor LSI for video-based driver assistance systems","S. Kyo; T. Koga; S. Okazaki; I. Kuroda","Multimedia Res. Labs., NEC Corp., Kawasaki, Japan; Multimedia Res. Labs., NEC Corp., Kawasaki, Japan; Multimedia Res. Labs., NEC Corp., Kawasaki, Japan; Multimedia Res. Labs., NEC Corp., Kawasaki, Japan","Proceedings of the 2003 IEEE International Conference on Intelligent Transportation Systems","","2003","1","","257","262 vol.1","This paper describes a fully programmable parallel processor LSI which integrates 128 SIMD RISC microprocessors, each operates in 100 MHz. The LSI achieves simultaneous and real-time multiple processing of driver assistance video recognition applications in software, while at the same time satisfies power efficiency requirement of an in-vehicle LSI. Based on four basic parallel methods and a software development environment including an optimizing compiler of an extended C language and video-based GUI tools, efficient development of real-time video recognition applications which effectively utilize the 128 micro-processors are facilitated. Result of a benchmark test using a high level language written for a robust lane-mark and vehicle detection application shows that the LSI can provide a four times better performance compared with a 2.4 GHz general purpose processor.","","0-7803-8125","10.1109/ITSC.2003.1251959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251959","","Large scale integration;Application software;Reduced instruction set computing;Microprocessors;Programming;Optimizing compilers;Graphical user interfaces;Benchmark testing;High level languages;Robustness","benchmark testing;C language;parallel processing;driver information systems;real-time systems;image recognition;object detection;road vehicles;reduced instruction set computing;large scale integration","programmable parallel processor;LSI;large scale integration;video based driver assistance systems;128 SIMD RISC microprocessors;single instruction multiple data;reduced instruction set computing;in-vehicle video recognition applications;power efficiency requirement;software development environment;optimizing compiler;C language;GUI tools;graphical user interface tools;benchmark test;high level language;vehicle detection;real-time system;100 MHz;2.4 GHz","","2","6","","","","","","IEEE","IEEE Conferences"
"Transportation: Microprocessors monitor rail car systems software optimizes the headway in Miami's people mover microprocessors replace relays in mechanical switches and signaling AC propulsion systems improve Adhesion, simplify maintenance","G. Kaplan","NA","IEEE Spectrum","","1987","24","1","59","61","A number of applications of computers to rail systems are described, namely: a fully automated urban rail system in Canada; two-track, fully automated People Mover system in Miami, Florida's central business district; a microprocessor-based unit for automatic routing began operating in the Washington, DC, subway system; increased acceptance of electronic interlocking; Amtrak's installation of a fault-tolerant-computer control center regarded by experts as one of the most advanced train control centers in the world; and the testing of new AC propulsion systems in revenue service by New York City's Transit Authority.","0018-9235;1939-9340","","10.1109/MSPEC.1987.6448064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6448064","","Microprocessors;Computers;Rails;Maintenance engineering;Control systems;Transportation;Consumer electronics","railways;transport computer control","2-track fully automated People Mover system;automatic routing microprocessor based unit;Washington subway system;AC population system testing;transportation;fully automated urban rail system;Canada;Miami;central business district;electronic interlocking;fault-tolerant-computer control center;train control centers;New York","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation of LOTOS based transformational environments","G. Leon; M. A. Ruz; J. Infante; C. Delgado Kloos; G. Gonzalez","Dept. Ing. de Sistemas Telematicos, Tech. Univ. of Madrid, Spain; Dept. Ing. de Sistemas Telematicos, Tech. Univ. of Madrid, Spain; Dept. Ing. de Sistemas Telematicos, Tech. Univ. of Madrid, Spain; Dept. Ing. de Sistemas Telematicos, Tech. Univ. of Madrid, Spain; Dept. Ing. de Sistemas Telematicos, Tech. Univ. of Madrid, Spain","COMPEURO'90: Proceedings of the 1990 IEEE International Conference on Computer Systems and Software Engineering - Systems Engineering Aspects of Complex Computerized Systems","","1990","","","552","553","A description is given of practical experience with the design and use of transformational environments oriented to the generation of LOTOS specifications. Three different prototypes have been used and evaluated. The first one is a transformational system built on top of the functional language CAML, the second one is an instantiation of the CIP environment to LOTOS (LOTOS-CIP), and the third is based on CSG and aims to support the industrial generation of LOTOS specifications (ASDE). The authors find that transformational environments are very valuable when a rich catalog of transformation rules and mechanisms for combining them are implemented.<<ETX>>","","0-8186-2041","10.1109/CMPEUR.1990.113679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=113679","","Prototypes;Variable speed drives;Testing;Telecommunications;Optimizing compilers;Cloning","formal specification;software tools;specification languages","LOTOS;transformational environments;specifications;functional language;CAML;CIP environment;CSG;ASDE;transformation rules","","","5","","","","","","IEEE","IEEE Conferences"
"In-Chip Configuration for Monitoring Power Consumption in Micro-processing Systems","V. Konstantakos; K. Kosmatopoulos; S. Nikolaidis; T. Laopoulos","Electronics Lab. Physics Dept., Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece, email: vkonstad@auth.gr; Electronics Lab. Physics Dept., Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece; Electronics Lab. Physics Dept., Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece; Electronics Lab. Physics Dept., Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece","2005 IEEE Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications","","2005","","","156","161","A power measurement technique is presented in this work. The problem addressed is the development of a circuit for accurate measurement of power consumption in micro-processing systems, by means of current sensing. Energy measurements performed over a time period of several clock cycles determine the hardware-software related power parameters of specific groups of instructions. Moreover, this information is useful in power optimization procedures. The proposed configuration can be integrated in the system as a peripheral unit for the evaluation of the core and of the system's peripherals, offering also built-in self test ability to the low power system.","","0-7803-9446-10-7803-9445","10.1109/IDAACS.2005.282962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062113","low-power circuits;embedded systems;current measurement;built-in testing structures","Monitoring;Energy consumption;Power measurement;Current measurement;Integrated circuit measurements;Energy measurement;Performance evaluation;Clocks;Automatic testing;System testing","built-in self test;low-power electronics;microprocessor chips;power consumption","in-chip configuration;power consumption monitoring;microprocessing system;power measurement technique;current sensing;energy measurement;clock cycle;hardware-software related power parameter;power optimization procedure;peripheral unit;built-in self test ability;low power system","","9","10","","","","","","IEEE","IEEE Conferences"
"FORAY-GEN: automatic generation of affine functions for memory optimizations","I. Issenin; N. Dutt","Donald Bren Sch. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; Donald Bren Sch. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA","Design, Automation and Test in Europe","","2005","","","808","813 Vol. 2","In today's embedded applications, a significant portion of energy is spent in the memory subsystem. Several approaches have been proposed to minimize this energy, including the use of scratch pad memories, with many based on static analysis of a program. However, it is often not possible to perform static analysis and optimization of a program's memory access behavior unless the program is specifically written for this purpose. We introduce the FORAY model of a program that permits aggressive analysis of the application's memory behavior that further enables such optimization since it consists of 'for' loops and array accesses which are easily analyzable. We present FORAY-GEN, an automated profile-based approach for extraction of the FORAY model from the original program. We also demonstrate how FORAY-GEN enhances applicability of other memory subsystem optimization approaches, resulting in an average doubling in the number of memory references that can be analyzed by existing static approaches.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395678","","Embedded computing;Energy consumption;Application software;Performance analysis;Scanning probe microscopy;Delay","optimisation;storage management;embedded systems","automatic affine function generation;memory optimization;embedded applications;scratch pad memories;static program analysis;memory behavior;for loops;array accesses;memory references","","9","10","","","","","","IEEE","IEEE Conferences"
"Efficient techniques for advanced data dependence analysis","K. Kyriakopoulos; K. Psarris","Dept. of Comput. Sci., Texas Univ., San Antonio, TX, USA; Dept. of Comput. Sci., Texas Univ., San Antonio, TX, USA","14th International Conference on Parallel Architectures and Compilation Techniques (PACT'05)","","2005","","","143","153","Scientific source code for high performance computers is extremely complex containing irregular control structures with complicated expressions. This complexity makes it difficult for compilers to analyze the code and perform optimizations. In particular with regard to program parallelization, complex expressions are often not taken intro consideration during the data dependence analysis phase. In this work we propose new data dependence analysis techniques to handle such complex instances of the dependence problem and increase program parallelization. Our method is based on a set of polynomial time techniques that can prove or disprove dependences in the presence of non-linear expressions, complex loop bounds, arrays with coupled subscripts, and if statement constraints. In addition our algorithm can produce accurate and complete direction vector information enabling the compiler to apply further transformations. To validate our method we performed an experimental evaluation and comparison against the I-Test, the Omega test and the Range test in the Perfect and SPEC benchmarks. The experimental results indicate that our dependence analysis tool is efficient and more effective in program parallelization than the other dependence tests. The improved parallelization of key loops results into higher speedups and better program execution performance in several benchmarks.","1089-795X","0-7695-2429","10.1109/PACT.2005.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515588","","Data analysis;Optimizing compilers;Benchmark testing;Polynomials;Computer science;High performance computing;Performance analysis;Performance evaluation;Algorithm design and analysis;Couplings","program control structures;program compilers;software performance evaluation;program testing","data dependence analysis;control structures;compilers;code analysis;program parallelization;polynomial time techniques;I-Test;Omega test;Range test;Perfect benchmark;SPEC benchmark;program execution performance","","4","22","","","","","","IEEE","IEEE Conferences"
"How faults can be simulated in self-testable VLSI digital circuits?","D. Bojanowicz","Inst. of Comput. Sci., Silesian Univ., Sosnowiec, Poland","Proceedings. 24th EUROMICRO Conference (Cat. No.98EX204)","","1998","1","","180","183 vol.1","Computer based simulation of self testable VLSI digital circuits is a time consuming process. This is why new methods are still being developed to optimise the simulation process and to reduce its duration. The paper presents a new method of fault simulation, intended for self testable digital circuits. In this method, fault masking performed by an in-circuit tester is estimated, based on only the signature itself which is stored in compressor. It is not necessary to carry out a time consuming analysis of the digital circuit's responses and compare them with stored model responses. Based on performed simulations, an observation was made that the developed method brings a substantial reduction of the duration of fault simulation processes performed for self testable digital circuits. It means the research laboratory needs considerably less time to verify the projects carried out on digital circuits.","1089-6503","0-8186-8646","10.1109/EURMIC.1998.711794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=711794","","Circuit faults;Circuit simulation;Built-in self-test;Very large scale integration;Digital circuits;Circuit testing;Computational modeling;Automatic testing;Performance evaluation;Computer simulation","circuit analysis computing;integrated circuit testing;automatic test software;VLSI;failure analysis;digital integrated circuits","fault simulation;computer based simulation;self testable VLSI digital circuits;fault masking;in-circuit tester;stored model responses;fault simulation processes","","","7","","","","","","IEEE","IEEE Conferences"
"An application of multipopulation genetic algorithm for optimization of adversaries' tactics and strategies in battlefield simulation","Wei Zhang; D. Ma; Hong-Jun Zhang; B. -. Wang; Yun-Tao Chen","Comput. Sch., Huazhong Univ. of Sci. & Technol., Hubei, China; Comput. Sch., Huazhong Univ. of Sci. & Technol., Hubei, China; Comput. Sch., Huazhong Univ. of Sci. & Technol., Hubei, China; NA; NA","Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)","","2003","3","","1704","1707 Vol.3","Simulation modeling the battlefield scenario should provide a realistic training ground for the soldiers where it is possible to test the soldiers' skills in a variety of situations. The design of opponents is one of significant facts to influence train level in battlefield simulation. This paper endeavors to show how method as multipopulation genetic algorithms can be used to address the problems such as how to make opponents' actions and strategies unpredictable and how to make battlefield simulation circumstance more realistic. Multipopulation genetic algorithms' inherent optimizing characteristic in subpopulations is just adaptive to solving our problem. The origin of this work is in the area of military training in battlefield simulation.","","0-7803-7865-20-7803-8131","10.1109/ICMLC.2003.1259771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259771","","Genetic algorithms;Computational modeling;Computer simulation;Military computing;Evolution (biology);Testing;Equations;Aerospace simulation;Weather forecasting;Application software","genetic algorithms;military computing;computer based training","multipopulation genetic algorithm;optimization;adversary tactics;adversary strategies;battlefield simulation;realistic training ground;military training","","","7","","","","","","IEEE","IEEE Conferences"
"An adaptive run-to-run optimizing controller for linear and nonlinear semiconductor processes","E. Del Castillo; Jinn-Yi Yeh","Dept. of Ind. Eng., Texas Univ., Arlington, TX, USA; NA","IEEE Transactions on Semiconductor Manufacturing","","1998","11","2","285","295","This paper presents a new run-to-run (R2R) multiple-input-multiple-output controller for semiconductor manufacturing processes. The controller, termed optimizing adaptive quality controller (OAQC), can act both as an optimizer-in case equipment models are not available-or as a controller for given models. The main components of the OAQC are shown and a study of its performance is presented. The controller allows one to specify input and output constraints and weights, and input resolutions. A multivariate control chart can be applied either as a deadband on the controller or simply to provide out of control alarms. Experimental designs can be utilized for on-line (recursive) model identification in the optimization phase. For testing purposes, two chemical mechanical planarization processes were simulated based on real equipment models. It is shown that the OAQC allows one to keep adequate control even if the input-output transfer function is severely nonlinear. Software implementation including the integration of the OAQC with the University of Michigan's Generic Cell Controller (GCC) is briefly discussed.","0894-6507;1558-2345","","10.1109/66.670178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670178","","Programmable control;Adaptive control;Semiconductor device modeling;MIMO;Manufacturing processes;Process control;Control charts;Design for experiments;Design optimization;Testing","adaptive control;MIMO systems;multivariable control systems;optimal control;integrated circuit manufacture;semiconductor device manufacture;process control;identification;production engineering computing","adaptive run-to-run optimizing controller;linear semiconductor processes;nonlinear semiconductor processes;multiple-input-multiple-output controller;MIMO controller;semiconductor manufacturing processes;optimizing adaptive quality controller;input constraints;output constraints;input resolutions;multivariate control chart;out-of-control alarms;online model identification;recursive model identification;optimization phase;chemical mechanical planarization processes;nonlinear input-output transfer function;software implementation;generic cell controller","","63","22","","","","","","IEEE","IEEE Journals & Magazines"
"Formal methods and iterative design","A. F. Monk","Dept. of Psychol., York Univ., UK","IEE Colloquium on Formal Methods and Human-Computer Interaction: II","","1988","","","1/1","1/4","Formal methods allow a system designer to describe precisely how the system will be. In the area of human-computer interaction this means describing the structure and detail of the user interface. Giving the designer tools to think clearly about the decisions made must be helpful. However, the problem of deciding how the user interface is to be designed remains. In particular, to use such tools effectively a designer must have a good understanding of how the users think about the task, what their expectations and priorities are. This kind of information is best obtained by user testing with prototypes. An initial design is put forward and then refined using feedback from typical users doing typical tasks with a prototype. This refinement should happen in parallel with the refinement of a formal model of the user interface by the application of domain independent principles. In the early stages the cost of this procedure can be minimised by using simulations and mock ups rather than full prototypes. This is known as iterative design. This paper describes, through an example, some techniques which can be used to get insights about how a user approaches a task and the difficulties they have with a particular prototype.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=209309","","Software engineering;User interfaces","software engineering;user interfaces","formal methods;user expectations;user priorities;iterative design;human-computer interaction;user interface;user testing;prototypes;feedback;refinement;formal model;domain independent principles;simulations;mock ups","","","","","","","","","IET","IET Conferences"
"Fast exploration of parameterized bus architecture for communication-centric SoC design","Chulho Shin; Young-Taek Kim; Eui-Young Chung; Kyu-Myung Choi; Jeong-Taek Kong; Soo-kwan Eo","CAE Center, Samsung Electron. Co. Ltd., Seoul, South Korea; CAE Center, Samsung Electron. Co. Ltd., Seoul, South Korea; CAE Center, Samsung Electron. Co. Ltd., Seoul, South Korea; CAE Center, Samsung Electron. Co. Ltd., Seoul, South Korea; CAE Center, Samsung Electron. Co. Ltd., Seoul, South Korea; CAE Center, Samsung Electron. Co. Ltd., Seoul, South Korea","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","352","357 Vol.1","For successful SoC design, efficient and scalable communication architecture is crucial. Some bus interconnects now provide configurable structures to meet this requirement of an SoC design. Furthermore, bus IP vendors provide software tools that automatically generate RTL codes of a bus once its designer configures it. Configurability, however, imposes more challenges upon designers because complexity involved in optimization increases exponentially as the number of parameters grows. In this paper, we present a novel approach with which effort requirement can be dramatically reduced. An automated optimization tool we developed is used and it exploits a genetic algorithm for fast design exploration. This paper shows that the time for the optimizing task can be reduced by more than 90% when the tool is used and, more significantly the task can be done without an expert's hand while ending up with a better solution.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268872","","Space exploration;Topology;Power system interconnection;Master-slave;System testing;Control systems;Graphics;Computer aided engineering;Design automation;Automatic testing","system-on-chip;integrated circuit design;circuit optimisation;genetic algorithms;system buses;integrated circuit interconnections","platform-based design;bus configuration;genetic algorithm;parameterized bus architecture;communication-centric SoC design;communication architecture;bus interconnects;configurable structures;software tools;RTL codes;circuit optimization","","10","26","","","","","","IEEE","IEEE Conferences"
"Let's ask the users [user interfaces]","J. Nielsen","Sunsoft, Mountain View, CA, USA","IEEE Software","","1997","14","3","110","111","Tools, techniques, and concepts to optimize user interfaces are presented. Many aspects of usability can best be studied by simply asking the users. This is especially true for issues related to user satisfaction and anxiety, which are hard to measure objectively. Questionnaires and interviews are two methods you can use to determine how people use a system and what features they particularly like or dislike. Both methods are indirect: they do not study the interface itself but rather users' opinions about it.","0740-7459;1937-4194","","10.1109/52.589250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589250","","User interfaces;Usability;Telephony;Testing;Feedback;Postal services;Java","human factors;interactive systems;software management","user interface optimization;usability;user satisfaction;anxiety;questionnaires;interviews;user opinions","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A design and performance study of 3D packaging for high performance memory applications","I. Mohammed; Byong-Su Seol; S. Krishnan","Tessera, Inc., San Jose, CA, USA; Tessera, Inc., San Jose, CA, USA; Tessera, Inc., San Jose, CA, USA","IEEE/CPMT/SEMI 28th International Electronics Manufacturing Technology Symposium, 2003. IEMT 2003.","","2003","","","149","155","To address the performance and miniaturization challenges being faced by the packaging industry, a novel packaging methodology is presented in this paper. A 3D packaging methodology that leverages the existing CSP (Chip Scale Packaging) infrastructure to design and build memory solutions that is high performing and highly dense is presented. The design includes the individual device package design, the 3D package design and module design. Both the electrical and thermal performance is optimized by modifying the three levels of design. The design trade-offs are studied in terms of performance and compared to the performance of single device packages. The 3D packages are analyzed electrically and thermally using finite element and finite difference-based commercial software. The electrical performance results are presented at a single device level and at the 3D package level. The thermal performance is determined under standard test conditions and actual operating environments. Finally, to illustrate the 3D packaging technique, a compact memory module is presented that offers high performance, has a low profile and enables dense memory systems.","1089-8190","0-7803-7933","10.1109/IEMT.2003.1225892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1225892","","Chip scale packaging;Microprocessors;Computer peripherals;Random access memory;SDRAM;DRAM chips;Seals;Design optimization;Software packages;Finite element methods","chip scale packaging;optimisation;modules;integrated memory circuits;finite element analysis","packaging industry;3D packaging methodology;CSP;chip scale packaging;high performance memory applications;device package design;module design;optimisation;single device packages;thermal performance;electrical performance;design level;commercial software;standard test conditions;compact memory module;dense memory systems;miniaturization;finite element commercial software;finite difference based commercial software","","3","7","","","","","","IEEE","IEEE Conferences"
"Energy-monitoring tool for low-power embedded programs","Dongkun Shin; Hojun Shim; Yongsoo Joo; Han-Saem Yun; Jihong Kim; Naehyuck Chang","Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., South Korea; Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., South Korea; Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., South Korea; Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., South Korea; Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., South Korea; Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., South Korea","IEEE Design & Test of Computers","","2002","19","4","7","17","Designing highly efficient embedded programs requires efficient tools to support performance monitoring and tuning of embedded software. Several such tools are available for various embedded processors. To effectively meet the energy consumption requirements of embedded systems, programmers try to understand the energy and power consumption of embedded systems as a high-priority monitoring target. The paper discusses SES, a highly integrated tool that delivers cycle-by-cycle power consumption data for optimizing embedded programs.","0740-7475;1558-1918","","10.1109/MDT.2002.1018129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1018129","","Programming profession;Energy consumption;Embedded system;Power measurement;Energy measurement;Hardware;Predictive models;Debugging;User interfaces;Monitoring","software tools;embedded systems;software performance evaluation","energy-monitoring tool;low-power embedded programs;performance monitoring;embedded software tuning;energy consumption;power consumption;SES;software tool;embedded program optimization","","16","12","","","","","","IEEE","IEEE Journals & Magazines"
"The single event upset response of the Analog Devices, ADSP2100A, digital signal processor","R. Harboe-Sorensen; H. Seran; P. Armbruster; L. Adams","ESA/ESTEC, Noordwivk, Netherlands; ESA/ESTEC, Noordwivk, Netherlands; ESA/ESTEC, Noordwivk, Netherlands; ESA/ESTEC, Noordwivk, Netherlands","IEEE Transactions on Nuclear Science","","1992","39","3","441","445","The authors present the results of a radiation evaluation program carried out on the ADSP2100A, which is a single-chip microprocessor optimized for 12.5 MIPS digital signal processing. Single event upset/latch-up (SEU/SEL) testing using Californium-252 was the primary aim of this program; however, accelerator heavy-ion and proton SEU/SEL data as well as total ionizing dose data are also presented. Californium-252 SEU testing covered both 12. 5- mu m and 21.3- mu m epitaxial layer DSPs, whereas only the 12.5- mu m type was tested with heavy ions and protons. Heavy-ion SEU testing covered the LET (linear energy transfer) range of 3.4 to 79.2 MeV/(mg/cm/sup 2/) and SEU testing covered the proton energies of 200, 500 and 800 MeV. A total ionizing dose rate of 64.0 rd(Si)/min was used for the cobalt-60 testing. The hardware design and software used are described and details of the various tests and test facilities are given. The authors report on the use of the SEU data for the calculation of expected in-orbit upset rates using the CREME suite of programs.<<ETX>>","0018-9499;1558-1578","","10.1109/23.277534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=277534","","Single event upset;Testing;Digital signal processing;Microprocessors;Life estimation;Ion accelerators;Proton accelerators;Epitaxial layers;Energy exchange;Hardware","aerospace instrumentation;computer testing;digital signal processing chips;integrated circuit testing;ion beam effects","latch-up testing;heavy ion data;proton data;space environment;single event upset response;Analog Devices;ADSP2100A;digital signal processor;radiation evaluation program;single-chip microprocessor;total ionizing dose data;SEU testing;linear energy transfer;total ionizing dose rate;hardware design;software;in-orbit upset rates;CREME suite of programs;12.5 micron;21.3 micron;200 to 800 MeV;/sup 252/Cf radiation;/sup 60/Co radiation","","2","7","","","","","","IEEE","IEEE Journals & Magazines"
"Multiple query optimization by cache-aware middleware using query teamwork","K. O'Gorman; D. Agrawal; A. El Abbadi","California Univ., Santa Barbara, CA, USA; NA; NA","Proceedings 18th International Conference on Data Engineering","","2002","","","274","","Queries with common sequences of disk accesses can make maximal use of a buffer pool. We developed middleware to promote the necessary conditions in concurrent query streams, and achieved a speedup of 2.99 in executing a workload derived from the TCP-H benchmark.","1063-6382","0-7695-1531","10.1109/ICDE.2002.994728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994728","","Query processing;Middleware;Teamwork;Buffer storage;Testing;Libraries;Predictive models;Spatial databases;Cache storage;File servers","query processing;cache storage;client-server systems;software performance evaluation;distributed databases","multiple query optimization;databases;cache-aware middleware;query teamwork;disk accesses;buffer pool;concurrent query streams;workload;TCP-H benchmark","","8","4","","","","","","IEEE","IEEE Conferences"
"Measuring the Effectiveness of Self-Healing Autonomic Systems","A. B. Brown; C. Redlin","IBM T.J. Watson Research Center; NA","Second International Conference on Autonomic Computing (ICAC'05)","","2005","","","328","329","Benchmarks are a central force in engineering progress, providing the objective ability to quantify improvement and justify design decisions. In previous work, we brought together the concepts of benchmarks and autonomic computing, describing a vision of benchmarks that quantitatively evaluate a computing system along the four core autonomic dimensions of self-healing, self-configuration, self-optimization, and self-protection. In this paper, we describe our experience with implementing a practical benchmark for the self-healing dimension of autonomic capability, which goes beyond simple measures of fault tolerance by including a measure of autonomic maturity. Our benchmark is capable of quantifying the autonomic self-healing capability of complex, production-scale enterprise solutions based on J2EE middleware (and indeed is currently being used for such purposes)","","0-7965-2276","10.1109/ICAC.2005.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498082","","Benchmark testing;Computer vision;Middleware;System testing;Databases;Steady-state;Trademarks;Space technology;Laboratories;Design engineering","benchmark testing;Java;middleware;security of data;software fault tolerance","self-healing autonomic systems;benchmarking;computing system;system self-healing;system self-configuration;system self-optimization;system self-protection;system fault tolerance;autonomic maturity;J2EE middleware","","8","6","","","","","","IEEE","IEEE Conferences"
"Using constraint logic programming in memory synthesis for general purpose computers","R. Beckmann; J. Herrmann","Dept. of Comput. Sci. XII, Dortmund Univ., Germany; NA","Proceedings European Design and Test Conference. ED & TC 97","","1997","","","619","","Summary form only. In modern computer systems the performance is dominated by the memory performance. Currently, there is neither a systematic design methodology nor a tool for the design of memory systems for general purpose computers. We present a first approach to CAD support for this crucial subtask of system level design. Dependencies between influencing factors and design decisions are explicitly represented by constraints, and constraint logic programming is used to make the design decisions. The memory design is optimized with respect to several objectives by iterating the (re)design cycle. Event driven simulation is used for evaluation of the intermediate results. The system is organized as an interactive design assistant.","1066-1409","0-8186-7786","10.1109/EDTC.1997.582432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582432","","Logic programming;Application software;Computer architecture;Discrete event simulation;Space technology;Computer science;Design methodology;Design automation;System-level design;Design optimization","memory architecture;general purpose computers;constraint handling;logic CAD;circuit CAD;discrete event simulation;integrated memory circuits","constraint logic programming;memory synthesis;general purpose computers;CAD support;system level design;design decisions;optimised memory design;event driven simulation;interactive design assistant","","1","1","","","","","","IEEE","IEEE Conferences"
"Extending value reuse to basic blocks with compiler support","J. Huang; D. J. Lilja","Sun Microsyst., Palo Alto, CA, USA; NA","IEEE Transactions on Computers","","2000","49","4","331","347","Speculative execution and instruction reuse are two important strategies that have been investigated for improving processor performance. Value prediction at the instruction level has been introduced to allow even more aggressive speculation and reuse than previous techniques. This study suggests that using compiler support to extend value reuse to a coarser granularity than a single instruction, such as a basic block, may have substantial performance benefits. We investigate the input and output values of basic blocks and find that these values can be quite regular and predictable. For the SPEC benchmark programs evaluated, 90 percent of the basic blocks have fewer than four register inputs, five live register outputs, four memory inputs, and two memory outputs. About 16 to 41 percent of all the basic blocks are simply repeating earlier calculations when the programs are compiled with the -O2 optimization level in the GCC compiler. Compiler optimizations, such as loop-unrolling and function inlining, affect the sizes of basic blocks, but have no significant or consistent impact on their value locality, nor the resulting performance. Based on these results, we evaluate the potential benefit of basic block reuse using a novel mechanism called the block history buffer. This mechanism records input and live output values of basic blocks to provide value reuse at the basic block level. Simulation results show that using a reasonably sized block history buffer to provide basic block reuse in a 4-way issue superscalar processor can improve execution time for the tested SPEC programs by 1 to 14 percent, with an overall average of 9 percent when using reasonable hardware assumptions.","0018-9340;1557-9956;2326-3814","","10.1109/12.844346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844346","","Registers;Optimizing compilers;History;Program processors;Testing;Hardware","optimising compilers;software performance evaluation;program testing","compiler support;speculative execution;instruction reuse;processor performance;value prediction;instruction level;SPEC benchmark programs;compiler optimizations;function inlining;block history buffer;simulation results","","16","21","","","","","","IEEE","IEEE Journals & Magazines"
"Managing OO projects better","P. Nesi","Dept. of Syst. & Inf., Florence Univ., Italy","IEEE Software","","1998","15","4","50","60","The author reveals that estimation of effort is key to managing OO development projects, then documents a few rules of thumb for doing this. One lesson he's learned from using these rules is that estimates for a whole project are not reliable if estimates for subcomponents are simply added up. As the project size increases, so does the overhead for communication and general interaction. He goes beyond effort metrics and recommends that one collects metrics throughout the project and uses them for continuous revalidation of assumptions and project performance. If one accepts that ""what you can't measure, you can't manage"", this is good advice.","0740-7459;1937-4194","","10.1109/52.687945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=687945","","Project management;Object oriented modeling;Lab-on-a-chip;Metalworking machines;Optimization methods;Computer aided software engineering;System testing;Industrial control;Computer numerical control;Libraries","project management;software development management;object-oriented methods;object-oriented programming;software metrics","object-oriented project management;object-oriented development projects;project size;communication overhead;interaction overhead;metrics;continuous revalidation;assumptions;project performance","","11","12","","","","","","IEEE","IEEE Journals & Magazines"
"Inductive Fault Analysis of MOS Integrated Circuits","J. P. Shen; W. Maly; F. J. Ferguson","Carnegie-Mellon University; Carnegie-Mellon University; Carnegie-Mellon University","IEEE Design & Test of Computers","","1985","2","6","13","26","Inductive Fault Analysis (IFA) is a systematic Procedure to predict all the faults that are likely to occur in MOS integrated circuit or subcircuit The three major steps of the IFA procedure are: (1) generation of Physical defects using statistical data from the fabrication process; (2) extraction of circuit-level faults caused by these defects; and (3) classification of faults types and ranking of faults based on their likelihood of occurrence Hence, given the layout of an IC, a fault model and a ranked fault list can be automatically generated which take into account the technology, layout, and process characteristics. The IFA procedure is illustrated by its applications to an example circuit. The results from this sample led to some very interesting observations regarding nonclassical faults.","0740-7475;1558-1918","","10.1109/MDT.1985.294793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069694","","Circuit faults;MOS integrated circuits;Circuit testing;Very large scale integration;Software testing;Logic testing;Integrated circuit testing;Integrated circuit modeling;Fabrication;Software tools","","","","346","15","","","","","","IEEE","IEEE Journals & Magazines"
"Transformational Implementation: An Example","R. Balzer","Information Sciences Institute, University of Southern California","IEEE Transactions on Software Engineering","","1981","SE-7","1","3","14","A system for mechanically transforming formal program specifications into efficient implementations under interactive user control is described and illustrated through a detailed example. The potential benefits and problems of this approach to software implementation are discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1981.230814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702797","Optimization;program manipulation system;program reliability;programming techniques;program transformation","Documentation;Control systems;Process design;Computer languages;Performance analysis;Performance evaluation;Design optimization;Instruments;Automatic testing;Magnetic heads","","Optimization;program manipulation system;program reliability;programming techniques;program transformation","","77","18","","","","","","IEEE","IEEE Journals & Magazines"
"A PC-based software receiver using a novel front-end technology","M. Laddomada; F. Daneshgaran; M. Mondin; R. M. Hickling","Politecnico di Torino, Italy; NA; NA; NA","IEEE Communications Magazine","","2001","39","8","136","145","Since the software radio concept was introduced, much progress has been made in the past few years in making it a reality. Many software radio based systems have been designed through the development efforts of both commercial and noncommercial organizations. While the term software radio has meant many things, the ultimate goal in software radio has been the realization of an agile radio that can transmit and receive signals at any carrier frequency using any protocol, all of which can be reprogrammed virtually instantaneously. Such a system places great demands on the limits of data converter and processor technologies since it requires real-time disposition of gigasamples of data produced by direct conversion of wireless signals into digital data. From a processing standpoint, the challenge in software radio is to exploit the three basic processor types-fixed architecture processors, FPGAs, and programmable DSPs/RISCs/CISCs-in such a way as to optimize the three-way trade-offs between speed, power dissipation, and programmability. With respect to the latter characteristic, the issues of high-level language interfaces, portability, and reprogramming speed must be considered. This article describes the architecture and operation of a PC-based software radio receiver. The development environment is a real-time PC-based platform that allows testing to be done in a simple manner using the main software functionality of a PC. The front-end of the receiver implemented in hardware represents a novel wideband design (bandwidth of up to 100 MHz centered at a carrier frequency of up to 2 GHz) that functionally converts wireless signals directly into a gigasample digital data stream in the receiver (and vice versa in the transmitter). This direct conversion approach shows the greatest promise in realizing the main goal of software radio.","0163-6804;1558-1896","","10.1109/35.952928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952928","","Software radio;Receivers;Computer architecture;RF signals;Frequency;Protocols;Real time systems;Signal processing;Field programmable gate arrays;Digital signal processing","microcomputer applications;radio receivers;software architecture;field programmable gate arrays;digital signal processing chips;reduced instruction set computing;satellite computers","PC-based software receiver architecture;front-end technology;agile radio;software radio based systems;carrier frequency;protocol;data converter;processor technology;wireless signals;direct conversion;digital data;fixed architecture processors;FPGA;programmable DSP;RISC;CISC;speed;power dissipation;wideband design;high-level language interfaces;portability;reprogramming speed;gigasample digital data stream;transmitter;100 MHz;2 GHz","","25","11","","","","","","IEEE","IEEE Journals & Magazines"
"Application of real-type tabu search in function optimization problems","Hyung-Su Kim; Kyeong Jun Mun; J. H. Park; Gi-Hyun Hwang","Dept. of Electr. Eng., Pusan Nat. Univ., South Korea; NA; NA; NA","ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No.01TH8570)","","2001","1","","613","618 vol.1","An optimization solution performance of tabu search is influenced by initial solution, selection of neighbor solution, and size of tabu list etc. In this paper, we proposed a real-type tabu search (RTS) for function optimization, which uses belief space to create a neighbor solution. Belief space is made of upper 60% neighbors to effectively restrict searching limit, so it can improve searching time and local or global searching capability of RTS. Also short-term and long-term memory based tabu lists adequate to RTS are implemented to search a different region. All of theses procedures are independently applied to each determinant value for quick convergance and effective searching process. In order to show the usefulness of the proposed method, the RTS is applied to the minimization problems such as, De Jong functions, Ackley function, and Griewank functions etc., the results are compared with those of genetic algorithm (GA) or evolutionary programming (EP).","","0-7803-7090","10.1109/ISIE.2001.931864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931864","","Testing;Genetic algorithms;Search methods;Genetic programming;Optimization methods;Analytical models;Simulated annealing;Application software;Minimization methods;Functional programming","search problems;optimisation;genetic algorithms;evolutionary computation","real-type tabu search;function optimization problems;initial solution;neighbor solution selection;belief space;searching limit restriction;searching time improvement;global searching capability;local searching capability;long-term memory based tabu lists;short-term memory based tabu lists;quick convergance;effective searching process;minimization problems;De Jong functions;Ackley function;Griewank functions;genetic algorithm;evolutionary programming;meta heuristics","","","9","","","","","","IEEE","IEEE Conferences"
"Towards verification via supercompilation","A. Lisitsa; A. Nemytykh","Dept. of Comput. Sci., Liverpool Univ., UK; NA","29th Annual International Computer Software and Applications Conference (COMPSAC'05)","","2005","2","","9","10 Vol. 1","Supercompilation, or supervised compilation is a technique for program specialization, optimization and, more generally, program transformation. We present an idea to use supercompilation for verification of parameterized programs and protocols, present a case study and report on our initial experiments.","0730-3157;0730-3157","0-7695-2413","10.1109/COMPSAC.2005.159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508067","","Protocols;Computer science;Power system modeling;History;Resource description framework;Computer languages;Concrete;Software testing;System testing;Computer applications","program compilers;formal verification","formal verification;supercompilation;program specialization;program optimization;program transformation","","1","8","","","","","","IEEE","IEEE Conferences"
"Ensuring resilience and graceful degradation of military SATCOM TCP/IP links","N. Frall; I. Griffin; G. Fairhurst; J. Tay","QinetiQ, Worcester, MA, USA; QinetiQ, Worcester, MA, USA; NA; NA","MILCOM 2002. Proceedings","","2002","1","","51","55 vol.1","The paper describes work undertaken in the UK by QinetiQ (formerly the Defence Evaluation Research Agency) and the University of Aberdeen to design a packet mode for a military satellite modem. Simulation of the protocols has shown significant improvements in performance. The design was optimised and validated at QinetiQ by direct recompilation in a SATCOM 'software radio' testbed. This novel approach has allowed rapid prototyping while allowing further simulation analysis to optimise the design.","","0-7803-7625","10.1109/MILCOM.2002.1180413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180413","","Resilience;Degradation;TCPIP;Design optimization;Military satellites;Modems;Protocols;Software radio;Software testing;Software prototyping","satellite links;transport protocols;military communication;packet radio networks;modems","military SATCOM;TCP/IP links;QinetiQ;Aberdeen University;packet mode;military satellite modem;protocols;software radio testbed","","","9","","","","","","IEEE","IEEE Conferences"
"Compiler Analysis of the Value Ranges for Variables","W. H. Harrison","IBM Thomas J. Watson Research Center","IEEE Transactions on Software Engineering","","1977","SE-3","3","243","250","Programs can be analyzed to determine bounds on the ranges of values assumed by variables at various points in the program. This range information can then be used to eliminate redundant tests, verify correct operation, choose data representations, select code to be generated, and provide diagnostic information. Sophisticated analyses involving the proofs of complex assertions are sometimes required to derive accurate range information for the purpose of proving programs correct. The performance of such algorithms may be unacceptable for the routine analysis required during the compilation process. This paper presents a discussion of mechanical range analysis employing techniques practical for use in a compiler. This analysis can also serve as a useful adjunct to the more sophisticated techniques required for program proving.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1977.231133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702432","Constant propagation, optimizing compiler, program analysis, proof of correctness, weak interpretation.","Testing;Algorithm design and analysis;Information analysis;Performance analysis;Flow graphs;Optimizing compilers;Tracking loops;Tires;Data flow computing","","Constant propagation, optimizing compiler, program analysis, proof of correctness, weak interpretation.","","41","5","","","","","","IEEE","IEEE Journals & Magazines"
"Thermal transient modeling and experimental validation in the European project PROFIT","H. Pape; D. Schweitzer; J. H. J. Janssen; A. Morelli; C. M. Villa","Assembly & Test Dept., Infineon Technol. AG Corp., Munich, Germany; Assembly & Test Dept., Infineon Technol. AG Corp., Munich, Germany; NA; NA; NA","IEEE Transactions on Components and Packaging Technologies","","2004","27","3","530","538","A major objective of the European project PROFIT is to generate boundary condition independent (BCI) dynamic compact thermal models (DCTM) of semiconductor products. Extending the methods for steady BCI-CTM developed in preceding projects DELPHI and SEED to the transient domain, a detailed numerical model of the component is needed, which is validated against four dual cold plate (DCP) experiments extracting heat along the main heat flow paths from a package. The validated detailed model is then used for numerical experiments in many environments represented by external BC. Results are used to optimize resistors and capacitors of a small network forming the DCTM. This work is focused on the first part of developing validated detailed dynamic models by comparison of modeling and measurements. Results of the European project PROFIT on thermal transient measurement and modeling of integrated circuit packages are presented. All together sixteen different packages from the three Semiconductor Manufacturers Infineon, Philips, and ST Microelectronics were measured in four DCP environments as defined in the preceding DELPHI and SEED projects. Solutions to measure TO-type and fine pitch packages in the DCP, especially for the critical DCP-4 boundary condition were demonstrated, as well as reduction of interface resistance and increased reproducibility by using Wood's alloy as an interface material. The measurements were simulated using the commercial software packages ANSYS, FLOTHERM, or MARC. The agreement between simulated and measured thermal impedance is 15% or better from steady state (t=1000 s) to transients with t>0.1, i.e., four orders of magnitude. In a few cases, this level of accuracy was kept even over seven orders of magnitude. Increasing relative inaccuracy with shorter transients corresponds to small absolute errors in temperature. So practical pulse temperature prediction will usually be correct within a few degrees. Extraction of geometrical and material parameters will need further improvement.","1521-3331;1557-9972","","10.1109/TCAPT.2004.831791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331549","","Semiconductor device packaging;Integrated circuit measurements;Electrical resistance measurement;Integrated circuit packaging;Boundary conditions;Integrated circuit modeling;Temperature;Numerical models;Cold plates;Resistors","semiconductor device packaging;transient analysis;integrated circuit packaging;software packages;project management;thermal analysis;thermal management (packaging);semiconductor device models","thermal transient modeling;experimental validation;European Project PROFIT;boundary condition independent;dynamic compact thermal models;semiconductor products;DELPHI;SEED;transient domain;numerical model;dual cold plate;heat flow paths;numerical experiments;resistors optimization;capacitors optimization;thermal transient measurement;integrated circuit packages;semiconductor manufacturers;Infineon;Philips;ST Microelectronics;DCP environments;TO-type packages;fine pitch packages;DCP-4 boundary condition;interface resistance;Wood alloy;interface material;software packages;ANSYS;FLOTHERM;MARC;thermal impedance;absolute errors;pulse temperature prediction;geometrical parameters;material parameters;thermal characterization;1000 s","","20","14","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of commercial reliability prediction programs","J. B. Bowles; L. A. Klein","South Carolina Univ., Columbia, SC, USA; NA","Annual Proceedings on Reliability and Maintainability Symposium","","1990","","","450","455","Discussed are demonstration versions of six reliability prediction software packages: FRATE, MilStress, PC Predictor, REAPmate, ReCalc 2 and RELEX. The programs are compared on the basis of price, data-input process, output-reporting options, error handling, user friendliness, and other options. No attempt is made to rank the programs in any way since any such ranking is highly dependent on the user's perspective and needs. Overall, the programs have many advanced features and provide powerful tools for assessing the reliability of a design.<<ETX>>","","","10.1109/ARMS.1990.68000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=68000","","Packaging;Circuit testing;Costs;Software packages;Integrated circuit testing;Chemicals;Software reliability;Telecommunications;Software design;Resistors","fault tolerant computing;software packages","reliability prediction software packages;FRATE;MilStress;PC Predictor;REAPmate;ReCalc 2;RELEX;price;data-input process;output-reporting options;error handling;user friendliness","","4","8","","","","","","IEEE","IEEE Conferences"
"Comments on ""Temporal logic-based deadlock analysis for Ada"" by G.M. Karam and R.J.A. Burh","M. Young; D. L. Levine; R. N. Taylor","Dept. of Comput. Science, Purdue Univ., West Lafayette, IN, USA; NA; NA","IEEE Transactions on Software Engineering","","1993","19","2","198","199","The commenters discuss several flaws they found in the above-titled paper by G.M. Koran and R.J.A. Burh (see ibid., vol.17, no.10, p.109-1125, (1991)). The commenters argue that the characterization of operational and axiomatic proof method is modified and inaccurate; the classification of modeling techniques for concurrent systems confuses the distinction between state-based and event-based models with the essential distinction between explicit enumeration of behaviors and symbolic manipulation of properties; the statements about the limitations of linear-time temporal logic in relation to nondeterminism are inaccurate; and the characterization of the computational complexity of the analysis technique is overly optimistic.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.214836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=214836","","System recovery;Logic;Computational complexity;Computer science;Interleaved codes;Testing;Heart;Software engineering;Safety;Reachability analysis","Ada;computational complexity;concurrency control;symbol manipulation;temporal logic","temporal logic-based deadlock analysis;state-based models;Ada;axiomatic proof method;event-based models;symbolic manipulation;nondeterminism;computational complexity","","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Compiler design issues for embedded processors","R. Leupers","Aachen Univ. of Technol., Germany","IEEE Design & Test of Computers","","2002","19","4","51","58","The growing complexity and high efficiency requirements of embedded systems call for new code optimization techniques and architecture exploration, using retargetable C and C++ compilers. The first commercial tools are already in industrial use. Meanwhile, researchers are developing new processor-specific code generation techniques that continually narrow the code quality gap between C compilers and assembly programming. The approaches that achieve the right balance of flexibility, code quality, retargeting effort, and compatibility with existing design tools will be successful.","0740-7475;1558-1918","","10.1109/MDT.2002.1018133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1018133","","Optimizing compilers;Computer architecture;Embedded system;Program processors;Design optimization;Iterative methods;Application specific processors;Software tools;Hardware;Process design","optimising compilers;embedded systems","retargetable C compilers;retargetable C++ compilers;embedded systems;code optimization techniques;architecture exploration;compiler design;assembly programming;embedded processors","","24","11","","","","","","IEEE","IEEE Journals & Magazines"
"Emerging technology and business solutions for system chips","N. C. Lu","Etron Technol., Hsinchu, Taiwan","2004 IEEE International Solid-State Circuits Conference (IEEE Cat. No.04CH37519)","","2004","","","25","31 Vol.1","In three decades, the IC industry grew from nothing to current GSI levels (with 10/sup 9/ devices on a chip). Its major driving force has been the use of device scaling, which has been especially effective in enhancing the performance of digital chips. However, increasingly, diverse system applications have created an additional driving force - the development of more and more system chips with an increased need to integrate more diverse functionality (digital, analog, memory, RF, etc.) within a limited form factor. In addition to current SoCs on a 2D die, a trend for the coming decade is multidimensional die integration on interconnected substrates in a compact package. Correspondingly, a metric analyzing technology trends is presented. At the same time, beyond the innovative foundry/fabless business structure of the 1990s, new business models are evolving for the realization of system chips. Such models, leading to an effective solution called virtual vertical integration, is discussed. System-chip development must also be vertically integrated to achieve optimized performance, but advanced technologies required to realize such integration cover various horizontal segments of knowledge, such as multidimensional-die architecture design, circuit design, and related design automation, as well as novel testing and packaging techniques, leading-edge device and wafer-fabrication technologies, and solution-oriented software coding. In this regard, critical challenges are highlighted in terms of power partitioning, integrated design, and built-in quality assurance for known-good-die, signal integrity in field applications, and technology optimization across different segments. The parallelism of technology solutions with business models, and their conjoined optimization in the coming system-chip era, is illustrated in this paper.","0193-6530","0-7803-8267","10.1109/ISSCC.2004.1332580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1332580","","Integrated circuit technology;Multidimensional systems;Packaging;Design optimization;Circuit testing;Radio frequency;Integrated circuit interconnections;Foundries;Computer architecture;Circuit synthesis","integrated circuit design;system-on-chip;integrated memory circuits;radiofrequency integrated circuits;circuit CAD;integrated circuit packaging;integrated circuit manufacture;integrated circuit technology;quality control;integrated circuit reliability;circuit optimisation;reviews","system chips;gigascale integration;GSI;IC industry;device scaling;digital chips;system applications;integrated functionality;digital functionality;analog functionality;memory functionality;RF functionality;form factor;SoC;multidimensional die integration;interconnected substrates;compact package;technology trend metric;foundry-fabless business structure;business models;virtual vertical integration;optimized performance;system-chip development;multidimensional-die architecture design;circuit design;design automation;testing;packaging techniques;wafer-fabrication technologies;device fabrication technologies;solution-oriented software coding;power partitioning;integrated design;built-in quality assurance;known-good-die;signal integrity;technology optimization;technology segments","","4","5","","","","","","IEEE","IEEE Conferences"
"Electronic bill auto-creation system and its visual development software","","","Tsinghua Science and Technology","","2001","6","2","167","167","The Electronic Bill Auto-Creating System and its Visual Development Software, XINCA, have been successfully released. It was developed by the Java Research Group led by Prof. Wang Kehong. At the end of October 2000, it also passed the authentication test of the Ministry of Education, China. The experts in the authentication group agreed unanimously that the technology used in this software and its applications demonstrates its superior status in China and in the international market. It is practical and useful development software in the field of E-Commerce. The System and its Visual Development Software are composed of two components, the XINCA PDF programming package and a visual development tool. It is used to develop special Java applications in a visual environment. The Java applications created by XINCA can be used to query the target database and automatically create the desired electronic documents in PDF. The application environment for XINCA is based on the Browser/WebServer model.","1007-0214","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083357","","Software;Visualization;Java;Consumer electronics;Portable document format;Optimization","","","","","","","","","","","TUP","TUP Journals & Magazines"
"Constructing multiple unique input/output sequences using metaheuristic optimisation techniques","Q. Guo; R. M. Hierons; M. Harman; K. Derderian","Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK","IEE Proceedings - Software","","2005","152","3","127","140","Multiple unique input/output sequences (UIOs) are often used to generate robust and compact test sequences in finite state machine (FSM) based testing. However, computing UIOs is NP-hard. Metaheuristic optimisation techniques (MOTs) such as genetic algorithms (GAs) and simulated annealing (SA) are effective in providing good solutions for some NP-hard problems. In the paper, the authors investigate the construction of UIOs by using MOTs. They define a fitness function to guide the search for potential UIOs and use sharing techniques to encourage MOTs to locate UIOs that are calculated as local optima in a search domain. They also compare the performance of GA and SA for UIO construction. Experimental results suggest that, after using a sharing technique, both GA and SA can find a majority of UIOs from the models under test.","1462-5970","","10.1049/ip-sen:20045001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1468677","","","finite state machines;simulated annealing;genetic algorithms;search problems;program testing;computational complexity","unique input/output sequences;metaheuristic optimisation techniques;finite state machine based testing;NP-hard problem;fitness function;sharing techniques;genetic algorithms;simulated annealing","","4","","","","","","","IET","IET Journals & Magazines"
"The digital HDTV grand alliance video subsystem: Development, optimizatlon and performance","P. J. Hearty; S. W. Siu-Wai Wu","NA; NA","1996. Digest of Technical Papers., International Conference on Consumer Electronics","","1996","","","302","","","","0-7803-3029","10.1109/ICCE.1996.517314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5726445","","Optimization;Hardware;TV;Prototypes;Streaming media;Software;Testing","","","","","","","","","","","IEEE","IEEE Conferences"
"Simulated Annealing For Treatment Plan Optimization In Stereotactic Interstitial Brachytherapy","V. R. Mandava; R. J. Maciunas; J. M. Fitzpatrick; T. S. Chen; D. Eisert","Vanderbilt University; NA; NA; NA; NA","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society Volume 13: 1991","","1991","","","299","300","","","0-7803-0216","10.1109/IEMBS.1991.683945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=683945","","Simulated annealing;Brachytherapy;Neoplasms;Surface fitting;Surface treatment;Optimization methods;Magnetic resonance imaging;Testing;Software tools;Sun","","","","","8","","","","","","IEEE","IEEE Conferences"
"Contingency Ranking for Voltage Stability Analysis of Large-Scale Power Systems","M. Poshtan; P. Rastgoufard; B. Singh","NA; NA; NA","IEEE PES Power Systems Conference and Exposition, 2004.","","2004","","","1506","1513 vol.3","In this paper we propose a screening method and suitable software program to analyze contingencies and their impact on static voltage stability in large power systems. The program substantially reduces the number of load flow simulation for contingency analysis and voltage stability studies and is based on the impact of the contingency on the load, and the available Mega Watt Margin (MWM). The program provides initially a ranking scheme for first and second level contingencies by comparing their loadability with the maximum loadability of the system. It then makes a short list of important contingencies for detailed analysis. Contingencies that result in a larger reduction of maximum loadability are placed in unacceptable contingency category, whereas, the contingencies pertaining to smaller reduction are put in acceptable contingency category. The contingencies falling in between acceptable and unacceptable categories are termed as significant contingencies and are placed in third category. The proposed method is tested first by the IEEE 118-bus test system, and then is applied to study a large-scale real power system with 25,000 buses. The program considers the Var limit of the synchronous generators and a switching scheme for the capacitor banks.","","0-7803-8718","10.1109/PSCE.2004.1397706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1397706","","Voltage;Stability analysis;Large-scale systems;Power system analysis computing;Power system stability;Power system simulation;System testing;Load flow;Analytical models;Load flow analysis","power system dynamic stability;large-scale systems;load flow;IEEE standards;synchronous generators;capacitor switching;power system analysis computing","contingency ranking;large-scale power system;screening method;software program;static voltage stability;load flow simulation;Mega Watt Margin;loadability;unacceptable contingency category;IEEE 118-bus test system;Var limit;synchronous generators;capacitor banks;voltage collapse","","4","20","","","","","","IEEE","IEEE Conferences"
"A Matlab-based power generator maintenance scheduler","C. Sharma; S. Bahadoorsingh","Dept. of Electr. Eng., Univ. of the West Indies, Trinidad and Tobago; NA","IEEE PES Power Systems Conference and Exposition, 2004.","","2004","","","1344","1348 vol.3","This paper presents a PC-based Windows application software package for production of optimised maintenance schedules, performing economic dispatch, predicting actual dates for long-term maintenance scheduling and querying the current status of a generating unit from data files. A new heuristic algorithm based on the tabu search has been proposed as a solution. Using The Power Generation Company of Trinidad and Tobago as the testing ground, a software package was developed and implemented in Matlab 6.5 providing user-friendly graphical user interfaces (GUIs). Numerical results have been obtained and the effectiveness of this developed software has been demonstrated. Selected outputs of the software are presented in this paper for illustration purposes.","","0-7803-8718","10.1109/PSCE.2004.1397663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1397663","","Computer languages;Power generation;Software packages;Power generation economics;Application software;Packaging;Production;Economic forecasting;Heuristic algorithms;Power generation dispatch","power generation scheduling;power generation dispatch;power generation economics;heuristic programming;mathematics computing;graphical user interfaces;numerical analysis;optimisation;preventive maintenance;software packages;power engineering computing","power generator maintenance scheduler;Windows application software package;economic dispatch;optimised maintenance scheduling;data files;heuristic algorithm;tabu search;Power Generation Company;Trinidad;Matlab 6.5;graphical user interfaces;GUI;numerical analysis;preventive maintenance","","3","6","","","","","","IEEE","IEEE Conferences"
"Structural performance measure of evolutionary testing applied to worst-case timing of real-time systems","H. -. Gross; B. F. Jones; D. E. Eyres","Sch. of Comput., Univ. of Glamorgan, Pontypridd, UK; NA; NA","IEE Proceedings - Software","","2000","147","2","25","30","Evolutionary testing is a new testing technique for automatically generating test cases which satisfy a given test criterion. For best or worst-case execution time assessment of real-time systems it can be used to generate test cases which minimise or maximise execution times or possibly violate the timing specification of the system. As a typical search or optimisation technique, evolutionary testing cannot guarantee to find test cases according to the test objective. The only outcome of such a search process is the time found, but there is no information on how close the result comes to the actual minimal or maximal time. Experiments with this testing technique established a relationship between the complexity of a test object and the success of the search process to find optimal or near optimal solutions. The paper can be seen as an initial attempt to define a predictive complexity measure which is able to indicate the degree of how successfully an evolutionary search might have performed on a test object. The measure is simple and easy to retrieve as it is based on a program's source code. It is extensible, which is important for a further improvement in accuracy. The application of the new measure has shown to be successful for many example test programs but also revealed weaknesses on test objects whose complexity is difficult to capture.","1462-5970","","10.1049/ip-sen:20000525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=871132","","","real-time systems","structural performance measure;evolutionary testing;worst-case timing;real-time systems;execution time;test case generation;evolutionary search;optimisation;experiments;predictive complexity measure;source code","","3","","","","","","","IET","IET Journals & Magazines"
"Exploiting conditional instructions in code generation for embedded VLIW processors","R. Leupers","Dept. of Comput. Sci., Dortmund Univ., Germany","Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)","","1999","","","105","109","This paper presents a new code optimization technique for a class of embedded processors. Modern embedded processor architectures show deep instruction pipelines and highly parallel VLIW-like instruction sets. For such architectures, any change in the control flow of a machine program due to a conditional jump may cause a significant code performance penalty. Therefore, the instruction sets of recent VLIW machines offer support for branch-free execution of conditional statements in the form of so-called conditional instructions. Whether an if-then-else statement is implemented by a conditional jump scheme or by conditional instructions has a strong impact on its worst-case execution time. However the optimal selection is difficult particularly for nested conditionals. We present a dynamic programming technique for selecting the fastest implementation for nested if-then-else statements based on estimations. The efficacy is demonstrated for a real-life VLIW DSP.","","0-7695-0078","10.1109/DATE.1999.761104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=761104","","VLIW;Digital signal processing;Telecommunication control;Instruction sets;Application software;Computer science;Pipelines;Dynamic programming;Birth disorders;Testing","instruction sets;embedded systems;parallel architectures;pipeline processing;dynamic programming;digital signal processing chips","conditional instructions;code generation;embedded VLIW processors;code optimization technique;deep instruction pipelines;VLIW-like instruction sets;control flow;code performance penalty;branch-free execution;if-then-else statement;worst-case execution time;nested conditionals;dynamic programming technique;DSP","","1","10","","","","","","IEEE","IEEE Conferences"
"A meta-heuristic based on simulated annealing for solving multiple-objective problems in simulation optimization","E. A. Avello; F. F. Baesler; R. J. Moraga","Departamento de Ingenieria Ind., Univ. del Bio-Bio, Conception, Chile; Departamento de Ingenieria Ind., Univ. del Bio-Bio, Conception, Chile; Departamento de Ingenieria Ind., Univ. del Bio-Bio, Conception, Chile","Proceedings of the 2004 Winter Simulation Conference, 2004.","","2004","1","","","513","This paper presents a new meta heuristic algorithm based on the search method called simulated annealing, and its application to solving multiobjective simulation optimization problems. Since the simulated annealing search method has been extensively applied as a modern heuristic to solve single objective simulation optimization problems, a modification to this method has been developed in order to solve multiobjective problems. The efficiency of this new algorithm was tested on a real case problem modeled under discrete simulation.","","0-7803-8786","10.1109/WSC.2004.1371355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371355","","Simulated annealing;Search methods;Optimization methods;Mathematical model;Heuristic algorithms;Testing;Constraint optimization;Software packages;Vectors;Linear programming","simulated annealing;search problems;discrete event simulation;linear programming;heuristic programming","meta heuristic algorithm;multiobjective simulation optimization problems;simulated annealing search method;discrete simulation;linear programming","","3","11","","","","","","IEEE","IEEE Conferences"
"Experiences with a wide area gigabit network","J. A. Terstriep; R. J. Menelli; T. T. Kwan","Nat. Center for Supercomput. Applications, Illinois Univ., Champaign, IL, USA; Nat. Center for Supercomput. Applications, Illinois Univ., Champaign, IL, USA; Nat. Center for Supercomput. Applications, Illinois Univ., Champaign, IL, USA","Conference Proceedings of the 1996 IEEE Fifteenth Annual International Phoenix Conference on Computers and Communications","","1996","","","179","187","The Blanca gigabit testbed is one of the national, high speed networking testbeds coordinated by the Corporation for National Research Initiative. After two years of continuous development, the Blanca high speed trunk now consists of two local-area HIPPI networks bridged by a wide-area OC-12 ATM link. Because there are many elements involved in the proper operation of the experimental network, application level testing often requires the coordination of multiple parties at different sites to monitor the hardware and systems software involved. In short, gathering application level performance data has been a real challenge. We describe and analyze the application level performance data gathered from the Blanca gigabit network. We have developed a high-speed networking library and a distributed image server application to help us understand the behavior of high speed network. Our measurements over Blanca showed that large TCP window sizes are crucial to achieving high bandwidth utilization. In addition, the host operating system idiosyncrasies significantly affect the networking performance. Using a variety of optimization techniques, we were able to improve the throughput by as much as 65 percent. The techniques described should prove useful to application developers who wish to exploit the bandwidth of high speed networks.","","0-7803-3255","10.1109/PCCC.1996.493631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493631","","High-speed networks;Application software;Bandwidth;Research initiatives;Software testing;System testing;Monitoring;Hardware;System software;Performance analysis","wide area networks;performance evaluation;local area networks;internetworking;asynchronous transfer mode;transport protocols;network servers;network interfaces;network operating systems","wide area gigabit network;Blanca gigabit testbed;high speed networking testbeds;Corporation for National Research Initiative;local-area HIPPI networks;wide-area OC-12 ATM link;experimental network;application level testing;systems software;application level performance data;high-speed networking library;distributed image server application;large TCP window sizes;high bandwidth utilization;networking performance;operating system;optimization techniques;throughput","","1","11","","","","","","IEEE","IEEE Conferences"
"Nonlinear programming applied to calibrating thermal and fluid models to test data","J. Baumann; B. Cullimore","C&R Technol., Littleton, CO, USA; NA","Eighteenth Annual IEEE Semiconductor Thermal Measurement and Management Symposium. Proceedings 2002 (Cat.No.02CH37311)","","2002","","","77","82","Thermal modeling is fraught with uncertainties such as film coefficients, contact resistances, dissipation rates, and effective conductances and capacitances of complex components. Adjusting the values of uncertainties in a thermal/fluid model to achieve a better fit with test data is a necessary step; this procedure is even codified into military standards for electronic equipment design, for example. Nonetheless, such ""correlation"" or ""calibration"" activities are typically done haphazardly and without any mathematical rigor, and are often impeded rather than aided by software. This paper shows how readily available nonlinear programming (NLP) techniques that were developed for optimization problems have been successfully used to automate this critical but laborious calibration task. This paper briefly introduces NLP concepts, and then demonstrates their application both to a simplified curve-fitting exercise as well as a real case: a transient with a serpentine condenser plate.","1065-2221","0-7803-7327","10.1109/STHERM.2002.991349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991349","","Uncertainty;Electronic equipment testing;Calibration;Thermal conductivity;Thermal resistance;Conductive films;Capacitance;Military standards;Electronic equipment;Impedance","nonlinear programming;calibration;correlation methods;thermal analysis;optimisation;electronic design automation;curve fitting;cooling;condensation;flow simulation","nonlinear programming techniques;thermal model test data calibration;fluid model test data correlation;thermal modeling;thermal analysis;parametric modeling;design automation;film coefficients;contact resistances;NLP;dissipation rates;effective conductance;effective capacitance;complex components;uncertainty value adjustment;military electronic equipment design standards;optimization problems;simplified curve-fitting exercise;serpentine condenser plate transient;validation","","2","2","","","","","","IEEE","IEEE Conferences"
"The base dopant out diffusion and the optimized setback layers in SiGe HBT","Zhiguo Li; Wanrong Zhang; Dong Wang; Yaohai Chang; Yinghua Sun","Reliability Phys. Lab, Beijing Polytech. Univ., China; NA; NA; NA; NA","2001 6th International Conference on Solid-State and Integrated Circuit Technology. Proceedings (Cat. No.01EX443)","","2001","1","","596","599 vol.1","The influences of the base dopant out-diffusion on SiGe HBT electrical characteristics are simulated numerically by ISE-TCAD software; at the some time, the action of the setback layers and difference between SiGe and Si setback layers are also simulated numerically. The simulation results indicated that the optimized width of SiGe and Si intrinsic setback layers are equal to and less than the length of the base dopant out-diffusion respectively. Finally, the model are confirmed by the experiment.","","0-7803-6520","10.1109/ICSICT.2001.981549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981549","","Silicon germanium;Germanium silicon alloys;Heterojunction bipolar transistors;Cutoff frequency;Poisson equations;Numerical simulation;Degradation;Testing;Semiconductor process modeling;Doping","Ge-Si alloys;semiconductor materials;heterojunction bipolar transistors;silicon;elemental semiconductors;surface diffusion;semiconductor doping","base dopant out diffusion;optimized setback layers;SiGe HBT;electrical characteristics;ISE-TCAD software;Si setback layers;SiGe setback layers;base dopant out-diffusion length;SiGe-Si","","","11","","","","","","IEEE","IEEE Conferences"
"A new optimized implementation of the SystemC engine using acyclic scheduling","D. G. Perez; G. Mouchard; O. Temam","ALCHEMY INRIA Futurs & LRI, Paris South Univ., France; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","552","557 Vol.1","SystemC is rapidly gaining wide acceptance as a simulation framework for SoC and embedded processors. While its main assets are modularity and the very fact it is becoming a de facto standard, the evolution of the SystemC framework (from version 0.9 to version 2.0.1) suggests the environment is particularly geared toward increasing the framework functionalities rather than improving simulation speed. For cycle-level simulation, speed is a critical factor as simulation can be extremely slow, affecting the extent of design space exploration. In this article, we present a fast SystemC engine that, in our experience, can speed up simulations by a factor of 1.93 to 3.56 over SystemC 2.0.1. This SystemC engine is designed for cycle-level simulators and for the moment, it only supports the subset of the SystemC syntax (signals, methods) that is most often used for such simulators. We achieved greater speed (1) by completely rewriting the SystemC engine and improving the implementation software engineering, and (2) by proposing a new scheduling technique, intermediate between SystemC dynamic scheduling technique and existing static scheduling schemes. Unlike SystemC dynamic scheduling, our technique removes many if not all useless process wake-ups, while using a simpler scheduling algorithm than in existing static scheduling techniques.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268903","","Engines;Dynamic scheduling;Processor scheduling;Scheduling algorithm;Software engineering;Space exploration;Signal design;Embedded system;Workstations;Design automation","simulation languages;digital simulation;processor scheduling;circuit optimisation;system-on-chip;embedded systems","SystemC engine;acyclic scheduling;SoC;embedded processors;SystemC framework;cycle-level simulation;design space exploration;software engineering;dynamic scheduling;static scheduling;process wake-ups;scheduling algorithm","","20","8","","","","","","IEEE","IEEE Conferences"
"An algorithm for dividing ambiguity sets for analog fault dictionary","Jinyan Cai; M. S. Alam","Dept. of Opt. & Electr. Eng., Shijia zhuang Mech. Eng. Coll., Shijiazhuang, China; NA","The 2002 45th Midwest Symposium on Circuits and Systems, 2002. MWSCAS-2002.","","2002","1","","I","89","A new algorithm for dividing ambiguity sets based on the lowest error probability for analog fault dictionary is proposed. The problem of tolerance affecting diagnostic accuracy in analog circuits is discussed. A statistical approach is used to derive the probability distribution of the tolerances of the output signal characteristics both in the absence and in the presence of faults in the circuit. For example, in this paper, Monte Carlo technique has been applied for the analysis of tolerance. The lowest error probabilities are computed according to Bayesian strategy. Using the PSpice software package, a detailed simulation program was developed to implement the proposed technique. The simulation software was packaged and then integrated with a symbolic analysis program that divides the ambiguity sets and structure the software package for the analysis before testing in the fault dictionary. Furthermore, the proposed approach can be easily extended to select the testing nodes leading to the selection of optimized nodes for the analog fault diagnosis.","","0-7803-7523","10.1109/MWSCAS.2002.1187163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1187163","","Dictionaries;Circuit faults;Software packages;Error probability;Computational modeling;Circuit simulation;Analog circuits;Probability distribution;Monte Carlo methods;Bayesian methods","analogue circuits;fault diagnosis;tolerance analysis;Monte Carlo methods;Bayes methods;SPICE;symbol manipulation;circuit testing","ambiguity set division;analog fault dictionary;error probability;tolerance analysis;analog circuit testing;Monte Carlo simulation;statistical algorithm;Bayesian method;PSpice software package;symbolic analysis;fault diagnosis","","1","9","","","","","","IEEE","IEEE Conferences"
"CpprofJ: aspect-capable call path profiling of multi-threaded Java applications","R. J. Hall","AT&T Labs.-Res., USA","Proceedings 17th IEEE International Conference on Automated Software Engineering,","","2002","","","107","116","A primary goal of program performance understanding tools is to focus the user's attention directly on optimization opportunities where significant cost savings may be found. Optimization opportunities fall into (at least) three broad categories: the call context of a general component may obviate the need for some of its generality; cross-cutting program aspects may be implemented suboptimally for the particular context of use; and thread dependencies may cause unintended delays. This paper enhances prior work in call path profiling in several ways. First, it provides two different call path oriented views on program performance, a server view and a thread view. The former helps one optimize for throughput, while the latter is useful for optimizing thread latency. The views incorporate a typed time notation for representing different program activities, such as monitor wait and thread preemption times. Second, the new framework allows aspect-oriented program profiling, even when the original program was not designed in an aspect oriented fashion. Finally, the approach is implemented in a tool, CPPROFJ, an aspect-capable call path profiler for Java. It exploits recent developments in the Java APIs to achieve accurate and portable sampling-based profiling. Three case studies illustrate its use.","1938-4300","0-7695-1736","10.1109/ASE.2002.1114999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1114999","","Java;Yarn;Cost function;Delay;Design optimization;Throughput;Monitoring;Testing;Computer bugs;Program processors","application program interfaces;Java;software performance evaluation;multi-threading;reverse engineering;object-oriented programming","CpprofJ;aspect-capable call path profiling;multi-threaded Java;program performance understanding tools;call context;thread dependencies;program performance;typed time notation;Java;monitor wait;thread preemption times;aspect-oriented program profiling","","1","14","","","","","","IEEE","IEEE Conferences"
"Validating novel CAC algorithms on ATM testbeds","J. Levendovszky; Z. Elek; C. Vegso","Dept. of Telecommun., Tech. Univ. Budapest, Hungary; NA; NA","1999 2nd International Conference on ATM. ICATM'99 (Cat. No.99EX284)","","1999","","","197","203","This paper is concerned with validating novel CAC algorithms developed in the framework of project COP579. The validation took place at the Expert Testing Facilities in Basel, Switzerland, where versatile source models and traffic traces were available from video, datafast and voice classes. The paper briefly summarizes the tested CAC algorithms then discusses mapping. The notion of mapping aims at finding an equivalent real environment in which all relevant aspects of the performance of different CAC algorithms can be tested without the validation process getting bogged down in side effects (e.g., unrealistically long measuring time to capture rare events etc.). After discussing the testing principles, different performance measures were introduced, based on how the CAC algorithms can be ranked. By analyzing the results a ""toplist"" of CAC algorithms was produced which can help when considering real implementation. Finally the reliability of the measurements and the inferred conclusions are addressed.","","0-7803-5428","10.1109/ICATM.1999.786803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=786803","","Testing;Quality of service;Traffic control;Communication system traffic control;State-space methods;Switches;Aggregates;Loss measurement;Software algorithms;Algorithm design and analysis","telecommunication congestion control;asynchronous transfer mode;telecommunication traffic;visual communication;data communication;voice communication;program testing;telecommunication computing","CAC algorithms;ATM testbeds;project COP579;Expert Testing Facilities;Basel;Switzerland;versatile source models;traffic traces;video classes;datafast classes;voice classes;will be discussed;validating process;performance measures;measurement reliability","","1","15","","","","","","IEEE","IEEE Conferences"
"Using Mathematica to aid simulation analysis","P. A. Savory","Nebraska Univ., Lincoln, NE, USA","Winter Simulation Conference Proceedings, 1995.","","1995","","","1324","1328","As computer power has increased, so has the capability of software developers to write programs that assist people with time-consuming tasks. Mathematica is such a program. The objective of this paper is to demonstrate how Mathematica, a symbolic programming environment, can be used to aid simulation analysis. In addition to a general discussion of Mathematica's uses, advantages, and disadvantages, several examples are presented. The examples include using Mathematica for distribution fitting, queueing analysis, random number generation, and creating a surface plot for optimization.","","0-78033018","10.1109/WSC.1995.479042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479042","","Analytical models;Testing;Computational modeling;Gaussian distribution;Surface fitting;Queueing analysis;Power system management;Engineering management;Power engineering and energy;Systems engineering and theory","digital simulation;software packages;symbol manipulation;random number generation;programming environments;statistical analysis;queueing theory;optimisation;surface fitting","simulation analysis;Mathematica;symbolic programming environment;distribution fitting;queueing analysis;random number generation;surface plot;optimization","","3","6","","","","","","IEEE","IEEE Conferences"
"CLADOR: a documentation method for C programmers","R. Ordonez; E. Diaz; G. Jimenez; J. Sevillano; J. Madrid","Grupo de Robotica y Procesado de Imagen, Sevilla Univ., Spain; Grupo de Robotica y Procesado de Imagen, Sevilla Univ., Spain; Grupo de Robotica y Procesado de Imagen, Sevilla Univ., Spain; Grupo de Robotica y Procesado de Imagen, Sevilla Univ., Spain; Grupo de Robotica y Procesado de Imagen, Sevilla Univ., Spain","[1991] Proceedings, Advanced Computer Technology, Reliable Systems and Applications","","1991","","","213","218","Software maintenance can be successfully accomplished if its documentation is structured clearly. The CLADOR (C LAnguage DOcumentation Rules) method is a proposal for a documentation method for C programmers. This method allows optimization of the programming time and the generated programs by generating structured and complete documentation.<<ETX>>","","0-8186-2141","10.1109/CMPEUR.1991.257384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257384","","Documentation;Programming profession;Libraries;Software maintenance;Proposals;Robot programming;Optimization methods;Debugging;Life testing;Software engineering","C language;optimisation;programming;system documentation","C Language Documentation Rules;software maintenance;program optimization;CLADOR;documentation method;programming time","","","32","","","","","","IEEE","IEEE Conferences"
"Web services-based collaborative and cooperative computing","W. T. Tsai; Z. Cao; Y. Chen; R. Paul","Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; NA","Proceedings Autonomous Decentralized Systems, 2005. ISADS 2005.","","2005","","","552","556","This paper presents an integrated development process for Web services. The key differences with the traditional software development are that this new process involves collaboration and cooperation among all parties involved: developers, brokers, and clients. The process defines a new way of developing trustworthy Web services based on the existing Internet infrastructure. The paper serves as a roadmap to research on Web services specification, discovery, ontology, composition, re-composition, testing, reliability assessing, ranking, and collaboration and cooperation among all parities involved in Web services research, development, and application.","1541-0056","0-7803-8963","10.1109/ISADS.2005.1452134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452134","","Collaboration;Web services;Testing;Publishing;Modular construction;Web and internet services;Runtime;Computer science;Programming;Collaborative software","Internet;groupware;software engineering","Web services;collaborative computing;cooperative computing;software development;Internet infrastructure;reliability assessment;service composition","","7","8","","","","","","IEEE","IEEE Conferences"
"SEMPA: software engineering for parallel scientific computing","P. Luksch; U. Maier; S. Rathmayer; M. Weidmann; F. Unger","Inst. fur Inf., Tech. Univ. Munchen, Germany; NA; NA; NA; NA","IEEE Concurrency","","1997","5","3","64","72","The Sempa project brings together researchers from computer science, mechanical engineering, and numerical analysis to define software-engineering methods for the parallelization of existing large-scale software packages in scientific computing. The parallel implementation of TfC, an industrial state-of-the-art computational-fluid-dynamics simulation program, serves as the central case study for defining and evaluating these methods. Sempa researchers have successfully implemented and tested a portable parallel version of TfC based on message-passing standards (PVM and MPI). In addition, Sempa researchers have demonstrated the potential of new languages and programming paradigms such as data parallelism and object orientation by reimplementing the algebraic multigrid solver-a key module in TfC-in Fortran 90, HPF, and C++. Networks of workstations have become very attractive as low-cost platforms. Efficiently using their resources for production runs of parallel software requires automated resource management that optimizes resource utilization for parallel batch jobs without impeding interactive use. Therefore, the Sempa project has also implemented a resource manager for batch execution of PVM programs.","1092-3063;1558-0849","","10.1109/4434.605920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=605920","","Software engineering;Scientific computing;Resource management;Computer science;Mechanical engineering;Numerical analysis;Large-scale systems;Software packages;Computer industry;Concurrent computing","software engineering;parallel programming","SEMPA;software engineering;parallel scientific computing;parallelization;computational-fluid-dynamics;message-passing standards;resource manager;PVM programs;algebraic multigrid solver","","2","12","","","","","","IEEE","IEEE Journals & Magazines"
"An optimized method for computer-aided DC measurements of power MOS transistors","J. Gracia; F. J. Aranceta","Dept. de Electr., Electron. y Autom., Centro de Estudios e Investigaciones Tecnicas de Guipuzcoa, San Sebastian, Spain; Dept. de Electr., Electron. y Autom., Centro de Estudios e Investigaciones Tecnicas de Guipuzcoa, San Sebastian, Spain","IEEE Transactions on Instrumentation and Measurement","","1988","37","3","393","397","An intelligent data acquisition system for the characterization of power NMOS transistors has been developed. The system uses a pulsed method with adaptive duty cycle. The temperature is empirically determined by using the transistor under test as a thermometer. An accurate database can be obtained for modeling or parameter-extraction purposes. Rules for the rejection of anomalous data due to thermal or electrical transitions have been included in the software. A criterion based on the first and second derivatives has shown that the acquired database quality is better when these rules are used. This criterion is independent of the model considered and the parameter-extraction algorithm used.<<ETX>>","0018-9456;1557-9662","","10.1109/19.7462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7462","","Optimization methods;Power measurement;MOSFETs;Databases;Temperature;Power system modeling;Intelligent systems;Parameter extraction;Power transistors;Data mining","characteristics measurement;circuit analysis computing;computerised instrumentation;data acquisition;electric variables measurement;insulated gate field effect transistors;power transistors;semiconductor device testing","thermal transitions;characteristics measurement;insulated gate FET;computer-aided DC measurements;power MOS transistors;intelligent data acquisition;NMOS;pulsed method;adaptive duty cycle;modeling;parameter-extraction;rejection of anomalous data;electrical transitions;parameter-extraction algorithm","","1","7","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic incremental development of agent systems, using Prometheus","M. Perepletchikov; L. Padgham","Sch. of Comput. Sci. & Inf. Technol., R. Melbourne Inst. of Technol., Vic., Australia; Sch. of Comput. Sci. & Inf. Technol., R. Melbourne Inst. of Technol., Vic., Australia","Fifth International Conference on Quality Software (QSIC'05)","","2005","","","413","418","This paper presents a mechanism for dividing an agent oriented application into the three IEEE defined scoping levels of essential, conditional and optional. This mechanism is applied after the initial system specification, and is then used to direct incremental development with three separate releases. The scoping described can be applied at any stage of a project, in order to guide consistent scoping back if such is needed. The three levels of scoping that are used are consistent with the approach used in many companies. The approach to scoping requires that scenarios are prioritised manually on a five point scale. All other aspects are then prioritised automatically, based on this information. The approach used allows a developer to indicate what size partitions - based on number of scenarios - are required for each scoping level. The mechanisms are applied to the Prometheus development methodology and are integrated into the Prometheus design tool (PDT).","1550-6002;2332-662X","0-7695-2472","10.1109/QSIC.2005.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579165","","Programming;Testing;Iterative methods;Computer science;Information technology;Application software;Software engineering;Software quality","multi-agent systems;formal specification;software agents","system specification;Prometheus development methodology;Prometheus design tool;agent systems incremental development","","2","14","","","","","","IEEE","IEEE Conferences"
"S-Check: a tool for tuning parallel programs","R. Snelick","Nat. Inst. of Stand. & Technol., Boulder, CO, USA","Proceedings 11th International Parallel Processing Symposium","","1997","","","107","112","We present a novel tool, called S-Check, for identifying performance bottlenecks in parallel and networked programs. S-Check is a highly-automated sensitivity analysis tool for programs that extends benchmarking and conventional profiling. It predicts how refinements in parts of a program are going to affect performance by making focal changes in code efficiencies and correlating these against overall program performance. This analysis is a sophisticated comparison that catches interactions arising from shared resources or communication links. S-Check's performance assessment ranks code segments ""bottleneck"" according to their sensitivity to the code efficiency changes. This rank-ordered list serves as a guide for tuning applications. In practice, S-Check code analysis yields faster parallel programs. A case study compares and contrasts sensitivity analyses of the same program on different architectures and offers solutions for performance improvement. An initial implementation of S-Check runs on Silicon Graphics multiprocessors and IBM SP machines. Particulars of the underlying methodology are only sketched with main emphasis given to details of the tool S-Check and its use.","1063-7133","0-8186-7793","10.1109/IPPS.1997.580861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=580861","","Delay;Sensitivity analysis;Software tools;NIST;Silicon;Graphics;Data mining;Neck;Image processing;Benchmark testing","parallel programming;software performance evaluation;multiprocessing systems","S-Check;parallel programs tuning;performance bottlenecks;networked programs;highly-automated sensitivity analysis tool;benchmarking;code efficiencies;shared resources;communication links;rank-ordered list;Silicon Graphics multiprocessors;IBM SP machines","","1","6","","","","","","IEEE","IEEE Conferences"
"Development and testing of a remote sensing instrument using GNSS reflectometry concepts","S. Gleason; M. Unwin","Surrey Space Centre, Surrey Satellite Technol. Ltd., Guildford, UK; Surrey Space Centre, Surrey Satellite Technol. Ltd., Guildford, UK","IGARSS 2003. 2003 IEEE International Geoscience and Remote Sensing Symposium. Proceedings (IEEE Cat. No.03CH37477)","","2003","7","","4332","4334 vol.7","A study has been undertaken by Surrey Satellite Technology Limited (SSTL) with support from the British National Space Centre (BNSC) to upgrade SSTL's Space GPS Receiver (SGR) into an ocean remote sensing instrument. Software algorithms were added to the SGR to permit on-board characterisation of GPS reflection opportunities. Subsequently, a hardware interface was added to allow for raw data sampling and in-depth data analysis. The upgraded SGR has undergone several levels of testing to date. The real time specular point calculations and an on board slewing capability were demonstrated using a Low Earth Orbit (LEO) spacecraft carrying an SGR for navigation purposes. The ability to map a reflected GPS signal was then accomplished during ground testing from an observatory on a hill above Barcelona, Spain. In preparing the instrument for an upcoming flight experiment, the raw data analysis capability has been undergoing substantial validation and optimisation. The ultimate goal of these experiments is to recover parameters relating to sea-state that may be useful for both scientific and commercial marine users. This paper is intended to detail the initial phases of development and testing of a future low cost ocean remote sensing instrument.","","0-7803-7929","10.1109/IGARSS.2003.1295506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1295506","","Testing;Remote sensing;Instruments;Satellite navigation systems;Reflectometry;Global Positioning System;Space technology;BNSC;Oceans;Data analysis","Global Positioning System;remote sensing;oceanography;geophysical equipment;electromagnetic wave reflection;UHF radio propagation","remote sensing test;GNSS reflectometry;Surrey Satellite Technology Limited;British National Space Centre;GPS signal reflection;hardware interface;low Earth orbit;spacecraft;navigation;Barcelona;Spain;sea-state;marine testing;ocean remote sensing instrument","","3","10","","","","","","IEEE","IEEE Conferences"
"Schedulability analysis and optimization for the synthesis of multi-cluster distributed embedded systems","P. Pop; P. Eles; Zebo Peng","Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","184","189","We present an approach to schedulability analysis for the synthesis of multi-cluster distributed embedded systems consisting of time-triggered and event-triggered clusters, interconnected via gateways. We have also proposed a buffer size and worst case queuing delay analysis for the gateways, responsible for routing inter-cluster traffic. Optimization heuristics for the priority assignment and synthesis of bus access parameters, aimed at producing a schedulable system with minimal buffer needs, have been proposed. Extensive experiments and a real-life example show the efficiency of our approaches.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253606","","Embedded system;Time division multiple access;Communication system control;Kernel;Computer architecture;Broadcasting;Communication channels;Packaging;Message passing;Application software","multiprocessing systems;network operating systems;embedded systems;optimisation;internetworking;telecommunication network routing;processor scheduling","multicluster distributed embedded systems;schedulability analysis;heuristic optimization;time-triggered clusters;event-triggered clusters;gateway interconnections;buffer size analysis;worst case queuing delay;inter-cluster traffic routing;priority assignment;bus access parameters;buffer requirement minimization;real-time applications","","11","18","","","","","","IEEE","IEEE Conferences"
"Data reuse exploration techniques for loop-dominated applications","T. Van Achteren; G. Deconinck; F. Catthoor; R. Lauwereins","ESAT/ACCA, Katholieke Univ., Leuven, Belgium; NA; NA; NA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","428","435","Efficient exploitation of temporal locality in the memory accesses on array signals can have a very large impact art the power consumption in embedded data dominated applications. The effective use of an optimized custom memory hierarchy or a customized software controlled mapping on a predefined hierarchy, is crucial for this. Only recently have effective systematic techniques to deal with this specific design step begun to appear They were still limited in their exploration scope. In this paper we introduce an extended formalized methodology based on an analytical model of the data reuse of a signal. The cost parameters derived from this model define the search space to explore and allow us to exploit the maximum data reuse possible. The result is an automated design technique to find power efficient memory hierarchies and generate the corresponding optimized code.","1530-1591","0-7695-1471","10.1109/DATE.2002.999206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999206","","Design automation;Automatic testing;Europe","low-power electronics;memory architecture;formal verification;cache storage;circuit optimisation;microprocessor chips","data reuse;exploration scope;loop-dominated applications;temporal locality;formalized methodology;cost parameters;search space;automated design technique;power efficient memory hierarchies;optimized code;array signals;power consumption;optimized custom memory hierarchy;customized software controlled mapping","","11","30","","","","","","IEEE","IEEE Conferences"
"Toward a fuzzy-logic based formal-approach for specifying imprecise requirements","J. Yen; J. Lee; Xiaoqing Liu","Texas A&M Univ., College Station, TX, USA; NA; NA","Proceedings of Annual Reliability and Maintainability Symposium (RAMS)","","1994","","","462","467","A major challenge with requirement engineering is that the requirements to be captured usually are described in qualitative terms which are imprecise in nature. However, most existing software specification methodologies require that the requirements be represented in a precise form. In this paper, we propose a fuzzy logic-based formal approach to formulate soft (imprecise) functional requirements. Based on such a formulation, the trade-offs among conflicting soft requirements can be analyzed using fuzzy multicriteria optimization techniques. The proposed approach not only can provide useful design guidelines that help the designer to focus on effective design trade-offs, but also enable a more realistic validation of the user's imprecise requirements.<<ETX>>","","0-7803-1786","10.1109/RAMS.1994.291152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=291152","","Fuzzy logic;Fuzzy sets;System testing;Optimization methods;Logic testing;Guidelines;Software safety;Problem-solving;Control systems;Expert systems","fuzzy logic;operations research;optimisation","fuzzy logic;imprecise requirement specification;requirement engineering;qualitative requirements;software specification;fuzzy multicriteria optimization techniques","","","17","","","","","","IEEE","IEEE Conferences"
"Application of 3D finite element CAD techniques in the design of current sensitive electromagnetic actuators","E. Li; P. M. McEwan; A. L. Kidd","Sch. of Eng. Inf. Technol., Sheffield City Polytech., UK; Sch. of Eng. Inf. Technol., Sheffield City Polytech., UK; NA","IEEE Transactions on Magnetics","","1992","28","2","1414","1417","Presents a method for evaluating and optimizing the design of an electromagnetic current-sensitive actuator, using a three-dimensional computer-aided design (CAD) finite-element (FE) package. The study is based on the use of the 3D FE-CAD (TOSCA) software package for the determination of the dynamic performance and the optimization of the actuator design when subjected to sinusoidal current. The analysis enables the relationship of the actuator sensitivity to manufacturing tolerance, sinusoidal current, and a wide range of design and control variables to be examined. The actuator sensitivity is quantified in terms of critical design factors for four practical actuators and a range of air-gap settings. The results of all the studies correlated closely with the results from tests and measurements.<<ETX>>","0018-9464;1941-0069","","10.1109/20.123958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=123958","","Finite element methods;Design automation;Actuators;Design optimization;Application software;Packaging;Software packages;Manufacturing;Air gaps;Testing","actuators;control system CAD;electromagnets;finite element analysis","TOSCA;3D finite element CAD techniques;current sensitive electromagnetic actuators;software package;sinusoidal current;manufacturing tolerance;critical design factors;air-gap settings","","1","4","","","","","","IEEE","IEEE Journals & Magazines"
"Forward-looking objective functions: concept and applications in high level synthesis","J. L. Wong; S. Megerian; M. Potkonjak","California Univ., Los Angeles, CA, USA; California Univ., Los Angeles, CA, USA; California Univ., Los Angeles, CA, USA","Proceedings 2002 Design Automation Conference (IEEE Cat. No.02CH37324)","","2002","","","904","909","The effectiveness of traditional CAD optimization algorithms is proportional to the accuracy of the targeted objective functions. However, behavioral synthesis tools are not used in isolation; they form a strongly connected design flow where each tool optimizes its own objective function without considering the consequences on the optimization goals of the subsequently applied tools. Therefore, efforts to optimize one aspect of a design often have unforeseen negative impacts on other phases of the design process. Our objective is to establish a systematic way of developing and validating new types of objective functions that consider the effects on subsequently applied synthesis steps. We demonstrate the generic forward-looking objective function (FLOF) strategy on three main steps in behavioral synthesis: (i) transformation, (ii) scheduling, and (iii) register assignment. We show how the FLOF can be used in the first two phases to reduce the total number of registers required in the third phase.","0738-100X","1-58113-461","10.1109/DAC.2002.1012750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012750","","High level synthesis;Design optimization;Design automation;Algorithm design and analysis;Permission;Processor scheduling;Process design;Logic design;Testing;Registers","processor scheduling;circuit CAD;circuit optimisation;software tools;shift registers","forward-looking objective functions;high level synthesis;CAD optimization algorithms;targeted objective functions;behavioral synthesis tools;connected design flow;tool objective function optimization;tool optimization goals;design phases;subsequently applied synthesis steps;generic forward-looking objective function strategy;FLOF;transformation;scheduling;register assignment;register reduction","","1","11","","","","","","IEEE","IEEE Conferences"
"Hybrid inductor modeling for successful filter design","W. R. Gaiewski; L. P. Dunleavy; L. A. Geis","Dept. of Electr. Eng., Univ. of South Florida, Tampa, FL, USA; Dept. of Electr. Eng., Univ. of South Florida, Tampa, FL, USA; NA","IEEE Transactions on Microwave Theory and Techniques","","1994","42","7","1426","1429","A method for deriving accurate L-band inductor models using I-port fixtured S-parameter measurements is presented, along with examples of successful filter designs using the models. Models were developed for a number of surface-mount and leaded devices with values ranging from 4.7 nH to 470 nH. The devices were measured in a coaxial test fixture on an HP8510B network analyzer, with fixture de-embedding performed using CAD software. The models are seen to fit measured data over a broadband, from low RF through resonance. To further validate the models, several filter circuits were developed and fabricated. The results of 5th- and 9th-order lowpass filter designs are presented here. The 9th-order filter was optimized for the desired performance using the parasitic inductor models. Measurements confirm that the use of the models led to a successful design on the ""first pass."" Finally, simulations show that this success would not have been achieved without the developed inductor models.<<ETX>>","0018-9480;1557-9670","","10.1109/22.299741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=299741","","Inductors;Filters;Fixtures;L-band;Scattering parameters;Surface fitting;Software measurement;Coaxial components;Circuit testing;Software testing","equivalent circuits;inductors;modelling;S-parameters;radiofrequency filters;microwave filters;low-pass filters;passive filters;linear network synthesis;lumped parameter networks","filter design;L-band inductor models;S-parameter measurements;surface-mount devices;leaded devices;coaxial test fixture;HP8510B network analyzer;fixture de-embedding;CAD software;lowpass filter designs;parasitic inductor models;VHF;UHF","","1","9","","","","","","IEEE","IEEE Journals & Magazines"
"An interrange comparison in support of the characterisation of space antenna systems and payload testing","S. F. Gregson; A. J. Robinson","Matra Marconi Space UK Ltd, UK; NA","IEE Colloquium on Antenna Measurements (Ref. No. 1998/254)","","1998","","","6/1","6/5","The construction at Matra Marconi Space Portsmouth of a new 1-50 GHz, 22 m by 8 m scan area planar near-field antenna measurement system, taken together with the existence of the large spherical near-field range on the same site, has presented opportunities to compare results obtained in two very different antenna measurement facilities. This paper gives a comparative description of the facilities, and an overview of the measurement process in each range as applied to the measurement of spaceborne antennas. Results of a study undertaken to establish the similarity between results obtained by each range when measuring the same instrument are then summarised. The results show encouraging similarity given that the measurement geometry, RF subsystems, control computers, positioner hardware, near field-far field transform software, software correction techniques, alignment correction methods and alignment measurement procedures are different in each facility. It should also be remembered that the planar and spherical techniques are optimised for different classes of antenna.","","","10.1049/ic:19980302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=710070","","","antenna testing","interrange comparison;space antenna systems;payload testing;Matra Marconi Space Portsmouth;scan area planar near-field antenna measurement system;large spherical near-field range;antenna measurement facilities;spaceborne antennas;1 to 50 GHz","","","","","","","","","IET","IET Conferences"
"Dynamic multiobjective optimization of war resource allocation using adaptive genetic algorithms","S. Palaniappan; S. Zein-Sabatto; A. Sekmen","Tennessee State Univ., Nashville, TN, USA; NA; NA","Proceedings. IEEE SoutheastCon 2001 (Cat. No.01CH37208)","","2001","","","160","165","Genetic algorithms (GA) are often well suited for multiobjective optimization problems. The major objective of this research is to optimize the war resource allocations of sorties, for a given war scenario, using genetic algorithms. The war is simulated using THUNDER software. THUNDER software is a stochastic, two-sided, analytical simulation of campaign-level military operations. The simulation is subject to internal unknown noises similar to real war cases. Due to these noises and discreteness in the simulation, as well as in real wars, an adaptive GA approach has been applied to solve this multiobjective optimization problem. Transforming this multiobjective optimization problem to a form suitable for direct implementation of GA was a major accomplishment of this research. A suitable fitness function was chosen after careful research and testing on the GA. Furthermore, the GA parameters were adaptively set to yield smoother and faster fitness convergence. Two fuzzy logic mechanisms were used to adapt the GA parameters. In the first mechanism, the mutation and crossover rates were changed adaptively. In the second mechanism, the fitness function coefficients are changed dynamically in each run. Testing results showed that the adaptive GA outperforms the conventional GA search in this multiobjective optimization problem and was effectively able to allocate forces for war scenarios.","","0-7803-6748","10.1109/SECON.2001.923107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923107","","Resource management;Genetic algorithms;Analytical models;Testing;Stochastic resonance;Convergence;Fuzzy logic;Genetic mutations;Intelligent systems;Military computing","military computing;resource allocation;adaptive systems;genetic algorithms;operations research;digital simulation;stochastic processes;fuzzy logic","dynamic multiobjective optimization;war resource allocation;adaptive genetic algorithms;multiobjective optimization problems;sorties;war scenario;THUNDER software;stochastic two-sided analytical simulation;campaign-level military operations;internal unknown noises;real war cases;adaptive GA approach;multiobjective optimization problem;direct implementation;fitness function;GA parameters;fitness convergence;fuzzy logic mechanisms;mutation;crossover rates;fitness function coefficients;conventional GA search;force allocation","","8","8","","","","","","IEEE","IEEE Conferences"
"Performance calculation of brushless DC motor","Ping Zheng; Jie Wang; Ranran Liu; Huawei Jiang; Shumei Cui; Shukang Cheng","Sch. of Electr. Eng. & Autom., Harbin Inst. of Technol., China; Sch. of Electr. Eng. & Autom., Harbin Inst. of Technol., China; Sch. of Electr. Eng. & Autom., Harbin Inst. of Technol., China; NA; NA; NA","2005 International Conference on Electrical Machines and Systems","","2005","1","","426","428 Vol. 1","In this paper the performances of a tangentially magnetized brushless DC motor are calculated and tested. For the calculation, both circuit method and finite-element method (FEM) provided by commercial software Ansoft are used. With the circuit method, motor design and performance calculation can be performed. A prototype motor is designed and calculated, and the curves of voltage, current, torque and efficiency are given. The measured results are also given in the paper, and the calculated and measured results are in good agreement. With the FEM, both no-load and loaded performances of the prototype motor are calculated in detail. The curves of no-load back electromotive force (BEMF), loaded torque, and loaded BEMF are given. The measured BEMF curve is also given, the calculated and measured BEMF curves are in good agreement. Since the results obtained from the circuit method and FEM are both consistent with the experimental results very well, reliable foundation has been established for further design and optimization.","","7-5062-7407","10.1109/ICEMS.2005.202561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1574794","","Brushless DC motors;DC motors;Prototypes;Torque;Performance evaluation;Circuit testing;Finite element methods;Design methodology;Software prototyping;Voltage","brushless DC motors;machine testing;finite element analysis;electric potential;design engineering","brushless DC motor;circuit method;finite-element method;FEM;commercial software;Ansoft;no-load back electromotive force","","","15","","","","","","IEEE","IEEE Conferences"
"Ridge waveguide branch-line directional couplers for wideband applications and LTCC technology","J. A. Ruiz-Cruz; Yunchi Zhang; K. A. Zaki; A. J. Piloto; J. M. Rebollar","NA; NA; NA; NA; NA","IEEE MTT-S International Microwave Symposium Digest, 2005.","","2005","","","1219","1222","A new branch-line directional coupler is proposed for achieving wide bandwidth. The device is implemented in ridge waveguide to exploit its wide monomode band. It can be physically realized either in empty ridge waveguide or in LTCC for integration in a chip module. Two prototypes of two and five branches are designed following a systematic procedure. An appropriate equivalent circuit provides initial dimensions and the desired optimum response is obtained by means of a final full-wave optimization based on the rigorous and efficient mode-matching method. The results are verified with the finite element method of the HFSS software and a prototype will be manufactured for testing","0149-645X","0-7803-8845","10.1109/MWSYM.2005.1516896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1516896","","Directional couplers;Wideband;Bandwidth;Prototypes;Equivalent circuits;Optimization methods;Mode matching methods;Finite element methods;Software prototyping;Manufacturing","directional couplers;equivalent circuits;finite element analysis;mode matching;optimisation;ridge waveguides","ridge waveguides;branch-line directional couplers;LTCC technology;wide monomode band;chip module integration;equivalent circuits;full-wave optimization;mode-matching method;finite element method","","1","16","","","","","","IEEE","IEEE Conferences"
"A coprocessor for accelerating visual information processing","W. Stechele; L. A. Carcel; S. Herrmann; J. L. Simon","Technische Univ. Munchen, Germany; NA; NA; NA","Design, Automation and Test in Europe","","2005","","","26","31 Vol. 3","Visual information processing will play an increasingly important role in future electronics systems. In many applications, e.g. video surveillance cameras, data throughput of microprocessors is not sufficient and power consumption is too high. Instruction profiling on a typical test algorithm has shown that pixel address calculations are the dominant operations to be optimized. Therefore AddressLib, a structured scheme for pixel addressing was developed, that can be accelerated by AddressEngine, a coprocessor for visual information processing. In this paper, the architectural design of AddressEngine is described, which in the first step supports a subset of the AddressLib. Dataflow and memory organization are optimized during architectural design. AddressEngine was implemented in an FPGA and was tested with the MPEG-7 global motion estimation algorithm. Results on processing speed and circuit complexity are given and compared to a pure software implementation. The next step will be the support for the full AddressLib, including segment addressing. An outlook on further investigations on dynamic reconfiguration capabilities is given.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395788","","Coprocessors;Acceleration;Information processing;Video surveillance;Cameras;Throughput;Microprocessors;Energy consumption;Testing;Design optimization","image segmentation;video signal processing;coprocessors;circuit optimisation;field programmable gate arrays;motion estimation;reconfigurable architectures","video object segmentation;visual information processing;video surveillance cameras;microprocessor data throughput;pixel address calculations;AddressLib pixel addressing scheme;AddressEngine accelerated coprocessor;dataflow optimization;memory organization;FPGA;MPEG-7 global motion estimation algorithm;dynamic reconfiguration","","4","6","","","","","","IEEE","IEEE Conferences"
"Thin layers in electrical engineering-example of shell models in analysing eddy-currents by boundary and finite element methods","L. Krahenbuhl; D. Muller","CEGELY, Ecole Centrale de Lyon, Ecully, France; CEGELY, Ecole Centrale de Lyon, Ecully, France","IEEE Transactions on Magnetics","","1993","29","2","1450","1455","The authors propose a didactical approach to thin regions in electromagnetics and, as an example, derive the boundary conditions and surface equation for eddy currents flowing inside a thin ferromagnetic shell. The numerical tests are done using the boundary-element-method (BEM) software package PH13-D, but the results could easily be transposed in a finite-element-method (FEM) context. The practical applications my concern the computation of losses (shield of electrical machines or transformers) or low-frequency electromagnetic perturbations (screen effects, electromagnetic compatibility) as well as special applications such as the optimization of the induction heating of pans.<<ETX>>","0018-9464;1941-0069","","10.1109/20.250676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250676","","Application software;Electromagnetic compatibility;Boundary conditions;Equations;Eddy currents;Software testing;Software packages;Finite element methods;Transformers;Electromagnetic induction","boundary-elements methods;eddy currents;electrical engineering computing;finite element analysis","thin layers;shell models;eddy-currents;finite element methods;thin regions;electromagnetics;boundary conditions;surface equation;ferromagnetic shell;boundary-element-method;software package PH13-D;losses;low-frequency electromagnetic perturbations;screen effects;electromagnetic compatibility;optimization;induction heating;pans","","95","12","","","","","","IEEE","IEEE Journals & Magazines"
"Lagrangian relaxation neural networks for job shop scheduling","P. B. Luh; Xing Zhao; Yajun Wang; L. S. Thakur","Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; NA; NA; NA","IEEE Transactions on Robotics and Automation","","2000","16","1","78","88","Manufacturing scheduling is an important but difficult task. In order to effectively solve such combinatorial optimization problems, the paper presents a Lagrangian relaxation neural network (LRNN) for separable optimization problems by combining recurrent neural network optimization ideas with Lagrangian relaxation (LR) for constraint handling. The convergence of the network is proved, and a general framework for neural implementation is established, allowing creative variations. When applying the network to job shop scheduling, the separability of problem formulation is fully exploited, and a new neuron-based dynamic programming is developed making innovative use of the subproblem structure. Testing results obtained by software simulation demonstrate that the method is able to provide near-optimal solutions for practical job shop scheduling problems, and the results are superior to what have been reported in the neural network scheduling literature. In fact, the digital implementation of LRNN for job shop scheduling is similar to the traditional LR approaches. The method, however, has the potential to be implemented in hardware with much improved quality and speed.","1042-296X;2374-958X","","10.1109/70.833193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=833193","","Lagrangian functions;Neural networks;Job shop scheduling;Constraint optimization;Recurrent neural networks;Manufacturing;Convergence;Dynamic programming;Software testing;Hardware","constraint handling;convergence;dynamic programming;optimisation;recurrent neural nets;production control;convex programming;combinatorial mathematics","Lagrangian relaxation neural networks;job shop scheduling;combinatorial optimization problems;separable optimization problems;recurrent neural network optimization;constraint handling;neuron-based dynamic programming","","25","21","","","","","","IEEE","IEEE Journals & Magazines"
"Customized simulation interface for packaged gas warehouse design","A. Simon; B. Rekiek; A. Delchambre","CAD-CAM Dept., Univ. Libre de Bruxelles, Brussels, Belgium; NA; NA","Proceedings of the IEEE International Symposium onAssembly and Task Planning, 2003.","","2003","","","289","294","The objective of this paper is to present the prototype of a software package embedding a simulation tool, customized for optimizing the facility layout of packaged gas warehouses. In those warehouses, empty packages are filled with different kinds of gases. To accelerate the filling process, pallets of similar packages are prepared in so-called sorting areas, while assorted pallets of filled ones are composed to fulfil the clients' needs. The main challenge for the authors' industrial partner is to be able to answer its customers' demands in less than 24 hours, while reducing the production costs. These costs have several sources: the numbers of packages within the system, creating stocks but also occupying significant space, the layout of the different areas and their interactions (internal transport, buffers, etc). Simulation has proven to be an efficient tool to help the designer to study the problems related to interactions between process and space. The philosophy of the prototype, called ""depot layout editor"", is to focus on the description of the architecture of the ware-house, while the simulation itself is left to a commercial package, AUTOMOD/spl trade/. Once the warehouse (or part of it) has been described, the software generates the simulation code, which can then be run within the AUTOMOD/spl trade/ environment. One of the advantages of using an external simulation kernel is that it provides extensive statistical analysis tools to evaluate the performance of a given design. The main feature of the approach is to be able to quickly test several architectures and organizations in order to find the one that best suits each warehouse constraints: geometry of the building, availability of stocks, customers' daily orders, etc. This paper will present the several aspects of the simulation preparation (products, process and layout definition), as well as the results obtained on several ware-house simulations studied for validation.","","0-7803-7770","10.1109/ISATP.2003.1217226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1217226","","Packaging;Software prototyping;Virtual prototyping;Software packages;Costs;Gases;Acceleration;Filling;Sorting;Production","prototypes;statistical analysis;computer interfaces;optimisation;software packages;warehouse automation;computational geometry","customized simulation interface;simulation tool;software package;gas warehouses;software prototype;production costs;depot layout editor;commercial package;AUTOMOD;statistical analysis tools;warehouse constraints;building geometry;prototype philosophy;embedded packages","","1","13","","","","","","IEEE","IEEE Conferences"
"Metro map layout using multicriteria optimization","J. M. Stott; P. Rodgers","Kent Univ., UK; Kent Univ., UK","Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.","","2004","","","355","362","We describe a system to automatically generate metro maps using a multicriteria approach. We have implemented a hill climbing optimizer which uses a fitness score generated from a sum of several aesthetic metrics. This is used to move from the initial geographic layout of the map to a schematic layout that is intended to aid travellers' navigation. We describe the software and show its application to a number of real world metro maps.","1093-9547","0-7695-2177","10.1109/IV.2004.1320168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1320168","","Navigation;Application software;Cities and towns;Circuits;Pipelines;Data visualization;Labeling;System testing;Contracts","cartography;optimisation;data visualisation","metro map layout;multicriteria optimization;hill climbing optimizer;fitness score;aesthetic metrics;geographic layout;schematic layout;travellers aid;navigation;public transport schematics;graph drawing","","6","13","","","","","","IEEE","IEEE Conferences"
"WebSphere Studio overview","F. Budinsky; G. DeCandio; R. Earle; T. Francis; J. Jones; J. Li; M. Nally; C. Nelin; V. Popescu; S. Rich; A. Ryman; T. Wilson","IBM Software Group, IBM Toronto Lab, 8200 Warden Ave, Markham, ON L6G 1C7, Canada; IBM Software Group, Research Triangle Park Lab, 3039 Cornwallis Road RTP, NC 27709; IBM Software Group, Research Triangle Park Lab, 3039 Cornwallis Road RTP, NC 27709; IBM Software Group, IBM Toronto Lab, 8200 Warden Ave, Markham, ON L6G 1C7, Canada; IBM Software Group, IBM Toronto Lab, 8200 Warden Ave, Markham, ON L6G 1C7, Canada; IBM Software Group, IBM Toronto Lab, 8200 Warden Ave, Markham, ON L6G 1C7, Canada; IBM Software Group, Research Triangle Park Lab, 3039 Cornwallis Road RTP, NC 27709; IBM Software Group, Research Triangle Park Lab, 3039 Cornwallis Road RTP, NC 27709; IBM Software Group, IBM Toronto Lab, 8200 Warden Ave, Markham, ON L6G 1C7, Canada; IBM Software Group, Research Triangle Park Lab, 3039 Cornwallis Road RTP, NC 27709; IBM Software Group, IBM Toronto Lab, 8200 Warden Ave, Markham, ON L6G 1C7, Canada; IBM Software Group, Research Triangle Park Lab, 3039 Cornwallis Road RTP, NC 27709","IBM Systems Journal","","2004","43","2","384","419","In this paper we provide an overview of IBM WebSphere Studio, a family of tools for developing distributed applications for J2EE servers for state-of-the-art information technology systems. In today's business environment such systems are complex, comprise multiple platforms, and make use of a wide range of technologies and standards. Through a representative development scenario we illustrate the way WebSphere Studio satisfies the challenging requirements for a modern integrated development environment. The scenario covers a variety of technologies and standards, including database access, Web services standards, Enterprise JavaBeans implementation, integrated application testing, Web page design, and performance optimization. We also describe the Eclipse Modeling Framework, the open source technology base on which WebSphere Studio is built.","0018-8670","","10.1147/sj.432.0384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386784","","","","","","1","","","","","","","IBM","IBM Journals & Magazines"
"The use of genetic algorithms for advanced instrument fault detection and isolation schemes","G. Betta; C. Liguori; A. Pietrosanto","Dept. of Ind. Eng., Cassino Univ., Italy; Dept. of Ind. Eng., Cassino Univ., Italy; NA","Quality Measurement: The Indispensable Bridge between Theory and Reality (No Measurements? No Science! Joint Conference - 1996: IEEE Instrumentation and Measurement Technology Conference and IMEKO Tec","","1996","2","","1129","1134 vol.2","An advanced scheme for Instrument Fault Detection and Isolation is proposed. It is mainly based on Artificial Neural Networks (ANNs), organized in layers and handled by knowledge-based analytical redundancy relationships. ANN design and training is performed by genetic algorithms which allow ANN architecture and parameters to be easily optimized. The diagnostic performance of the proposed scheme is evaluated with reference to a measurement station for automatic testing of induction motors.","","0-7803-3312","10.1109/IMTC.1996.507340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507340","","Genetic algorithms;Instruments;Fault detection;Artificial neural networks;Redundancy;Algorithm design and analysis;Design optimization;Biological cells;Electrical fault detection;Induction motors","fault diagnosis;genetic algorithms;neural net architecture;diagnostic expert systems;automatic test equipment;automatic test software;redundancy;learning (artificial intelligence);induction motors;machine testing","genetic algorithms;instrument fault detection and isolation;advanced scheme;ANN design and training;knowledge-based analytical redundancy relationships;ANN architecture;diagnostic performance;measurement station;automatic testing;induction motors;fitness function;data acquisition system","","14","20","","","","","","IEEE","IEEE Conferences"
"MOCSYN: multiobjective core-based single-chip system synthesis","R. P. Dick; N. K. Jha","Dept. of Electr. Eng., Princeton Univ., NJ, USA; NA","Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)","","1999","","","263","270","In this paper we present a system synthesis algorithm, called MOCSYN, which partitions and schedules embedded system specifications to intellectual property cores in an integrated circuit. Given a system specification consisting of multiple periodic task graphs as well as a database of core and integrated circuit characteristics, MOCSYN synthesizes real-time heterogeneous single-chip hardware software architectures using an adaptive multiobjective genetic algorithm that is designed to escape local minima. The use of multiobjective optimization allows a single system synthesis run to produce multiple designs which trade off different architectural features. Integrated circuit price, power consumption, and area are optimized under hard real-time constraints. MOCSYN differs from previous work by considering problems unique to single-chip systems. It solves the problem of providing clock signals to cores composing a system-on-a-chip. It produces a bus structure which balances ease of layout with the reduction of bus contention. In addition, it carries out floorplan block placement within its inner loop allowing accurate estimation of global communication delays and power consumption.","","0-7695-0078","10.1109/DATE.1999.761132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=761132","","Integrated circuit synthesis;Energy consumption;Delay estimation;Partitioning algorithms;Scheduling algorithm;Embedded system;Intellectual property;Spatial databases;Real time systems;Hardware","hardware-software codesign;embedded systems;genetic algorithms;logic partitioning;integrated circuit layout;circuit layout CAD;data structures;directed graphs","MOCSYN algorithm;multiobjective core-based single-chip system synthesis;embedded system specifications;intellectual property cores;integrated circuit;partitioning;scheduling;multiple periodic task graphs;real-time heterogeneous architectures;single-chip hardware software architectures;adaptive multiobjective genetic algorithm;multiple designs;power consumption;IC area;IC price;hard real-time constraints;system-on-a-chip cores;bus structure;ease of layout;bus contention reduction;floorplan block placement;inner loop;global communication delays;data structures;directed acyclic graph;clock selection;task assignment","","17","32","","","","","","IEEE","IEEE Conferences"
"Two-ported cache alternatives for superscalar processors","A. Wolfe; R. Boleyn","Department of Electrical Engineering, Princeton University; Department of Electrical Engineering, Princeton University","Proceedings of the 26th Annual International Symposium on Microarchitecture","","1993","","","41","48","The quantitative approach to computer architecture and processor design requires extensive experimentation to assess the potential performance benefits of individual techniques. Using trace-driven simulation (TDS) tools in conjunction with optimizing compilers, a designer can quickly characterize the dynamic behavior and the resultant performance of a candidate machine running a realistic benchmark. Most current TDS tools have two shortcomings, namely the lack of retargetability and the lack of visualization support. The authors present the development of a highly retargetable TDS tool suite, called EXPLORER, that incorporates powerful visualization and interactive capabilities. A prototype of EXPLORER has been implemented and examples of retargeting and visualization-based simulation of the RS/6000 and the MPC 601 have been performed to demonstrate its effectiveness.<<ETX>>","","0-8186-5280","10.1109/MICRO.1993.282740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=282740","","Visualization;Computational modeling;Computer architecture;Process design;Optimizing compilers;Virtual prototyping;Computer simulation;Software tools;Data mining;Reduced instruction set computing","computer architecture;digital simulation;parallel programming;program testing;software portability;software tools","EXPLORER;retargetable;visualization-based;trace-driven simulator;superscalar processors;computer architecture;optimizing compilers;realistic benchmark;visualization support;TDS tool suite;interactive;retargeting;RS/6000;MPC 601","","6","21","","","","","","IEEE","IEEE Conferences"
"A Constraint Network Based Approach to Memory Layout Optimization","G. Chen; M. Kandemir; M. Karakoy","The Pennsylvania State University; NA; NA","Design, Automation and Test in Europe","","2005","","","1156","1161","While loop restructuring based code optimization for array intensive applications has been successful in the past, it has several problems such as the requirement of checking dependences (legality issues) and transformation of all of the array references within the loop body indiscriminately (while some of the references can benefit from the transformation, others may not). As a result, data transformations, i.e., transformations that modify memory layout of array data instead of loop structure have been proposed. One of the problems associated with data transformations is the difficulty of selecting a memory layout for an array that is acceptable to the entire program (not just to a single loop). In this paper, we formulate the problem of determining the memory layouts of arrays as a constraint network, and explore several methods of solution in a systematic way. Our experiments provide strong support in favor of employing constraint processing, and point out future research directions.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395750","","Constraint optimization;Computer science;Computer networks;Educational institutions;Application software;Proposals;Engineering profession;Robustness;Design automation;Automatic testing","","","","4","16","","","","","","IEEE","IEEE Conferences"
"MSVQ design for packet networks with application to LSF quantization","H. Khalil; K. Rose","Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA; NA","2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)","","2000","5","","2597","2600 vol.5","The design of a multi-stage vector quantizer (MSVQ) based source-channel coding system is optimized for packet networks. Resilience to packet loss is further enhanced by a proposed interleaving approach that ensures that a single lost packet only eliminates a subset of the vector stages. The design is optimized while taking into account compression efficiency, packet loss rate, and the interleaving technique in use. The new source-channel-optimized MSVQ is tested on memoryless line spectral frequency (LSF) parameter quantization in speech coders. A source-channel-optimization MSVQ is shown to yield a gain of up to 2.0 dB in SNR, for coding the LSFs, over traditional MSVQ and to substantially enhance the robustness of packetized speech transmission. Although the formulation is given in the context of packet networks, the work is directly extendible to the broader category of erasure channels.","1520-6149","0-7803-6293","10.1109/ICASSP.2000.860994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860994","","Quantization;Robustness;Decoding;Interleaved codes;Design optimization;Resilience;Laboratories;Signal design;Speech coding;Application software","vector quantisation;combined source-channel coding;packet switching;telecommunication networks;optimisation;memoryless systems;vocoders;spectral analysis","LSF quantization;MSVQ design;packet networks;multi-stage vector quantizer;source-channel coding system;packet loss resilience;interleaving approach;compression efficiency;packet loss rate;source-channel-optimized MSVQ;memoryless LSF parameter quantization;line spectral frequency;speech coders;SNR;packetized speech transmission;erasure channels","","1","8","","","","","","IEEE","IEEE Conferences"
"Performance-optimized feature ordering in content-based image retrieval","H. Eidenberger; C. Breiteneder","Austrian Libraries Network, Ministry of Science and Transport, Garnisongasse 7/21, A-1090 Vienna, Austria; Institute for Software Technology, Interactive Systems Group, Vienna University of Technology, Favoritenstrasse 9-11/188, A-1040 Vienna, Austria","2000 10th European Signal Processing Conference","","2000","","","1","4","We present a method to improve the performance of content-based image retrieval (CBIR) systems. The idea is based on the concept of query models [1], which generalizes the notion of similarity in multi-feature queries. In a query model features are organized in layers. Each succeeding layer has to investigate only a subset of the image set the preceding layer had to examine. For the purpose of performance acceleration we group features into two types: features for quick elimination of rather not similar images and features for the detailed analysis of result set candidates. Performance optimization is based on a model for predicting the number of images to be retrieved and on a model describing relationships between features. Results in our test environment show significant reduction of query execution time.","","978-952-1504-43","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7075779","","Predictive models;Vectors;Optimization;Databases;Computational modeling;Feature extraction;Image color analysis","","","","","","","","","","","IEEE","IEEE Conferences"
"Accumulator size minimization for a fast cumulant-based motion estimator","J. S. Cardoso; L. Corte-Real","Dept. de Engenharia Electrotecnica, Univ. do Porto/INESC Porto, Porto, Portugal; Dept. de Engenharia Electrotecnica, Univ. do Porto/INESC Porto, Porto, Portugal","IEEE Transactions on Circuits and Systems for Video Technology","","2005","15","12","1660","1664","The implementation of fast dedicated processor for block matching motion estimation based on cumulants matching criteria implies the optimization of all of its components. Special care should be spent with the multiply-accumulate unit that is the core of many digital signal processing systems. Therefore, its optimization may be of outmost importance, specially if a significative number of such units are present in the platform. In this paper, the minimization of the size of one such unit is provided for a specific application, although the results have relevance in other scenarios.","1051-8215;1558-2205","","10.1109/TCSVT.2005.858614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546012","Cumulants;higher order statistics;multiply-accumulate unit;optimization","Motion estimation;Field programmable gate arrays;Signal processing algorithms;Digital signal processing;Higher order statistics;Computational modeling;Software testing;Coprocessors;Kernel","motion estimation;higher order statistics;optimisation","accumulator size minimization;fast cumulant-based motion estimator;cumulants matching criteria;digital signal processing systems","","1","4","","","","","","IEEE","IEEE Journals & Magazines"
"A new single layer broadband CPW fed-printed monopole antenna for wireless applications","Y. Coulibaly; T. A. Denidni; L. Talbi; A. R. Sebak","Personal Communications Group, INRS-EMT, Montreal, Que., Canada; Personal Communications Group, INRS-EMT, Montreal, Que., Canada; NA; NA","Canadian Conference on Electrical and Computer Engineering 2004 (IEEE Cat. No.04CH37513)","","2004","3","","1541","1544 Vol.3","Broadband antennas have been increasingly investigated in recent years for wireless communication systems. Printed antennas fed by coplanar waveguide can provide a wide bandwidth and can be easily integrated with active circuits. In the same perspective, we proposed a new coplanar fed broadband antenna.. The investigated antenna uses printed monopole with two parasitic elements to widen the bandwidth. The design methodology is outlined. This antenna was numerically designed using Ansoft HFSS simulation software package. A parametric study using an electromagnetic simulator was performed. Based on the optimized design, an experimental prototype was fabricated and measured. Simulations and experimental measurements were carried out, and the comparison between them gives a good agreement. Using this approach, a fractional bandwidth of 47% at the center frequency of 2.35 GHz was achieved. This proposed antenna can be used for broadband wireless communications networks.","0840-7789","0-7803-8253","10.1109/CCECE.2004.1349700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1349700","","Broadband antennas;Coplanar waveguides;Bandwidth;Wireless communication;Circuit simulation;Antenna measurements;Electromagnetic measurements;Electromagnetic waveguides;Active circuits;Design methodology","coplanar waveguide components;antenna feeds;monopole antennas;broadband antennas;microstrip antennas;optimisation;radio equipment;electronic design automation;telecommunication computing;telecommunication equipment testing","single layer broadband CPW fed-printed monopole antenna;wireless applications;broadband antennas;coplanar waveguide fed broadband antenna;numerically designed antenna;parasitic elements;Ansoft HFSS simulation software package;parametric electromagnetic simulator;optimized design;experimental prototype;fractional bandwidth;center frequency;simulations;broadband wireless communications networks;2.35 GHz","","2","7","","","","","","IEEE","IEEE Conferences"
"Implementations and performance of nonlinear CG methods by TAO on Dawning2000+","Jian Wang; Xuebin Chi; Tongxiang Gu","Comput. Network Inf. Center, Chinese Acad. of Sci., Beijing, China; Comput. Network Inf. Center, Chinese Acad. of Sci., Beijing, China; NA","Proceedings. Seventh International Conference on High Performance Computing and Grid in Asia Pacific Region, 2004.","","2004","","","252","255","Nonlinear conjugate gradient methods (CG) are typical unconstrained optimization methods. As the optimization problems to be solved become larger, the dependence on efficient and scalable software is severe. Toolkit for Advanced Optimization (TAO) is a parallel package that can currently solve several kinds of optimization problems. In this paper, we give the framework of several variants of CG: CGFR, CGPR, CGPRP and their implementations in TAO 1.5, which have been tested up to 64 processors on Dawning2000 to solve problems with up to 10 variables. The results show that the scalability of CG implementations in TAO 1.5 is excellent.","","0-7695-2138","10.1109/HPCASIA.2004.1324042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1324042","","Character generation;Design optimization;Physics computing;Gradient methods;Computer architecture;Vectors;Computer networks;Laboratories;Mathematics;Research and development","mathematics computing;mathematical programming;conjugate gradient methods;software packages;parallel programming;object-oriented languages;software tools","nonlinear CG methods;Dawning2000+;nonlinear conjugate gradient methods;unconstrained optimization;Toolkit for Advanced Optimization;parallel package;CGFR;CGPR;CGPRP;TAO 1.5","","","12","","","","","","IEEE","IEEE Conferences"
"Dual purpose simulation: new data link test and comparison with VDL-2","D. C. Robinson","NASA Glenn Res. Center, Cleveland, OH, USA","Proceedings. The 21st Digital Avionics Systems Conference","","2002","1","","3C6","3C6","While the results of this paper are similar to those of previous research, in this paper the technical difficulties present previously are eliminated, producing better results, enabling one to more readily see the benefits of Prioritized CSMA (PCSMA). A new analysis section also helps to generalize this research so that it is not limited to exploration of the new concept of PCSMA. Commercially available network simulation software, OPNET version 7.0, simulations are presented involving an important application of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) over the Very High Frequency Data Link Mode 2 (VDL-2). Communication is modeled for essentially all incoming and outgoing nonstop air-traffic for just three United States cities: Cleveland, Cincinnati, and Detroit. Collision-less PCSMA is successfully tested and compared with the traditional CSMA typically associated with VDL-2. The performance measures include latency, throughput, and packet loss. As expected, PCSMA is much quicker and more efficient than traditional CSMA. These simulation results show the potency of PCSMA for implementing low latency, high throughput and efficient connectivity. We are also testing a new and better data link that could replace CSMA with relative ease.","","0-7803-7367","10.1109/DASC.2002.1067949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1067949","","Testing;Multiaccess communication;Delay;Throughput;Application software;Communication system control;Telecommunication control;Frequency;Cities and towns;Performance loss","carrier sense multiple access;aircraft communication;air traffic control;digital simulation;data communication;digital radio;telecommunication computing","prioritized CSMA;collision-less PCSMA;network simulation software;OPNET version 7.0;Aeronautical Telecommunications Network;ATN;controller-pilot data link communications;CPDLC;latency;throughput;packet loss;performance measures;retransmission analysis;very high frequency data link;VHF data link Mode 2;VDL-2;communication modelling;ATC ground stations;air traffic control","","","7","","","","","","IEEE","IEEE Conferences"
"ePASS-a software-based POWER simulator","A. Goldstein; S. Weiss","Dept. of Electr. Eng., Tel Aviv Univ., Israel; NA","Proceedings of the Seventh Israeli Conference on Computer Systems and Software Engineering","","1996","","","46","54","The latest RISC computer chips provide very high performance though complex pipelined implementations, using independent integer and floating point units, branch prediction mechanisms, multiple instruction dispatch per clock cycle and more. For a chosen hardware configuration to deliver optimized performance, various design aspects, resulting in many possible configurations, need to be thoroughly evaluated. Among these are such issues as synchronization between pipelines, cache organization and access policy, dynamic vs static branch prediction, detecting and handling dependency among instructions in different pipeline states. Addressing the above necessitates the use of simulators that provide the design environment in which to model and evaluate candidate configurations and then fine-tune the chosen implementation. The authors describe ePASS, an experimental software based simulator for the IBM RS/6000 and its POWER architecture. They discuss the three basic modules ePASS consists of, each representing a different view of the system: an Instruction Interpreter and Register Set Simulator, a Functional Simulator and a Memory Simulator. Finally they present a simple test case program to run on the simulator, involving branch prediction and execution as well as making use of the interlock mechanism to guarantee synchronization of the branch unit with the fixed point pipeline.","","0-8186-7536","10.1109/ICCSSE.1996.554848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=554848","","Pipelines;Synchronization;Computational modeling;Reduced instruction set computing;Computer aided instruction;High performance computing;Clocks;Hardware;Design optimization;Power system modeling","virtual machines","software-based POWER simulator;ePASS;RISC computer chips;pipelined implementations;independent integer units;independent floating point units;branch prediction mechanisms;multiple instruction dispatch;hardware configuration;optimized performance;design aspects;synchronization;cache organization;access policy;instruction dependency;IBM RS/6000;POWER architecture;Instruction Interpreter;Functional Simulator;Memory Simulator;interlock mechanism;fixed point pipeline","","","10","","","","","","IEEE","IEEE Conferences"
"Method of fire image identification based on optimization theory","L. Jiecheng; D. Ding; W. Longbiao; S. Weiguo","Dept. of Electronic Science and Technology, University of Science and Technology of China, Hefei 230026, P. R. China; Dept. of Electronic Science and Technology, University of Science and Technology of China, Hefei 230026, P. R. China; Dept. of Electronic Science and Technology, University of Science and Technology of China, Hefei 230026, P. R. China; Dept. of Electronic Science and Technology, University of Science and Technology of China, Hefei 230026, P. R. China","Journal of Systems Engineering and Electronics","","2002","13","2","78","83","In view of some distinctive characteristics of the early-stage flame image, a corresponding method. of characteristic extraction is presented. Also introduced is the application of the improved BP algorithm based on the optimization theory to identifying fire image characteristics. First the optimization of BP neural network adopting Levenberg-Marquardt algorithm with the property of quadratic convergence is discussed, and then a new system of fire image identification is devised. Plenty of experiments and field tests have proved that this system can detect the earlystage fire flame quickly and reliably.","1004-4132","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6077451","Fire flame;Characteristic extraction;Optimization theory;Levenberg-Marquardt algorithm","Fires;Optical switches;Cameras;Optimization;Optical imaging;Image edge detection;Logic gates","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"A methodology and design environment for DSP ASIC fixed point refinement","R. Cmar; L. Rijnders; P. Schaumont; S. Vernalde; I. Bolsens","IMEC, Leuven, Belgium; NA; NA; NA; NA","Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)","","1999","","","271","276","Complex signal processing algorithms are specified in floating point precision. When their hardware implementation requires fixed point precision, type refinement is needed. The paper presents a methodology and design environment for this quantization process. The method uses independent strategies for fixing MSB and LSB weights of fixed point signals. It enables short design cycles by combining the strengths of both analytical and simulation based methods.","","0-7695-0078","10.1109/DATE.1999.761133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=761133","","Design methodology;Digital signal processing;Application specific integrated circuits;Quantization;Signal processing algorithms;Analytical models;Electrical capacitance tomography;Signal processing;Modems;Monitoring","fixed point arithmetic;floating point arithmetic;digital signal processing chips;application specific integrated circuits;quantisation (signal);error statistics;object-oriented methods;hardware description languages;integrated circuit design;circuit optimisation;hardware-software codesign","DSP ASIC;fixed point refinement;design environment;complex signal processing algorithms;floating point precision;hardware implementation;type refinement;quantization process;independent strategies;LSB weights;MSB weights;short design cycles;fast convergence;simulation statistics;C++ object oriented description;VHDL;error statistics","","38","5","","","","","","IEEE","IEEE Conferences"
"UW-ISL document image analysis toolbox: an experimental environment","J. Liang; R. Rogers; R. M. Haralick; I. T. Phillips","Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; NA; NA; NA","Proceedings of the Fourth International Conference on Document Analysis and Recognition","","1997","2","","984","988 vol.2","A document image analysis toolbox including a collection of data structures and algorithms to support a variety of applications, is described in this paper. An experimental environment is built to allow developers to develop, test and optimize their algorithms and systems. Appropriate and quantitative performance metrics for each kind of information a document analysis technique infers have been developed. The performance of each algorithm has been evaluated based on these metrics and the UW-III document image database which contains a total of 1600 English document images randomly selected from scientific and technical journals.","","0-8186-7898","10.1109/ICDAR.1997.620657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620657","","Text analysis;Image analysis;Image sequence analysis;System testing;Performance analysis;Information analysis;Algorithm design and analysis;Image databases;Measurement;Data structures","document image processing;data structures;software performance evaluation;software metrics;visual databases;image segmentation","UW-ISL;document image analysis toolbox;experimental environment;data structures;testing;optimization;performance metrics;document image database;English document images;scientific journals;technical journals;image segmentation","","4","10","","","","","","IEEE","IEEE Conferences"
"Design method and automation of comparator generation for flash A/D converter","Daegyu Lee; Jincheol Yoo; Kyusun Choi","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","Proceedings International Symposium on Quality Electronic Design","","2002","","","138","142","The design methods and the automation of the comparator circuit layout generation for a flash A/D converter are presented in this paper. The threshold inverter quantization (TIQ) based A/D converters require 2/sup n/ - 1 comparators, each one different from all others. Optimal design method of the TIQ comparator presented in this paper significantly improves the linearity of the A/D converter against the CMOS process variation. Especially the DNL dependence on the CMOS process variation can be almost eliminated. The design method has been incorporated into a software package and the 2/sup n/ - 1 optimized TIQ comparator layouts are generated as an output of the software package. The simulation results are presented to show the effectiveness of the design methods. Also, the prototype chip has been fabricated, with initial test results confirming the DNL reduction.","","0-7695-1561","10.1109/ISQED.2002.996716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996716","","Design methodology;Design automation;CMOS process;Software packages;Circuits;Inverters;Quantization;Linearity;Design optimization;Software prototyping","integrated circuit design;comparators (circuits);analogue-digital conversion;circuit layout CAD;integrated circuit layout;CMOS integrated circuits","design method;comparator generation;flash A/D converter;circuit layout generation;threshold inverter quantization;TIQ;linearity;CMOS process variation;DNL dependence;software package","","7","14","","","","","","IEEE","IEEE Conferences"
"Visual comparison of JPEG 2000 versus conventional JPEG","T. T. Chinen","Fujifilm Software, San Jose, CA, USA","Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429)","","2003","2","","II","283","One difficulty in image compression research is designing meaningful performance metrics. Purely numerical measures such as PSNR are unsatisfactory because they do not correlate well with human assessment. We introduce a method of subjective image evaluation for image compression called calibrated rank ordering (CRO). CRO is attractive because it produces substantial numerical results without excessive burden on the observers. Using CRO we compare traditional JPEG with JPEG 2000 in a variety of modes. We also consider the effect of differing images sources, i.e., digital still camera vs. film scan. Finally, we compare and contrast the artifacts of both JPEG and JPEG 2000.","1522-4880","0-7803-7750","10.1109/ICIP.2003.1246672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246672","","Transform coding;Image coding;Testing;Bit rate;PSNR;Distortion measurement;Quantization;Digital cameras;Software performance;Humans","image coding;cameras;data compression","Joint Photographic Experts Group;JPEG 2000;image compression;peak signal-to-noise ratio;PSNR;calibrated rank ordering;images source;digital still camera;film scan","","1","7","","","","","","IEEE","IEEE Conferences"
"Development of a user-friendly, open-system software for the design and evaluation of protective relaying applications","M. Khederzadeh; H. Ansari-Shahrezai","NA; NA","2004 Eighth IEE International Conference on Developments in Power System Protection","","2004","1","","232","235 Vol.1","In this paper, a novel modeling and simulation environment is developed that could serve as an aid to relay designers and manufacturers to evaluate their protective algorithms and hardware structures. Both relay performance and power system behavior are simulated in one unique environment. Each relay has all the common protective devices. Any protective device can be enable/disabled by the user. All the protective devices are collected and optimized in one unique S-function that emulates the actual relay. The relay operates in a dynamic manner and its status is visible on the screen. Advanced protective algorithms like thermal overload could be best tested and debugged by the developed environment. It allows evaluation of both individual protective devices as well as the interactions among them under a variety of power system operating conditions. The sampling interval is adaptively adjusted by the system frequency.","0537-9989","0-86341-385","10.1049/cp:20040106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1364849","","","relay protection;software engineering;digital simulation;power system faults;power system protection;CAD","open-system software;protective relay design;protective relay evaluation;protective algorithms;relay hardware structures;power system behavior;S-function;power system faults;power system protection","","","","","","","","","IET","IET Conferences"
"A workbench for computer architects","C. L. Mitchell; M. J. Flynn","Comput. Syst. Lab., Stanford Univ., CA, USA; NA","IEEE Design & Test of Computers","","1988","5","1","19","29","The authors present a high-level simulator that supports a top-down architectural analysis of embedded, custom applications. This tool characterizes more than 50 instruction-set variants and allows data such as instruction cached performance, data cache performance, register set size, and register allocation policy to be evaluated for all the architectures simultaneously. Designers also have more flexibility because they can trade off among high-level design constructs. Thus, they can evaluate relative performance before having to complete the machine specification at a lower level.<<ETX>>","0740-7475;1558-1918","","10.1109/54.668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668","","Computer architecture;Hardware;Costs;Software tools;Registers;Design engineering;Computational modeling;Analytical models;Application software;Design optimization","buffer storage;CAD;computer architecture;performance evaluation;storage allocation;virtual machines","computer architecture simulation;workbench;high-level simulator;top-down architectural analysis;instruction-set variants;instruction cached performance;data cache performance;register set size;register allocation policy;high-level design constructs;relative performance;machine specification","","17","11","","","","","","IEEE","IEEE Journals & Magazines"
"Application of artificial neural network in noise mixed partial discharge recognition","Zhong Zheng; Kexiong Tan","Dept. of Electr. Eng., Tsinghua Univ., Beijing, China; NA","Canadian Conference on Electrical and Computer Engineering 2001. Conference Proceedings (Cat. No.01TH8555)","","2001","1","","673","677 vol.1","To test partial discharge (PD) recognition ability under different noise conditions, systemic research is carried out. In a noise-screened high voltage lab and using a high speed, wide-band digital measuring system, different kinds of PD current waveforms are recorded. Noises of different types are investigated. Then the PD signals are immersed into different noises with certain signal-noise ratios (SNR). By applying the segmented time domain data compression method, the features vectors of mixed waveforms are extracted. Employing a backpropagation algorithm, a feedforward triple-layered artificial neural network (ANN) program is generated and optimized. The mixed waveforms are tested and influence of each noise types in different SNR conditions are studied.","0840-7789","0-7803-6715","10.1109/CCECE.2001.933765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=933765","","Artificial neural networks;Partial discharges;Signal to noise ratio;System testing;Voltage;Wideband;Current measurement;Partial discharge measurement;Velocity measurement;Noise measurement","insulation testing;partial discharge measurement;automatic test software;feedforward neural nets;multilayer perceptrons;data compression;noise;backpropagation","noise mixed partial discharge recognition;artificial neural network;PD recognition ability;wide-band digital measuring system;PD current waveforms;signal-noise ratios;segmented time domain data compression method;features vectors;backpropagation algorithm;feedforward triple-layered artificial neural net","","","4","","","","","","IEEE","IEEE Conferences"
"Mod-p decision diagrams: a data structure for multiple-valued functions","H. Sack; E. Dubrova; C. Meinel","Trier Univ., Germany; NA; NA","Proceedings 30th IEEE International Symposium on Multiple-Valued Logic (ISMVL 2000)","","2000","","","233","238","Multiple-valued decision diagrams (MDDs) give a way of approaching problems by using symbolic variables which are often more naturally associated with the problem statement than the variables obtained by a binary encoding. We present a more general class of MDDs, containing not only branching nodes but also functional nodes, labeled by addition modulo p operation, p-prime, and give algorithms for their manipulation Such decision diagrams have a potential of being more space-efficient than MDDs, However they are not a canonical representation of multiple-valued functions and thus the equivalence test of two Mod-p-DDs is more difficult then the test of two MDDs. To overcome this problem, we design a fast probabilistic equivalence test for Mod-p-DDs that requires time linear in the number of nodes.","0195-623X","0-7695-0692","10.1109/ISMVL.2000.848625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848625","","Data structures;Boolean functions;Encoding;Logic testing;Logic design;Design optimization;Formal verification;Application software;Computer aided software engineering","decision diagrams;multivalued logic;data structures","decision diagrams;data structure;multiple-valued functions;equivalence test;probabilistic equivalence test","","3","15","","","","","","IEEE","IEEE Conferences"
"A heuristics approach to automatic data flow diagram layout","K. P. Tan; G. H. Ong; P. Wong","Dept. of Inf. Syst. & Comput. Sci., Nat. Univ. of Singapore, Singapore; NA; NA","Proceedings of 6th International Workshop on Computer-Aided Software Engineering","","1993","","","314","323","A heuristics method for the automatic placement of data flow diagrams (DFDs) is presented. It fulfills the whole set of aesthetics requirements for a nice DFD layout within an acceptable time bound. The system allocates the process with most data flows at the center of a 2-D grid space and effectively handles the positional preference for entities, processes, and data stores. The test cut function detects any crisscrossing of data flows and any cutting of DFD objects. The pop function pops up to those objects of high intersection weight to the upper layer under aesthetics optimization. This 3-D DFD contains only linear data flows with all crisscrossings eliminated. A noncutting connection between two objects can be seen clearly by rotating the 3-D DFD at every 90/spl deg/ step. Thus, the heuristics method realizes the abstraction of automatic diagram creation.","1066-1387","0-8186-3480","10.1109/CASE.1993.634835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=634835","","Design for disassembly;Computer aided software engineering;Very large scale integration;Integrated circuit layout;Algorithm design and analysis;Printed circuits;Manufacturing;System testing","computer aided software engineering","automatic data flow diagram layout;heuristics method;aesthetics requirements;positional preference;entities;processes;data stores;test cut function;pop function;intersection weight;aesthetics optimization;automatic diagram creation","","","12","","","","","","IEEE","IEEE Conferences"
"A benchmark tutorial","W. J. Price","Motorola Inc., Tempe, AZ, USA","IEEE Micro","","1989","9","5","28","43","The author describes benchmarking and discusses some of the specific benchmark tests for computer systems that are in use today. He examines some of the pitfalls involved with benchmark comparison and analysis and points out how to avoid, or at least minimize the impact of, such problems. The goal is to learn how to gather and interpret meaningful comparison data. Benchmark test results are tabulated for the Dhrystone 1.1, Digital Review, Dodec, Khornerstone, Linpack, Livermore Fortran Kernel, SPICE, Stanford, and single-precision Whetstone benchmarks.<<ETX>>","0272-1732;1937-4143","","10.1109/40.45825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=45825","","Tutorials;Benchmark testing;System testing;Optimizing compilers;System performance;Software testing","computer testing;performance evaluation","benchmarking;benchmark tests;computer systems;Dhrystone 1.1;Digital Review;Dodec;Khornerstone;Linpack;Livermore Fortran Kernel;SPICE;Stanford;single-precision Whetstone benchmarks","","20","","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing NANOS OpenMP for the IBM Cyclops multithreaded architecture","D. Rodenas; X. Martorell; G. Almasi; C. Cascaval","CEPBA, IBM Res. Inst., Barcelona, Spain; CEPBA, IBM Res. Inst., Barcelona, Spain; CEPBA, IBM Res. Inst., Barcelona, Spain; CEPBA, IBM Res. Inst., Barcelona, Spain","19th IEEE International Parallel and Distributed Processing Symposium","","2005","","","9 pp.","","In this paper, we present two approaches to improve the execution of OpenMP applications on the IBM Cyclops multithreaded architecture. Both solutions are independent and they are focused to obtain better performance through a better management of the cache locality. The first solution is based on software modifications to the OpenMP runtime library to balance stack accesses across all data caches. The second solution is a small hardware modification to change the data cache mapping behavior, with the same goal. Both solutions help parallel applications to improve scalability and obtain better performance in this kind of architectures. In fact, they could also be applied to future multi-core processors. We have executed (using simulation) some of the NAS benchmarks to prove these proposals. They show how, with small changes in both the software and the hardware, we achieve very good scalability in parallel applications. Our results also show that standard execution environments oriented to multiprocessor architectures can be easily adapted to exploit multithreaded processors.","1530-2075","0-7695-2312","10.1109/IPDPS.2005.317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419937","","Yarn;Computer architecture;Application software;Hardware;Runtime library;Scalability;Random access memory;Proposals;Resource management;Pipelines","multi-threading;open systems;cache storage;software libraries;benchmark testing;parallel architectures;performance evaluation","NANOS OpenMP;IBM Cyclops multithreaded architecture;software modification;OpenMP runtime library;data caches;multicore processors;multiprocessor architectures","","","26","","","","","","IEEE","IEEE Conferences"
"Repeatable quality assurance techniques for requirements negotiations","P. Grunbacher; M. Halling; S. Biffl; H. Kitapci; B. W. Boehm","Syst. Eng. & Autom., Johannes Kepler Univ., Linz, Austria; Syst. Eng. & Autom., Johannes Kepler Univ., Linz, Austria; NA; NA; NA","36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the","","2003","","","9 pp.","","Many software projects fail because early life-cycle defects such as ill-defined requirements are not identified and removed. Therefore, quality assurance (QA) techniques for defect detection and prevention play an important role. The effectiveness and efficiency of QA approaches has been empirically evaluated. In this paper we discuss QA techniques optimized for requirements negotiations. In particular, we focus on negotiations using the EasyWinWin approach. We present (1) repeatable techniques for checking quality throughout negotiations as well as (2) role-oriented inspection techniques helping a project team to reduce unnecessary complexity and to mitigate risks stemming from defects in requirements negotiation results. We present the results of a thorough feasibility study we conducted to test our approach.","","0-7695-1874","10.1109/HICSS.2003.1173672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173672","","Quality assurance;Collaborative work;Software quality;Testing;Collaborative software;Project management;Systems engineering and theory;Automation;Software engineering;Inspection","quality assurance;group decision support systems;software management;formal specification;systems analysis;risk management","repeatable quality assurance techniques;requirements negotiations;software projects;life-cycle defects;ill-defined requirements;defect detection;defect prevention;QA techniques optimization;EasyWinWin approach;quality checking;role-oriented inspection techniques;complexity reduction;risk mitigation;GSS support","","2","26","","","","","","IEEE","IEEE Conferences"
"Utilizing output signatures to enhance semantic matching","C. W. Leigeber; T. E. Doom","Electr. Eng., Dayton Univ., OH, USA; NA","Proceedings of the 44th IEEE 2001 Midwest Symposium on Circuits and Systems. MWSCAS 2001 (Cat. No.01CH37257)","","2001","2","","686","689 vol.2","The importance of functional logic verification has grown considerably and spans many fields of interest, such as design verification, reengineering, and technology mapping. We present an iterative algorithm that efficiently creates and utilizes function signatures to identify functional correspondence, thus reducing the complexity of determining a semantic matching between a library circuit and a circuit under test. Previous approaches to this problem have been unable to limit certain types of correspondence between symmetric functions. The reduction of extraneous correspondences is crucial, as the verification of each match is computationally expensive. By utilizing output signatures, we will demonstrate an algorithm that is effective at handling many cases of circuit symmetry.","","0-7803-7150","10.1109/MWSCAS.2001.986281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=986281","","Circuit testing;Boolean functions;Logic design;Design engineering;Circuit synthesis;Input variables;Iterative algorithms;Libraries;Computational modeling;Circuit simulation","logic simulation;formal verification;iterative methods;software libraries;Boolean functions;circuit optimisation","output signatures;semantic matching;functional logic verification;design verification;technology mapping;iterative algorithm;function signatures;functional correspondence;library circuit;circuit under test;correspondence;symmetric functions;extraneous correspondences;circuit symmetry","","","8","","","","","","IEEE","IEEE Conferences"
"Fuzzy Logic-based Recognition of Gait Changes due to Trip-related Falls","R. Hassan; R. Begg; S. Taylor","Dept. of Computer Science &amp; Software Engineering, The University of Melbourne, Carlton, Vic 3010, Australia.; NA; NA","2005 IEEE Engineering in Medicine and Biology 27th Annual Conference","","2005","","","4970","4973","The main aim of this paper is to explore application of fuzzy rules for automated recognition of gait changes due to falling behaviour. Minimum foot clearance (MFC) during continuous walking on a treadmill was recorded on 10 healthy elderly and 10 elderly with reported balance problem and tripping falls. MFC histogram characteristic features were used as inputs to the set of fuzzy rules; the features were extracted based on estimating the clusters in the data. Each of the clusters found corresponded to a new fuzzy rule, which were then applied to associate the input space to an output region. Gradient descent method was used to optimise the rule parameters. Both cross-validation and Jack-knife (leave-one-out) techniques were utilized for training the models and subsequently, testing the performance of the optimized fuzzy model. Receiver operating characteristics (ROC) plots, as well as accuracy rates were used to evaluate the performance of the developed model. Test results indicated up to a maximum of 95% accuracy in discriminating the healthy and balance-impaired gait patterns. These results suggest good potentials for fuzzy logic to use as gait diagnostics","1094-687X;1558-4615","0-7803-8741","10.1109/IEMBS.2005.1615590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615590","","Fuzzy logic;Senior citizens;Testing;Foot;Legged locomotion;Histograms;Fuzzy sets;Feature extraction;Data mining;Optimization methods","feature extraction;fuzzy logic;gait analysis;geriatrics;learning (artificial intelligence);medical image processing;pattern clustering","fuzzy logic;gait changes;trip-related falls;fuzzy rules;automated recognition;falling behaviour;minimum foot clearance;continuous walking;treadmill;balance problem;tripping falls;feature extraction;cluster estimation;gradient descent method;cross-validation technique;Jack-knife technique;training;optimized fuzzy model;receiver operating characteristics;healthy gait patterns;balance-impaired gait patterns;gait diagnostics","","7","11","","","","","","IEEE","IEEE Conferences"
"Architectural concepts for a system simulator for concurrent prototyping of equipment and controls","K. P. White; R. Fritz; S. Horvath; C. Orellana; J. Wohlers; R. G. Fairbrother; W. S. Terry","Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA; Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA; Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA; Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA; Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA; NA; NA","Proceedings of the Winter Simulation Conference","","2002","2","","1117","1122 vol.2","AutoMod/sup /spl reg// is a leading discrete-event simulation package widely applied in the modeling and analysis of distribution systems. Included in the AutoMod software suite is the Model Communications Module (MCM), which allows an executing simulation to open socket connections and to send and receive messages via TCP/IP network protocol. In this paper we report on a pilot study which explores the functionality of the MCM. In particular, we develop and implement an architecture that can be used to design, test, verify, and optimize control system software interacting with a discrete-event simulation of the system to be controlled. This architecture supports concurrent engineering of controls and hardware prototypes. Application of this architecture can significantly reduce the duration and cost of development cycles for new equipment and systems. In addition, this architecture can be applied to investigate the feasibility of implementing engineering changes in systems currently deployed.","","0-7803-7614","10.1109/WSC.2002.1166365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166365","","Virtual prototyping;Computer architecture;Software prototyping;Discrete event simulation;Control system synthesis;Communication system control;Packaging machines;Sockets;TCPIP;IP networks","discrete event simulation;software architecture;transport protocols;control system CAD;concurrent engineering;software packages;postal services","AutoMod;discrete-event simulation package;distribution system analysis;modeling;Model Communications Module;socket connections;TCP/IP network protocol;architecture;control system design;control system testing;verification;optimization;control system software;concurrent engineering","","","7","","","","","","IEEE","IEEE Conferences"
"Development of a high performance TSPC library for implementation of large digital building blocks","B. Le Chapelain; A. Mechain; Y. Savaria; G. Bois","Dept. of Electr. & Comput. Eng., Ecole Polytech., Montreal, Que., Canada; NA; NA; NA","ISCAS'99. Proceedings of the 1999 IEEE International Symposium on Circuits and Systems VLSI (Cat. No.99CH36349)","","1999","1","","443","446 vol.1","In this paper, we present a heuristic study of split-output latches built according to the true single-phase clocked circuits (TSPC) design style. Previous studies have shown that TSPC circuits can operate at very high frequencies. Our goal is to implement large digital building blocks operating at 1 GHz or more (in 0.35 ""m CMOS technology). Our methodology started with the development of a first set of cells based on ad-hoc design. A fast semi automatic method was then developed to reduce the number of simulations. Based on the experience acquired with TSPC optimization, we proposed an automatic algorithm to optimize TSPC cells and a method to realize libraries of logical standard cells. This paper describes the different steps followed to elaborate the TSPC library. The method and the cell library are validated by characterizing small digital building blocks.","","0-7803-5471","10.1109/ISCAS.1999.777908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777908","","Software libraries;Circuit testing;CMOS technology;Inverters;Frequency;Latches;Clocks;Optimization methods;VHF circuits;Circuit synthesis","CMOS logic circuits;cellular arrays;logic CAD;clocks;flip-flops;software libraries","high performance TSPC library;digital building blocks;heuristic study;split-output latches;true single-phase clocked circuits;ad-hoc design;semi automatic method;logical standard cells;CMOS logic;1 GHz;0.35 micron","","1","7","","","","","","IEEE","IEEE Conferences"
"The modified Alopex algorithm and its circuit architecture","Zhang Hang; Peng Yizhong; Song Junde","Beijing Univ. of Posts & Telecommun., China; Beijing Univ. of Posts & Telecommun., China; Beijing Univ. of Posts & Telecommun., China","[Proceedings] Singapore ICCS/ISITA `92","","1992","","","1093","1095 vol.3","A modified algorithm of pattern extraction is presented and its circuit architecture described. The algorithm has been implemented and tested by a C language program on a SUN4/330 Workstation. The software package is applied to placement in VLSI design and gate arrangement in gate matrix layout.<<ETX>>","","0-7803-0803","10.1109/ICCS.1992.255093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=255093","","Circuits;Stochastic processes;Cost function;Equations;Computer architecture;Workstations;Software packages;Artificial neural networks;Design optimization;Algorithm design and analysis","C language;circuit layout CAD;pattern recognition;software packages;VLSI","modified Alopex algorithm;algorithm of pattern extraction;circuit architecture;software package;placement;VLSI design;gate arrangement","","","3","","","","","","IEEE","IEEE Conferences"
"Simulation Optimization: Methods And Applications","Y. Carson; A. Maria","State University of New York at Binghamton; NA","Winter Simulation Conference Proceedings,","","1997","","","118","126","","","0-7803-4278","10.1109/WSC.1997.640387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=640387","","Optimization methods;Computational modeling;Input variables;Traffic control;Application software;Mathematical model;Testing;Design optimization;Industrial engineering;Software tools","","","","36","65","","","","","","IEEE","IEEE Conferences"
"Cathodoluminescent white phosphors optimization for lighting applications","N. N. Chubun; A. G. Chakhovskoi; C. E. Hunt","Electr. & Comput. Eng., California Univ., Davis, CA, USA; Electr. & Comput. Eng., California Univ., Davis, CA, USA; Electr. & Comput. Eng., California Univ., Davis, CA, USA","Technical Digest of the 17th International Vacuum Nanoelectronics Conference (IEEE Cat. No.04TH8737)","","2004","","","36","39","In recent years, a significant progress has been reported in the area of field emission cathodes and cathode materials for cathodoluminescent vacuum light sources. In this research, we investigated mixes of individual industrially available red, green and blue phosphors in an attempt to create a thermally stable and energy efficient phosphor mixes capable of operating for prolonged time in the conditions of the vacuum field emission light sources. Two mixtures of NICHIA made phosphors, referred to as M1 and M2 have been studied. The M1 mixture uses high efficient P-22 type green phosphor while M2 mixture uses the wide spectrum green phosphor Y/sub 2/SiO/sub 5/:Tb. As a result of our investigation, we were able to identify phosphor powder mixes capable of emitting white light under continuous dc electron beam excitation at accelerating energies of up to 8 kW. The prepared mixes exhibit stable light emission and color characteristics while maintaining the efficiency up to 40 Lm/Wt.","","0-7803-8397","10.1109/IVNC.2004.1354886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1354886","","Phosphors;Light sources;Testing;Cathode ray tubes;TV;Temperature sensors;Application software;Electrons;Glass;Design engineering","yttrium compounds;zinc compounds;II-VI semiconductors;phosphors;cathodoluminescence;light sources;field emission;thermal stability;colour","white phosphors optimization;lighting applications;red phosphor;green phosphor;blue phosphor;thermally stable phosphor mixes;energy efficient phosphor mixes;vacuum field emission light sources;P-22 type green phosphor;dc electron beam excitation;accelerating energies;stable light emission;color characteristics;cathodoluminescent vacuum light sources;ZnS:Au,Cu,Al;Y/sub 2/SiO/sub 5/:Tb","","1","1","","","","","","IEEE","IEEE Conferences"
"A loop transformation for maximizing parallelism from single loops with nonuniform dependencies","Chul-Kwon Cho; Jae-Chan Shim; Mann-Ho Lee","ADD, Taejon, South Korea; NA; NA","Proceedings High Performance Computing on the Information Superhighway. HPC Asia '97","","1997","","","696","699","This paper describes several loop splitting methods for exploiting parallelism from single loops, and also proposes a generalized and optimal loop transformation technique for exploiting parallelism from single loops with nonuniform dependencies. The proposed algorithm is based on partitioning a serial loop by using the size of dependence distance such that it varies between different instances of the dependence. It outperforms the two methods proposed by C.D. Polychronopoulos (see Compiler optimizations for enhancing parallelism and their impact on architecture design, IEEE Trans. Comput., vol.37, no.8, p. 991-1004, 1988).","","0-8186-7901","10.1109/HPC.1997.592234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=592234","","Parallel processing;Testing;Equations;Computer architecture;Computer science;Partitioning algorithms;Data mining;Algorithm design and analysis","parallelising compilers;optimising compilers;program control structures;parallel programming;software performance evaluation","loop transformation;parallelism;single loops;nonuniform dependencies;loop splitting methods;loop transformation technique;dependence distance;compiler optimizations;architecture design","","1","9","","","","","","IEEE","IEEE Conferences"
"Optimising Pseudoknot in /spl Gamma/CMC","G. G. da Cruz Neto; R. M. F. Lima; R. D. Lins; A. L. M. Santos","Dept. de Inf., Univ. Federal de Pernambuco, Recife, Brazil; NA; NA; NA","Proceedings of EUROMICRO 96. 22nd Euromicro Conference. Beyond 2000: Hardware and Software Design Strategies","","1996","","","120","126","Benchmarking implementations is fundamental to allow analysing performance amongst different platforms. The choice of a benchmark that makes possible a reliable and fair comparison of a particular aspect is a difficult task, however. The Pseudoknot benchmark is a floating-point intensive application taken from molecular biology which was used to compare the compile-time and execution-time performance of over 25 different implementations of functional languages. Amongst those implementations was /spl Gamma/CMC, an abstract machine for efficient implementation of lazy functional languages. /spl Gamma/CMC pioneered the transference of the control of the execution flow to C, as much as possible, to take advantage of the extremely low cost of procedure calls in modern RISC architectures. /spl Gamma/CMC was amongst the machines that presented good Pseudoknot figures, although it did not use some of the sophisticated optimisations of most of the other implementations. The experience of implementing Pseudoknot in /spl Gamma/CMC was most valuable in providing insights for new ways in optimising it. This paper describes several optimisations introduced to /spl Gamma/CMC which bring a better Pseudoknot performance.","1089-6503","0-8186-7487","10.1109/EURMIC.1996.546373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=546373","","Functional programming;Performance analysis;Costs;Reduced instruction set computing;Benchmark testing;Biology computing;Collaboration;Arithmetic;Optimizing compilers","functional languages","/spl Gamma/CMC;Pseudoknot optimisation;benchmarking implementations;performance analysis;floating-point intensive application;molecular biology;functional languages;abstract machine;procedure calls;RISC architectures","","","12","","","","","","IEEE","IEEE Conferences"
"An optimal PCM codec soft IP generator and its application","Gwo-Yang Wu; Liang-Bi Chen; Yuan-Long Jeang; Gwo-Jia Jong","Graduate Inst. of Electron. & Inf. Eng., Nat. Kaohsiung Univ. of Appl. Sci., Taiwan; Graduate Inst. of Electron. & Inf. Eng., Nat. Kaohsiung Univ. of Appl. Sci., Taiwan; Graduate Inst. of Electron. & Inf. Eng., Nat. Kaohsiung Univ. of Appl. Sci., Taiwan; Graduate Inst. of Electron. & Inf. Eng., Nat. Kaohsiung Univ. of Appl. Sci., Taiwan","2002 IEEE International Conference on Field-Programmable Technology, 2002. (FPT). Proceedings.","","2002","","","315","317","In this paper, we propose a soft IP generator which can add or remove PCM codec modules arbitrarily. It can be applied to PCM codec IP designs that need to change their related modules, corresponding to different working environments. It also can help us to easily manage our soft IP modules, produce optimized modules, and remove unnecessary modules, in order to reduce the implementation cost. In addition, users can implement their own Verilog HDL code of PCM codec by following our predefined interface specification, and integrate it with our optimal PCM codec module to produce the users' own optimized system.","","0-7803-7574","10.1109/FPT.2002.1188700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1188700","","Phase change materials;Codecs;Hardware design languages;Design automation;Encoding;Automatic testing;Integrated circuit modeling;Cost function;Error analysis;Software testing","codecs;VLSI;integrated circuit design;circuit CAD;logic CAD;circuit optimisation;pulse code modulation;industrial property;hardware description languages","VLSI CAD tools;PCM codec optimization;soft IP generator;PCM codec modules;implementation cost reduction;Verilog HDL code;interface specification","","2","14","","","","","","IEEE","IEEE Conferences"
"Scheduling in the presence of conditional constructs and optimisation of control structure","J. Ainscough; D. M. Southall; C. Thompson","Dept. of Electr. & Electron. Eng., Manchester Metropolitan Univ., UK; Dept. of Electr. & Electron. Eng., Manchester Metropolitan Univ., UK; Dept. of Electr. & Electron. Eng., Manchester Metropolitan Univ., UK","Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit","","1994","","","58","62","Traditionally scheduling of the individual operations is restricted to their respective sequential blocks. This paper demonstrates that it is possible to extend the work so those complex structures containing a number of distinct control structures can be scheduled as a single entity, thus exploiting the parallelism that exists across and within the individual structures. However, to achieve an optimal implementation the analysis of the structures is significantly more complex, and consequentially computationally more expensive. The issues surrounding this are discussed.<<ETX>>","","0-7803-2020","10.1109/ASIC.1994.404609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=404609","","Processor scheduling;Testing;Parallel processing;Computer aided software engineering;Control system synthesis;Current control","logic CAD;circuit CAD;integrated circuit design;scheduling","conditional constructs;control structure optimisation;scheduling","","","8","","","","","","IEEE","IEEE Conferences"
"New aspects of simulation in hot-embossing","M. Worgull; M. Heckele","Inst. fur Mikrostrukturtech., Forschungszentrum Karlsruhe AC, Germany; Inst. fur Mikrostrukturtech., Forschungszentrum Karlsruhe AC, Germany","Symposium on Design, Test, Integration and Packaging of MEMS/MOEMS 2003.","","2003","","","272","274","Hot embossing is especially well suited for manufacturing small and medium volume series. However a wider diffusion of this process is currently seriously hampered by the lack of adequate simulation tools for process optimization and part design. This lack of simulation tools is becoming critical as the dimensions of the microstructures continuously shrink from micron and sub-micron towards nano-scales and as productivity requirements dictate the enlargement of formats to process larger numbers of devices in parallel. Having no macroscopic equivalent the micro hot embossing process cannot be described by simple down scaling of existing software tools like in injection molding. In this paper a first access is described how numerical simulation also can be applied to the hot embossing process.","","0-7803-7066","10.1109/DTIP.2003.1287051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287051","","Embossing;Polymers;Injection molding;Filling;Temperature;Compression molding;Software tools;Application software;Softening;Computational modeling","polymer melts;micromachining;injection moulding;embossing;micromechanical devices","hot-embossing;microstructure;micro hot embossing process;injection molding;numerical simulation","","1","1","","","","","","IEEE","IEEE Conferences"
"Testing and optimizing ADC performance: a probabilistic approach","N. Giaquinto; M. Savino; A. Trotta","NA; NA; NA","Proceedings of 1995 IEEE Instrumentation and Measurement Technology Conference - IMTC '95","","1995","","","650","","","","0-7803-2615","10.1109/IMTC.1995.515399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=515399","","Histograms;System testing;Legged locomotion;Analog-digital conversion;Computer simulation;Software measurement;Employment;Computer errors;Error correction;Software algorithms","","","","2","11","","","","","","IEEE","IEEE Conferences"
"The optimization of poly-Si cells","S. S. Kim; D. G. Lim; H. W. Kim; J. Yi","Dept. of Electr. Eng., Kyun Kwan Univ., Kyunggi, South Korea; NA; NA; NA","Proceedings of 5th International Conference on Properties and Applications of Dielectric Materials","","1997","2","","642","645 vol.2","We investigated the optimization of poly-Si solar cell characteristics for various factors. The influencing factors were investigated in terms of emitter thickness, thermal treatment, surface treatment, grid design, and contact metal. First, we carried out solar cell simulation using a PC1D software package. Then poly-Si solar cells were fabricated and characterized in terms of structural, electrical, and optical properties. Pretreatment temperature of 900/spl deg/C, chemical surface polishing of poly-Si substrate, emitter thickness of 0.43 /spl mu/m, top Yb metal, and grid finger shading area of 7% gave an improved conversion efficiency of poly-Si solar cell.","","0-7803-2651","10.1109/ICPADM.1997.616517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=616517","","Photovoltaic cells;Doping;Conductivity;Gettering;Passivation;Charge carrier lifetime;Surface treatment;Chemicals;Costs;Grain boundaries","elemental semiconductors;silicon;solar cells;semiconductor device testing;digital simulation;polishing","polysilicon solar cells;emitter thickness;thermal treatment;surface treatment;grid design;solar cell simulation;PC1D software package;pretreatment temperature;chemical surface polishing;grid finger shading area;conversion efficiency;900 degC;0.43 micron;Si","","","6","","","","","","IEEE","IEEE Conferences"
"Analytic example of a Schweppe likelihood-ratio detector","T. H. Kerr","MIT Lincoln Lab., Lexington, MA, USA","IEEE Transactions on Aerospace and Electronic Systems","","1989","25","4","545","558","A low-dimensional test problem with a known solution is used to verify various computer implementations of F.C. Schweppe's likelihood detector (1965). In this case a closed-form solution is provided for a Schweppe likelihood detector in terms of an intermediate Kalman filter, as utilized in its implementation, for detecting the presence of a two-state signal model in Gaussian white noise. The associated error probabilities are also evaluated following a procedure that utilizes optimized Chernoff-like bounds for a tight approximation. A methodology is demonstrated for appropriately setting the decision threshold for this example as a tradeoff against allowable observation time. By using this or similar examples, certain qualitative and quantitative aspects of the software implementation can be checked for conformance to anticipated behavior as an intermediate benchmark, prior to modular replacement of the various high-order matrices appropriate to the particular application.<<ETX>>","0018-9251;1557-9603;2371-9877","","10.1109/7.32087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=32087","","Software performance;Application software;Software testing;Detectors;Probability;Radar detection;Software debugging;Algorithms;Riccati equations;Laboratories","computerised signal processing;error statistics;Kalman filters;signal detection;telecommunications computing;white noise","likelihood-ratio detector;low-dimensional test;closed-form solution;intermediate Kalman filter;two-state signal model;Gaussian white noise;error probabilities;optimized Chernoff-like bounds;approximation;decision threshold;observation time;software implementation;intermediate benchmark","","4","27","","","","","","IEEE","IEEE Journals & Magazines"
"Bus access optimization for distributed embedded systems based on schedulability analysis","P. Pop; P. Eles; Z. Peng","Dept. of Comput. & Inf. Sci., Linkoping Univ., Sweden; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","567","574","We present an approach to bus access optimization and schedulability analysis for the synthesis of hard real-time distribution embedded systems. The communication model is based on a time-triggered protocol. We have developed an analysis for the communication delays proposing four different message scheduling policies over a time-triggered communication channel. Optimization strategies for the bus access scheme are developed, and the four approaches to message scheduling are compared using extensive experiments.","","0-7695-0537","10.1109/DATE.2000.840842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840842","","Embedded system;Time division multiple access;Communication system control;Kernel;Clocks;Processor scheduling;Access protocols;Communication channels;Synchronization;Software architecture","embedded systems;processor scheduling;message passing;access protocols","bus access optimization;distributed embedded systems;schedulability analysis;communication model;time-triggered protocol;communication delays;message scheduling policies;time-triggered communication channel;optimization strategies","","12","20","","","","","","IEEE","IEEE Conferences"
"A combined ANN and expert system tool for transformer fault diagnosis","Zhenyuan Wang; Yilu Liu; P. J. Griffin","Dept. of Electr. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; NA; NA","IEEE Transactions on Power Delivery","","1998","13","4","1224","1229","A combined artificial neural network and expert system tool (ANNEPS) is developed for transformer fault diagnosis using dissolved gas-in-oil analysis (DGA). ANNEPS lakes advantage of the inherent positive features of each method and offers a further refinement of present techniques. The knowledge base of its expert system (EPS) is derived from IEEE and IEC DGA standards and expert experiences to include as many known diagnosis rules as possible. The topology and training data set of its artificial neural network (ANN) are carefully selected to extract known as well as unknown diagnosis correlations implicitly. The combination of the ANN and EPS outputs has an optimization mechanism to ensure high diagnosis accuracy for all general fault types. ANNEPS is database enhanced to facilitate archive management of equipment conditions, trend analysis and further revision of the diagnosis rules, Test results show that the system has better performance than ANN or EPS used individually.","0885-8977;1937-4208","","10.1109/61.714488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=714488","","Expert systems;Artificial neural networks;Diagnostic expert systems;Dissolved gas analysis;Fault diagnosis;IEC standards;Lakes;Network topology;Training data;Data mining","power transformer insulation;power transformer testing;insulation testing;electric breakdown;automatic test software;diagnostic expert systems;neural nets;learning (artificial intelligence);fault diagnosis;power engineering computing","power transformer fault diagnosis;artificial neural network;expert system tool;dissolved gas-in-oil analysis;insulation breakdown testing;standards;diagnosis rules;topology;training data set;optimization mechanism;diagnosis accuracy","","137","18","","","","","","IEEE","IEEE Journals & Magazines"
"Short flux paths optimise the efficiency of a 5-phase switched reluctance drive","A. Michaelides; C. Pollock","Dept. of Eng., Warwick Univ., Coventry, UK; Dept. of Eng., Warwick Univ., Coventry, UK","IAS '95. Conference Record of the 1995 IEEE Industry Applications Conference Thirtieth IAS Annual Meeting","","1995","1","","286","293 vol.1","A new configuration of switched reluctance drive is proposed in which the windings of a five-phase motor are configured to encourage short flux paths within the motor. The five-phase prototype motor can be fed from a new power converter which uses only six switching devices in a shared switch configuration. Experimental results demonstrate that the five-phase prototype achieves higher efficiency than prior art switched reluctance and induction drives constructed in the same frame size, and with the benefit of the new electronic circuit it is very cost effective.","0197-2618","0-7803-3008","10.1109/IAS.1995.530313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=530313","","Reluctance motors;Hysteresis motors;Stators;Switches;Prototypes;Switching converters;Torque;Iron;Costs;Magnetic flux","switching circuits;reluctance motor drives;machine windings;magnetic flux;electric machine analysis computing;machine theory;machine testing;optimisation;software packages","switched reluctance drive;five-phase motor;windings;short flux paths;power converter;switching devices;shared switch configuration;efficiency;SRM;SRDESIGN software package","","10","10","","","","","","IEEE","IEEE Conferences"
"Universal simulation kit in micro-production","O. Wiechers; M. Zehtaban; J. Seidelmann; J. Schliesser","Dept. Cleanroom Manuf., Fraunhofer IPA, Stuttgart, Germany; Dept. Cleanroom Manuf., Fraunhofer IPA, Stuttgart, Germany; Dept. Cleanroom Manuf., Fraunhofer IPA, Stuttgart, Germany; Dept. Cleanroom Manuf., Fraunhofer IPA, Stuttgart, Germany","IEEE/CPMT/SEMI 28th International Electronics Manufacturing Technology Symposium, 2003. IEMT 2003.","","2003","","","343","346","Due to its broad fields of applications, micro technology has become a key technology for the 21st century. In the scope of a successfully performed project for ""Principles of universal device kit for micro-production"", a simulation kit for testing and optimizing the tools and logistic concepts has been introduced by Fraunhofer IPA, Department Cleanroom Manufacturing. In this paper the approach and results of the developed simulation kit will be presented. The goal of this study was to test the functionality and efficiency of different equipment configurations and logistic plans during development and planning phase by means of this simulation kit. eM-Plant simulation software is used which benefits from an object-oriented and open system architecture and enables the user to create well-structured hierarchical simulation models. Based on this virtual simulation platform important manufacturing data like throughput, cycle time, equipment utilization as well as process controlling strategies can be determined for later application in production phase. By means of this simulation kit, development and optimization of new application for a tool and for factory logistics is also possible.","1089-8190","0-7803-7933","10.1109/IEMT.2003.1225926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1225926","","Object oriented modeling;Logistics;Virtual manufacturing;Performance evaluation;Software testing;System testing;Open systems;Computer architecture;Manufacturing processes;Throughput","object-oriented methods;virtual reality;electronic engineering computing;electronics industry;digital simulation;manufacturing data processing;process control","universal simulation kit;microproduction;microtechnology;key technology;universal device kit;logistic concepts;eM-plant simulation software;object oriented architecture;open system architecture;hierarchical simulation models;virtual simulation platform;manufacturing data;tool logistics;factory logistics;cycle time;process controlling strategies;optimisation;equipment utilisation;planning phase;production phase","","","8","","","","","","IEEE","IEEE Conferences"
"Verification of a complex SoC; the PRO/sup 3/ case-study","F. Andritsopoulos; C. Charopoulos; G. Doumenis; F. Karoubalis; Y. Mitsos; F. Petreas; I. Theologitou; S. Perissakis; D. Reisis","Telecommun. Lab., Nat. Tech. Univ. of Athens, Greece; Telecommun. Lab., Nat. Tech. Univ. of Athens, Greece; Telecommun. Lab., Nat. Tech. Univ. of Athens, Greece; Telecommun. Lab., Nat. Tech. Univ. of Athens, Greece; Telecommun. Lab., Nat. Tech. Univ. of Athens, Greece; Telecommun. Lab., Nat. Tech. Univ. of Athens, Greece; Telecommun. Lab., Nat. Tech. Univ. of Athens, Greece; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","224","229 suppl.","In this paper we present the experience gained from the design and verification of a complex network processor The PRO/sup 3/ processor can operate in either ATM or IP based multiprotocol networking environments, supporting link rates up to 2.4 Gbps. We describe the methodology followed during the verification process, from specifications to silicon prototype test and highlight the problems encountered during the post-layout procedure. To accommodate the application verification a proprietary debug tool is integrated in the system. The paper emphasizes the importance of the verification, addressing it as a parallel process to system design, and highlights the need for easy to verify designs.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253833","","Asynchronous transfer mode;Application software;Bandwidth;Process design;Protocols;Laboratories;Testing;Spine;Hardware;Physics","system-on-chip;network computers;formal verification;computer debugging;hardware-software codesign;timing;circuit CAD;circuit optimisation","complex network processor;PRO/sup 3/ processor;ATM based multiprotocol networking environment;IP based multiprotocol networking environment;verification process;debug tool;2.4 Gbit/s","","","6","","","","","","IEEE","IEEE Conferences"
"Real-time Handel-C based implementation of DV decoder","M. Gorgon; S. Cichon; M. Pac","Dept. of Automatics, AGH Univ. of Sci. & Technol., Krakow, Poland; Dept. of Automatics, AGH Univ. of Sci. & Technol., Krakow, Poland; Dept. of Automatics, AGH Univ. of Sci. & Technol., Krakow, Poland","International Conference on Field Programmable Logic and Applications, 2005.","","2005","","","130","135","In the paper, an FPGA realization is described for a DV decoder, working in real-time with digital signal conformant to IEC-6 1834-2 standard. The FPGA decoder covers the following stages of the signal decompression: Inverse VLC, Inverse Quantization, Inverse Weighting, Inverse DCT and all the auxiliary operations. Each stage of the operation has been realized by creation of processing element, based on the function codes implemented in Handel-C language. In order to achieve a real-time performance, optimization has been carried out for algorithms and all function codes. Next, a pipeline implementation for the video stream has been completed. Finally, the function execution has been parallelized in the pipeline at the single video block level. Hardware-software test stand has been designed and set-up and then testing has been carried out on real data transmitted on-line from a DV camcorder.","1946-147X;1946-1488","0-7803-9362","10.1109/FPL.2005.1515711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515711","","Decoding;Image coding;Field programmable gate arrays;Discrete cosine transforms;Streaming media;Video compression;Testing;Image sampling;Quantization;Pipelines","field programmable gate arrays;signal processing;programming languages;database languages;hardware-software codesign;optimisation;real-time systems;IEC standards","FPGA realization;DV decoder;digital signal conformant;IEC-6 1834-2 standard;FPGA decoder;signal decompression;inverse VLC;inverse quantization;inverse weighting;inverse DCT;auxiliary operations;creation of processing element;Handel-C language;real-time performance optimization;pipeline implementation;video stream;single video block level;hardware-software test stand;real data transmitted on-line;DV camcorder","","","12","","","","","","IEEE","IEEE Conferences"
"A virtual boundary model for a quick drop-impact analysis of PCB in electronic products","K. H. Low; Yuqi Wang; K. H. Hoon; W. K. Wai","Sch. of Mech. & Production Eng., Nanyang Technol. Univ., Singapore; Sch. of Mech. & Production Eng., Nanyang Technol. Univ., Singapore; Sch. of Mech. & Production Eng., Nanyang Technol. Univ., Singapore; Sch. of Mech. & Production Eng., Nanyang Technol. Univ., Singapore","Proceedings of the 5th Electronics Packaging Technology Conference (EPTC 2003)","","2003","","","275","281","Modeling and simulation of printed circuit boards (PCB) during drop test are often performed for electronic device assemblies. As sensitive components in electronic devices, components on the PCB are easily damaged during a drop-impact process. While different PCB positions in the electronic devices can cause different stress and strain on the board, these impact behaviors can also make a difference to the force along the solders that connect the chips to the PCB. Through an existing FEA software tool, we can find an optimal position of the PCB on the electronic device to improve product quality. To illustrate the proposed method, the drop simulation of a PCB specimen mounted on a packaged TV product was performed. Dynamic modeling and simulation of drop test was established and presented. The position of the PCB was optimized through the proposed virtual boundary method. With the method, three optimization models were considered, and the designs were proven to successful by comparing the stress and deflection of concerned points on the PCB. It is also found that the method can quickly determine several PCB optimal positions in electronic devices directly by the magnitudes of the deflection and stress.","","0-7803-8205","10.1109/EPTC.2003.1271528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1271528","","Stress;Circuit testing;Circuit simulation;Printed circuits;Electronic equipment testing;Performance evaluation;Assembly;Capacitive sensors;Software tools;Electronics packaging","electronics packaging;circuit simulation;optimisation;impact (mechanical);finite element analysis;television receivers;dynamic response;printed circuits","drop-impact analysis;virtual boundary model;electronic product PCB position optimization;electronic device assemblies;drop-impact process;board stress;board strain;FEA software tool;drop simulation;packaged TV product;dynamic modeling;PCB deflection","","","13","","","","","","IEEE","IEEE Conferences"
"A multiagent genetic algorithm for global numerical optimization","Weicai Zhong; Jing Liu; Mingzhi Xue; Licheng Jiao","Nat. Key Lab. for Radar Signal Process. & the Inst. of Intelligent Inf. Process., Xidian Univ., Xi'an, China; Nat. Key Lab. for Radar Signal Process. & the Inst. of Intelligent Inf. Process., Xidian Univ., Xi'an, China; Nat. Key Lab. for Radar Signal Process. & the Inst. of Intelligent Inf. Process., Xidian Univ., Xi'an, China; Nat. Key Lab. for Radar Signal Process. & the Inst. of Intelligent Inf. Process., Xidian Univ., Xi'an, China","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2004","34","2","1128","1141","In this paper, multiagent systems and genetic algorithms are integrated to form a new algorithm, multiagent genetic algorithm (MAGA), for solving the global numerical optimization problem. An agent in MAGA represents a candidate solution to the optimization problem in hand. All agents live in a latticelike environment, with each agent fixed on a lattice-point. In order to increase energies, they compete or cooperate with their neighbors, and they can also use knowledge. Making use of these agent-agent interactions, MAGA realizes the purpose of minimizing the objective function value. Theoretical analyzes show that MAGA converges to the global optimum. In the first part of the experiments, ten benchmark functions are used to test the performance of MAGA, and the scalability of MAGA along the problem dimension is studied with great care. The results show that MAGA achieves a good performance when the dimensions are increased from 20-10,000. Moreover, even when the dimensions are increased to as high as 10,000, MAGA still can find high quality solutions at a low computational cost. Therefore, MAGA has good scalability and is a competent algorithm for solving high dimensional optimization problems. To the best of our knowledge, no researchers have ever optimized the functions with 10,000 dimensions by means of evolution. In the second part of the experiments, MAGA is applied to a practical case, the approximation of linear systems, with a satisfactory result.","1083-4419;1941-0492","","10.1109/TSMCB.2003.821456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275544","","Genetic algorithms;Signal processing algorithms;Multiagent systems;Scalability;Evolution (biology);Linear systems;Information processing;Problem-solving;Application software;Benchmark testing","genetic algorithms;optimisation;numerical analysis","multiagent genetic algorithm;global numerical optimization;lattice like environment;linear system","","206","26","","","","","","IEEE","IEEE Journals & Magazines"
"Complex ASICs verification with SystemC","A. Randjic; N. Ostapcuk; I. Soldo; P. Markovic; V. Mujkovic","HDL Design House, Belgrade, Serbia; HDL Design House, Belgrade, Serbia; HDL Design House, Belgrade, Serbia; HDL Design House, Belgrade, Serbia; HDL Design House, Belgrade, Serbia","2002 23rd International Conference on Microelectronics. Proceedings (Cat. No.02TH8595)","","2002","2","","671","674 vol.2","This paper aims to present a way of complex ASIC verification by C/C++ oriented hardware description language using SystemC libraries. The solution of testbench organization is presented, as a result of complexity of design under verification, network processor PcomP. It is a general solution, new tests and parameter changes are very easy to be added, as is usage of the same testbench concept for other design.","","0-7803-7235","10.1109/MIEL.2002.1003347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003347","","Application specific integrated circuits;Hardware design languages;Testing;Controllability;Observability;Emulation;Algorithm design and analysis;Software algorithms;Design optimization;Programming","C++ language;C language;application specific integrated circuits;formal verification;hardware description languages;software libraries;integrated circuit design","complex ASICs;verification;C/C++ oriented hardware description language;testbench organization;design under verification;network processor;PcomP;parameter changes;SystemC libraries","","1","6","","","","","","IEEE","IEEE Conferences"
"On-chip cache algorithm design for multimedia SOC","A. Pratoomtong; Yu Hen Hu","Wisconsin Univ., Madison, WI, USA; Wisconsin Univ., Madison, WI, USA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","","2005","2","","ii/337","ii/340 Vol. 2","In order to optimally implement real time, high throughput, data intensive multimedia applications, it is crucial to optimize the performance of the memory subsystem to minimize excessive off-chip memory bandwidth subject to the constraint of available on-chip memory cache size. This can be accomplished by customizing algorithm transformation and designing a customized cache address mapping algorithm for a specific class of multimedia applications. In this paper, we propose an algorithm transformation and customized cache mapping to improve the data reusability and reduce address conflict which in turn, reduces the cache miss and memory I/O bandwidth for the block-based full-search motion estimation algorithm. Simulation results using test video sequences demonstrate marked performance improvement.","1520-6149;2379-190X","0-7803-8874","10.1109/ICASSP.2005.1415410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415410","","Algorithm design and analysis;Bandwidth;Hardware;Software algorithms;Cache memory;Throughput;Constraint optimization;Motion estimation;Computer architecture;Multimedia computing","cache storage;multimedia computing;system-on-chip;video coding;optimisation;search problems;motion estimation;image sequences","on-chip cache algorithm;multimedia SOC;optimisation;performance;memory subsystem;excessive off-chip memory bandwidth;data reusability;address conflict;block-based full-search motion estimation;video sequences","","2","6","","","","","","IEEE","IEEE Conferences"
"The design and development of a distributed scheduler agent","L. S. Peh; A. L. Ananda","Nat. Univ. of Singapore, Singapore; NA","Proceedings of 1996 IEEE Second International Conference on Algorithms and Architectures for Parallel Processing, ICA/sup 3/PP '96","","1996","","","108","115","The main objective of the distributed scheduler agent is to provide task placement advice either to parallel and distributed applications directly, or to a distributed scheduler which will despatch normal applications transparently to the advised hosts. To accomplish this, the distributed scheduler agent needs to know the global load situation across all machines and be able to pick the host which best suits the specific resource requirements of individual jobs. Issues concerning the collecting and distribution of load information throughout the system are discussed. This load information is then fed to a ranking algorithm which uses a 3-dimensional load space to generate the most suitable host based on weights which indicate the relative importance of resources to a task. Performance tests are carried out to determine the response times and overhead of the distributed scheduler agent. An application, a distributed ray tracer, is also customised to make use of the distributed scheduler agent and the results presented.","","0-7803-3529","10.1109/ICAPP.1996.562864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=562864","","Processor scheduling;Single machine scheduling;Workstations;Testing;Local area networks;Resource management;Algorithm design and analysis;Scheduling algorithm;Fault tolerance;Fault detection","local area networks;workstations;processor scheduling;resource allocation;software performance evaluation;parallel processing;software engineering;software agents","distributed scheduler agent development;distributed scheduler agent design;task placement advice;parallel applications;distributed applications;advised hosts;global load situation;machines;resource requirements;load information collection;load information distribution;ranking algorithm;3-dimensional load space;relative resource importance weights;performance tests;response times;overhead;distributed ray tracer","","","17","","","","","","IEEE","IEEE Conferences"
"Power-performance trade-offs for energy-efficient architectures: A quantitative study","Hongbo Yang; R. Govindarajan; G. R. Gao; K. B. Theobald","Dept. of Electr. & Comput. Eng., Delaware Univ., Newark, DE, USA; NA; NA; NA","Proceedings. IEEE International Conference on Computer Design: VLSI in Computers and Processors","","2002","","","174","179","The drastic increase in power consumption by modern processors emphasizes the need for power-performance trade-offs in architecture design space exploration and compiler optimizations. This paper reports a quantitative study on the power-performance trade-offs in software pipelined schedules for an Itanium-like EPIC architecture with dual-speed pipelines, in which functional units are partitioned into fast ones and slow ones. We have developed an integer linear programming formulation to capture the power-performance tradeoffs for software pipelined loops. The proposed integer linear programming formulation and its solution method have been implemented and tested on a set of SPEC2000 benchmarks. The results are compared with an Itanium-like architecture (baseline) in which there are four functional units (FUs) and all of them are fast units. Our quantitative study reveals that by introducing a few slow FUs in place of fast FUs in the baseline architecture, the total energy consumed by FUs can be considerably reduced. When 2 out of 4 FUs are set as slow, the total energy consumed by FUs is reduced by up to 31.1% (with an average reduction of 25.2%) compared with the baseline configuration, while the performance degradation caused by using slow FUs is small. If performance demand is less critical, then energy reduction of up to 40.3% compared with the baseline configuration can be achieved.","1063-6404","0-7695-1700","10.1109/ICCD.2002.1106766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106766","","Energy efficiency;Computer architecture;Integer linear programming;Energy consumption;Space exploration;Design optimization;Optimizing compilers;Processor scheduling;Pipelines;Testing","optimising compilers;performance evaluation;computer architecture;pipeline processing;parallel architectures;power consumption","power consumption;power-performance trade-offs;architecture design;space exploration;compiler optimizations;software pipelined schedules;microprocessors;EPIC architecture;integer linear programming","","1","21","","","","","","IEEE","IEEE Conferences"
"Adaptive control with NeuCOP, the neural control and optimization package","T. J. Graettinger; N. V. Bhat; J. S. Buck","NeuralWare Inc., Pittsburgh, PA, USA; NA; NA","Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)","","1994","4","","2389","2393 vol.4","NeuCOP is the neural control and optimization package that has been jointly developed by Texaco and NeuralWare. It is a state-of-the-art, multivariable, adaptive controller that combines the nonlinear modeling power of neural networks with nonlinear optimization algorithms. The NeuCOP controller belongs to the general class of model predictive controllers. One novel feature of the NeuCOP controller is its use of a nonlinear, neural network process model. We describe the identification subsystem that has been developed. More specifically, we address the issue of system re-identification, after the system is put online. The re-identification process allows the model to adapt to changing process conditions.<<ETX>>","","0-7803-1901","10.1109/ICNN.1994.374593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=374593","","Adaptive control;Packaging;Predictive models;Neural networks;Industrial control;Testing;Programmable control;Robust control;Power generation economics;Economic forecasting","adaptive control;neurocontrollers;neural nets;optimisation;software packages;identification;multivariable control systems;predictive control","NeuCO;neural control;optimization;Texaco;NeuralWare;multivariable, adaptive controller;nonlinear modeling;neural networks;model predictive controllers;identification","","1","9","","","","","","IEEE","IEEE Conferences"
"Design and development of energy efficient sensorless direct torque controlled induction motor drive based on real time simulation","S. Vamsidhar; B. G. Fernandas","Energy Syst. Eng., Indian Inst. of Technol., Mumbai, India; NA","30th Annual Conference of IEEE Industrial Electronics Society, 2004. IECON 2004","","2004","2","","1349","1354 Vol. 2","This paper presents energy efficient direct torque controlled sensorless induction motor drive using space vector modulation. The entire design and optimisation of controller parameters of the drive is carried out using hardware-in-the-loop (HIL) simulation. The simulation is performed in real time using TMS320F240 digital signal processor. The dynamic machine model with 5 state variables is solved in real time to determine the state variables. These variables are then used to estimate the various output quantities. Using the same controller and software used in HIL simulation, and with additional signal conditioning interface circuitry, the results obtained from real time simulation are experimentally validated on a 1.5-kW induction motor drive. A simple method based on minimum average input power is proposed to determine the optimal flux reference at light loads. A method for online-controller parameter tuning is also proposed so as to have an optimal dynamic as well as steady state performance.","","0-7803-8730","10.1109/IECON.2004.1431774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431774","","Energy efficiency;Torque control;Sensorless control;Induction motor drives;Modeling;Space technology;System testing;Electric variables control;Control systems;Design optimization","induction motor drives;machine control;control system synthesis;control engineering computing;optimisation;digital signal processing chips","induction motor drive;energy efficient sensorless direct torque control;space vector modulation;optimisation;hardware-in-the-loop simulation;TMS320F240;digital signal processor;online-controller parameter tuning;1.5 kW","","","12","","","","","","IEEE","IEEE Conferences"
"Distribution feeder reconfiguration for service restoration and load balancing","Qin Zhou; D. Shirmohammadi; W. -. E. Liu","San Francisco, CA, USA; NA; NA","IEEE Transactions on Power Systems","","1997","12","2","724","729","This paper describes two feeder reconfiguration algorithms for the purpose of service restoration and load balancing in a real-time operation environment. The developed methodologies combine optimization techniques with heuristic rules and fuzzy logic for efficiency and robust performance. Many of practical operating concerns of feeder reconfiguration and the coordination with other distribution automation applications are also addressed. The developed algorithms have been implemented as a production grade software. Test results on PG&E distribution feeders show that the performance is efficient and robust.","0885-8950;1558-0679","","10.1109/59.589664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589664","","Load management;Switches;Fuzzy logic;Substations;Robustness;Automation;Production systems;Software algorithms;Real time systems;Joining processes","distribution networks;power system planning;power system restoration;power system stability;optimisation;fuzzy logic;heuristic programming","distribution feeder reconfiguration planning;service restoration;load balancing;reconfiguration algorithms;real-time operation environment;optimization techniques;heuristic rules;fuzzy logic;efficiency;robust performance;distribution automation;production grade software;computer simulation","","122","9","","","","","","IEEE","IEEE Journals & Magazines"
"On the optimal total processing time using checkpoints","B. Dimitrov; Z. Khalil; N. Kolev; P. Petrov","Inst. of Math., Acad. of Sci., Sofia, Bulgaria; NA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","5","436","442","The authors investigate the problem of optimizing the expected blocking time duration by providing a schedule of checkpoints during the required job processing time. They give a general approach for determining the optimal checkpoint schedule and derive some cases when the optimal checkpointing is uniform. The model has applications in unreliable computing systems, multiclient computer service, data transmissions, etc.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=90446","","Electric breakdown;Processor scheduling;Checkpointing;Data communication;Testing;Performance evaluation;Councils;Mathematics;Application software;Computer applications","optimisation;programming theory;scheduling","optimal total processing time;optimal checkpoint schedule;unreliable computing systems;multiclient computer service;data transmissions","","6","9","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting basic block value locality with block reuse","Jian Huang; D. J. Lilja","Dept. of Comput. Sci. & Eng., Minnesota Univ., Minneapolis, MN, USA; NA","Proceedings Fifth International Symposium on High-Performance Computer Architecture","","1999","","","106","114","Value prediction at the instruction level has been introduced to allow more aggressive speculation and reuse than previous techniques. We investigate the input and output values of basic blocks and find that these values can be quite regular and predictable, suggesting that using compiler support to extend value prediction and reuse to a coarser granularity may have substantial performance benefits. For the SPEC benchmark programs evaluated, 90% of the basic blocks have fewer than 4 register inputs, 5 live register outputs, 4 memory inputs and 2 memory outputs. About 16% to 41% of all the basic blocks are simply repeating earlier calculations when the programs are compiled with the -O2 optimization level in the GCC compiler. We evaluate the potential benefit of basic block reuse using a novel mechanism called a block history buffer. This mechanism records input and live output values of basic blocks to provide value prediction and reuse at the basic block level. Simulation results show that using a reasonably sized block history buffer to provide basic block reuse in a 4-way issue superscalar processor can improve execution time for the tested SPEC programs by 1% to 14% with an overall average of 9%.","","0-7695-0004","10.1109/HPCA.1999.744342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744342","","Registers;Computer science;Computer aided instruction;Ear;Optimizing compilers;Program processors;History;Testing;Accuracy","parallel architectures;parallel programming;instruction sets;software reusability;program compilers;storage allocation;buffer storage","basic block value locality;block reuse;value prediction;instruction level;aggressive speculation;compiler support;coarser granularity;performance benefits;SPEC benchmark programs;register inputs;live register outputs;memory inputs;memory outputs;-O2 optimization level;GCC compiler;basic block reuse;block history buffer;4-way issue superscalar processor;execution time;SPEC programs","","30","10","","","","","","IEEE","IEEE Conferences"
"SPSA/SIMMOD optimization of air traffic delay cost","N. L. Kleinman; S. D. Hill; V. A. Ilenda","Dept. of Math., Brigham Young Univ., Provo, UT, USA; NA; NA","Proceedings of the 1997 American Control Conference (Cat. No.97CH36041)","","1997","2","","1121","1125 vol.2","The cost of delay is a serious and increasing problem in the airline industry. Air travel is increasing, and already domestic airports incur thousands of hours of delay daily, costing the industry $2 billion a year. One strategy for reducing total delay costs is to hold planes for a short time at the gate in order to reduce costly airborne congestion. In a network of airports involving hundreds of flights, it is difficult to determine the amount to hold each flight at the gate. This paper discusses how the optimization procedure simultaneous perturbation stochastic approximation (SPSA) can be used to process delay cost measurements from air traffic simulation packages and produce an optimal gate holding schedule. As a test case, the SIMMOD air traffic simulation package was used to model a simple four-airport network. Initial delay costs are reduced up to 10.3%.","0743-1619","0-7803-3832","10.1109/ACC.1997.609707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=609707","","Cost function;Airports;Telecommunication traffic;Traffic control;Packaging;Costing;Delay effects;Stochastic processes;Job shop scheduling;Testing","approximation theory;air traffic control;software packages;digital simulation;scheduling;aerospace simulation;optimisation","SPSA/SIMMOD optimization;air traffic delay cost;airline industry;domestic airports;airborne congestion;simultaneous perturbation stochastic approximation;delay cost measurements;optimal gate holding schedule;SIMMOD air traffic simulation package;four-airport network","","10","20","","","","","","IEEE","IEEE Conferences"
"Geographic information retrieval (GIR) ranking methods for digital libraries","R. R. Larson; P. Frontiera","Sch. of Inf. Manage. & Syst., California Univ., Berkeley, CA, USA; NA","Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 2004.","","2004","","","415","","This demo presents results from an evaluation of algorithms for ranking results by probability of relevance for geographic information retrieval (GIR) applications. We demonstrate an algorithm for GIR ranking based on logistic regression from samples of the test collection. We also show the effects of different representations of the geographic regions being searched, including minimum bounding rectangles, convex hulls, and complex polygons.","","1-58113-832","10.1109/JCDL.2004.240497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336219","","Information retrieval;Software libraries","geographic information systems;relevance feedback;visual databases;digital libraries;probability","relevance probability;geographic information retrieval;logistic regression;geographic regions;digital libraries;spatial databases","","3","","","","","","","IEEE","IEEE Conferences"
"Spectral analysis for characterizing program power and performance","R. Joseph; M. Martonosi; Zhigang Hu","Dept. of Electr. Eng., Princeton Univ., NJ, USA; Dept. of Electr. Eng., Princeton Univ., NJ, USA; NA","IEEE International Symposium on - ISPASS Performance Analysis of Systems and Software, 2004","","2004","","","151","160","Performance and power analysis in modern processors requires managing a large amount of complex information across many time-scales. For a example, thermal control issues are a power subproblem with relevant time constants of millions of cycles or more, while the so-called dI/dT problem is also a power subproblem but occurs because of current variability on a much finer granularity: tens to hundreds of cycles. Likewise, for performance issues, program phase analysis for selecting simulation regions requires looking for periodicity on the order of millions of cycles or more, while some aspects of cache performance optimization requires understanding repetitive patterns on much finer granularities. Fourier analysis allows one to transform waveform into a sum of component (usually sinusoidal) waveforms in the frequency domain; in this way, the waveform's fundamental frequencies (periodicities of repetition) can be clearly identified. This paper shows how one can use Fourier analysis to produce frequency spectra for some of the time waveforms seen in processor execution. By working in the frequency domain, one can easily identify key application tendencies. For example, we demonstrate how to use spectral analysis to characterize the power behavior of real programs. As we show, this is useful for understanding both the temperature profile of a program and its voltage stability. These are particularly relevant issues for architects since thermal concerns and the dI/dT problem have significant influence on processor design. Frequency analysis can also be used to examine program performance. In particular, it can also identify periodic occurrences of important microarchitectural events like cache misses. Overall, the paper demonstrates the value of using frequency analysis in different research efforts related to characterizing and optimizing application performance and power.","","0-7803-8385","10.1109/ISPASS.2004.1291367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1291367","","Spectral analysis;Performance analysis;Frequency domain analysis;Information analysis;Energy management;Pattern analysis;Analytical models;Optimization;Fourier transforms;Temperature","spectral analysis;Fourier analysis;program testing;software performance evaluation;power conversion","spectral analysis;program power characterization;program performance characterization;dI/dT problem;power subproblem;granularity;performance optimization;Fourier analysis;frequency domain;frequency analysis","","6","24","","","","","","IEEE","IEEE Conferences"
"A hybrid parametric, non-parametric approach to Bayesian target tracking","J. V. Black; C. M. Reed","DRA, Malvern, UK; NA","Proceeding of 1st Australian Data Fusion Symposium","","1996","","","178","183","This article describes a versatile approach to nonlinear, nonGaussian noise target tracking which makes use of both parametric and nonparametric techniques within a Bayesian framework. It produces a Gaussian mixture model (GMM) of a track, but resorts to a sampling technique within the tracking process to handle nonlinearity. GMMs are recovered from samples using the expectation-maximisation method. The approach has been implemented in PV-WAVE software and tested against a Kalman-filter tracker in a simulator with air-defence scenarios. Sample results are presented for a scenario with a single surveillance-radar and a single target following a weaving path. These show that the tracker produces significantly better position estimates and comparable heading and speed estimates. Computation times are about 30 times greater than for the Kalman-filter tracker, but there is scope for reducing that substantially by tolerating fewer samples.","","0-7803-3601","10.1109/ADFS.1996.581103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581103","","Bayesian methods;Target tracking;Sampling methods;Economic forecasting;Linearity;Software testing;Weaving;State estimation;Maximum likelihood estimation;Probability distribution","target tracking;tracking;Bayes methods;noise;statistical analysis;optimisation","parametric techniques;nonparametric techniques;Bayesian target tracking;nonlinear nonGaussian noise target tracking;Gaussian mixture model;nonlinearity;expectation-maximisation method;PV-WAVE software;Kalman-filter tracker;air-defence scenarios;surveillance radar","","3","13","","","","","","IEEE","IEEE Conferences"
"Interpretation of digital partial discharge measurements","B. Fruth; D. Gross","Power Diagnostix Syst. GmbH, Aachen, Germany; Power Diagnostix Syst. GmbH, Aachen, Germany","IEE Colloquium on PD Display Systems and Analytical Software","","1996","","","3/1","3/3","The presence of partial discharges is one of the most prominent indicators of defects and ongoing degradation processes in electrical insulation systems. On-line partial discharge measurements provide information about progressing degradation under operational stress or defects which are only excited by the operational stress (e.g. vibration of e.g. motors). Thus, a complementary insight in the state of the apparatus can be gained which is not accessible through off-line testing. Provided a ""diagnostic connector"" or sensor is integrated into or attached to the high voltage system, periodic measurements, trend recording or even on-line monitoring becomes possible. Digital partial discharge measurements with enhanced signal conditioning techniques allow better visualization and interpretation of data in terms of risk and defect type and provide better noise reduction and identification capabilities. They are not only applied to the R&D environment, but are today used in quality assurance and online situations. The interpretation of digital pd measurements not only includes the defect assessment (physics of partial discharge) but also an identification of noise sources and non-pd signals. These ""interpretations"" lead to a choice of sensors and signal processing techniques to be chosen from a so-called toolbox which serves to optimize the measurement procedures.","","","10.1049/ic:19960232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=543319","","Partial discharges;Insulation testing;Detectors;High-voltage techniques;Signal processing;Random noise","partial discharges;insulation testing;electric sensing devices;high-voltage techniques;signal processing;random noise","digital partial discharge measurements;degradation processes;electrical insulation systems;defects;on-line partial discharge measurements;operational stress;motors;diagnostic connector;high voltage system;on-line monitoring;enhanced signal conditioning techniques;noise reduction;quality assurance;defect assessment;noise sources;sensors","","","","","","","","","IET","IET Conferences"
"CASTOR: a computer aided system testability optimizer","C. Bolchini","Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy","Proceedings of Twentieth Euromicro Conference. System Architecture and Integration","","1994","","","314","321","CASTOR is an expert module that solves testability problems detected in a hardware design by advising the designer on the application of design for testability techniques. The system evaluates all possible solutions and implements the most economic one in terms of overheads. The optimal solution is obtained by applying a branch and bound strategy to carry out the exhaustive search of the best solution, optimizing system resources. A suitable cost function is used to quantify the effects of the application of each DfT technique to the circuit examined.<<ETX>>","","0-8186-6430","10.1109/EURMIC.1994.390378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=390378","","System testing;Circuit testing;Expert systems;Design for testability;Hardware;Application software;Cost function;Flow production systems;Built-in self-test;Automatic testing","design for testability;expert systems","CASTOR;computer aided system testability optimizer;expert module;testability problems;hardware design;design for testability techniques;branch and bound strategy;system resources;cost function","","","17","","","","","","IEEE","IEEE Conferences"
"Built-in diagnostics for advanced power management","M. Darty; Li Pi Su; C. Bosco","McDonnell Douglas Aerosp., Huntsville, AL, USA; NA; NA","Proceedings of AUTOTESTCON '94","","1994","","","399","407","The Army's Diagnostic Analysis and Repair Tool Set (DARTS) is an advanced software product used to perform automated fault diagnostics that results in reduced logistics costs, decreased downtime and enhanced mission performance. DARTS enabled automated, knowledge based fault diagnostics to be embedded in the Advanced Modular Power Control System (AMPCS). AMPCS is an integrated hardware and software product for aerospace power management. DARTS was used in a concurrent engineering design environment as a computer aided engineering tool to optimize the fault detection and fault isolation characteristics of the AMPCS prototype design. Project results indicate a new method for linking the diagnostic knowledge base captured during design with the hardware under development to achieve automated fault isolation throughout the hardware life cycle. A successful demonstration of real-time fault diagnostics was conducted using the AMPCS prototype unit and the DARTS software in a hardware-in-the-loop laboratory system. This project reduced to practice the concept of automated, knowledge based diagnostics for electronic systems.<<ETX>>","","0-7803-1910","10.1109/AUTEST.1994.381592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381592","","Energy management;Hardware;Power system management;Software prototyping;Performance analysis;Software performance;Software tools;Logistics;Costs;Power control","military computing;fault diagnosis;automatic test equipment;economics;power control;knowledge based systems;aerospace computing;real-time systems;fault tolerant computing;circuit CAD;design for testability;power supplies to apparatus","built-in diagnostics;advanced power management;advanced software product;automated fault diagnostics;reduced logistics costs;decreased downtime;enhanced mission performance;knowledge based fault diagnostics;Advanced Modular Power Control System;integrated hardware/software product;aerospace power management;concurrent engineering design environment;computer aided engineering tool;fault detection;fault isolation;AMPCS prototype design;diagnostic knowledge base;automated fault isolation;real-time fault diagnostics;DARTS software;hardware-in-the-loop laboratory system;automated knowledge based diagnostics","","2","2","","","","","","IEEE","IEEE Conferences"
"Means-ends analysis: a hardware implementation","D. L. Mitchell; H. Kobayashi","Dept. of Electr. & Comput. Eng., South Carolina Univ., Columbia, SC, USA; Dept. of Electr. & Comput. Eng., South Carolina Univ., Columbia, SC, USA","Proceedings. IEEE Energy and Information Technologies in the Southeast'","","1989","","","989","994 vol.3","The authors describe a hardware implementation for means-ends analysis (MEA), an artificial intelligence (AI) technique that involves comparing a given goal with a current task-domain situation to extract a difference between them. This general problem-solving technique had been implemented in software and used in certain planners such as STRIPS and ABSTRIPS. The authors discuss hardware alternatives for each software subfunction. A simple application of 'getting a cup of coffee' is presented as a test example. This hardware implementation is one alternative for implementing the MEA algorithm. The main goal is to illustrate the merging of AI with VLSI. The system is presented at the functional or logic design level and is technology-independent. Logic synthesis and some design optimization are done.<<ETX>>","","","10.1109/SECON.1989.132557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=132557","","Hardware;Artificial intelligence;Logic design;Problem-solving;Strips;Application software;Testing;Merging;Very large scale integration;Design optimization","artificial intelligence;logic CAD;problem solving","hardware implementation;means-ends analysis;artificial intelligence;goal;current task-domain situation;problem-solving technique;STRIPS;ABSTRIPS;MEA algorithm;VLSI;logic design;design optimization","","","6","","","","","","IEEE","IEEE Conferences"
"Non coherent spectral analysis of ADC using filter bank","C. Rebai; D. Dallet; P. Marchegay","Lab. IXL, Bordeaux I Univ., Talence, France; Lab. IXL, Bordeaux I Univ., Talence, France; Lab. IXL, Bordeaux I Univ., Talence, France","IMTC/2002. Proceedings of the 19th IEEE Instrumentation and Measurement Technology Conference (IEEE Cat. No.00CH37276)","","2002","1","","183","187 vol.1","The spectral analysis of ADC digital data has traditionally been done with discrete Fourier transform. This method imposes restrictions to optimize the results: one of these is coherent sampling. Recently, some filter structures have been used for the spectral analysis of sinusoidal signal corrupted by harmonics and noise. In this paper, we present a filter bank structure used for decomposition of a signal into his main spectral components. The main application examined is ADC spectral parameters extraction (SINAD, SNR, THD) with non coherent sampling. Computer simulations are used to demonstrate the performance of the proposed filter structure.","1091-5281","0-7803-7218","10.1109/IMTC.2002.1006837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006837","","Spectral analysis;Filter bank;Sampling methods;Discrete Fourier transforms;Optimization methods;Power harmonic filters;Application software;Parameter extraction;Signal to noise ratio;Computer simulation","analogue-digital conversion;spectral analysis;fast Fourier transforms;signal sampling;digital filters;notch filters;harmonic distortion;integrated circuit testing","ADC digital data;noncoherent spectral analysis;sinusoidal signal;filter bank structure;signal decomposition;spectral parameters extraction;computer simulations;ADC testing;notch filter;digital filtering;second order filters;bandpass filter;biquadratic filter;FFT","","4","7","","","","","","IEEE","IEEE Conferences"
"Transmitter-based multiuser interference rejection for the down-link of a wireless CDMA system in a multipath environment","M. Brandt-Pearce","Dept. of Electr. Eng., Virginia Univ., Charlottesville, VA, USA","IEEE Journal on Selected Areas in Communications","","2000","18","3","407","417","In wireless code division multiple access (CDMA) communications systems, there has been interest in processing the transmitted down-link signal in order to shift signal processing to the transmitter where power and computational resources are plentiful, thus simplifying receiver operation and reducing the power it requires. Multiuser interference (MUI) and multipath effects observed by the receiver are anticipated and suppressed at the transmitter; channel equalization and multiuser detection are therefore not required. This paper introduces two methods that are able to combat both degradations, yet allow the receiver to remain as simple as a single user receiver for a perfect channel. For mild multipath channels, the performance of the algorithms is excellent, within a few decibels of the single user ideal channel case, at the cost of additional computation at the base station at which complete knowledge of the channels and the receiver codewords is required. One method, the decorrelating prefilter, is most flexible and applicable to existing systems yet less powerful than other previously published methods. The second, the jointly optimized sequences algorithm, has a performance on average superior to published methods. In addition to theoretical analysis and simulation of the algorithms' potential, these algorithms have also been implemented and tested on a software radio testbed and experimental data are shown. The jointly optimized sequences performed particularly well even in severe multipath and multiuser interference environments.","0733-8716;1558-0008","","10.1109/49.840200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840200","","Signal processing algorithms;Multiaccess communication;Signal processing;Radio transmitters;Software algorithms;Software testing;Communication systems;Interference suppression;Multiuser detection;Degradation","code division multiple access;radio links;land mobile radio;radio transmitters;interference suppression;multipath channels;decorrelation;filtering theory;optimisation;spread spectrum communication;indoor radio;wireless LAN;multiuser channels","transmitter-based multiuser interference rejection;multipath effects;wireless CDMA system;multipath environment;direct sequence code division multiple access;down-link signal processing;receiver;perfect channel;multipath channels;algorithm performance;single user ideal channel;base station;receiver codewords;decorrelating prefilter;jointly optimized sequences algorithm;simulation;software radio testbed;experimental data;indoor wireless LAN","","71","21","","","","","","IEEE","IEEE Journals & Magazines"
"Construction and 60 kV tests of the prototype pulser for the LHC injection kicker system","M. J. Barnes; G. D. Wait; E. Carlier; L. Ducimetiere; G. H. Schroder; E. B. Vossenberg","TRIUMF, Vancouver, BC, Canada; NA; NA; NA; NA; NA","Digest of Technical Papers. 12th IEEE International Pulsed Power Conference. (Cat. No.99CH36358)","","1999","2","","777","780 vol.2","The European Laboratory for Particle Physics (CERN) is constructing the Large Hadron Collider (LHC). Two counter-rotating proton beams will be injected into the LHC at an energy of 450 GeV by two kicker magnet systems, producing magnetic field pulses of approximately 900 ns rise time and 6.6 /spl mu/s flat top duration with a ripple of less than /spl plusmn/0.5%. Both injection systems are composed of 4 travelling wave kicker magnets of 2.17 m length each, powered by pulse forming networks (PFNs). To achieve the high-required kick strength of 1.2 Tm, for a compact and cost efficient design, a characteristic impedance of 5 Ohms has been chosen. The design strategy for the magnets and generators has been defined after detailed analysis of existing systems. The electrical circuit has been optimised using the circuit analysis software PSpice. Most known parasitics have been modelled. A prototype PFN has been constructed at CERN and successfully tested at 60 kV. A calibration procedure has been developed and utilised for obtaining correction data for a high voltage probe and oscilloscope amplifier. Measurements carried out with a precision of approximately /spl plusmn/0.1% show that the prototype PFN conforms to the specifications and the PSpice predictions.","","0-7803-5498","10.1109/PPC.1999.823629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=823629","","Testing;Prototypes;Large Hadron Collider;Laboratories;Particle beams;Magnetic fields;Costs;Impedance;Magnetic analysis;Circuits","hadrons;colliding beam accelerators;pulsed power supplies;magnetic fields;pulse generators;accelerator magnets","LHC injection kicker system;Large Hadron Collider;counter-rotating proton beams;kicker magnet systems;magnetic field pulses;pulse forming networks;design strategy;circuit analysis software;PSpice;calibration procedure;60 kV;450 keV;900 ns;6.6 mus;2.17 m;5 ohm","","3","11","","","","","","IEEE","IEEE Conferences"
"A performance comparison of the Rete and TREAT algorithms for testing database rule conditions","Yu-Wang Wang; E. N. Hanson","Utek Corp., Lisle, IL, USA; NA","[1992] Eighth International Conference on Data Engineering","","1992","","","88","97","The authors present the results of a simulation comparing the performance of the two most widely used production rule condition testing algorithms, Rete and TREAT, in the context of a database rule system. The results show that TREAT almost always outperforms Rete. TREAT requires less storage than Rete, and is less sensitive to optimization decisions than Rete. Based on these results, it is concluded that TREAT is the preferred algorithm for testing join conditions of database rules. Since Rete does outperform TREAT in some cases, this study suggests a next step which would be to develop a hybrid version of Rete and TREAT with an optimizer that would decide which strategy to use based on the rule definition and statistics about the data and update patterns.<<ETX>>","","0-8186-2545","10.1109/ICDE.1992.213202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213202","","System testing;Production systems;Design optimization;Context modeling;Relational databases;Statistical analysis;Process design;Computer science;Military computing;Application software","database theory;deductive databases;digital simulation;knowledge based systems;performance evaluation","performance simulation comparison;Rete algorithm;TREAT algorithm;data patterns;database rule conditions;production rule condition testing algorithms;database rule system;join conditions;rule definition;update patterns","","10","19","","","","","","IEEE","IEEE Conferences"
"Circuit partitioning using a Tabu search approach","S. Areibi; A. Vannelli","Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada","1993 IEEE International Symposium on Circuits and Systems","","1993","","","1643","1646 vol.3","Tabu Search is a simple combinatorial optimization strategy that has been applied in graph coloring, scheduling, and space planning. The application of the Tabu Search heuristic to the circuit partitioning problem is described. Results obtained indicate that in most cases Tabu Search yields netlist partitions with 10% fewer cut nets than the best netlist partitions obtained by using an interchange method or simulated annealing. The Tabu Search method is 3 to 20 times faster than simulated annealing on tested problems. The benefit of integrating Tabu Search with simulated annealing is discussed.<<ETX>>","","0-7803-1281","10.1109/ISCAS.1993.394055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=394055","","Circuit simulation;Simulated annealing;Partitioning algorithms;Processor scheduling;Application software;Search methods;Circuit testing;Printed circuits;Polynomials;Scheduling algorithm","search problems;combinatorial mathematics;circuit optimisation;network topology;simulated annealing","Tabu search approach;combinatorial optimization strategy;circuit partitioning problem;netlist partitions;cut nets;simulated annealing","","7","5","","","","","","IEEE","IEEE Conferences"
"Quadratic assignment problems generated with the Palubetskis algorithm are degenerate","D. Cyganski; R. F. Vaz; V. G. Virball","Dept. of Electr. & Comput. Eng., Worcester Polytech. Inst., MA, USA; Dept. of Electr. & Comput. Eng., Worcester Polytech. Inst., MA, USA; Dept. of Electr. & Comput. Eng., Worcester Polytech. Inst., MA, USA","IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications","","1994","41","7","481","484","The quadratic assignment problem (QAP) is a combinatorial optimization problem that arises in many applications such as the allocation of processes in distributed computer systems. The QAP is NP-hard and therefore no algorithms are known for solving the QAP in polynomial time. For this reason a variety of heuristic methods have been proposed for this problem. In order to evaluate heuristics, Palubetskis proposed an algorithm that generates QAPs with known optimal solution value. We show in this paper that given a Palubetskis instance (but not its optimal value) the corresponding optimal value can be determined via a linear program, polynomial in the input data, i.e., in polynomial time. This implies that problems generated by the Palubetskis method belong to a simple and degenerate subclass of QAPs and are therefore not appropriate for algorithm testing. The proof technique suggests moreover a new lower bound for Euclidean QAPs.<<ETX>>","1057-7122;1558-1268","","10.1109/81.298362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=298362","","Polynomials;Transmission line matrix methods;Cost function;Distributed computing;Testing;Minimization;Linear matrix inequalities;Symmetric matrices;Application software;Resource management","optimisation;resource allocation;heuristic programming;linear programming;matrix algebra;polynomials;combinatorial mathematics","quadratic assignment problems;Palubetskis algorithm;degenerate problems;combinatorial optimization problem;resource allocation;NP-hard;heuristic methods;known optimal solution value;linear program;polynomial time;Euclidean QAPs","","2","8","","","","","","IEEE","IEEE Journals & Magazines"
"Hardware and software platform for real-time processing and visualization of echographic radiofrequency signals","M. Scabia; E. Biagi; L. Masotti","Dipt. di Elettronica a Telecomunicazioni, Universita degli Studi di Firenze, Italy; Dipt. di Elettronica a Telecomunicazioni, Universita degli Studi di Firenze, Italy; Dipt. di Elettronica a Telecomunicazioni, Universita degli Studi di Firenze, Italy","IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control","","2002","49","10","1444","1452","In this paper the architecture of a hardware and software platform, for ultrasonic investigation is presented. The platform, used in conjunction with an analog front-end hardware for driving the ultrasonic transducers of any commercial echograph, having the radiofrequency echo signal access, make it possible to dispose of a powerful echographic system for experimenting any processing technique, also in a clinical environment in which real-time operation mode is an essential prerequisite. The platform transforms any echograph into a test-system for evaluating the diagnostic effectiveness of new investigation techniques. A particular user interface was designed in order to allow a real-time and simultaneous visualization of the results produced in the different stages of the chosen processing procedure. This is aimed at obtaining a better optimization of the processing algorithm. The most important platform aspect, which also constitutes the basic differentiation with respect to similar systems, is the direct processing of the radiofrequency echo signal, which is essential for a complete analysis of the particular ultrasound-media interaction phenomenon. The platform completely integrates the architecture of a personal computer (PC) giving rise to several benefits, such as the quick technological evolution in the PC field and an extreme degree of programmability for different applications. The PC also constitutes the user interface, as a flexible and intuitive visualization support, and performs some software signal processing, by custom algorithms and commercial libraries. The realized close synergy between hardware and software allows the acquisition and real-time processing of the echographic radiofrequency (RF) signal with fast data representation.","0885-3010;1525-8955","","10.1109/TUFFC.2002.1041086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041086","","Hardware;Visualization;Radio frequency;Signal processing;Signal processing algorithms;Computer architecture;User interfaces;Ultrasonic transducers;Real time systems;Testing","biomedical ultrasonics;acoustic signal processing;medical signal processing;real-time systems;data visualisation","echographic radiofrequency signal;real-time processing;data visualization;hardware platform;software platform;ultrasonic imaging;clinical diagnosis;user interface;personal computer","Computer Graphics;Equipment Design;Image Enhancement;Image Enhancement;Microcomputers;Programming Languages;Signal Processing, Computer-Assisted;Software Design;Ultrasonography;Ultrasonography;User-Computer Interface","34","35","","","","","","IEEE","IEEE Journals & Magazines"
"Using the P/sup 3/T to guide the parallelization and optimization effort under the Vienna Fortran compilation system","T. Fahringer","Dept. of Software Technol. & Parallel Syst., Wien Univ., Austria","Proceedings of IEEE Scalable High Performance Computing Conference","","1994","","","437","444","Performance prediction of parallel programs is a key issue for the next generation of parallelizing compilers. The paper reports on experiences with the P/sup 3/T, an automatic parameter based performance prediction tool, which is based on an analytical performance model. It supports the Vienna Fortran Compilation System (VFCS) in parallelizing and optimizing Fortran programs for distributed memory parallel computers. The P/sup 3/T automatically computes at compile time a set of parallel program parameters which predict the outcome of three of the most crucial performance aspects of parallel programs: work distribution, communication overhead, and data locality. After analyzing the strengths and [imitations of the performance estimator, experiments are shown that demonstrate the ability of the P/sup 3/T to successfully guide both programmer and compiler in the search for efficient data distribution strategies and program transformations. It is shown that the P/sup 3/T detects not only crossover points of the goodness of different distribution, strategies but also undulations (or change in slope) of the performance curve for parallel programs.<<ETX>>","","0-8186-5680","10.1109/SHPCC.1994.296676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296676","","Concurrent computing;Distributed computing;Distribution strategy;Program processors;Performance analysis;Programming profession;Runtime;Software performance;Electronic mail;Costs","parallel programming;program compilers;FORTRAN;performance evaluation;Vienna development method;distributed memory systems;program testing","P/sup 3/T;optimization effort;Vienna Fortran compilation system;distributed memory parallel computers;parallel programs;parallelizing compilers;automatic parameter based performance prediction tool;parallel program parameters;work distribution;communication overhead;data locality;data distribution strategies;program transformations;crossover points;performance curve","","3","16","","","","","","IEEE","IEEE Conferences"
"Constraint-driven system partitioning","M. L. Lopez-Vallejo; J. Grajal; J. C. Lopez","ETSI Telecomunicacion, Univ. Politecnica de Madrid, Spain; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","411","416","This paper describes how optimization techniques can be applied to efficiently solve the constrained co-design problem. This is performed by the formulation of different cost functions which will drive the hardware-software partitioning process. The use of complex cost functions allows us to capture more aspects of the design. Besides, the appropriate formulation of this kind of functions has a great impact on the results that can be obtained regarding both quality and algorithm convergence rate. A strong point of the proposed formulation is its generality. Therefore, it does not depend on the problem and can be easily extended for considering new design constraints.","","0-7695-0537","10.1109/DATE.2000.840304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840304","","Partitioning algorithms;Cost function;Convergence;Software standards;Hardware;Simulated annealing;Genetic algorithms;Design optimization;Algorithm design and analysis","constraint handling;hardware-software codesign;logic partitioning","constraint-driven system partitioning;co-design problem;cost functions;hardware-software partitioning process;algorithm convergence rate;generality","","4","9","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of a perceptual ringing distortion metric for digital video","Z. Zhel; H. R. Wu; Z. Yu; T. Ferguson; D. Tan","Sch. of Comput. Sci. & Software Eng., Monash Univ., Vic., Australia; Sch. of Comput. Sci. & Software Eng., Monash Univ., Vic., Australia; NA; NA; NA","2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).","","2003","3","","III","613","This paper evaluates a perceptual impairment measure for ringing artifacts, which are common in hybrid MC/DPCM/DCT coded video, as a predictor of the mean opinion score (MOS) obtained in the standard subjective assessment. The perceptual ringing artifacts measure is based on a vision model and a ringing distortion region segmentation algorithm, which is converted into a new perceptual ringing distortion metric (PRDM) on a scale of 0 to 5. This scale corresponds to a modified double-stimulus impairment scale variant II (DSIS-II) method. The Pearson correlation, the Spearman rank order correlation and the average absolute error are used to evaluate the performance of the PRDM compared with the subjective test data. The results show a strong correlation between the PRDM and the MOS with respect to ringing artifacts.","1520-6149","0-7803-7663","10.1109/ICASSP.2003.1199549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199549","","Distortion measurement;Video coding;Gain control;Australia;Discrete cosine transforms;Quality assessment;Pulse modulation;Humans;Computer science;Software engineering","video coding;distortion;differential pulse code modulation;discrete cosine transforms;transform coding;image segmentation","performance evaluation;perceptual ringing distortion metric;digital video quality;perceptual impairment measure;ringing artifacts;hybrid MC/DPCM/DCT coded video;mean opinion score;MOS;vision model;ringing distortion region segmentation algorithm;modified DSIS-II method;double-stimulus impairment scale variant II;Pearson correlation;Spearman rank order correlation;average absolute error","","","16","","","","","","IEEE","IEEE Conferences"
"Development and testing of a reliability performance index for modular robotic systems","D. L. Schneider; D. Tesar; J. W. Barnes","USAF Inst. of Technol., Wright-Patterson AFB, OH, USA; NA; NA","Proceedings of Annual Reliability and Maintainability Symposium (RAMS)","","1994","","","263","271","One issue pertaining to modular robotic systems is the integration of modules into a fully functional robot system. Two criteria for modules integration not previously investigated are the reliability and accuracy of the system. This paper presents the results of the development of a framework for a ""criterion"" embodying these two modular robot characteristics. Using a probabilistic representation of manipulator kinematics and a reliability block diagram model of the manipulator system, a reliability performance index (RPI) representing the probability of no hardware or software failure and the manipulator achieving a specified position and orientation is developed. The RPI is tested with a case study consisting of a three degree-of-freedom planar manipulator assembled from a choice of six joint modules of varying reliability and precision and a choice of six link module combinations of varying lengths and machining tolerances. A straight-line, square trajectory is specified and the RPI is calculated for each combination of joint modules and links, a total of 1296 different combinations. An analysis of variance (ANOVA) is performed on the results using the different joint locations and link options as factors and the different joint modules and link options as factor levels. The different factors are tested for significance and the Tukey Studentized Range Test is performed to determine significance of the different joint modules and link options. Using this statistical testing, a 70% reduction in the module design space is achieved using the RPI. Optimization using other appropriate manipulator criteria can then be performed to generate the final configuration. Additional extensive case studies are needed to fully develop the RPI to a stage necessary for implementation into a computer-aided design system for modular robot configuration design. The RPI may also be useful in the quantification of the overall system reliability and performance of any system based upon measured error, such as control systems.<<ETX>>","","0-7803-1786","10.1109/RAMS.1994.291118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=291118","","Testing;Performance analysis;Economic indicators;Manipulators;Robots;Analysis of variance;Kinematics;Hardware;Software performance;Assembly","reliability;statistical analysis;kinematics;performance index","reliability performance index;modular robotic systems;fully functional robot system;accuracy;probabilistic representation;manipulator kinematics;reliability block diagram model;three degree-of-freedom planar manipulator;straight-line square trajectory;analysis of variance;ANOVA;Tukey Studentized Range Test;joint modules;link options;statistical testing;configuration design","","10","17","","","","","","IEEE","IEEE Conferences"
"A symbolic optimization approach for tuning of PID controllers","C. -. Calistru","Dept. of Autom. Control & Ind. Inf., Iasi Univ., Romania","Proceedings of International Conference on Control Applications","","1995","","","174","175","The paper presents an approach of tuning PID controllers using the integral criteria and symbolic calculus facilities offered by one of the most powerful symbolic calculus packages, MATHEMATICA. The approach, called SYMBOLIC-TUNE, considers the Krasovschii-Pospelov formulae. Different controller types and different plants are considered. Illustrative results obtain by simulation are presented.","","0-7803-2550","10.1109/CCA.1995.555663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=555663","","Three-term control;Automatic control;Error correction;Bismuth;Transfer functions;Laplace equations;Nonlinear systems;System testing;Calculus;Control engineering","tuning","symbolic optimization;PID controller tuning;symbolic calculus;software package;MATHEMATICA;SYMBOLIC-TUNE;Krasovschii-Pospelov formulae","","6","8","","","","","","IEEE","IEEE Conferences"
"Optimizing parallel applications for wide-area clusters","H. E. Bal; A. Plaat; M. G. Bakker; P. Dozy; R. F. H. Hofman","Dept. of Math. & Comput. Sci., Vrije Univ., Amsterdam, Netherlands; NA; NA; NA; NA","Proceedings of the First Merged International Parallel Processing Symposium and Symposium on Parallel and Distributed Processing","","1998","","","784","790","Recent developments in networking technology cause a growing interest in connecting local area clusters of workstations over wide area links, creating multilevel clusters, or metacomputers. Often, latency and bandwidth of local area and wide area networks differ by two orders of magnitude or more. One would expect only very coarse grain applications to achieve good performance. To test this intuition, we analyze the behavior of several existing medium-grain applications on a wide-area multicluster. We find that high performance can be obtained if the programs are optimized to take the multilevel network structure into account. The optimizations reduce intercluster traffic and hide intercluster latency, and substantially improve performance on wide area multiclusters. As a result, the range of metacomputing applications is larger than previously assumed.","1063-7133","0-8186-8404","10.1109/IPPS.1998.670017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670017","","Computer networks;Biology computing;Testing;Application software;Concurrent computing;Local area networks;Wide area networks;Delay;Computer vision;Physics computing","optimisation;parallel algorithms;wide area networks;performance evaluation;local area networks","parallel applications;local area clusters;workstation clusters;wide area networks;multilevel clusters;metacomputers;latency;bandwidth;coarse grain applications;performance;medium grain applications;wide area multicluster;program optimization;multilevel network structure;intercluster traffic;metacomputing applications","","12","14","","","","","","IEEE","IEEE Conferences"
"Statistical approach to the optimization of optical fiber fusion splicing in the field","E. Serafini","SIRTI SpA, Milan, Italy","Journal of Lightwave Technology","","1989","7","2","431","435","The optimization of fusion splicing in the field is analyzed and a method for interpretation laboratory splicing data is presented. Two main parameters have been recognized to guide field splicing in long-haul applications: (a) maximum permissible splice loss and (b) maximum number of trials. The influence of these parameters on the average splice loss and on the total number of splices of a well-defined splicing system is illustrated. Computer simulations, based on laboratory test, have been performed to choose the optimum values for the two above-mentioned parameters to minimize the achievable mean splice loss and the number of splices to be made in actual installations.<<ETX>>","0733-8724;1558-2213","","10.1109/50.17790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=17790","","Optical fibers;Splicing;Optical losses;Optical fiber cables;Laboratories;Performance evaluation;Optical fiber losses;Loss measurement;Optimization methods;Application software","data analysis;optical fibres;optical workshop techniques","computer simulations;fusion splicing;long-haul applications;splice loss","","1","6","","","","","","IEEE","IEEE Journals & Magazines"
"Two case studies in optimization-based computer-aided design of control systems","M. K. H. Fan; C. D. Walrath; C. Lee; A. L. Tits; W. T. Nye; M. Rimer; R. Grant; W. S. Levine","University of Maryland, College Park; Westinghouse Defence and Electronics Center, Baltimore, Md; University of Maryland, College Park; University of Maryland, College Park; University of California, Berkeley; Grumman Aerospace Corporation, Bethpage, N.Y.; Grumman Aerospace Corporation, Bethpage, N.Y.; University of Maryland, College Park","1985 24th IEEE Conference on Decision and Control","","1985","","","1794","1794","There have been many approaches proposed for the computer-aided design of control systems. The co-authors of this paper include several strong proponents of a design methodology emphasizing designer's intuition, man-machine interaction and sophisticated optimization techniques [1]. This methodology has been implemented by our group as part of the DELIGHT MaryLin system [2, 3], an offshoot of Berkeley's DELIGHT system [4, 5]. A logical way to test this approach to computer-aided design is to apply DELIGHT. MaryLin to a number of real control system design problems and see how well it performs. This paper describes just such applications to two control problems supplied by industry. In both cases, the problems had been previously solved by other techniques. Thus, we are able to compare the solution obtained with DELIGHT. MaryLin to solutions that are acceptable in actual practice.","","","10.1109/CDC.1985.268869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4048628","","Design optimization;Design automation;Control systems;Control system synthesis;Aerospace control;Military computing;Military aircraft;Application software;Design engineering;Systems engineering and theory","","","","1","8","","","","","","IEEE","IEEE Conferences"
"Early community building: a critical success factor for XP projects","G. Broza","Ind. Logic (Canada), Toronto, Ont., Canada","Agile Development Conference (ADC'05)","","2005","","","167","172","Extreme programming (XP) literature and discussions often view successful projects only as customer-driven product development: planning, coding and testing an unfolding series of prioritized units of vertical functionality. I claim, however, that a successful project also requires a prospering community, comprising an introspective group of committed professionals communicating effectively, and using a well-understood, stable process. Weakness on any of these fronts presents a high risk of failure; therefore, I advise every XP project's members to actively engage in building their community, such that it reaches its critical level of development already by the first internal release. To help in this endeavor, I provide a comprehensive list of activities and attitudes to practice and avoid during the first release.","","0-7695-2487","10.1109/ADC.2005.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609817","XP;community building;first release;risk","Programming profession;Logic programming;Logic testing;Project management;Collaboration;Gas insulated transmission lines;Functional programming;Product development;Buildings;Software systems","software engineering","early community building;extreme programming;customer-driven product development;product planning;product testing","","","19","","","","","","IEEE","IEEE Conferences"
"ATRI program at UCD","N. C. Luhmann","Dept. of Appl. Sci., California Univ., Davis, CA, USA","International Conference on Plasma Science (papers in summary form only received)","","1995","","","112","","Summary form only given. The Advanced Thermionics Research Initiative Program (ATRI 2000) is now established at the University of California at Davis. Graduate students continue to receive fellowships as they train in the field of thermionics and microwave technology. Being in the Bay Area, the program benefits from the synergism between the microwave tube industries, SLAG, LLNL and LBNL, and the excellent faculties at UC Berkeley, Stanford and UC Davis. The primary charter of ATRI is still to support basic multidisciplinary graduate research in thermionics with an aim at providing engineers and scientists for the strategic microwave tube industry, to incorporate new technology and methods into the field, and to transfer that technology and disseminate information. The philosophy of the ATRI research program is to first understand a new mechanism on an analytical level, then use available computer software or codes developed by the students to numerically model the processes before testing and optimizing the device in the laboratory. Using this procedure, we have made significant contributions to the science of high power microwave generation. By having a strong research program in innovative rf devices, such as gyrotron amplifiers, photocathode rf linacs, free electron lasers, reaccelerated-electron-beam klystrons and also conventional TWTs, the students are exposed to the relevant science and technology in an active way.","0730-9244","0-7803-2669","10.1109/PLASMA.1995.531463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=531463","","Microwave generation;Microwave tubes;Gyrotrons;Photocathodes;Free electron lasers;Klystrons;Traveling wave tubes;Electronics engineering education","thermionic tubes;microwave generation;electron tube testing;microwave tubes;gyrotrons;photocathodes;free electron lasers;klystrons;travelling wave tubes;research initiatives;electronic engineering education","ATRI 2000;Advanced Thermionics Research Initiative Program;thermionics;microwave technology;microwave tube industries;SLAG;LLNL;LBNL;strategic microwave tube industry;analytical level;computer software;computer codes;numerical modelling;testing;optimization;high power microwave generation;RF devices;gyrotron amplifiers;photocathode RF linacs;free electron lasers;reaccelerated-electron-beam klystrons;TWT","","","","","","","","","IEEE","IEEE Conferences"
"GBS IP multicast tunneling","L. Allen; C. Murray; M. DiFrancisco; C. Ellis; S. Hughes; M. Blanding","Booz, Allen & Hamilton Inc., McLean, VA, USA; NA; NA; NA; NA; NA","MILCOM 2000 Proceedings. 21st Century Military Communications. Architectures and Technologies for Information Superiority (Cat. No.00CH37155)","","2000","2","","699","703 vol.2","Internet protocol (IP) multicast over the DoD's Global Broadcast Service (GBS) provides a one-way communications path to the warfighter as part of an interactive communications system. GBS provides the capability to deliver information products of varying size, timeliness requirements, and security levels. The products share satellite resources based on CINC/JTF commanders priorities, operational locations, availability of the satellite resources, and platform capabilities of the deployed users. Currently, IP multicast (IPMC) is not supported by the Defense Information Infrastructure (DII). This document defines a real-time IP solution for tunneling multicast data across the Defense Information Services Network (DISN), through GBS components, and to the end-user LANs. This paper describes a cost-effective, operationally sound, incremental approach for rapid prototyping and integration of these technologies. It outlines each of the implementation approaches researched to date, the designs selected for systematic testing, and an overview of the plan for testing selected designs. The initial architecture is based on commercial-off-the-shelf IP routers, ATM switches, multicast software, and VPN techniques.","","0-7803-6521","10.1109/MILCOM.2000.904019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904019","","Tunneling;Satellite broadcasting;Multicast protocols;System testing;Web and internet services;Communication system security;Information security;Availability;Software prototyping;Prototypes","transport protocols;LAN interconnection;information networks;military communication;telecommunication network routing;multicast communication;asynchronous transfer mode;packet switching;interactive systems;broadcasting;direct broadcasting by satellite;military computing","GBS IP multicast tunneling;DoD;Global Broadcast Service;one-way communications path;interactive communications system;information products;security levels;satellite resource sharing;operational locations;satellite resource availability;Defense Information Infrastructure;DII;real-time IP solution;multicast data tunneling;Defense Information Services Network;DISN;LAN;rapid prototyping;systematic testing;commercial-off-the-shelf IP routers;ATM switches;multicast software;VPN techniques","","","3","","","","","","IEEE","IEEE Conferences"
"The research based on the design of a 1.3GHz cascode LNA","Ma Hongbo; Feng Quanyuan; Gong Kunlin","Inst. of Microelectron., Southwest Jiaotong Univ., Chengdu, China; Inst. of Microelectron., Southwest Jiaotong Univ., Chengdu, China; Inst. of Microelectron., Southwest Jiaotong Univ., Chengdu, China","2005 Asia-Pacific Microwave Conference Proceedings","","2005","2","","4 pp.","","A 1.3GHz cascode low noise amplifier based on 0.25/spl mu/m CMOS technology has been successfully designed in this paper. From the aspect of noise optimization, gain and impedance match, the design methodology for LNA is analyzed in detail, the influence of capacitance C/sub gd/, C/spl I.bar/match/spl I.bar/in, and W/sub 2/ on LNA is also discussed, whose noise factor and S-parameter is simulated and tested by the ADS software. The result has argued that the good NF of 1.42dB, moderate gain of 13.687dB, acceptable S11 of -14.769dB, S22 of -14.530dB, S12 of -52.955dB are achieved under the frequency of 1.3GHz.","2165-4727;2165-4743","0-7803-9433","10.1109/APMC.2005.1606469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1606469","","CMOS technology;Low-noise amplifiers;Design optimization;Impedance;Design methodology;Capacitance;Scattering parameters;Analytical models;Software testing;Noise measurement","low noise amplifiers;UHF amplifiers;CMOS integrated circuits;S-parameters;capacitance","cascode LNA;low noise amplifier;CMOS technology;noise optimization;gain;impedance match;noise factor;S-parameter;1.3 GHz;13.687 dB;1.42 dB;0.25 micron","","1","5","","","","","","IEEE","IEEE Conferences"
"Synthesizing self-testable filters via scaling and redundant operator elimination","L. Goodby; A. Orailoglu","Dept. of Electr. & Comput. Eng., California Univ., San Diego, La Jolla, CA, USA; NA","Conference Record of The Twenty-Ninth Asilomar Conference on Signals, Systems and Computers","","1995","1","","132","136 vol.1","A synthesis-based approach to improving the testability of digital filters is presented, with the aim of producing designs that achieve very high fault coverage under low-overhead built-in self-test methodologies. The synthesis-based approach permits high coverages to be achieved without the addition of special test hardware or other manipulation of the gate-level netlist. The testability of a design is enhanced at the register-transfer level (RTL), prior to synthesis. Using scaling as a redundancy elimination technique, it is possible to reduce the area required by a design, as well as identify further redundancies that can be eliminated through the automatic selection of optimized RTL structures drawn from a parameterized VHDL library.","1058-6393","0-8186-7370","10.1109/ACSSC.1995.540527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540527","","Built-in self-test;Redundancy;Signal design;Finite impulse response filter;Automatic testing;Digital signal processing;Signal synthesis;Logic testing;Adders;Design engineering","design for testability;digital filters;hardware description languages;automatic test software","self-testable filters synthesis;redundant operator elimination;digital filters testability;very high fault coverage;low-overhead built-in self-test;synthesis-based approach;gate-level netlist;design testability;register-transfer level;scaling;automatic selection;parameterized VHDL library","","4","13","","","","","","IEEE","IEEE Conferences"
"Evolutionary computer controlled design of a reluctance motor drive system","K. S. Chai; C. Pollock","Dept. of Eng., Leicester Univ., UK; Dept. of Eng., Leicester Univ., UK","38th IAS Annual Meeting on Conference Record of the Industry Applications Conference, 2003.","","2003","3","","1480","1487 vol.3","This paper demonstrates the benefits of employing computer-automated design methods in design and optimization of electrical machines and drives. The proposed design and optimization software for a reluctance motor and drive has been used to design a complete flux switching motor without human intervention. The capability of accurately predicting dynamic performances of the full system (motor and drive) allows a genetic algorithm to confidently adjust the design within reasonable boundaries while steadily improving the fitness function of the designs. The motor and drive has been constructed and test results have verified the accuracy of the simulation. The motor and drive as designed has delivered superior performance to comparable motors of the same frame size.","","0-7803-7883","10.1109/IAS.2003.1257752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1257752","","Control systems;Reluctance motors;Drives;Design methodology;Design optimization;Algorithm design and analysis;Software design;Humans;Genetic algorithms;Testing","reluctance motor drives;machine control;magnetic flux;finite element analysis;electric machine analysis computing;switching;genetic algorithms","evolutionary computer controlled design;reluctance motor drive system;optimization software;design software;reluctance motor drive;flux switching motor;human intervention;genetic algorithm;fitness function;FEM software","","5","13","","","","","","IEEE","IEEE Conferences"
"Optimizing the performance of a surface mount placement machine","K. P. Ellis; F. J. Vittes; J. E. Kobza","Grado Dept. of Ind. & Syst. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; NA; NA","IEEE Transactions on Electronics Packaging Manufacturing","","2001","24","3","160","170","Process planning is an important and integral part of effectively operating a printed circuit board (PCB) assembly system. A PCB assembly system generally consists of different types of placement machines, testing equipment, and material handling equipment. This research develops a new solution approach to determine the component placement sequence and feeder arrangement for a turret style surface mount-placement machine often used in PCB assembly systems. This solution approach can be integrated into a process planning system to reduce assembly time and improve productivity. The algorithm consists of a construction procedure that uses a set of rules to generate an initial component placement sequence and feeder arrangement along with an improvement procedure to improve the initial solution. An industrial case study conducted at Ericsson, Inc., using a Fuji CP4-3 machine and actual PCB data, is presented to demonstrate the performance of the proposed solution approach. The solutions obtained using the proposed solution approach are compared to those obtained using state of the art PCB assembly process optimization software. For all PCBs in the case study, the proposed solution approach yielded lower placement times than the commercial software, thus generating additional valuable production capacity. This research is applicable for both researchers and practitioners in printed circuit board assembly systems.","1521-334X;1558-0822","","10.1109/6104.956801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=956801","","Assembly systems;Process planning;Printed circuits;Circuit testing;Materials testing;System testing;Materials handling equipment;Productivity;Construction industry;Machinery production industries","assembly planning;printed circuit manufacture;surface mount technology;optimisation","assembly time;process planning;PCB assembly system;component placement sequence;feeder arrangement;turret style surface mount-placement machine;productivity;construction procedure;initial component placement sequence;Ericsson;Fuji CP4-3 machine;placement times;production capacity","","36","27","","","","","","IEEE","IEEE Journals & Magazines"
"Constrained optimisation procedure for evaluating cyclic loading of power transformers","T. T. Nguyen","Dept. of Electr. & Electron. Eng., Western Australia Univ., Nedlands, WA, Australia","IEE Proceedings - Generation, Transmission and Distribution","","1995","142","3","240","246","The paper reports new developments in computational methods for evaluating the normal cyclic rating (NCR) and long-term emergency rating (LTER) of transformers. It makes two major improvements to the procedures given in the international standard in the subject: IEC354. One improvement derives from an interpretation of NCR and LTER formulations as a constrained optimisation procedure. Starting from a given load profile, the procedure maximises transformer loading levels subject to constraints formed from maximum values of permissible internal temperatures and of a defined insulation ageing factor. The second advance is based on a separation of variables in the evaluation of the integral which gives the relative ageing factor. This measure makes major savings in the computing overheads of practical evaluations. The two developments together lead to reliable procedures for NCR or LTER evaluations in practice as encountered in a power authority or utility. They can be relied on to give a solution without user intervention. Convergence to a solution is independent of the transformer thermal characteristics relevant to rating studies and of the loading profiles for which they are carried out. The results summarise some of the studies used in validating a comprehensive software system that implements the procedures which the paper reports. The new procedures have been tested and compared with existing procedures for many practical cases and found to achieve significant advantage, sometimes dramatically so.<<ETX>>","1350-2360","","10.1049/ip-gtd:19951746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=388356","","Aging;Optimization methods;Power transformer insulation;Power transformers;Standards","ageing;optimisation;power transformer insulation;power transformers;standards","cyclic loading;power transformers;constrained optimisation procedure;normal cyclic rating;long-term emergency rating;IEC354;international standard;load profile;internal temperatures;insulation ageing factor;relative ageing factor;software system","","2","","","","","","","IET","IET Journals & Magazines"
"An optimal algorithm for scheduling soft aperiodic tasks in dynamic-priority preemptive systems","I. Ripoll; A. Crespo; A. Garcia-Fornes","Dept. de Ingenieria de Sistemas, Comput. y Autom., Univ. Politecnica de Valencia, Spain; NA; NA","IEEE Transactions on Software Engineering","","1997","23","6","388","400","The paper addresses the problem of jointly scheduling tasks with both hard and soft real time constraints. We present a new analysis applicable to systems scheduled using a priority preemptive dispatcher, with priorities assigned dynamically according to the EDF policy. Further, we present a new efficient online algorithm (the acceptor algorithm) for servicing aperiodic work load. The acceptor transforms a soft aperiodic task into a hard one by assigning a deadline. Once transformed, aperiodic tasks are handled in exactly the same way as periodic tasks with hard deadlines. The proposed algorithm is shown to be optimal in terms of providing the shortest aperiodic response time among fixed and dynamic priority schedulers. It always guarantees the proper execution of periodic hard tasks. The approach is composed of two parts: an offline analysis and a run time scheduler. The offline algorithm runs in pseudopolynomial time O(mn), where n is the number of hard periodic tasks and m is the hyperperiod/min deadline.","0098-5589;1939-3520;2326-3881","","10.1109/32.601081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601081","","Scheduling algorithm;Dynamic scheduling;Processor scheduling;Real time systems;Testing;Delay;Timing;Computer Society;Runtime","scheduling;real-time systems;minimisation;computational complexity;distributed algorithms","optimal algorithm;soft aperiodic task scheduling;dynamic priority preemptive systems;soft real time constraints;priority preemptive dispatcher;EDF policy;online algorithm;acceptor algorithm;aperiodic work load servicing;hard deadlines;aperiodic response time;dynamic priority schedulers;periodic hard tasks;offline analysis;run time scheduler;pseudopolynomial time;earliest deadline first","","24","13","","","","","","IEEE","IEEE Journals & Magazines"
"An intelligent FFT-analyzer","G. Betta; M. D'Apuzzo; C. Liguori; A. Pietrosanto","Dept. of Ind. Eng., Cassino Univ., Italy; NA; NA; NA","IEEE Transactions on Instrumentation and Measurement","","1998","47","5","1173","1179","An intelligent FFT-analyzer capable of adapting its operating parameters on the basis of the signal spectrum was set up and characterized. The realized instrument is based on a parameter optimization procedure which provides the instrument an autoconfiguration capability. It was implemented on a multiple processor DSP architecture in order to achieve a real-time behavior. The experimental tests carried out on a large number of signals highlight the instrument capability of correctly detecting tones with a good frequency resolution for any signal spectrum type.","0018-9456;1557-9662","","10.1109/19.746578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=746578","","Signal processing algorithms;Instruments;Signal analysis;Algorithm design and analysis;Spectral analysis;Frequency;Application software;Associate members;Digital signal processing;Computer architecture","spectral analysers;digital instrumentation;automatic test software;fast Fourier transforms;digital signal processing chips;analogue-digital conversion","intelligent FFT-analyzer;signal spectrum;parameter optimization procedure;autoconfiguration capability;multiple processor DSP architecture;real-time behavior;frequency resolution;automatic test software","","20","23","","","","","","IEEE","IEEE Journals & Magazines"
"Generating synchronous timed descriptions of digital receivers from dynamic data flow system level configuration","P. Zepter; T. Grotker","Aachen Univ. of Technol., Germany; Aachen Univ. of Technol., Germany","Proceedings of European Design and Test Conference EDAC-ETC-EUROASIC","","1994","","","672","","The system and architecture design of digital receivers for high to medium throughput communication links and similar signal processing hardware has special characteristics. The algorithm development on the system level is performed using a data flow driven simulation tool. To shorten the turn-around time in the joint optimization of algorithm and architecture we developed the concept of a tool and library support for a smooth direct transition from the untimed system level dynamic data flow specification and simulation to a synchronous timed ASIC implementation using a hardware description language. Multiple and dynamic data rates can be converted. To reuse design knowledge a library of several generic implementations is provided, which should allow to cover various trade-offs for the particular functions. The timing interface of the library allows for introduction of timing and implementation-dependent information in terms of data rates, iteration intervals (i.e. the number of clock cycles between two data items), port related latencies and control conditions (for models with dynamic rates). The system incorporates algorithms for checking whether the system is consistent and deadlock-free as well as the computation of the arrival times for the data items on the edges. The main task in the implementation of the dynamic data flow is the automatic creation of the gated clock or control signal system to enable the correct setting of algorithmic states in the system. Furthermore algorithms for detecting the registers representing the algorithmic states (which are different from pipeline registers when dealing with dynamic subgraphs) and deciding on the feasibility of the implementation have been developed.<<ETX>>","","0-8186-5410","10.1109/EDTC.1994.326923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326923","","Synchronous generators;Signal processing algorithms;Libraries;Timing;Clocks;Automatic control;Signal design;Throughput;Digital signal processing;Hardware","digital communication systems;receivers;application specific integrated circuits;specification languages;parallel processing;signal processing equipment;data communication equipment;circuit CAD;software tools;signal processing","synchronous timed descriptions;digital receivers;dynamic data flow;architecture design;communication links;signal processing hardware;data flow driven simulation tool;turn-around time;joint optimization;smooth direct transition;synchronous timed ASIC implementation;hardware description language;timing interface;data rates;iteration intervals;lock cycles;port related latencies;control conditions;deadlock-free;arrival times;gated clock","","6","3","","","","","","IEEE","IEEE Conferences"
"A multi-application FFT analyzer based on a DSP architecture","G. Betta; C. Liguori; M. D'Apuzzo","Dept. of Autom., Electromagn., Inf. Eng. & Ind. Math., Cassino Univ., Italy; NA; NA","IMTC/99. Proceedings of the 16th IEEE Instrumentation and Measurement Technology Conference (Cat. No.99CH36309)","","1999","2","","713","719 vol.2","A DSP-based FFT analyzer, capable of adapting its behavior on the basis of the specific application requirements was set up. A suitably designed user-interface allows the specific application to be chosen and its main parameters to be defined. Then an automatic optimization procedure organizes both the hardware and with the aim of satisfying the user requirements. The implementation on a multi-DSP architecture allows a real-time behavior to be assured. The experimental tests carried out highlight the instrument capability to optimally operate in a large number of typical applications.","1091-5281","0-7803-5276","10.1109/IMTC.1999.776961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=776961","","Digital signal processing;Application software;Signal analysis;Hardware;Instruments;Frequency;Signal processing algorithms;User interfaces;Computer architecture;Algorithm design and analysis","spectral analysers;fast Fourier transforms;computerised instrumentation;digital signal processing chips;user interface management systems;automatic test software","multi-application FFT analyzer;DSP architecture;specific application requirements;user-interface design;automatic optimization procedure;hardware organization;implementation;real-time behavior;intelligent behavior;software upgrade;development environments;reconfigurability argument;software modules;tone detection","","1","24","","","","","","IEEE","IEEE Conferences"
"IF sampling 4/sup th/ order bandpass /spl Delta//spl Sigma/ modulator for digital receiver applications","J. F. Jensen; A. E. Cosand; H. C. Choe","HRL Labs., LLC, Malibu, CA, USA; HRL Labs., LLC, Malibu, CA, USA; NA","25th Annual Technical Digest 2003. IEEE Gallium Arsenide Integrated Circuit (GaAs IC) Symposium, 2003.","","2003","","","200","203","Bandpass modulators sampling at high IFs (/spl sim/200 MHz) allows direct sampling of the RF signal, reducing analog hardware, and makes it easier to realize completely software programmable receivers. This paper presents the circuit design of and test results from a continuous time tunable IF sampling 4/sup th/ order bandpass delta-sigma modulator implemented in InP HBT IC technology for use in a multi-mode digital receiver application. The bandpass /spl Delta//spl Sigma/ modulator is fabricated in AlInAs/GaInAs heterojunction bipolar technology with a peak unity current gain cutoff frequency (f/sub T/) of 130 GHz and a maximum frequency of oscillation (f/sub MAX/) of 130 GHz. The 4/sup th/ order bandpass /spl Delta//spl Sigma/ modulator consists of two bandpass resonators that can be tuned to optimize both wideband and narrowband operation. The IF is tunable from 140 MHz to 210 MHz in this /spl Delta//spl Sigma/ modulator for use in multiple platform applications. Operating from /spl plusmn/5 V power supplies, the fabricated 4/sup th/ order /spl Delta//spl Sigma/ modulator sampling at 4 GSPS demonstrates stable behavior and achieves a signal to (noise+distortion) ratio (SNDR) of 78 dB @ 1 MHz BW and 50 dB @ 60 MHz BW. The average SNDR performance measured on over 250 parts is 72.5 dB @ 1 MHz BW and 47.7 dB @ 60 MHz BW.","1064-7775","0-7803-7833","10.1109/GAAS.2003.1252394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252394","","Sampling methods;Delta modulation;Circuit testing;Tunable circuits and devices;Cutoff frequency;Modulation coding;Hardware;Circuit synthesis;Integrated circuit testing;Digital modulation","software radio;delta-sigma modulation;bipolar integrated circuits;band-pass filters;circuit tuning;aluminium compounds;indium compounds;gallium compounds;III-V semiconductors","software radios;IF sampling /spl Delta//spl Sigma/ modulator;bandpass /spl Delta//spl Sigma/ modulator;multimode digital receivers;direct RF signal sampling;software programmable receivers;continuous time tunable delta-sigma modulator;HBT IC technology;bandpass resonators;wideband operation;narrowband operation;tunable IF;130 GHz;140 to 210 MHz;5 V;-5 V;1 MHz;60 MHz;AlInAs-GaInAs","","","4","","","","","","IEEE","IEEE Conferences"
"Target architecture oriented high-level synthesis for multi-FPGA based emulation","O. Bringmann; C. Menn; W. Rosenstiel","FZI, Karlsruhe, Germany; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","326","332","This paper presents a new approach on combined high-level synthesis and partitioning for FPGA-based multi-chip emulation systems. The goal is to synthesize a prototype with maximal performance under the given area and interconnection constraints of the target architecture. Interconnection resources are handled similarly to functional resources, enabling the scheduling and the sharing of inter-chip connections according to their delay. Moreover, data transfer serialization is performed completely or partially, depending on the mobility of the data transfers, in order to satisfy the given interconnection constraints. In contrast to conventional partitioning approaches, the constraints of the target architecture are fulfilled by construction.","","0-7695-0537","10.1109/DATE.2000.840291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840291","","High level synthesis;Emulation;Integrated circuit interconnections;Prototypes;Delay;Hardware;Circuit synthesis;Software prototyping;Hip;Acceleration","high level synthesis;field programmable gate arrays;logic partitioning;integrated circuit interconnections;scheduling;circuit optimisation","target architecture oriented synthesis;high-level synthesis;multi-FPGA based emulation;partitioning;interconnection constraints;area constraints;scheduling;inter-chip connections;data transfer serialization;mobility","","3","18","","","","","","IEEE","IEEE Conferences"
"Faster min-cut computation in unweighted hypergraphs/circuit netlists","Wai-Kei Mak","Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan","2005 IEEE VLSI-TSA International Symposium on VLSI Design, Automation and Test, 2005. (VLSI-TSA-DAT).","","2005","","","67","70","A hypergraph minimum cut algorithm which is the fastest algorithm to date for computing a minimum cut in a weighted hypergraph was presented in W. K. Mak and D. F. Wong (2000). It is based on the repeated application of a special minimum s-t cut procedure. Unlike most minimum s-t cut procedures, it does not rely on any flow computation. And it runs in O(p + nlogn) time for a general weighted hypergraph where p is the total number of terminals of all hyperedges and n is the number of nodes in the input hypergraph. In this paper, we reduce its running time to O(p) if the input hypergraph is unweighted. Since a circuit netlist is an unweighted hypergraph where p = O(n) typically, this improvement can reduce the running time in circuit partitioning application significantly.","","0-7803-9060","10.1109/VDAT.2005.1500022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1500022","","Circuits;Partitioning algorithms;Computer science;Application software;Algorithm design and analysis;Computer networks;Ear;Data structures;Terminology","circuit analysis computing;circuit optimisation;graph theory","min-cut computation;unweighted hypergraphs/circuit netlists;hypergraph minimum cut algorithm;minimum s-t cut;circuit partitioning","","","12","","","","","","IEEE","IEEE Conferences"
"High frequency piezo-composite transducer array designed for ultrasound scanning applications","A. Nguyen-Dinh; L. Ratsimandresy; P. Mauchamp; R. Dufait; A. Flesch; M. Lethiecq","VERMON, Tours, France; NA; NA; NA; NA; NA","1996 IEEE Ultrasonics Symposium. Proceedings","","1996","2","","943","947 vol.2","A 20 MHz high density linear array transducer is presented in this paper, This array has been developed using an optimized ceramic-polymer composite material. The electro-mechanical behaviour of this composite, especially designed for high frequency applications, is characterised and the results are compared to theoretical predictions. To support this project, a new method of transducer simulation has been implemented. This simulation software takes into account the elementary boundary phenomena and allows prediction of inter-element coupling modes in the array. The model also yields realistic computed impulse responses of transducers, A miniature test device and water tank have been constructed to perform elementary acoustic beam pattern measurements. It is equipped with highly accurate motion controls and a specific needle-shaped target has been developed. The smallest displacement available in the three main axes of this system is 10 microns. The manufacturing of the array transducer has involved high precision dicing and micro interconnection techniques. The flexibility of the material provides us with the possibility of curving and focusing the array transducer. Performance of this experimental array are discussed and compared to the theoretical predictions. The results demonstrate that such array transducers will allow high quality near field imaging. This work presents the efforts to extend the well known advantages of composite piezoelectric transducers to previously unattainable frequencies.","1051-0117","0-7803-3615","10.1109/ULTSYM.1996.584147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584147","","Frequency;Acoustic transducers;Piezoelectric transducers;Computational modeling;Composite materials;Application software;Predictive models;Acoustic testing;Impulse testing;Performance evaluation","ultrasonic transducer arrays;ultrasonic materials testing;filled polymers;lead compounds;piezoceramics;piezoelectric transducers;ultrasonic measurement;ultrasonic imaging","piezo-composite transducer array;ultrasound scanning applications;linear array transducer;electro-mechanical behaviour;transducer simulation;elementary boundary phenomena;inter-element coupling modes;computed impulse responses;acoustic beam pattern measurements;micro interconnection techniques;near field imaging;ceramic-polymer composite material;PZT ceramic hard epoxy composite;20 MHz;PZT;PbZrO3TiO3","","18","5","","","","","","IEEE","IEEE Conferences"
"Proceedings International Parallel and Distributed Processing Symposium","","","Proceedings International Parallel and Distributed Processing Symposium","","2003","","","","","The following topics are dealt with: Grid and distributed computing; scheduling task systems; shared-memory multiprocessors; imaging and visualization; testing and debugging; performance analysis and real-time systems; scheduling for heterogeneous resources; networking; peer-to-peer and mobile computing; compiler technology and run-time systems; load balancing; network routing; parallel programming models; parallel algorithms; scheduling and storage; parallel and distributed performance; software for high performance clusters; decentralized algorithms; multithreading and VLIW; parallel and distributed real-time systems; high-level parallel programming models and supportive environments; Java for parallel and distributed computing; nature inspired distributed computing; high performance computational biology; advances in parallel and distributed computational models; reconfigurable architectures; communication architecture for clusters; next generation systems; fault-tolerant parallel and distributed systems; wireless, mobile and ad hoc networks; parallel and distributed image processing, video processing, and multimedia; formal methods for parallel programming; Internet computing and e-commerce; parallel and distributed scientific and engineering computing with applications; massively parallel processing; performance modeling, evaluation, and optimization of parallel and distributed systems; and parallel and distributed systems: testing and debugging.","1530-2075","0-7695-1926","10.1109/IPDPS.2003.1213072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213072","","Processor scheduling;Shared memory systems;Software debugging;Software testing;Real time systems;Program compilers;Resource management;Parallel programming;Parallel algorithms;Memory management;Parallel architectures;Biomedical computing;Reconfigurable architectures;Computer fault tolerance;Video signal processing;Image processing;Software requirements and specifications;Multimedia computing;Internet","grid computing;processor scheduling;shared memory systems;program visualisation;program debugging;program testing;performance evaluation;real-time systems;mobile computing;program compilers;resource allocation;routing protocols;parallel programming;parallel algorithms;multi-threading;storage management;workstation clusters;parallel architectures;Java;biology computing;reconfigurable architectures;fault tolerant computing;ad hoc networks;video signal processing;image processing;formal specification;formal verification;multimedia computing;Internet;electronic commerce;engineering computing;natural sciences computing","Grid computing;distributed computing;task systems scheduling;shared-memory multiprocessors;visualization;debugging;testing;performance analysis;real-time systems;peer-to-peer computing;mobile computing;compiler technology;run-time systems;load balancing;network routing;parallel programming models;parallel algorithms;storage;high performance clusters;decentralized algorithms;multithreading;VLIW;Java;computational biology;nature inspired distributed computing;reconfigurable architectures;next generation systems;fault-tolerant systems;mobile ad hoc networks;image processing;video processing;multimedia;formal methods;Internet computing;e-commerce;scientific computing;engineering computing;massively parallel processing","","","","","","","","","IEEE","IEEE Conferences"
"Checking program profiles","P. Moseley; S. Debray; G. Andrews","Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA","Proceedings Third IEEE International Workshop on Source Code Analysis and Manipulation","","2003","","","193","202","Execution profiles have become increasingly important for guiding code optimization. However, little has been done to develop ways to check automatically that a profile does, in fact, reflect the actual execution behavior of a program. We describe a framework that uses program monitoring techniques in a way that allows the automatic checking of a wide variety of profile data. We also describe our experiences with using an instance of this framework to check edge profiles. The profile checker uncovered profiling anomalies that were previously unknown and that would have been very difficult to identify using existing techniques.","","0-7695-2005","10.1109/SCAM.2003.1238045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238045","","Testing;Counting circuits;Optimizing compilers;Computer science;Computerized monitoring;Programming profession;Hardware;Runtime;Flow graphs;Feedback","program diagnostics;automatic programming;optimising compilers;program testing","code optimization;program execution behavior;program monitoring techniques;automatic profile data checking;edge profile checking;program profile checking","","1","26","","","","","","IEEE","IEEE Conferences"
"FFT Processor IP Cores synthesis on the base of configurable pipeline architecture","A. Melnyk; B. Dunets","INTRON Ltd., Lviv, Ukraine; INTRON Ltd., Lviv, Ukraine","The Experience of Designing and Application of CAD Systems in Microelectronics, 2003. CADSM 2003. Proceedings of the 7th International Conference.","","2003","","","211","213","The INTRON FFT Processor IP Cores Generator, based on newest INTRON IP Core Generator creation methodology, which allows generating optimized FFT IP Cores on base of scalable source IT modules is presented.","","966-553-278","10.1109/CADSM.2003.1255034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1255034","","Pipelines;Discrete Fourier transforms;Field programmable gate arrays;Signal processing;Test pattern generators;Optimization methods;Fast Fourier transforms;Image processing;Hardware design languages;Clocks","pipeline processing;reconfigurable architectures;field programmable gate arrays;hardware description languages;hardware-software codesign;discrete Fourier transforms;digital signal processing chips","FFT processor IP cores synthesis;configurable pipeline architecture;optimized cores;scalable source IP modules;VHDL;FPGA;user-defined parameters;discrete Fourier transform;high-speed cores","","3","2","","","","","","IEEE","IEEE Conferences"
"On-line security of an electric power system using a transient stability contingency screening and ranking technique","C. M. Machado Ferreira; J. A. Dias Pinto; F. P. Maciel Barbosa","Dep. Engenharia Electrotecnica, Inst. Superior de Engenharia de Coimbra, Portugal; Dep. Engenharia Electrotecnica, Inst. Superior de Engenharia de Coimbra, Portugal; NA","11th IEEE Mediterranean Electrotechnical Conference (IEEE Cat. No.02CH37379)","","2002","","","331","335","In this paper a three-step technique is presented for on-line contingency screening, ranking and transient stability assessment based on a hybrid method. This efficient and robust approach combines the advantages of the time domain integration schemes to compute the initial system trajectory with the equal area criterion. The computer programs developed by the authors were integrated in the software package TRANsySTEM. In order to validate the established mathematical models as well as the software package, the transient stability of the New England test power network was studied. The results obtained with the hybrid formulation were compared with the solutions produced by a time domain simulation program and show a very close agreement. Consequently, the proposed technique classifies correctly, without any dangerous diagnostic or false alarm, all the analysed situations. In addition, this approach provides the transient stability margins as well as the critical machines of the severe contingencies, therefore enabling further design of corrective security actions. Finally, some conclusions that provide a valuable contribution to the understanding of the transient stability assessment of a multimachine power system are pointed out.","","0-7803-7527","10.1109/MELECON.2002.1014584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1014584","","Power system security;Power system transients;Power system stability;Power system analysis computing;Power system control;Power system simulation;Power system interconnection;Power systems;Power system planning;Power system relaying","power system transient stability;power system control;power system security;power system simulation;time-domain analysis","on-line security;on-line contingency screening;on-line contingency ranking;transient stability assessment;hybrid method;TRANsySTEM software package;robust approach;time domain integration schemes;initial system trajectory;equal area criterion;computer programs;New England test power network;time domain simulation program;power system control;transient stability margins;critical machines;corrective security actions;multimachine power system","","","16","","","","","","IEEE","IEEE Conferences"
"Efficient Utilization of SIMD Extensions","F. Franchetti; S. Kral; J. Lorenz; C. W. Ueberhuber","Vienna Univ. of Technol., Austria; Vienna Univ. of Technol., Austria; Vienna Univ. of Technol., Austria; Vienna Univ. of Technol., Austria","Proceedings of the IEEE","","2005","93","2","409","425","This paper targets automatic performance tuning of numerical kernels in the presence of multilayered memory hierarchies and single-instruction, multiple-data (SIMD) parallelism. The studied SIMD instruction set extensions include Intel's SSE family, AMD's 3DNow!, Motorola's AltiVec, and IBM's BlueGene/L SIMD instructions. FFTW, ATLAS, and SPIRAL demonstrate that near-optimal performance of numerical kernels across a variety of modern computers featuring deep memory hierarchies can be achieved only by means of automatic performance tuning. These software packages generate and optimize ANSI C code and feed it into the target machine's general-purpose C compiler to maintain portability. The scalar C code produced by performance tuning systems poses a severe challenge for vectorizing compilers. The particular code structure hampers automatic vectorization and, thus, inhibits satisfactory performance on processors featuring short vector extensions. This paper describes special-purpose compiler technology that supports automatic performance tuning on machines with vector instructions. The work described includes: 1) symbolic vectorization of digital signal processing transforms; 2) straight-line code vectorization for numerical kernels; and 3) compiler back ends for straight-line code with vector instructions. Methods from all three areas were combined with FFTW, SPIRAL, and ATLAS to optimize both for memory hierarchy and vector instructions. Experiments show that the presented methods lead to substantial speedups (up to 1.8 for two-way and 3.3 for four-way vector extensions) over the best scalar C codes generated by the original systems as well as roughly matching the performance of hand-tuned vendor libraries.","0018-9219;1558-2256","","10.1109/JPROC.2004.840491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1386659","Automatic vectorization;digital signal processing (DSP);fast Fourier transform (FFT);short vector single instruction, multiple data (SIMD);symbolic vectorization","Computer aided instruction;Kernel;Digital signal processing;Parallel processing;Spirals;Boosting;Computer applications;Signal processing algorithms;Registers;Concurrent computing","instruction sets;parallel programming;software libraries;mathematics computing;operating system kernels;software packages;parallelising compilers;software portability","SIMD extensions;single instruction multiple data parallelism;automatic performance tuning;multilayered memory hierarchies;Abbreviated Test Language for Avionic Systems;numerical kernels;software packages;ANSI C code;American National Standards Institute;C compiler;software portability;vectorizing compilers;automatic vectorization;short vector extensions;special purpose compiler technology;vector instructions;digital signal processing transforms;straight line code vectorization;compiler back ends;hand tuned vendor libraries;performance tuning systems","","18","54","","","","","","IEEE","IEEE Journals & Magazines"
"Rank-order filter design with a sampled-analog multiple-winners-take-all core","U. Cilingiroglu; L. E. Dake","Dept. of Electr. Eng., Texas A&M Univ., College Station, TX, USA; NA","IEEE Journal of Solid-State Circuits","","2002","37","8","978","984","We propose a sampled-analog rank-order filter (ROF) architecture of complexity O(n/sup 2/). It yields a very compact structure because the devices used are essentially of minimum geometry. Its sole active building block being the simple CMOS inverter, the circuit exhibits an excellent low-voltage compatibility. Furthermore, it can support a rail-to-rail common-mode input range. It is inherently fast due to fully parallel signal processing and speed is expected to increase with technological scaling at the same rate as purely digital circuitry. Finally, it supports full programmability of the rank by means of an analog reference voltage. The ROF is based on a pair of multiple-winners-take-all (mWTA) circuits and a set of AND gates. The paper includes a description of the architecture and a detailed analysis of the mWTA. Most relevant design issues are addressed and experimental results obtained from a fabricated ROF are presented.","0018-9200;1558-173X","","10.1109/JSSC.2002.800985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1020236","","Signal processing;Voltage;Biomedical signal processing;Application software;Nonlinear filters;Hardware;Costs;Circuit testing;Geometry;Inverters","active filters;nonlinear filters;circuit complexity;CMOS analogue integrated circuits;low-power electronics;programmable filters;logic gates;parallel processing","rank-order filter design;sampled-analog multiple-winners-take-all circuit;circuit complexity;CMOS inverter;active filter;AND gate;low-voltage circuit;rail-to-rail common-mode input range;parallel signal processing;nonlinear filter;programmable circuit","","12","8","","","","","","IEEE","IEEE Journals & Magazines"
"Prototype learning algorithms for nearest neighbor classifier with application to handwritten character recognition","Cheng-Lin Liu; M. Nakagawa","Venture Bus. Lab., Tokyo Univ. of Agric. & Technol., Japan; NA","Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)","","1999","","","378","381","This paper reviews some prototype learning algorithms for nearest neighbor (NN) classifier design land evaluates their performances in handwritten character recognition. The algorithms include the well-known LVQ and those that globally optimize an objective function, as well as some newly derived variants. Experimental results of handwritten numeral recognition and Chinese character recognition show that the global optimization algorithms generally outperform LVQ. Particularly, the generalized LVQ of Sato and Yamada (1998) and a new algorithm MAXP2 yield best results.","","0-7695-0318","10.1109/ICDAR.1999.791803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791803","","Prototypes;Nearest neighbor searches;Character recognition;Handwriting recognition;Neural networks;Application software;Testing;Databases;Laboratories;Computer science","handwritten character recognition;image classification;learning (artificial intelligence);optimisation","prototype learning algorithms;nearest neighbor classifier;handwritten character recognition;performance evaluation;LVQ algorithms;objective function;handwritten numeral recognition;Chinese character recognition;global optimization algorithms;MAXP2","","","12","","","","","","IEEE","IEEE Conferences"
"NFL theorem is unusable on structured classes of problems","B. Weinberg; E. -. Talbi","Univ. des Sci. et Technol. de Lille, Villeneuve d' Ascq, France; Univ. des Sci. et Technol. de Lille, Villeneuve d' Ascq, France","Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)","","2004","1","","220","226 Vol.1","Nowadays, in the heuristic and metaheuristics community, there is a schism between researchers who say: ""we proved experimentally that a given heuristic provides good results on a given problem and we can guess that it is enough general to be applied for other problems"", and those who claim: ""the No Free Lunch theorem (NFL) proves that there exists no absolute efficient heuristic"". The formers suspect the existence of a structure in the solved problem and that structure can occur in other problems. The latters fear that heuristics are especially adapted for the testbed problem this paper addresses structural aspect of combinatorial optimization problems. In a first time, we recall some related works which provide a frame to our work. Particularly, we recall the existence of deceptive problems which are proved to be hard to optimize, the definitions of the five scenarios of knowledge in optimization problem, and some works which already discuss the reach of NFL theorem. In the next part, we give a short overview of how NFL works and discuss its significance with regards to complexity. This leads to the observation that the notion of structure of optimization problems is missing in NFL use. Then, we prove that k-coloring problems respect such a notion of structure, for any k. In the last part we discuss the relevance of our work on four points: the polynomial reduction of NP-complete problems and structure preservation, the connection between our work and the study which squeeze NFL using neighborhood search operators, the position of our study on the five scenarios of knowledge, and finally the difference between metaheuristics and heuristics.","","0-7803-8515","10.1109/CEC.2004.1330860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1330860","","Testing;Polynomials;NP-complete problem;Logic;Information theory;Compression algorithms;Mathematics;Data compression;Optimization methods;Application software","computational complexity;optimisation;combinatorial mathematics;search problems;graph colouring","NFL theorem;structured problem classes;heuristic;metaheuristics;No Free Lunch theorem;testbed problem;combinatorial optimization problems;optimization problem;computational complexity;k-coloring problems;polynomial reduction;NP-complete problems;structure preservation;neighborhood search operators;heuristics","","3","18","","","","","","IEEE","IEEE Conferences"
"An experimental study on the performance of visual information retrieval similarity models","H. Eidenberger; C. Breiteneder","Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Austria","2002 IEEE Workshop on Multimedia Signal Processing.","","2002","","","233","236","This paper is an experimental study on the performance of the two major methods for macro-level similarity measurement: linear weighted merging and logical retrieval. Performance is measured as the average query execution time for a significant number of tests. The two models were implemented in the standard version (as they are applied in a number of prototypes) and in an optimized version. The results show that optimized logical retrieval clearly outperforms optimized linear weighted merging.","","0-7803-7713","10.1109/MMSP.2002.1203289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203289","","Information retrieval;Merging;Equations;Image retrieval;Content based retrieval;Paper technology;Time measurement;Testing;Prototypes;Euclidean distance","content-based retrieval;image retrieval;visual databases;query formulation","visual information retrieval;macrolevel similarity measurement;linear weighted merging;optimized logical retrieval;content-based image retrieval;content-based visual retrieval;query optimization;triangle inequality;average query execution time","","1","6","","","","","","IEEE","IEEE Conferences"
"A generalized two-threshold detection procedure","S. Fleisher; H. Singh; E. Shwedyk","Dept. of Electr. Eng., Tech. Nova Scotia Univ., Halifax, NS, Canada; NA; NA","IEEE Transactions on Information Theory","","1988","34","2","347","352","A modified sequential procedure for testing binary hypotheses with different means, proposed by C.C. Lee and J.B. Thomas (ibid., vol.IT-30, no.1, p.16-23, Jan. 1984), is generalized for application to the case of multiple hypotheses with different means/variances of the Gaussian distribution. The method constitutes a two-threshold test for fixed-size packages of samples with a sequential procedure of discarding the package for which no decision is reached and subsequently testing a new package. The objective is to find an optimum package size N/sub 0/ which leads to the minimum overall average sample number (ASN) for a given overall error probability. An optimization algorithm is developed to extend the application of the Lee-Thomas procedure to the M-ary case. Performance characteristics of the generalized two-threshold (GTT) test procedure are compared with those of conventional sequential as well as fixed-sample-size (FSS) methods. It is shown for the M-ary different means/variances cases that for low error rates the number of samples required by the GTT test is, on the average, approximately half that needed by a FSS test. However, it is somewhat more than the ASN obtained with a conventional sequential test. With decreasing error probabilities the GTT test performance approaches that of conventional sequential methods.<<ETX>>","0018-9448;1557-9654","","10.1109/18.2650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=2650","","Packaging;Testing;Sequential analysis;Error probability;Gaussian distribution;Frequency selective surfaces;Error analysis;Councils;Information theory;Application software","information theory","information theory;generalized two-threshold detection procedure;sequential procedure;multiple hypotheses;Gaussian distribution;optimum package size;average sample number;optimization algorithm;M-ary case;test procedure","","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Design for manufacturability: a path from system level to high yielding chips","A. J. Strojwas","Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA","Proceedings 2000. Design Automation Conference. (IEEE Cat. No.00CH37106)","","2000","","","375","376","This tutorial describes a wide spectrum of the Design for Manufacturability (DFM) activities. We start by presenting a new approach to IC design, which takes full advantage of leading edge technology. Then we propose a new methodology for acceleration of yield ramping which accounts for all the dominant yield loss mechanisms and a set of software tools that have been developed to address the yield learning problems. Several real-life examples demonstrate the practical results of employing such a yield ramping strategy.","","0-7803-5973","10.1109/ASPDAC.2000.835127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=835127","","Design for manufacture;Fabrication;Acceleration;Software tools;Manufacturing processes;Data analysis;Circuit testing;Semiconductor device manufacture;Time to market;Production","integrated circuit design;design for manufacture;integrated circuit yield;circuit optimisation;circuit CAD","design for manufacturability;system level;high yielding chips;IC design;yield ramping;software tools;design analysis;yield prediction;yield data analysis;yield diagnosis","","","","","","","","","IEEE","IEEE Conferences"
"A parallel stochastic optimization algorithm for solving 2D bin packing problems","R. P. Pargas; R. Jain","Dept. of Comput. Sci., Clemson Univ., SC, USA; Dept. of Comput. Sci., Clemson Univ., SC, USA","Proceedings of 9th IEEE Conference on Artificial Intelligence for Applications","","1993","","","18","25","This study describes a stochastic approach to the problem of packing two-dimensional figures in a rectangular area efficiently. The techniques employed are similar to those used in genetic algorithms or in simulated annealing algorithms, algorithmic methods which are grouped under the general classification of stochastic optimization. A parallel processing system, an Intel i860 hypercube, is used to speed up execution. Execution time is quite lengthy due to the costly process of evaluating the lengths of layouts. Load balancing is quite efficient and near-perfect load balancing is achieved. Four different data sets were tested, the simplest consisting of 129 figures, each of seven possible shapes and of differing sizes. The goal of a minimum of 80% efficiency or utilization based on bin length was achieved in all runs performed.<<ETX>>","","0-8186-3840","10.1109/CAIA.1993.366666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366666","","Stochastic processes;Computer science;Load management;Solids;Application software;Job shop scheduling;Processor scheduling;Software algorithms;Genetic algorithms;Annealing","problem solving;parallel algorithms;optimisation;resource allocation","problem solving;execution time;parallel stochastic optimization algorithm;2D bin packing problems;two-dimensional figures;rectangular area;genetic algorithms;simulated annealing;classification;parallel processing system;Intel i860 hypercube;load balancing;data sets;bin length","","10","17","","","","","","IEEE","IEEE Conferences"
"Applications of global flow analysis in logic synthesis","L. Berman; L. Trevillyan; D. Brand","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","1988., IEEE International Symposium on Circuits and Systems","","1988","","","901","904 vol.1","The authors describe a representation for multilevel circuits derived using the techniques of data-flow analysis. The usefulness of this representation is illustrated by describing algorithms for two of the fundamental problems of logic design: reducing the size of a circuit and increasing its testability. The authors obtain algorithms for circuit-size reduction by characterizing a class of function-preserving circuit transformations in terms of the representation and describe a novel algorithm from this class that uses approximate summary information to map the problem of reducing circuit size to that of finding a small cut in an associated graph. The algorithms for improving testability rely on the fact that the proposed representation permits conditional deductions to be performed efficiently. These algorithms have been implemented as part of an automatic design system in use within IBM.<<ETX>>","","","10.1109/ISCAS.1988.15069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=15069","","Logic design;Circuit testing;Circuit synthesis;Optimization methods;Optimizing compilers;Application software;Data analysis;Logic testing;Performance evaluation;Algorithm design and analysis","logic CAD","logic reduction;design for testability;global flow analysis;logic synthesis;representation for multilevel circuits;data-flow analysis;logic design;circuit-size reduction;function-preserving circuit transformations;improving testability;automatic design system;IBM","","2","11","","","","","","IEEE","IEEE Conferences"
"The Transient Electromechanical Oscillations Of The Synchronous Machine, In A Special Case","A. Campeanu","University of Craiova","Proceedings of the 6th International Conference on Optimization of Electrical and Electronic Equipments","","1998","1","","295","298","","","973-98511-2","10.1109/OPTIM.1998.710491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=710491","","Synchronous machines;Computer aided software engineering;Torque;Stability;Joining processes;Stators;Concrete;Stress;Testing;Shafts","","","","1","9","","","","","","IEEE","IEEE Conferences"
"Control flow driven splitting of loop nests at the source code level","H. Falk; P. Marwedel","Dept. of Comput. Sci., Dortmund Univ., Germany; Dept. of Comput. Sci., Dortmund Univ., Germany","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","410","415","This paper presents a novel source code transformation for control flow optimization called loop nest splitting which minimizes the number of executed if-statements in loop nests of embedded multimedia applications. The goal of the optimization is to reduce runtimes and energy consumption. The analysis techniques are based on precise mathematical models combined with genetic algorithms. Due to the inherent portability of source code transformations, a very detailed benchmarking using 10 different processors can be performed. The application of our implemented algorithms to three real-life multimedia benchmarks leads to average speed-ups by 23.6%-62.1% and energy savings by 19.6%-57.7%. Furthermore, our optimization also leads to advantageous pipeline and cache performance.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253644","","Energy consumption;Pipelines;Runtime;MPEG 4 Standard;Computer science;Application software;Algorithm design and analysis;Mathematical model;Genetic algorithms;Biomedical image processing","embedded systems;genetic algorithms;multimedia computing;optimising compilers","source code transformation;control flow optimization;embedded multimedia software;runtime;if-statement;energy consumption;mathematical model;genetic algorithm;loop nest splitting","","7","17","","","","","","IEEE","IEEE Conferences"
"Model reuse through hardware design patterns","F. Rincon; F. Moya; J. Barba; J. C. Lopez","Univ. de Castilla-La Mancha, Ciudad Real, Spain; Univ. de Castilla-La Mancha, Ciudad Real, Spain; Univ. de Castilla-La Mancha, Ciudad Real, Spain; Univ. de Castilla-La Mancha, Ciudad Real, Spain","Design, Automation and Test in Europe","","2005","","","324","329 Vol. 1","Increasing reuse opportunities is a well-known problem for software designers as well as for hardware designers. Nonetheless, current software and hardware engineering practices have embraced different approaches to this problem. Software designs are usually modelled after a set of proven solutions to recurrent problems called design patterns. This approach differs from the component-based reuse usually found in hardware designs: design patterns do not specify unnecessary implementation details. Several authors have already proposed translating structural design patterns concepts to hardware design. In this paper we extend the discussion to behavioural design patterns. Specifically, we describe how the hardware version of the Iterator can be used to enhance model reuse.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395580","","Algorithm design and analysis;Software design;Design optimization;Hardware design languages;High level synthesis;Productivity;Process design;Resource management;Software engineering;Standardization","hardware description languages;industrial property;object-oriented methods;electronic design automation;system-on-chip","model reuse;intellectual property;VHDL;hardware design patterns;SoC;software design;hardware design;behavioural design patterns;Iterator","","6","13","","","","","","IEEE","IEEE Conferences"
"The synthesis of passive circuits using the FDTD","R. C. Tupynamba; A. S. Omar","Arbeitsbereich Hochfrequenztechnik, Tech. Univ. Hamburg-Harburg, Germany; Arbeitsbereich Hochfrequenztechnik, Tech. Univ. Hamburg-Harburg, Germany","1996 IEEE MTT-S International Microwave Symposium Digest","","1996","2","","749","752 vol.2","An optimization procedure for the synthesis of passive circuits is presented. This procedure uses an FDTD code for the circuit analysis. The versatility of the FDTD allows the calculation of the circuit as a whole, with considering all discontinuities and interactions between the parts of the structure. The result is a powerful software package capable of producing an accurate circuit design.","0149-645X","0-7803-3246","10.1109/MWSYM.1996.511047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=511047","","Circuit synthesis;Passive circuits;Finite difference methods;Time domain analysis;Equations;Scattering parameters;Microwave integrated circuits;MMICs;Circuit testing;Libraries","passive networks;circuit optimisation;finite difference time-domain analysis","synthesis;passive circuits;optimization;FDTD code;circuit analysis;discontinuities;software package;circuit design","","2","9","","","","","","IEEE","IEEE Conferences"
"Evolving transformation sequences using genetic algorithms","D. Fatiregun; M. Harman; R. M. Hierons","Dept. of Inf. Syst. & Comput., Brunel Univ., Middlesex, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Middlesex, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Middlesex, UK","Source Code Analysis and Manipulation, Fourth IEEE International Workshop on","","2004","","","65","74","Program transformation is useful in a number of applications including program comprehension, reverse engineering and compiler optimization. In all these applications, transformation algorithms are constructed by hand for each different transformation goal. Loosely speaking, a transformation algorithm defines a sequence of transformation steps to apply to a given program. It is notoriously hard to find good transformation sequences automatically, and so much (costly) human intervention is required. This work shows how search-based meta-heuristic algorithms can be used to automate, or partly automate the problem of finding good transformation sequences. In this case, the goal of transformation is to reduce program size, but the approach is sufficiently general that it can be used to optimize any source-code level metric. The search techniques used are random search (RS), hill climbing (HC) and genetic algorithms (GA). The paper reports the result of initial experiments on small synthetic program transformation problems. The results are encouraging. They indicate that the genetic algorithm performs significantly better than either hill climbing or random search.","","0-7695-2144","10.1109/SCAM.2004.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1386160","","Genetic algorithms;Information systems;Reverse engineering;Program processors;Optimizing compilers;Humans;Software engineering;Software maintenance;Software testing;Amorphous materials","program compilers;genetic algorithms","transformation sequences;genetic algorithms;program comprehension;reverse engineering;compiler optimization;transformation algorithms;search-based meta-heuristic algorithms;source-code level metric;search techniques;random search;hill climbing;synthetic program transformation","","22","17","","","","","","IEEE","IEEE Conferences"
"Autonomic Computing - a means of achieving dependability?","R. Sterritt; D. Bustard","NA; NA","10th IEEE International Conference and Workshop on the Engineering of Computer-Based Systems, 2003. Proceedings.","","2003","","","247","251","Autonomic computing is emerging as a significant new approach to the design of computing systems. Its goal is the development of systems that are self-configuring, self-healing, self-protecting and self-optimizing. Dependability is a long-standing desirable property of all computer-based systems. The purpose of the paper is to consider how autonomic computing can provide a framework for dependability.","","0-7695-1917","10.1109/ECBS.2003.1194805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194805","","Systems engineering and theory;Computer errors;Fault tolerant systems;Fault tolerance;Communities;Software systems;Testing;Mathematics;Design engineering;Informatics","software fault tolerance;automatic programming;system recovery","autonomic computing;computing systems design;computer-based systems;dependability framework;self-healing systems;self optimizing systems","","59","12","","","","","","IEEE","IEEE Conferences"
"What is systems engineering?","","","IEEE Aerospace and Electronic Systems Magazine","","2000","15","10","9","10","There are probably more definitions of ""Systems Engineering"" than there are AESS members. In its simplest form systems engineering is the design of the whole as opposed to the design of the parts. The vast number, complexity and diversity of elements can overwhelm and degrade system performance and reliability. Embedded processing and software can be both a boon and a bane. A systems engineer analyzes and optimizes an ensemble of elements that relate to the flow of energy, mass and communications into a design that performs the desired function. ""Systems engineering"" is used herein to cover a very broad spectrum of processes and controls to engineer a product at the many levels required to satisfy all aspects of the original requirement. Our definition is not intended to either include or exclude systems engineering and integration as used in the computer field. In any case, systems engineering is the application of solid engineering principles to design and develop a large enterprise within cost and schedule to satisfy the needs of the ultimate user. It involves conceptualization, design, development, test, implementation, approval/certification and operation (including human factors) of a system. In essence, systems engineering is a problem-solving discipline for the modern world.","0885-8985;1557-959X","","10.1109/62.879392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879392","","Systems engineering and theory;Design engineering;Reliability engineering;Power engineering and energy;Aerospace and Electronic Systems Society;Degradation;System performance;Embedded software;Performance analysis;Design optimization","systems engineering","systems engineering;embedded processing;embedded software;flow of energy;mass;communications;conceptualization;design;development;test;implementation;approval/certification;operation;human factors","","2","","","","","","","IEEE","IEEE Journals & Magazines"
"Transformation of Lotos specifications to Estelle specifications","H. El-Gendy; H. Baraka","GATIS Corp., Giza, Egypt; NA","Proceedings Second IEEE Symposium on Computer and Communications","","1997","","","215","220","A technique for the automated transformation of a Lotos specification to an Estelle specification is presented. First, a restricted behaviour tree is constructed from the Lotos specification in a somewhat similar way to generating a reachability tree for a finite-state machine. The restricted behaviour tree has a finite size even when the communications protocol specified represents infinite behaviour. We develop an algorithm for constructing the Estelle specifications from the restricted behaviour tree. A minimization rule is also developed to optimize the size of the Estelle specification by reducing both the number of states and the number of transitions. We conclude by pointing out areas for further research.","","0-8186-7852","10.1109/ISCC.1997.615999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=615999","","Protocols;Testing;Costs;Certification;Software systems;Concurrent computing;Context;ISO standards;Natural languages","specification languages;formal specification;minimisation;protocols;trees (mathematics);computer aided software engineering;reachability analysis;ISO standards;telecommunication standards;telecommunication computing","automated specification transformation;Lotos specifications;Estelle specifications;restricted behaviour tree;reachability tree;finite-state machine;communications protocol;infinite behaviour;minimization rule;specification size optimization;state number reduction;transition number reduction","","2","8","","","","","","IEEE","IEEE Conferences"
"Improving the Performance of an Optimistic Concurrency Control Algorithm Through Timestamps and Versions","M. J. Carey","Department of Computer Sciences, University of Wisconsin","IEEE Transactions on Software Engineering","","1987","SE-13","6","746","751","This correspondence describes and analyzes two schemes for improving the performance of serial validation, an optimistic concurrency control algorithm proposed by Kutng and Robinson. It is shown that timestamp-based techniques can be used to implement serial validation, yielding an equivalent algorithm with a much lower validation cost. A multiple version variant of serial validation is then presented, and simulation results indicate that multiversion serial validation has significant performance advantages over the single version algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702279","Concurrency control;database systems;modeling and simulation;transaction processing","Concurrency control;Testing;Transaction databases;Costs;Database systems;Proposals;Performance analysis;Algorithm design and analysis;Control system synthesis;Concurrent computing","","Concurrency control;database systems;modeling and simulation;transaction processing","","5","20","","","","","","IEEE","IEEE Journals & Magazines"
"Robust modelling of local image structures and its application to medical imagery","Li Wang; A. Bhalerao; R. Wilson","Dept. of Comput. Sci., Warwick Univ., Coventry, UK; Dept. of Comput. Sci., Warwick Univ., Coventry, UK; Dept. of Comput. Sci., Warwick Univ., Coventry, UK","Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.","","2004","3","","534","537 Vol.3","A robust modelling method for detecting and measuring isotropic, linear features and bifurcations is described and applied to analysing 2D electrophoresis and retinal images. Features are modelled as a superposition of Gaussian functions with the Hermite expansion and estimated by a combination of a multiresolution, windowed Fourier approach followed by an EM type of spatial regression. A penalised likelihood test, the Akakie information criteria (AIC) is used to select the best model and scale for feature segments. Results are shown by using samples on both gel and retinal images.","1051-4651","0-7695-2128","10.1109/ICPR.2004.1334584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334584","","Robustness;Biomedical imaging;Retina;Application software;Bifurcation;Image analysis;Signal resolution;Testing;Image segmentation;Computer vision","medical image processing;feature extraction;image resolution;Gaussian processes;image segmentation;regression analysis;maximum likelihood estimation;optimisation;Fourier transforms;electrophoresis;eye;image sampling;approximation theory","robust modelling method;image structure modeling method;medical imagery;isotropic linear feature detection;feature measurement;bifurcation;2D electrophoresis analysis;retinal image analysis;Gaussian function;Hermite expansion;windowed Fourier method;EM algorithm;expectation maximization algorithm;spatial regression analysis;penalised likelihood test;Akakie information criteria;feature segmentation;Hermite approximation","","2","11","","","","","","IEEE","IEEE Conferences"
"Improving the accuracy of support-set finding method for power estimation of combinational circuits","Hoon Choi; Seung Ho Hwang","Dept. of Electr. Eng., Korea Adv. Inst. of Sci. & Technol., Seoul, South Korea; NA","Proceedings European Design and Test Conference. ED & TC 97","","1997","","","526","530","We address a way to improve the accuracy of support-set finding method for a probability-based power estimation of combinational circuits. Support-set finding methods to build local BDDs have been proposed to handle large circuits. However because they consider only the shallow reconvergence, they are not accurate enough to be used in the power optimization. To solve this problem, we propose a new algorithm, Feather algorithm, which can efficiently detect minimal support-set with 100% reconvergent node detection rate. The experimental results show that the average error of our proposed method is 0.1% for the total power and 1.6% for the node-specific power.","1066-1409","0-8186-7786","10.1109/EDTC.1997.582411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582411","","Combinational circuits;Binary decision diagrams;Energy consumption;Boolean functions;Feathers;Portable computers;Data structures;Computer errors;Application software;Design optimization","combinational circuits","support-set finding method;power estimation;combinational circuit;probability;local BDD;Feather algorithm;reconvergent node detection rate","","3","7","","","","","","IEEE","IEEE Conferences"
"A multi-level comparative performance characterization of SPECjbb2005 versus SPECjbb2000","R. Morin; A. Kumar; E. Ilyina","Intel Corp., Santa Clara, CA, USA; Intel Corp., Santa Clara, CA, USA; Intel Corp., Santa Clara, CA, USA","IEEE International. 2005 Proceedings of the IEEE Workload Characterization Symposium, 2005.","","2005","","","67","75","SPEC has released SPECjbb2005, a new server-side Java benchmark which supersedes SPECjbb2000. SPECjbb2005 is a substantial update to SPECjbb2000, intended to make the workload more representative based on current Java development practices. SPECjbb2000 has been in existence for about five years and it has been a valuable tool for optimizing the performance of commercial JVMs as well as supporting research activities. Since SPECjbb2005 replaces SPECjbb2000, it is important to understand the key differences between the two, as well as implications for JVM and hardware designers. In this paper, we present a comparative characterization of these two workloads based on detailed measurements on an Intel/spl reg/ Xeon/spl trade/ processor-based commercial server. First, we describe key functional changes introduced in SPECjbb2005. Using low-intrusion application profiling tools we compare application execution profiles. Through JVM monitoring tools, we compare JVM behavior including JIT optimization and garbage collection. Using operating system monitoring tools we compare key system level metrics including CPU utilization. With the aid of processor performance monitoring events, we compare key architectural characteristics such as cache miss rates, memory/bus utilization, and branch behavior. Finally, we summarize key findings, provide recommendations to JVM developers and hardware designers, and suggest areas for future work.","","0-7803-9461","10.1109/IISWC.2005.1526002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1526002","","Java;Hardware;Monitoring;Operating systems;Scalability;Libraries;Measurement;Productivity;Runtime environment;Optimizing compilers","software performance evaluation;benchmark testing;Java;storage management;system monitoring;software metrics","multilevel comparative performance characterization;SPECjbb2005;SPECjbb2000;server-side Java benchmark;Intel Xeon processor-based commercial server;low-intrusion application profiling tools;JVM monitoring tools;JIT optimization;garbage collection;operating system monitoring tools;system level metrics;CPU utilization;processor performance monitoring events;cache miss rates;memory utilization;bus utilization;branch behavior","","2","23","","","","","","IEEE","IEEE Conferences"
"Multi-signal flow graphs: a novel approach for system testability analysis and fault diagnosis","S. Deb; K. R. Pattipati; V. Raghavan; M. Shakeri; R. Shrestha","Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA","Proceedings of AUTOTESTCON '94","","1994","","","361","373","In this paper, we present a comprehensive methodology for a formal, but intuitive, cause-effect dependency modeling using multi-signal directed graphs that correspond closely to hierarchical system schematics and develop diagnostic strategies to isolate faults in the shortest possible time without making the unrealistic single fault assumption. A key feature of our methodology is that our models lend naturally to real-world necessities, such as system integration and hierarchical troubleshooting.<<ETX>></ETX>","","0-7803-1910","10.1109/AUTEST.1994.381596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381596","","Flow graphs;System testing;Fault diagnosis;Design for testability;Costs;Observability;Manufacturing;Design automation;Maintenance engineering;Design optimization","fault diagnosis;failure analysis;design for testability;automatic test software;signal flow graphs","multi-signal flow graph;system testability analysis;fault diagnosis;cause-effect dependency modeling;hierarchical system schematics;system integration;hierarchical troubleshooting;cassette player;TEAMS software package;feedback loop analysis;static fault analysis;test sequencing algorithm;modular diagnosis;DFT","","43","14","","","","","","IEEE","IEEE Conferences"
"SPSA for non-smooth optimization with application in ECG analysis","L. Gerencser; G. Kozmann; Z. Vago","Comput. & Autom. Inst., Hungarian Acad. of Sci., Budapest, Hungary; NA; NA","Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171)","","1998","4","","3907","3908 vol.4","It is shown that nonsmooth optimization problems can be solved by a suitable extension of the simultaneous perturbation stochastic approximation (SPSA) method. The new optimization method has been tested in a min-max classification problem using both simulated and real data. The latter are ECG signals which were collected for the detection of so-called late potentials.","0191-2216","0-7803-4394","10.1109/CDC.1998.761839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=761839","","Electrocardiography;Application software","electrocardiography;medical signal processing;stochastic programming;perturbation techniques;minimax techniques;approximation theory","simultaneous perturbation stochastic approximation;nonsmooth optimization;ECG analysis;min-max classification;late potentials","","5","6","","","","","","IEEE","IEEE Conferences"
"Modelling tool for chemical-mechanical polishing design and evaluation","S. R. Runnels; I. Kim; J. Schleuter; C. Karlsrud; M. Desai","Southwest Res. Inst., San Antonio, TX, USA; NA; NA; NA; NA","IEEE Transactions on Semiconductor Manufacturing","","1998","11","3","501","510","Wafer-scale phenomenological modeling, combined with an automatic model validation algorithm, has been implemented in a chemical-mechanical polishing (CMP) modeling software environment called Plane-View. The wafer-scale material removal model is an enhanced Preston equation that admits a nonuniform pressure distribution. Automatic model validation is performed using the Levenberg-Marquart minimization scheme and film thickness data. A three-step model validation procedure is established where sets of material removal profiles are automatically read by Plane-View, which then adjusts the model parameters to optimize the model's fit, creates formulae that capture the behavior of those parameters as a function of process conditions, and stores formulae in a library for easy access by users. This article describes the software architecture, the wafer-scale model, the theoretical aspects of model validation, and a demonstration of the model and validation results using a set of five test wafers.","0894-6507;1558-2345","","10.1109/66.705385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705385","","Chemicals;Semiconductor device modeling;Physics;Equations;Software libraries;Software architecture;Algorithm design and analysis;Software algorithms;Software testing","polishing;semiconductor process modelling;programming environments","chemical-mechanical polishing;wafer-scale phenomenological modeling;automatic model validation algorithm;modeling software environment;Plane-View;wafer-scale material removal model;enhanced Preston equation;nonuniform pressure distribution;Levenberg-Marquart minimization scheme;three-step model validation procedure;material removal profiles;process conditions;test wafers","","30","15","","","","","","IEEE","IEEE Journals & Magazines"
"Hardware support for QoS-based function allocation in reconfigurable systems","M. Ullmann; Wansheng Jin; J. Becker","Karlsruhe Univ., Germany; Karlsruhe Univ., Germany; Karlsruhe Univ., Germany","Design, Automation and Test in Europe","","2005","","","259","264 Vol. 3","This paper presents a new approach for allocating suitable function-implementation variants depending on given quality-of-service function requirements for run-time reconfigurable multi-device systems. Our approach adapts methodologies from the domain of knowledge-based systems which can be used for doing run-time hardware/software resource usage optimizations.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395831","CBR;Algorithm;Resource Management","Hardware;Runtime;Application software;Quality of service;Field programmable gate arrays;Control systems;Communication system control;System-on-a-chip;Automotive engineering;Digital signal processing","case-based reasoning;quality of service;reconfigurable architectures;resource allocation;field programmable gate arrays;embedded systems;digital signal processing chips;hardware-software codesign","QoS-based function allocation;quality of service;FPGA;CBR;knowledge-based systems;hardware/software resource usage optimization;run-time reconfigurable multi-device systems","","1","10","","","","","","IEEE","IEEE Conferences"
"Quantitative extraction and compensation of excess inductance: case study of a digital attenuator chip","S. C. Choi; D. H. Kwon; K. H. Kim; S. W. Hwang","Dept. of Electron. Eng., Korea Univ., Seoul, South Korea; Dept. of Electron. Eng., Korea Univ., Seoul, South Korea; Dept. of Electron. Eng., Korea Univ., Seoul, South Korea; Dept. of Electron. Eng., Korea Univ., Seoul, South Korea","2002 IEEE MTT-S International Microwave Symposium Digest (Cat. No.02CH37278)","","2002","3","","2249","2252 vol.3","The excess inductance of the conductor connecting the ground pins of the MMIC and the ground plane is extracted from a simplified procedure of measurement and optimization. The method is applied to a digital attenuator chip with a positive bias circuit. The addition of bypass capacitors with the value compensating the extracted excess inductance of the conductor clearly improves the performance of the chip.","0149-645X","0-7803-7239","10.1109/MWSYM.2002.1012321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012321","","Inductance;Computer aided software engineering;Attenuators;Circuit testing;Pins;Semiconductor device measurement;Conductors;Equivalent circuits;Capacitors;Attenuation","gallium arsenide;MESFET integrated circuits;field effect MMIC;attenuators;field effect digital integrated circuits;inductance;circuit optimisation;compensation;III-V semiconductors","digital attenuator chip;conductor excess inductance;MMIC ground pins;ground plane;optimization;positive bias circuit;bypass capacitors;chip performance;GaAs MESFET MMIC chips;GaAs","","","8","","","","","","IEEE","IEEE Conferences"
"A widely configurable EPROM memory compiler for embedded applications","H. Lim; A. Shubat; V. Duvalyan; S. Dandamudi; S. Raviv; A. Kablanian","Virage Logic Corp., Fremont, CA, USA; NA; NA; NA; NA; NA","Proceedings. International Workshop on Memory Technology, Design and Testing (Cat. No.98TB100236)","","1998","","","12","16","An EPROM memory configurable in size and word width has been designed into a silicon compiler framework. The memory compiler software enables the hardware design to be encapsulated to facilitate re-use. The compiler supports memory densities ranging from 64 kb to 512 kb. The design is implemented in a 0.6 um 2-metal EPROM process. The 32 Kb/spl times/8 instance measures 1.32 mm/spl times/1.83 mm and at nominal process and environment (T/sub j/=25/spl deg/C, VDD=5.0 V) the simulated address access time is 29.4 nsec, read cycle time is 32.6 nsec, and power dissipation is 71.4 mW at 30 MHz operating speed. A streamlined design flow for integrating a new design into the compiler framework allowed for a three month design cycle from product definition to tapeout of the first instance.","","0-8186-8494","10.1109/MTDT.1998.705940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705940","","EPROM;Circuit testing;Production;Logic;Lakes;Redundancy;Optimizing compilers;Power supplies;Low voltage;Hardware design languages","EPROM;integrated memory circuits;circuit layout CAD;integrated circuit layout","configurable EPROM memory compiler;word width configurable;size configurable;silicon compiler framework;2-metal EPROM proces;embedded applications;64 to 512 kbit;0.6 micron;5 V;29.4 ns;32.6 ns;71.4 mW;30 MHz","","3","3","","","","","","IEEE","IEEE Conferences"
"Gaussian mixture distance for information retrieval","X. Q. Li; I. King","Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, Hong Kong; NA","IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)","","1999","4","","2544","2549 vol.4","We propose a Gaussian mixture distance for performing accurate nearest-neighbor search for information retrieval. Under an established Gaussian finite mixture model for the distribution of the data in the database, the Gaussian mixture distance is formulated based on minimizing the Kullback-Leibler divergence between the distribution of the retrieval data and the data in database. We compared the performance of the Gaussian mixture distance with the well-known Euclidean and Mahalanobis distance based on a precision performance measurement. Experimental results demonstrate that the Gaussian mixture distance function is superior in the others for different types of testing data.","1098-7576","0-7803-5529","10.1109/IJCNN.1999.833474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=833474","","Information retrieval;Covariance matrix;Nearest neighbor searches;Euclidean distance;Computer science;Application software;Measurement;Testing;Multimedia databases;Spatial databases","query processing;database management systems;search problems;database theory;Gaussian processes;probability;optimisation","Gaussian mixture distance;information retrieval;nearest-neighbor search;database;Kullback-Leibler divergence;probability;optimisation","","4","6","","","","","","IEEE","IEEE Conferences"
"A method for automatic evaluation of fault effects in the advanced intelligent network","H. Suzuki; H. Kawamura; T. Akiyama; N. Takahashi","NTT Commun. Switching Labs., Tokyo, Japan; NTT Commun. Switching Labs., Tokyo, Japan; NTT Commun. Switching Labs., Tokyo, Japan; NTT Commun. Switching Labs., Tokyo, Japan","Proceedings of GLOBECOM '93. IEEE Global Telecommunications Conference","","1993","","","234","238 vol.1","The method described here provides network operators with criteria for deciding the priorities with which services degraded by network faults should be restored. This method consists of four processes: the first lists the unavailable services of customers affected by the fault, the second predicts the mean time needed to repair the fault, the third predicts the traffic trends for the affected services, and the fourth calculates the criteria used to decide the service restoration priorities. This method enables network operators to choose suitable means restoring services and to avoid future congestion due to faults. It also lets them respond more quickly and precisely to customer claims. We are introducing this method into our advanced IN operations systems.<<ETX>>","","0-7803-0917","10.1109/GLOCOM.1993.318129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318129","","Intelligent networks;Degradation;Computer architecture;Communication switching;Laboratories;Telecommunication traffic;Stability;Monitoring;Customer service;Application software","intelligent networks;telecommunication network management;automatic test equipment;telecommunications computing;network operating systems;telecommunication traffic","automatic evaluation;fault effects;advanced intelligent network;network faults;unavailable services;mean time to repair;traffic trends prediction;service restoration priorities;operations systems","","7","6","","","","","","IEEE","IEEE Conferences"
"Rapid prototyping of a test modem for terrestrial broadcasting of digital television","A. Garcia Armada; M. Calvo Ramon","Grupo de Radiacion, Polytech. Univ. of Madrid, Spain; NA","IEEE Transactions on Consumer Electronics","","1997","43","4","1100","1109","This paper introduces rapid prototyping of a complex system and the difficulties that appear when a real-time prototype of such a system is approached. These concepts are applied to the construction of a test modem for terrestrial broadcasting of digital television, in which orthogonal frequency division multiplexing (OFDM) is the transmission technique. The possibility of reducing the number of sub-carriers in the OFDM signal and obtaining enough information about the system performance in hostile environments (multipath, phase noise, non-linear, ...) is considered and discarded. As a conclusion, a non-real time prototype is presented which allows one to optimize parameters, verify the possibility of implementation and evaluate subjective qualities.","0098-3063;1558-4127","","10.1109/30.642377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=642377","","Prototypes;Modems;TV broadcasting;Digital TV;Digital video broadcasting;Satellite broadcasting;OFDM;System testing;Real time systems;Europe","digital television;television broadcasting;software prototyping;frequency division multiplexing;telecommunication computing;multipath channels;phase noise;modems","test modem;terrestrial broadcasting;digital television;rapid prototyping;real-time prototype;orthogonal frequency division multiplexing;OFDM signal;system performance;multipath;phase noise;nonreal time prototype;parameter optimisation;subjective qualities evaluation","","11","27","","","","","","IEEE","IEEE Journals & Magazines"
"Using cultural algorithms to evolve strategies in agent-based models","D. A. Ostrowski; T. Tassier; M. Everson; R. G. Reynolds","NA; NA; NA; NA","Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)","","2002","1","","741","746 vol.1","Cultural algorithms are self-adaptive models that support the collective evolution process through the employment of a population and a belief space. The cultural approach is applied to derive a generalized set of beliefs from successive populations of parameter configurations from an agent-based simulation of transactions within a durable goods market. The maintenance of this information allows for the guided evolution of the agent-based system over successive simulations. In order to more effectively evaluate parameter configurations, software engineering techniques of white and black box testing are applied. In this paper, a methodology for the use of cultural algorithms to optimize strategies in agent-based models is presented. This approach is demonstrated in an application used to model pricing strategies in the context of an agent-based model under a simulated real-world market scenario and a heterogeneous population.","","0-7803-7282","10.1109/CEC.2002.1007018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007018","","Cultural differences;Global communication;Testing;Algorithm design and analysis;Software algorithms;Process design;State-space methods;Measurement;Genetic mutations;Communication channels","multi-agent systems;digital simulation;costing;evolutionary computation;belief maintenance;software engineering;economics;financial data processing","cultural algorithms;self-adaptive models;collective evolution process;belief space;population;parameter configurations;agent-based models;strategy evolution;agent-based simulation;transactions;durable goods market;guided evolution;successive simulations;software engineering techniques;white box testing;black box testing;pricing strategies;simulated real-world market scenario;heterogeneous population","","9","12","","","","","","IEEE","IEEE Conferences"
"Solving global optimal problems by using a dynamical evolutionary algorithm","Yuanxiang Li; Xiufen Zou","State Key Lab. of Software Eng., Wuhan Univ., Hubei, China; NA","Fifth International Conference on Algorithms and Architectures for Parallel Processing, 2002. Proceedings.","","2002","","","170","173","We introduce a new dynamical evolutionary algorithm and use it to solve global optimal problems. A brief theoretical explanation for this algorithm is obtained from statistical mechanics. The algorithm has been evaluated numerically using a wide set of test functions which are nonlinear, multimodal and multidimensional. Numerical results show that the algorithm has the potential to obtain a global optimum or more accurate solutions than other methods for hard problems.","","0-7695-1512","10.1109/ICAPP.2002.1173569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173569","","Heuristic algorithms;Parallel processing","evolutionary computation;optimisation;numerical analysis","global optimal problem solving;statistical mechanics;numerical evaluation;nonlinear test functions;multimodal test functions;multidimensional test functions;hard problems;dynamical evolutionary algorithm","","5","15","","","","","","IEEE","IEEE Conferences"
"Progressive ranking and composition of Web services using covering arrays","C. J. Colbourn; Yinong Chen; Wei-Tek Tsai","Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA; Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA","10th IEEE International Workshop on Object-Oriented Real-Time Dependable Systems","","2005","","","179","185","Major computer companies and government agencies are adopting Web services (WS) technology. Web services must ensure interoperability and security, and be reliable and trustworthy. Consumers must make runtime decisions based on an intelligent and unbiased evaluation. We propose one component of a test suite generator to address this critical issue. The goal is to transform the labor-intensive software development process into a largely automated Web services development process. The method operates within a service-oriented architecture, with the models and tools implemented as services that can be added, removed and replaced at runtime. We propose a generic greedy algorithm based on covering arrays for application in this environment.","1530-1443;2378-573X","0-7695-2347","10.1109/WORDS.2005.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544792","","Web services;Testing;Runtime;Certification;Web and internet services;Simple object access protocol;Computer science;Reliability engineering;Government;Programming","Internet;open systems;software architecture;greedy algorithms","Web services;covering arrays;interoperability;security;software development;service-oriented architecture;generic greedy algorithm","","2","31","","","","","","IEEE","IEEE Conferences"
"Effective iterative techniques for fingerprinting design IP","A. E. Caldwell; Hyun-Jin Choi; A. B. Kahng; S. Mantik; M. Potkonjak; Gang Qu; J. L. Wong","Cadence Design Syst. Inc., San Jose, CA, USA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2004","23","2","208","215","Fingerprinting is an approach that assigns a unique and invisible ID to each sold instance of the intellectual property (IP). One of the key advantages fingerprinting-based intellectual property protection (IPP) has over watermarking-based IPP is the enabling of tracing stolen hardware or software. Fingerprinting schemes have been widely and effectively used to achieve this goal; however, their application domain has been restricted only to static artifacts, such as image and audio, where distinct copies can be obtained easily. In this paper, we propose the first generic fingerprinting technique that can be applied to an arbitrary synthesis (optimization or decision) or compilation problem and, therefore to hardware and software IPs. The key problem with design IP fingerprinting is that there is a need to generate a large number of structurally unique but functionally and timing identical designs. To reduce the cost of generating such distinct copies, we apply iterative optimization in an incremental fashion to solve a fingerprinted instance. Therefore, we leverage on the optimization effort already spent in obtaining previous solutions, yet we generate a uniquely fingerprinted new solution. This generic approach is the basis for developing specific fingerprinting techniques for four important problems in VLSI CAD: partitioning, graph coloring, satisfiability, and standard-cell placement. We demonstrate the effectiveness of the new fingerprinting-based IPP techniques on a number of standard benchmarks.","0278-0070;1937-4151","","10.1109/TCAD.2003.822126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1262458","","Fingerprint recognition;Intellectual property;Hardware;Iterative methods;Protection;Watermarking;Application software;Timing;Cost function;Very large scale integration","industrial property;iterative methods;optimisation;VLSI;benchmark testing;integrated circuit design","iterative techniques;design IP fingerprinting;fingerprinting-based intellectual property protection;watermarking-based IPP;static artifacts;arbitrary synthesis;decision;compilation problem;hardware IP;software IP;iterative optimization;VLSI CAD;partitioning;graph coloring;satisfiability;standard-cell placement;benchmarking","","32","28","","","","","","IEEE","IEEE Journals & Magazines"
"Stochastic Petri nets applied to the performance evaluation of static task allocations in heterogeneous computing environments","A. R. McSpadden; N. Lopez-Benitez","Dept. of Comput. Sci., Texas Tech. Univ., Lubbock, TX, USA; NA","Proceedings Sixth Heterogeneous Computing Workshop (HCW'97)","","1997","","","185","194","A stochastic Petri net (SPN) is systematically constructed from a task graph whose component subtasks are statically allocated onto the processor suite of a heterogeneous computing system (HCS). Given that subtask execution times are exponentially distributed an exponential distribution can be generated for the overall completion time. In particular the enabling functions and rate functions used to specify the SPN model provide needed versatility to integrate processor heterogeneity, task priorities, allocation schemes, communication costs, and other factors characteristic of a HCS into a comprehensive performance analysis. The manner in which these parameters are incorporated into the SPN allows the model to be transformed into a testbed for optimization schemes and heuristics. The proposed approach can be applied to arbitrary task graphs including non-series-parallel.","","0-8186-7879","10.1109/HCW.1997.581420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581420","","Stochastic processes;Petri nets;Delay estimation;Stochastic systems;Processor scheduling;Computer science;Educational institutions;Cost function;Testing;Tiles","Petri nets;stochastic processes;resource allocation;software performance evaluation;distributed processing;open systems;exponential distribution;optimisation;heuristic programming","stochastic Petri net;performance evaluation;static task allocations;heterogeneous computing environments;task graph;processor suite;subtask execution times;exponential distribution;completion time;enabling functions;rate functions;SPN model;processor heterogeneity;task priorities;allocation schemes;communication costs;optimization schemes;heuristics;non-series-parallel graphs","","5","23","","","","","","IEEE","IEEE Conferences"
"A tutorial approach to individualized instruction for an electronic computer aided design laboratory","J. A. Parker","EET Dept., Purdue Univ., West Lafayette, IN, USA","Proceedings Frontiers in Education 1995 25th Annual Conference. Engineering Education for the 21st Century","","1995","1","","2a2.18","2a2.20 vol.1","This electronic computer-aided design (CAD) course (EET 320) moves the student from theoretical digital and analog circuits to the actual printed circuit board (PCB) design and layout as quickly as possible. Since the design of PCBs requires increasingly smaller board sizes and higher component densities, the utilization of extensive electronic CAD software packages is required to achieve circuit performance. These packages are windows- and menu-driven with many and varied options. Since each student works at his own computer and at his own pace with his own individual computer skills, a system of individualized instruction is required far a rapid introduction and fast-track development of these computer skills. Tutorials on (i) schematic capture, (ii) circuit simulation, (iii) PLD (programmable logic design) implementation, and (iv) PCB placement and routing on actual circuits advance skill levels rapidly. After going through these tutorials and circuit examples with step-by-step instructions, the student is ready to tackle more complicated circuits on an individual basis. Orcad CAD packages are used to implement all four main areas of this course. Using the PCB and integrated circuits, the actual circuit can then be constructed and tested. How well the actual circuit performs is the ultimate evaluation.","0190-5848","0-7803-3022","10.1109/FIE.1995.483021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=483021","","Tutorial;Design automation;Computer aided instruction;Circuit testing;Analog circuits;Printed circuits;Software packages;Circuit optimization;Electronics packaging;Circuit simulation","circuit CAD;circuit layout CAD;analogue circuits;digital circuits;printed circuit design;circuit analysis computing;network routing;electronic engineering education;courseware;laboratories","tutorial approach;individualized instruction;electronic computer aided design laboratory;electronic CAD course;digital circuits;analog circuits;printed circuit board;PCB design;PCB layout;board size;component densities;circuit performance;windows;menu-driven software packages;student;computer skills;schematic capture;circuit simulation;programmable logic design implementation;PCB placement;routing;Orcad","","","5","","","","","","IEEE","IEEE Conferences"
"A practical approach for bus architecture optimization at transaction level","O. Ogawa; S. Bayon de Noyer; P. Chauvet; K. Shinohara; Y. Watanabe; H. Niizuma; T. Sasaki; Y. Takai","Semicond. Co., Kyoto, Japan; NA; NA; NA; NA; NA; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","176","181 suppl.","For multimedia applications, the System LSI design trend is to integrate an increasing number of applications running on a single chip. Traditional architectures have reached their limit in terms of performance. New architectures must be explored to fulfill the system application needs. Complex bus structures have been introduced. These bus architectures open a much larger exploration space than traditional hardware-software partitioning trade-offs. We have been researching methods to leverage these new architectural elements. We also introduce a design environment to apply practical and efficient methods in today's design flow. Two key technologies are supporting our method and environment: Automatic bus architecture synthesis for easy configuration of bus architecture and transaction level of abstraction for communication for improvement of simulation performance. In this paper, we show the design method, an overview of the design environment and its usefulness through experimental results.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253825","","Design methodology;Large scale integration;Computer architecture;Space exploration;Space technology;Hardware;Cities and towns;Electronic mail;Multimedia systems;Fabrication","large scale integration;multimedia systems;circuit optimisation;integrated circuit design","bus architecture optimization;transaction level;multimedia applications;System LSI design trend;exploration space;design environment;simulation performance","","31","68","","","","","","IEEE","IEEE Conferences"
"P-3PC: a point-to-point communication model for automatic and optimal decomposition of regular domain problems","F. J. Seinstra; D. Koelma","Intelligent Sensory Inf. Syst., Amsterdam Univ., Netherlands; Intelligent Sensory Inf. Syst., Amsterdam Univ., Netherlands","IEEE Transactions on Parallel and Distributed Systems","","2002","13","7","758","768","One of the most fundamental problems automatic parallelization tools are confronted with is to find an optimal domain decomposition for a given application. For regular domain problems (such as simple matrix manipulations), this task may seem trivial. However, communication costs in message-passing programs often depend significantly on the memory layout of data blocks to be transmitted. As a consequence, straightforward domain decompositions may be non-optimal. In this paper, we introduce a new point-to-point communication model, called P-3PC (Parameterized model based on the Three Paths of Communication), that is specifically designed to overcome this problem. In comparison with related models (e.g. LogGP), P-3PC is similar in complexity, but more accurate in many situations. Although the model is aimed at MPI's standard point-to-point operations, it is applicable to similar message-passing definitions as well. The effectiveness of the model is tested in a framework for automatic parallelization of low-level image processing applications. Experiments are performed on two Beowulf-type systems, each having a different interconnection network and a different MPI implementation. The results show that, where other models frequently fail, P-3PC correctly predicts the communication costs related to any type of domain decomposition.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2002.1019863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019863","","Costs;Message passing;Image processing;Matrix decomposition;Automatic testing;Multiprocessor interconnection networks;Predictive models;Optimization;Software performance;Software design","application program interfaces;message passing;software performance evaluation;image processing;problem solving;multiprocessor interconnection networks;communication complexity","P-3PC;point-to-point communication model;automatic optimal domain decomposition;regular domain problems;automatic parallelization tools;matrix manipulations;communication costs;message-passing programs;memory layout;data block transmission;parameterized model;communication paths;complexity;accuracy;MPI;automatic parallelization;low-level image processing applications;Beowulf-type systems;interconnection networks;performance optimization;performance modeling","","9","28","","","","","","IEEE","IEEE Journals & Magazines"
"Simulation of restaurant operations using the Restaurant Modeling Studio","D. M. Brann; B. C. Kulick","Autom. Associates Inc., Solana Beach, CA, USA; Autom. Associates Inc., Solana Beach, CA, USA","Proceedings of the Winter Simulation Conference","","2002","2","","1448","1453 vol.2","The operation of quick service restaurants (QSR) is a highly engineered process, with many factors coming into play: physical layout, equipment availability, and worker staffing levels, positioning, and priorities. The Restaurant Modeling Studio (RMS) provides an analysis platform for investigating the impacts of these factors on critical performance metrics, especially speed of service and service capacity. The key components of the RMS are a simulation engine built in Arena, and two custom applications built on Microsoft Visio - the Kitchen and Process Designers. The simulation engine supports a large number of behaviors, including parallel operations, inventory replenishment, prioritized task selection and many more. The Kitchen Designer and Process Designer provide the user with powerful tools for specifying the physical layout and order fulfillment processes. This paper presents the components of the RMS and its use in an analysis kitchen design comparison and labor deployment standards.","","0-7803-7614","10.1109/WSC.2002.1166417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166417","","Engines;Process design;Testing;Computer architecture;User interfaces;Application software;Marketing and sales;Analytical models;Automation;Availability","catering industry;digital simulation;personnel;CAD","quick service restaurants;physical layout;equipment availability;worker staffing levels;positioning;priorities;Restaurant Modeling Studio;performance metrics;service speed;service capacity;Arena;Microsoft Visio;Process Designer;Kitchen Designer;parallel operations;inventory replenishment;prioritized task selection;labor deployment standards","","2","","","","","","","IEEE","IEEE Conferences"
"Design challenges for system-in-package vs system-on-chip","C. Trigas","Motorola Semicond. Product Sector, Adv. Interconnect Syst. Lab., Munich, Germany","Proceedings of the IEEE 2003 Custom Integrated Circuits Conference, 2003.","","2003","","","663","666","System-in-package (SiP) or multi-technology designs, as seen from a semiconductor industry point of view, have created a new set of design challenges. SiP designs are typically only attempted when a wall is reached -such as size or performance constraints and conventional system-on-chip (SoC) solutions are too expensive to implement. SoC involves accessing and working with one design/technology library and partitioning decisions are highly dependent on the library construction and routing capabilities that are identified within the software. The higher integration capacity of SiP reduces the number of components in the system and reduces the size and routing complexity of the printed circuit board. SiP design challenges arise due to the lack of similar design infrastructure between semiconductor technologies and the multitude of layout possibilities. Packaging concepts include chip stacked on chip, flip chip stacked on chip, chips placed side by side in a package, among other concepts which complicate the design partitioning process. This paper discusses these challenges to optimizing system design.","","0-7803-7842","10.1109/CICC.2003.1249482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249482","","System-on-a-chip;Integrated circuit interconnections;Routing;Testing;Libraries;Semiconductor device packaging;Application software;Cost function;Electromagnetic compatibility;CMOS technology","system-on-chip;integrated circuit design;chip scale packaging;flip-chip devices;multichip modules","system-in-package;system-on-chip;SiP integration capacity;software routing capabilities;software library construction;SoC;multi-technology designs;design partitioning;printed circuit board routing complexity;stacked chip packaging;flip chip packaging;system design optimization","","8","6","","","","","","IEEE","IEEE Conferences"
"A case study of the multi-domain system design capture concept","N. T. Hoang; N. E. Karangelen","Dahlgren Div., Naval Surface Warfare Center, Silver Spring, MD, USA; NA","[1993] Proceedings of the IEEE Workshop on Real-Time Applications","","1993","","","171","175","The principal objective of the system development process is to establish a design which satisfies the system requirements and constraints while optimizing the key trade-offs and issues associated with system functionality, behavior, and implementation. The paper describes a formalism for the design of large, complex, and real-time systems that can be captured and analyzed to better meet the systems requirements. An example, the air traffic control system, is used to show the proof of concept of the method.<<ETX>>","","0-8186-4130","10.1109/RTA.1993.263093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263093","","Computer aided software engineering;Real time systems;System analysis and design;Design engineering;Systems engineering and theory;Hardware;Humans;Design optimization;Software performance;Software testing","air traffic computer control;formal specification;real-time systems","multi-domain system design capture concept;system requirements;system functionality;real-time systems;air traffic control system","","","6","","","","","","IEEE","IEEE Conferences"
"Distributing a Chemical Process Optimization Application Over a Gigabit Network","R. L. Clay; P. A. Steenkiste","Carnegie Mellon University; NA","Supercomputing '95:Proceedings of the 1995 ACM/IEEE Conference on Supercomputing","","1995","","","45","45","We evaluate the impact of a gigabit network on the implementation of a distributed chemical process optimization application. The optimization problem is formulated as a stochastic Linear Assignment Problem and was solved using the Thinking Machines CM-2 (SIMD) and the Cray C-90 (vector) computers at PSC, and the Intel iWarp (MIMD) system at CMU, connected by the Gigabit Nectar testbed. We report our experience distributing the application across this heterogeneous set of systems and present measurements that show how the communication requirements of the application depend on the structure of the application. We use detailed traces to build an application performance model that can be used to estimate the elapsed time of the application for different computer system and network combinations. Our results show that the application benefits from the high-speed network, and that the need for high network throughput is increasing as computer systems get faster. We also observed that supporting high burst rates is critical, although structuring the application so that communication is overlapped with computation relaxes the bandwidth requirements.","","0-89791-816","10.1109/SUPERC.1995.241499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383181","chemical process optimization;distributed computing;heterogeneous computing;gigabit networks;stochastic linear assignment problem;optimal resource allocation","Chemical processes;Application software;Computer networks;Stochastic systems;Vectors;System testing;Computer applications;High-speed networks;Throughput;Bandwidth","","chemical process optimization;distributed computing;heterogeneous computing;gigabit networks;stochastic linear assignment problem;optimal resource allocation","","","27","","","","","","IEEE","IEEE Conferences"
"Enhancing the semiconductor fab layout process","J. J. Plata","Texas Instrum. Inc., Dallas, TX, USA","Proceedings of 1994 IEEE/SEMI Advanced Semiconductor Manufacturing Conference and Workshop (ASMC)","","1994","","","11","15","With the dramatically increasing cost of building semiconductor facilities, all aspects of factory design and operation must be evaluated to search for cost reduction opportunities. Enhancing the factory layout process using advanced software tools may be one of those opportunities. SEMATECH's future factory design group is evaluating new software products that attempt to increase the productivity and effectiveness of the layout task and thus increase the value of the factory layout. This article focuses on Texas Instruments' findings as a part of SEMATECH's software beta test. This paper explores new semiconductor fab layout software technology and demonstrates the layout productivity gains and value added to the final layout when it was exercised by Texas Instruments newest 200 mm fab. Additionally, semiconductor layout methodology, metrics, and inherit difficulties are discussed. Emphasis is placed on the software's ability to integrate quantitative and qualitative inputs into the layout process, and provide real time feedback to diverse groups of layout users, dramatically increasing the involvement of the customer and thus the value of the final layout, while decreasing the cycle time in generating an optimized layout. Reference is made to the potential added value to the actual manufacturing operation and process. Extendibility of the layout drawings to aid in the operation and future planning of the fab are discussed.","","0-7803-2053","10.1109/ASMC.1994.588156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=588156","","Production facilities;Costs;Productivity;Instruments;Buildings;Software tools;Software testing;Software quality;Feedback;Manufacturing processes","integrated circuit manufacture","semiconductor fab layout;semiconductor facilities;factory design;cost;software tools;SEMATECH;productivity;Texas Instruments;beta test;real time feedback;customers;manufacturing;cycle time","","3","1","","","","","","IEEE","IEEE Conferences"
"Static and dynamic models of induction motors","A. M. Sharaf; M. M. Rayan","Dept. of Electr. Eng., New Brunswick Univ., Fredericton, NB, Canada; NA","The Proceedings of the Twenty-First Annual North American Power Symposium","","1989","","","110","119","The authors describe the modeling of squirrel cage induction motors and the derivation of steady-state and dynamic models that can be utilized in harmonic penetration studies. They present the results of laboratory testing of three-phase and single-phase induction motors to estimate these static and dynamic models. Such models are extremely useful in: identifying effective control strategies to enhance efficiency of variable voltage/variable frequency motor drives; assessing machine performance with excitation voltage and frequency variations; and identifying nonlinear load impedance models of induction motor loads in harmonic penetration programs such as Cyme and Harmflo software. These models were utilized in deriving efficient open-loop control strategies based on optimized volt/hertz (V/sub m//f/sub s/) ratio characteristics for minimum input power, minimum losses, and maximum efficiency.<<ETX>>","","0-8186-2005","10.1109/NAPS.1989.77082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=77082","","Induction motors;Open loop systems;Frequency;Steady-state;Testing;Voltage control;Motor drives;Impedance;Load modeling;Software performance","electric machine analysis computing;harmonics;machine theory;squirrel cage motors","machine theory;electric machine analysis computing;models;squirrel cage induction motors;harmonic penetration;testing;variable voltage/variable frequency motor drives;nonlinear load impedance;software;open-loop control strategies;losses;efficiency","","","5","","","","","","IEEE","IEEE Conferences"
"Process optimization and productivity improvement by real-time data collection system","Tuung Luoh; Ling-Wuu Yang; Hsueh-Hao Shih; Chi-Tung Huang; Kuang-Chao Chen; Yaw-Lin Hwang; C. Hsueh; H. Chung","Macronix Int. Co. Ltd., Hsinchu, Taiwan; Macronix Int. Co. Ltd., Hsinchu, Taiwan; Macronix Int. Co. Ltd., Hsinchu, Taiwan; Macronix Int. Co. Ltd., Hsinchu, Taiwan; Macronix Int. Co. Ltd., Hsinchu, Taiwan; Macronix Int. Co. Ltd., Hsinchu, Taiwan; Macronix Int. Co. Ltd., Hsinchu, Taiwan; Macronix Int. Co. Ltd., Hsinchu, Taiwan","Semiconductor Manufacturing Technology Workshop, 2002","","2002","","","140","143","Factory automation data management (FADM) system is a real-time computer control system. It can record all the process and hardware parameters during processing. The functions in the software include GUI, data communication, digital I/O, D/S control, etc. FADM was adopted in two single-wafer thermal process chambers, in-situ steam generation (ISSG) oxidation and LPCVD poly-silicon (POLYgen), to real-time monitoring the process parameters during processing. Mini-marathon tests for POLYgen and ISSG processes demonstrated that FADM could help engineers to obtain the process stability and its limitation. According to the FADM results, the chambers were retrofitted for process optimization.","","0-7803-7604","10.1109/SMTW.2002.1197394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1197394","","Productivity;Real time systems;Automatic control;Communication system control;Manufacturing automation;Control systems;Hardware;Graphical user interfaces;Data communication;Oxidation","graphical user interfaces;data acquisition;factory automation;real-time systems;process control;data handling;integrated circuit manufacture","computer control system;factory automation data management system;hardware parameter;GUI software;real-time system;digital I/O control;single wafer thermal process chamber;in situ stream generation oxidation;process optimization;LPCVD polysilicon;POLYgen","","","2","","","","","","IEEE","IEEE Conferences"
"DPF: a data parallel Fortran benchmark suite","Yu Hu; S. L. Johnsson; D. Kehagias; N. Shalaby","Div. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA, USA; NA; NA; NA","Proceedings 11th International Parallel Processing Symposium","","1997","","","219","226","We present the Data Parallel Fortran (DPF) benchmark suite, a set of data parallel Fortran codes for evaluating data parallel compilers appropriate for any target parallel architecture, with shared or distributed memory. The codes are provided in basic, optimized and several library versions. The functionality of the benchmarks cover collective communication functions, scientific software library functions, and application kernels that reflect the computational structure and communication patterns in fluid dynamic simulations, fundamental physics and molecular studies in chemistry or biology. The DPF benchmark suite assumes the language model of High Performance Fortran, and provides performance evaluation metrics of busy and elapsed times and FLOP rates, FLOP count, memory usage, communication patterns, focal memory access, and arithmetic efficiency as well as operation and communication counts per iteration. An instance of the benchmark suite was fully implemented in CM-Fortran and tested on the CM-5.","1063-7133","0-8186-7793","10.1109/IPPS.1997.580896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=580896","","Software libraries;Biological system modeling;Parallel architectures;Application software;Kernel;Biology computing;Physics computing;Fluid dynamics;Computational modeling;Chemistry","parallel languages;FORTRAN;program testing;software performance evaluation","data parallel Fortran benchmark suite;DPF;data parallel Fortran codes;data parallel compilers;parallel architecture;library versions;collective communication functions;scientific software library functions;application kernels;communication patterns;fluid dynamic simulations;fundamental physics;molecular studies;chemistry;biology;High Performance Fortran;FLOP rates;FLOP count;memory usage;focal memory access","","","17","","","","","","IEEE","IEEE Conferences"
"Learning lazy naive Bayesian classifiers for ranking","Liangxiao Jiang; Yuanyuan Guo","Fac. of Comput. Sci., China Univ. of Geosciences, Hubei, China; Fac. of Comput. Sci., China Univ. of Geosciences, Hubei, China","17th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'05)","","2005","","","5 pp.","416","Naive Bayes (simply NB) has been well-known as an effective and efficient classification algorithm. However, it is based on the conditional independence assumption that it is often violated in applications. In addition, in many real-world data mining applications, however, an accurate ranking of instances is often required rather than an accurate classification. For example, a ranking of customers in terms of the likelihood that they buy one's products is useful in direct marketing. In this paper, we firstly investigate the ranking performance of some lazy learning algorithms for extending naive Bayes. The ranking performance is measured by Hand and Till (2001) and Bradley (1997). We observe that they can not significantly improve naive Bayes' ranking performance. Motivated by this fact and aiming at improving naive Bayes with accurate ranking, we present a new lazy learning algorithm, called lazy naive Bayes (simply LNB), to extend naive Bayes for ranking. We experimentally tested our algorithm, using the whole 36 UCI data sets (Blake and Merz, 2000) recommended by Weka, and compared it to NB and C4.4 (Provost and Domingos, 2003) measured by AUC. The experimental results show that our algorithm significantly outperforms both NB and C4.4","1082-3409;2375-0197","0-7695-2488","10.1109/ICTAI.2005.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1562971","","Bayesian methods;Niobium;Data mining;Error analysis;Application software;Testing;Computer science;Geology;Classification algorithms;Equations","Bayes methods;learning (artificial intelligence);pattern classification","lazy naive Bayesian classifier learning;naive Bayes method;classification algorithm;lazy learning algorithm","","3","15","","","","","","IEEE","IEEE Conferences"
"A new optimised design of integrated three phase gas insulated bus duct","R. Rajaraman; S. Satyanarayana; A. Bhoomaiah; H. S. Jain","R&D Div., BHEL, Hyderabad, India; NA; NA; NA","1999 Eleventh International Symposium on High Voltage Engineering","","1999","5","","414","417 vol.5","The paper highlights the techniques adopted for design optimisation of a three phase single enclosure, 145 kV class gas insulated bus duct. The design is based on extensive electrostatic field studies in three dimensions using special software. A unique feature of this design is the introduction of individual rib insulators for supporting each phase conductor, in place of the conventional single conical shaped insulator. A model bus duct has been fabricated and successfully tested for its specified dielectric duties.","0537-9989","0-85296-719","10.1049/cp:19990970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=818322","","","gas insulated switchgear","integrated three phase gas insulated bus duct;design optimisation;three phase single enclosure;3D electrostatic field studies;software;rib insulators;phase conductor;busbars;145 kV","","","","","","","","","IET","IET Conferences"
"Dynamic programming with ARMA, Markov, and NARMA models vs. Q-learning-case study","J. Chrobak; A. Pacut; A. Karbowski","Warsaw Univ. of Technol., Poland; NA; NA","Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium","","2000","3","","265","270 vol.3","Two approaches to control policy synthesis for unknown systems are investigated. The indirect approach is based on the identification of ARMA, NARMA, or Markov chain models, and applications of dynamic programming to these models with or without the use of a certainty equivalence principle. The direct approach is represented by Q-learning, with the lookup table or with the use of radial basis function approximation. We implemented both methods to optimization of a stock portfolio and tested on the Warsaw stock exchange data.","1098-7576","0-7695-0619","10.1109/IJCNN.2000.861314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=861314","","Dynamic programming;Computer aided software engineering;Testing;Control system synthesis;Investments;Share prices;Optimization methods;Stock markets;Table lookup;Function approximation","investment;radial basis function networks;function approximation;dynamic programming;autoregressive moving average processes;hidden Markov models;table lookup;learning (artificial intelligence);stock markets;probability","dynamic programming;ARMA model;probability;NARMA models;Q-learning;Markov chain models;lookup table;radial basis function neural networks;optimization;stock market;portfolio;function approximation","","1","7","","","","","","IEEE","IEEE Conferences"
"Evolving PC system hardware configurations","D. Dasgupta; A. Stoliartchouk","Math. Sci. Dept., Memphis Univ., TN, USA; Math. Sci. Dept., Memphis Univ., TN, USA","Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)","","2002","1","","517","522 vol.1","The task of determining proper configuration of a PC system may be very difficult for a customer having little knowledge about computer hardware components. Also, it becomes more and more difficult even for a specialist, (with time constraints) to determine the proper specification, because the market offers a wide variety of products to choose from. We represented this hardware configuration problem in the form of tree structures, where nodes represent all possible hardware components of a PC system. We then used structured genetic algorithms to solve this multi-level optimization problem. Accordingly, we developed a prototype system called ""Computer Hardware Designing System"" and successfully tested it on a number of empirical examples.","","0-7803-7282","10.1109/CEC.2002.1006288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006288","","Hardware;Tree data structures;Application software;Computer science;Time factors;Genetic algorithms;Prototypes;System testing;Investments;Design optimization","reconfigurable architectures;microcomputers;evolutionary computation;configuration management","evolving hardware configurations;personal computer configuration;time constraints;tree structures;hardware components;structured genetic algorithms;multi-level optimization;Computer Hardware Designing System","","","10","","","","","","IEEE","IEEE Conferences"
"Balanced optimization of 1.31 /spl mu/m tunnel-junction VCSELs","J. Piprek; V. Jayaraman; M. Mehta; J. E. Bowers","Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA; Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA; Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA; Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA","IEEE/LEOS 3rd International Conference on Numerical Simulation of Semiconductor Optoelectronic Devices, 2003. Proceedings","","2003","","","45","46","A hybrid approach that combines the advantages of GaAs mirrors and InP active layers in InP/GaAs wafer bonded 1.31 /spl mu/m VCSELs is described. This novel device concept features intra-cavity ring contacts, five strain-compensated AlGaInAs quantum wells, and an AlInAs/InP tunnel junction in order to reduce absorption by p-doped layers. The tunnel junction is literally confined forming an aperture for current injection and wave guiding. A self-consistent VCSEL simulation software is employed in order to analyze and optimize the internal device physics.","","0-7803-7992","10.1109/NUSOD.2003.1259042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259042","","Vertical cavity surface emitting lasers;Indium phosphide;Gallium arsenide;Apertures;Optical fiber communication;Optical fiber devices;Optical fiber testing;Thermal conductivity;Mirrors;Absorption","surface emitting lasers;laser mirrors;gallium arsenide;III-V semiconductors;indium compounds;aluminium compounds;semiconductor device models;integrated optoelectronics;quantum well lasers;laser cavity resonators;semiconductor quantum wells","tunnel-junction VCSELs;GaAs mirrors;InP active layers;intracavity ring contacts;strain-compensated AlGaInAs quantum wells;AlInAs/InP tunnel junction;current injection;wave guiding;VCSEL simulation software;1.31 mum;InP-GaAs;AlInAs-InP;AlGaInAs","","1","3","","","","","","IEEE","IEEE Conferences"
"A statistical approach for parallel optimization with application to VLSI placement","K. Efe","Center for Adv. Comput. Studies, Southwestern Louisiana Univ., Lafayette, LA, USA","[1991] Proceedings. 11th International Conference on Distributed Computing Systems","","1991","","","518","525","A massively parallel optimization approach based on simple neighborhood search techniques was developed and applied to the problem of VLSI cell placement. Statistical models are developed to analyze the performance of the approach in general, and to derive statistical bounds on the quality of obtainable results. The results of these analyses suggest a simple framework for approximate solution of difficult problems. The approach is inherently parallel, and it can be implemented on any type of parallel computer. It was implemented on a simulated hypercube MIMD machine using Cosmic C on a network of Sun workstations. The method is empirically verified.<<ETX>>","","0-8186-2144","10.1109/ICDCS.1991.148720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148720","","Very large scale integration;Simulated annealing;Computational modeling;Costs;Application software;Concurrent computing;Search methods;Hypercubes;Sun;Testing","circuit layout CAD;optimisation;search problems;VLSI","statistical approach;parallel optimization;VLSI placement;neighborhood search techniques;statistical bounds;simulated hypercube MIMD machine;Cosmic C;Sun workstations","","2","9","","","","","","IEEE","IEEE Conferences"
"Taming control flow: a structured approach to eliminating goto statements","A. M. Erosa; L. J. Hendren","Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada; Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada","Proceedings of 1994 IEEE International Conference on Computer Languages (ICCL'94)","","1994","","","229","240","In designing optimizing and parallelizing compilers, it is often simpler and more efficient to deal with programs that have structured control flow. Although most programmers naturally program in a structured fashion, there remain many important programs and benchmarks that include some number of goto statements, thus rendering the entire program unstructured. Such unstructured programs cannot be handled with compilers built with analyses and transformations for structured programs. In this paper we present a straight-forward algorithm to structure C programs by eliminating all goto statements. The method works directly on a high-level abstract syntax tree (AST) representation of the program and could easily be integrated into any compiler that uses an AST-based intermediate representation. The actual algorithm proceeds by eliminating each goto by first applying a sequence of goto-movement transformations followed by the appropriate goto-elimination transformation. We have implemented the method in the McCAT (McGill Compiler Architecture Testbed) optimizing/parallelizing C compiler and we present experimental results that demonstrate that the method is both efficient and effective.<<ETX>>","","0-8186-5640","10.1109/ICCL.1994.288377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=288377","","Program processors;Optimizing compilers;Programming profession;Switches;Computer science;Design optimization;Software testing;Software engineering;Information analysis;Flow graphs","program compilers;C language;parallel programming","goto statements;control flow;parallelizing compilers;optimizing compilers;C programs;high-level abstract syntax tree;AST representation;intermediate representation;goto-movement transformations;goto-elimination;McCAT;McGill Compiler Architecture Testbed","","17","18","","","","","","IEEE","IEEE Conferences"
"Increasing the portability and re-usability of protocol code","B. Krupczak; K. L. Calvert; M. H. Ammar","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA","IEEE/ACM Transactions on Networking","","1997","5","4","445","459","Deploying protocols is an expensive and time-consuming process today. One reason is the high cost of developing, testing, and installing protocol implementations. To reduce this difficulty, protocols are developed and executed within environments called protocol subsystems, and protocol software is often ported instead of being coded from scratch. Unfortunately, today a variety of protocol subsystems offer a plethora of features, functionality, and drawbacks; the differences among them often reduce the portability and reusability of protocol code, and therefore present barriers to the deployment of new protocols. In this paper, we consider differences in subsystems and their effect on the portability and reusability of protocols and protocol implementations. We then propose two different approaches, each optimized for a different situation, that allow protocol code implemented in one subsystem to be used without modification within other subsystems, and thus reduce the barriers to protocol deployment. We relate our experiences designing, implementing, and measuring the performance of each approach using, as a baseline, an AppleTalk protocol stack we have developed.","1063-6692;1558-2566","","10.1109/90.649455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649455","","Protocols;Programming profession;Costs;Testing;Operating systems;Programming environments;Internet;Contracts;Military computing;Design optimization","protocols;software portability;software reusability;telecommunication computing","portability;re-usability;protocol code;protocol subsystems;protocol software;functionality;drawbacks;implementations;design;implementation;performance;AppleTalk protocol stack","","6","22","","","","","","IEEE","IEEE Journals & Magazines"
"Tool path simulation using a virtual 5-axis milling machine","M. Munlin","Dept. of Inf. Technol., Thammasat Univ., Pathumthani, Thailand","2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.","","2002","1","","193","198 vol.1","Presents the algorithms to simulate nonlinear kinematics of a 5-axis milling machine. The simulator is based on 3D representation and employing the inverse kinematics approach to derive the corresponding rotational and translation movement of the mechanism. The simulator makes it possible to analyze the accuracy of a 3D tool-path based on a prescribed set of the cutter location (CL) points as well as a set of the cutter contact (CC) points with tool inclination angle. The resulting trajectory of the tool is not unique and depends on the initial set up of the machine which in turn is problem dependent. Furthermore, the simulator can be used to simulate the milling process, verify the final cut and estimate the errors of the actual tool-path before the real workpiece is actually being tested with the real machine. Thus, reducing the cost of iterative trial and error. Tool path simulation is verified by a series of cutting experiments performed by means of the proposed software and evaluates the accuracy of milling. It has been shown that the proposed graphical 3D software presents an efficient interactive approach to the interactive modification of a tool path based on an appropriate set of transformations as well as to verification of the tool path optimization algorithms.","","0-7803-7657","10.1109/ICIT.2002.1189889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189889","","Metalworking machines;Kinematics;Milling;Software tools;Analytical models;Testing;Costs;Performance evaluation;Software performance;Software algorithms","machining;cutting;machine tools;computerised numerical control;kinematics;CAD/CAM;digital simulation","tool path simulation;virtual 5-axis milling machine;nonlinear kinematics;3D representation;inverse kinematics;rotational movement;translation movement;3D tool path;cutter location points;cutter contact points;tool inclination angle;graphical 3D software;interactive approach","","2","16","","","","","","IEEE","IEEE Conferences"
"Optimization of backside micromachined CMOS inductors for RF applications","M. Ozgur; M. E. Zaghloul; M. Gaitan","Dept. of Electr. Eng. & Comput. Sci., George Washington Univ., Washington, DC, USA; NA; NA","2000 IEEE International Symposium on Circuits and Systems. Emerging Technologies for the 21st Century. Proceedings (IEEE Cat No.00CH36353)","","2000","5","","185","188 vol.5","The quality factor of backside micromachined CMOS inductors is optimized for high frequency applications. Up to 90% improvement of the peak quality factor is predicted for 10 nH inductors according to the simulations over previously published results in this technology. Extensive tests will be performed for the fabricated inductors with an improved post-processing procedure.","","0-7803-5482","10.1109/ISCAS.2000.857394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857394","","Inductors;Radio frequency;Q factor;Stress;Parasitic capacitance;Polyimides;Biomembranes;Application software;CMOS technology;Semiconductor device modeling","CMOS integrated circuits;inductors;Q-factor;micromachining","optimization;backside micromachining;CMOS inductor;RF applications;quality factor;high frequency applications","","19","10","","","","","","IEEE","IEEE Conferences"
"BDL, a language of distributed reactive objects","J. -. Talpin; A. Benveniste; B. Caillaud; C. Jard; Z. Bouziane; H. Canon","IRISA, Rennes, France; NA; NA; NA; NA; NA","Proceedings First International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC '98)","","1998","","","196","205","We introduce the definition of a language of distributed reactive objects, a Behaviour Description Language (BDL), as a unified medium for specifying, verifying, compiling and validating object-oriented distributed reactive systems. One of the novelties in BDL is its seamless integration into the Unified Modeling Language approach (UML). BDL supports a description of objects interaction which respects both the functional architecture of system designs and the declarative style of diagram descriptions. This support is implemented by means of a partial-order theoretical framework. This framework allows to specify both the causality and the control models of object interactions independently of any hypothesis on the actual configuration of the system. Given the description of such a configuration, the use of BDL offers new perspectives for a flexible verification of systems by modeling them as an asynchronous network of synchronous components. It allows an optimized code generation by using compilation techniques developed for synchronous languages. It permits an accurate validation and test of applications by supporting the manipulation of both causal and control dependencies. BDL aims at maximizing the re-usability of high-level specifications while minimizing programming effort and test-case based validation of distributed systems.","","0-8186-8430","10.1109/ISORC.1998.666789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=666789","","Electrical capacitance tomography;Unified modeling language;System testing;Software systems;Engineering management;Software engineering;Software testing;Natural languages;Computers;Proposals","distributed processing;specification languages;formal verification","distributed reactive objects;Behaviour Description Language;BDL;specifying;verifying;compiling;validating;object-oriented distributed reactive systems;distributed systems","","4","20","","","","","","IEEE","IEEE Conferences"
"Performance analysis of generics in scientific computing","L. Dragan; S. M. Watt","Ontario Res. Centre for Comput. Algebra, Western Ontario Univ., London, Ont., Canada; Ontario Res. Centre for Comput. Algebra, Western Ontario Univ., London, Ont., Canada","Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)","","2005","","","8 pp.","","This paper studies the performance of generics, or templates as they are sometimes called, for scientific computing in various programming languages. In order to understand the cost of using generics, we develop a test suite for generics based on a standard numeric benchmark. We compare the results of this new benchmark for generics in C++, C# and Java, both between language implementations and against the specialized, non-generic benchmark. We also compare the efficiency of C++ with Aldor a language originally for computer algebra relying entirely on generics. We find that the implementation of generics in current compilers must be improved before they are used for efficiency-critical scientific applications, and we identify specific areas for potential optimization.","","0-7695-2453","10.1109/SYNASC.2005.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595834","","Performance analysis;Scientific computing;Benchmark testing;Computer languages;Costs;Standards development;Java;Algebra;Application software;Optimizing compilers","natural sciences computing;software performance evaluation;software libraries","performance analysis;scientific computing;generics performance;C++ programming language;C# programming language;Java programming language","","6","15","","","","","","IEEE","IEEE Conferences"
"Benchmark-based design strategies for single chip heterogeneous multiprocessors","J. M. Paul; D. E. Thomas; A. Bobrek","Electr. & Comput. Eng. Dept., Carnegie Mellon Univ., Pittsburgh, PA, USA; Electr. & Comput. Eng. Dept., Carnegie Mellon Univ., Pittsburgh, PA, USA; Electr. & Comput. Eng. Dept., Carnegie Mellon Univ., Pittsburgh, PA, USA","International Conference on Hardware/Software Codesign and System Synthesis, 2004. CODES + ISSS 2004.","","2004","","","54","59","Single chip heterogeneous multiprocessors are arising to meet the computational demands of portable and handheld devices. These computing systems are not fully custom designs traditionally targeted by the design automation (DA) community, general purpose designs traditionally targeted by the computer architecture (CA) community, nor pure embedded designs traditionally targeted by the real-time (RT) community. An entirely new design philosophy will be needed for this hybrid class of computing. The programming of the device will be drawn from a narrower set of applications with execution that persists in the system over a longer period of time than for general purpose programming. But the devices will still be programmable, not only at the level of the individual processing element, but across multiple processing elements and even the entire chip. The design of other programmable single chip computers has enjoyed an era where the design trade-offs could be captured in simulators such as SimpleScalar and performance could be evaluated to the SPEC benchmarks. Motivated by this, we describe new benchmark-based design strategies for single chip heterogeneous multiprocessors. We include an example and results.","","1-58113-937","10.1109/CODESS.2004.240797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1360479","","Application software;Computer architecture;Computational modeling;Design automation;Design optimization;Bandwidth;Application specific integrated circuits;Embedded system;Embedded computing;Benchmark testing","system-on-chip;computer architecture;benchmark testing","benchmark-based design strategies;single chip heterogeneous multiprocessors;portable devices;handheld devices;design automation;computer architecture;real-time community;general purpose programming;programmable single chip computers;SimpleScalar;SPEC benchmarks;system-on-chips;SoC;scenario-oriented design","","3","16","","","","","","IEEE","IEEE Conferences"
"Extended application of Ada to cover ECBS with O4S","I. Ogren","Romet AB, Vaddo, Sweden","Proceedings 1994 Tutorial and Workshop on Systems Engineering of Computer-Based Systems","","1994","","","145","151","The system concept is presented as a structure constituted from objects from the categories mission, operator, software and hardware. It is shown how Ada's semantics and syntax can be used in the pseudo language Odel to describe system structure and object behaviour, regardless of object category. Experience form 10 years of Ada-based methods work has led to an understanding of the evolutionary and iterative nature of systems work, expressed in the O4S method with its ""ball-bearing"" development model. It is shown how the method makes it possible to unify system and project structure and thus optimize project resource utilization. Application areas for the O4S are shown concentrating on system development with cooperation between users and developers.<<ETX>>","","0-8186-5715","10.1109/ECBS.1994.331673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=331673","","Hardware;Computer languages;Software packages;Packaging;Software systems;Testing;Logic programming;Application software;Iterative methods;Optimization methods","Ada;programming environments;systems analysis","Ada;ECBS;O4S;system concept;semantics;syntax;pseudo language Odel;object behaviour;project resource utilization;system development;engineering of computer based systems;objects for systems","","","6","","","","","","IEEE","IEEE Conferences"
"Software processing performance in network processors","I. Papaefstathiou; G. Kornaros; N. Zervos","Inst. of Comput. Sci., Found. of Res. & Technol. Hellas, Crete, Greece; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","3","","186","191 Vol.3","To meet the demand for higher performance, flexibility, and economy in today's state-of-the-art networks, an alternative to the ASICs that traditionally were used to implement packet-processing functions in hardware, called network processors (NPs), has emerged. In this paper, we briefly outline the architecture of such an innovative network processor aiming at the acceleration of protocol processing in high-speed network interfaces, and we use this architecture as a case study for our measurements. We focus on the performance of the general purpose processors used for executing high level protocol processing, since this part proves to be the bottleneck of the design. The performance is analyzed by executing a set of widely used, real applications and by applying network traffic according to certain stochastic criteria. The performance of the RISC used is compared with that of other well-known CPU architectures so as to verify that our results are applicable to the general network processors era. As our results demonstrate, the bottleneck of the majority of the network processors is the general-purpose processing units used, since today's network protocols need a great amount of high-level processing. On the other hand the specific purpose processors or co-processors, optimized for certain part of the network packet processing, involved in such systems, can provide the power needed, even at today's ultra high network speeds.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269228","","Software performance;Protocols;Hardware;Computer architecture;Accelerometers;Acceleration;High-speed networks;Performance analysis;Telecommunication traffic;Stochastic processes","network interfaces;telecommunication traffic;microprocessor chips;transport protocols;performance evaluation","software processing performance;network processors;high speed network interfaces;network traffic;high level protocol processing;CPU architectures;central processing unit;RISC;reduced instruction set computing;network packet processing","","","12","","","","","","IEEE","IEEE Conferences"
"Implementation and evaluation of an on-demand parameter-passing strategy for reducing energy","M. Kandemir; L. Kolcu; W. Zhang","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","1058","1063","In this paper, we present an energy-aware parameter-passing strategy called on-demand parameter-passing. The objective of this strategy is to eliminate redundant actual parameter evaluations if the corresponding formal parameter in a subroutine is not used during execution. This on-demand parameter-passing is expected to be very successful in reducing energy consumption of large, multi-routine embedded applications at the expense of a slight implementation complexity.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253744","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253744","","Energy consumption;Algorithms;Computer languages;Circuits;Optimizing compilers;Application software;Program processors;Energy efficiency;Energy management;Hardware","subroutines;program control structures;optimising compilers;low-power electronics;embedded systems","programming language support;optimizing compiler;on-demand parameter-passing strategy;energy consumption reduction;multiroutine embedded applications;energy-aware parameter-passing strategy;redundant parameter evaluation reduction;subroutine execution","","","12","","","","","","IEEE","IEEE Conferences"
"Reference standard digital recorder with 200 MS/s-12 bit for HV impulse tests","W. Strauss","Strauss Syst.-Elektron. GmbH, Gundelsheim, Germany","1999 Eleventh International Symposium on High Voltage Engineering","","1999","1","","226","229 vol.1","The paper describes the hardware of a new generation of digital recorders, designed and optimized to fulfil all requirements for measuring of full or chopped lightning, switching and current impulses according to all applicable standards. Such digital recorders are required for Accredited Laboratories and National Metrological Laboratories. The results of the official calibration of the first digital recorder type TR-AS 200-12 performed by the Physikalisch-Technische Bundesanstalt (PTB) in Braunschweig are shown. This digital recorder is available now as Reference Standard of Measurement in our DKD Calibration Laboratory for electrical impulse parameter. The technical data and software properties are described.","0537-9989","0-85296-719","10.1049/cp:19990548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=820385","","","digital instrumentation","reference standard digital recorder;HV impulse tests;digital recorders;chopped lightning impulse measurement;switching impulse measurement;current impulse measurment;Accredited Laboratories;National Metrological Laboratories;TR-AS 200-12 digital recorder;Physikalisch-Technische Bundesanstalt;Braunschweig;DKD Calibration Laboratory;electrical impulse parameter;software properties;technical data","","","","","","","","","IET","IET Conferences"
"Determination of material properties of thin layers using angle beam ultrasonic spectroscopy","L. Adler; S. Rokhlin; A. Baltazar","Adler Consultants, Inc., Columbus, OH, USA; NA; NA","2001 IEEE Ultrasonics Symposium. Proceedings. An International Symposium (Cat. No.01CH37263)","","2001","1","","701","704 vol.1","The objective of this paper is to describe a dual beam scanning approach for the characterization of free layers, coatings and embedded layers. We have developed and tested a prototype of the Angle Beam Ultrasonic Spectroscopy (ABUS) scanner. In addition to the capabilities of a conventional ultrasonic scanner, the system features dual beam scanning and it is optimized to display normal and oblique incidence frequency response of thin layers from which the above mentioned acoustical properties can be reconstructed. Software and hardware were developed to insure system operation and signal processing in real-time. The original feature of the system is the built-in layer property determination and display capability. The layer thickness, density, elastic moduli and attenuations can be determined at any location using the live signals or over the whole area in a postprocessing mode. The property determination algorithm is based on the optimized inversion of the normal and oblique reflection spectra. Experimental results and images of elastic properties obtained with the ABUS system will presented for immersed layers, coating and adhesive joint.","","0-7803-7177","10.1109/ULTSYM.2001.991822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991822","","Material properties;Acoustic beams;Coatings;Displays;Signal processing algorithms;Acoustic testing;Software prototyping;Prototypes;Spectroscopy;Frequency response","thin films;coatings;elastic moduli measurement;thickness measurement;density measurement;ultrasonic materials testing","material properties;thin layers;angle beam ultrasonic spectroscopy;free layers;coatings;embedded layers;dual beam scanning;real-time system operation;density;layer thickness;elastic moduli;attenuations","","","4","","","","","","IEEE","IEEE Conferences"
"BEATNP: a tool for partitioning Boolean networks","H. Cho; G. Hachtel; M. Nash; L. Setiono","Dept. of Electr. & Comput. Eng., Colorado Univ., Boulder, CO, USA; Dept. of Electr. & Comput. Eng., Colorado Univ., Boulder, CO, USA; Dept. of Electr. & Comput. Eng., Colorado Univ., Boulder, CO, USA; Dept. of Electr. & Comput. Eng., Colorado Univ., Boulder, CO, USA","[1988] IEEE International Conference on Computer-Aided Design (ICCAD-89) Digest of Technical Papers","","1988","","","10","13","BEATNP (BoolEAn Tools Network Partitioner) was designed to extend the application size capability of the BOLD (Boulder Optimal Logic Design) system. BEATNP partitions a Boolean network into subnetworks which satisfy user specified size constraints. Most of the tools in the BOLD tools suite solve problems which are in NP or Co-NP, so they can be assumed to have exponential complexity. Because the BEATNP algorithms have log-linear worst-case complexity, the CPU time requirements of optimization tools can be reduced greatly in difficult cases. When used with the BOLD minimizer on a set of well known benchmark examples, BEATNP reduced CPU time by 1 to 3 orders of magnitude while retaining a significant majority of the optimization savings available in the unpartitioned case.<<ETX>>","","0-8186-0869","10.1109/ICCAD.1988.122452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=122452","","Partitioning algorithms;Logic design;Minimization;Clustering algorithms;Computer networks;Design engineering;Application software;Benchmark testing;Design optimization;Boolean functions","computational complexity;logic CAD;minimisation of switching nets","Boolean network partitioner;BEATNP;application size capability;Boulder Optimal Logic Design;Boolean network;subnetworks;user specified size constraints;exponential complexity;log-linear worst-case complexity;CPU time requirements;optimization tools;BOLD minimizer;CPU time","","11","10","","","","","","IEEE","IEEE Conferences"
"On transforming Petri net model to Moore machine","C. K. Chang; H. Huang","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","Proceedings., Fourteenth Annual International Computer Software and Applications Conference","","1990","","","267","272","The absence of satisfactory methods for verifying the liveness and fairness properties limits the analysis power of Petri net theories. An approach is introduced to connect the Petri net model with the Model Checker. A translator is used to transform the reachability graph of the Petri net to the Moore machine. The Moore machine and the behaviors specified by temporal logic are the inputs of the Model Checker, which is able to verify the properties of liveness and fairness. During the transformation, local and global behaviors of the Petri net model are separated, which means that a certain modularity can be achieved. An optimization technique is presented to trim the unnecessary local information from the local reachability graphs. The space complexity of manipulating the global reachability graph, which is generated by combing the trimmed local reachability graph, can be reduced. Moreover, a new approach is proposed to verify the concurrency behavior by using the Model Checker.<<ETX>>","","0-8186-2054","10.1109/CMPSAC.1990.139365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=139365","","Performance analysis;Automatic testing;Concurrent computing;Roentgenium;Logic;Control system analysis;Power generation;Software engineering;Computer aided software engineering;Software systems","computational complexity;finite automata;parallel programming;Petri nets;program verification;temporal logic","verification;Petri net;Model Checker;reachability graph;Moore machine;temporal logic;liveness;fairness;optimization technique;local reachability graphs;space complexity;concurrency behavior","","1","8","","","","","","IEEE","IEEE Conferences"
"Computer-assisted experimental design for the optimization of electrostatic separation processes","M. Mihailescu; A. Samuila; A. Urs; R. Morar; A. Iuga; L. Dascalescu","EGH, Cluj-Napoca, Romania; NA; NA; NA; NA; NA","IEEE Transactions on Industry Applications","","2002","38","5","1174","1181","Electrostatic separation is a typical multifactorial process. Its efficiency depends on the characteristics of the granular mixtures to be sorted, the feed rate, the configuration of the electrode system, the applied high voltage, the environmental conditions, and so on. The aim of this paper is to demonstrate the usefulness of computer-assisted experimental design in the optimization of such a process. The example analyzed in the paper was suggested by a typical application of electrostatic separation technique in the recycling industry: the selective sorting of metals and insulating materials from chopped wire and cable waste. The objective was to maximize the benefits from the recycling of both constituents of a binary copper-polyvinyl chloride granular mixture. A preliminary set of electrostatic separation tests, performed on a custom-designed laboratory unit, guided the choice of the starting values of the parameters considered in the computer-assisted experimental procedure of process optimization. The results of a first experiment, carried out in conformity with a fractional factorial scheme, were used for the computation of the coefficients of a linear mathematical model of the electrostatic process. The model was then employed to predict the values of the operating variables for which the optimum of the process is attained. A second experiment was performed in order to confirm the accuracy of the prediction. The procedure presented in this paper and the accompanying computer programs can be easily adapted to other electrostatic process applications.","0093-9994;1939-9367","","10.1109/TIA.2002.802897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035166","","Design for experiments;Design optimization;Separation processes;Application software;Recycling;Electrostatic processes;Feeds;Electrodes;Voltage;Electrostatic analysis","recycling;cables (electric);organic insulating materials;electrostatic devices;electrical engineering computing;design of experiments","computer-assisted experimental design;electrostatic separation processes optimisation;granular mixtures;feed rate;electrode system;high voltage;environmental conditions;selective materials sorting;chopped wire and cable waste;insulating materials sorting;metals sorting;binary copper-polyvinyl chloride granular mixture;process optimization;fractional factorial scheme;linear mathematical model","","29","23","","","","","","IEEE","IEEE Journals & Magazines"
"Memory-enhanced univariate marginal distribution algorithms for dynamic optimization problems","Shengxiang Yang","Dept. of Comput. Sci., Leicester Univ., UK","2005 IEEE Congress on Evolutionary Computation","","2005","3","","2560","2567 Vol. 3","Several approaches have been developed into evolutionary algorithms to deal with dynamic optimization problems, of which memory and random immigrants are two major schemes. This paper investigates the application of a direct memory scheme for univariate marginal distribution algorithms (UMDAs), a class of evolutionary algorithms, for dynamic optimization problems. The interaction between memory and random immigrants for UMDAs in dynamic environments is also investigated. Experimental study shows that the memory scheme is efficient for UMDAs in dynamic environments and that the interactive effect between memory and random immigrants for UMDAs in dynamic environments depends on the dynamic environments.","1089-778X;1941-0026","0-7803-9363","10.1109/CEC.2005.1555015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1555015","","Heuristic algorithms;Evolutionary computation;Testing;Computer science;Application software;Design optimization;Electric breakdown;Genetic algorithms;Electronic design automation and methodology;Mathematical analysis","distributed algorithms;evolutionary computation;dynamic programming;storage management","univariate marginal distribution algorithms;dynamic optimization problems;evolutionary algorithms;memory immigrants;random immigrants;direct memory scheme;UMDA","","4","21","","","","","","IEEE","IEEE Conferences"
"Array prefetching for irregular array accesses in Titanium","J. Su; K. Yelick","Div. of Comput. Sci., California Univ., Berkeley, CA, USA; Div. of Comput. Sci., California Univ., Berkeley, CA, USA","18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.","","2004","","","158","","Summary form only given. Compiling irregular applications, such as sparse matrix vector multiply and particle/mesh methods in a SPMD parallel language is a challenging problem. These applications contain irregular array accesses, for which the array access pattern is not known until runtime. Numerous research projects have approached this problem under the inspector executor paradigm in the last 15 years. The value added by the work described in this paper is in using performance modeling to choose the best data communication method in the inspector executor model. We explore our ideas in a compiler for Titanium, a dialect of Java designed for high performance computing. For a sparse matrix vector multiply benchmark, experimental results show that the optimized Titanium code has comparable performance to C code with MPI using the Aztec library.","","0-7695-2132","10.1109/IPDPS.2004.1303148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303148","","Prefetching;Titanium;Sparse matrices;Runtime;Phased arrays;Application software;Data communication;Java;Libraries;Processor scheduling","sparse matrices;parallel languages;Java;optimising compilers;message passing;C language;software libraries;data communication;benchmark testing","sparse matrix vector multiply;mesh methods;SPMD parallel language;irregular array access pattern;data communication method;inspector executor model;Java;high performance computing;optimized Titanium code;C code;MPI;Aztec library","","","19","","","","","","IEEE","IEEE Conferences"
"Computation and design of portable adaptive antenna with chiral coating","F. Bogdanov; D. Karkashadze; R. Zaridze; G. Bit-Babic; K. Tavzarashvili","Tbilisi State Univ., Georgia; NA; NA; NA; NA","Proceedings of the Second International Symposium of Trans Black Sea Region on Applied Electromagnetism (Cat. No.00TH8519)","","2000","","","130","","Summary form only given. A portable adaptive antenna made of linear sources imbedded in a screened dielectric was proposed by Zaridze et al. (see Proc. IEEE AP Int. Symp., Orlando, Florida, p.2524-27, 1999). In this article, based on the method of Zaridze, an adaptive antenna with improved polarization properties is suggested due to the use, instead of a dielectric, chiral medium. A software package for computation and visualization of characteristics of such an antenna is presented and demonstrated. The interaction of antenna with the outside lossy dielectric body is also simulated and evaluated. The antenna considered is represented as an aggregate of linear sources imbedded in partially screened chiral medium of the given shape, dimensions and material properties. The number of excitation sources, their amplitudes, phases and situations, as well as wavenumber of excitation are the input parameters for the user. One can also choose the shape of antenna, its dimensions, undercoat material parameters and percent of the screened boundary. The lossy dielectric body, simulating the human head attached to the antenna, is also included in this scheme. The user may specify dimensions and complex permittivity of dielectric body, as well as its disposition with respect to the antenna. The created software package has been tested in a wide frequency range and for different geometrical, material and excitation parameters. The efficiency of software package to compute and optimize parameters of adaptive antenna alone or in the presence of lossy dielectric body is also demonstrated.","","0-7803-6428","10.1109/AEM.2000.943284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=943284","","Portable computers;Adaptive arrays;Dielectric losses;Software packages;Shape;Dielectric materials;Polarization;Visualization;Computational modeling;Aggregates","mobile antennas;adaptive antenna arrays;chirality;inhomogeneous media;dielectric bodies;electromagnetic wave polarisation;software packages;electrical engineering computing;absorbing media","portable adaptive antenna;chiral coating;linear sources;screened dielectric;polarization properties;software package;dielectric chiral medium;partially screened chiral medium;excitation sources;antenna dimensions;undercoat material parameters;human head;complex permittivity;lossy dielectric body","","","2","","","","","","IEEE","IEEE Conferences"
"An expert system for filter optimum location [in distribution networks]","A. Berizzi; F. Castelli-Dezza; A. Silvestri; D. Zaninelli","Dipt. di Elettronica, Politecnico di Milano, Italy; Dipt. di Elettronica, Politecnico di Milano, Italy; Dipt. di Elettronica, Politecnico di Milano, Italy; Dipt. di Elettronica, Politecnico di Milano, Italy","IEEE Power Engineering Society. 1999 Winter Meeting (Cat. No.99CH36233)","","1999","2","","1269","1274 vol.2","The paper deals with the implementation of an expert system able to determine the optimum location of power harmonic filters in distribution networks. The structure of the expert system is described and the results of its application to some examples are compared with those obtained by intensive computation based on harmonic power flow software programs.","","0-7803-4893","10.1109/PESW.1999.747397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=747397","","Expert systems;Power harmonic filters;Power system harmonics;Impedance;Load flow;Harmonic filters;Pollution;Filtering;System testing;Power supplies","power distribution planning;expert systems;power harmonic filters;power system harmonics;harmonic distortion;power system CAD;optimisation","distribution network CAD;power harmonic filters;expert system;location optimisation;harmonic power flow software programs","","","15","","","","","","IEEE","IEEE Conferences"
"Optimization of automated high-speed modular placement machines using knowledge-based systems","P. Csaszar; P. C. Nelson; R. R. Rajbhandari; T. M. Tirpak","Motorola Adv. Technol. Center, Schaumburg, IL, USA; NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","","2000","30","4","408","417","This paper introduces an optimizer for a new family of modular, multistation, walking beam, high-speed chip mounters. The objective is to optimize the machines in a manner that would streamline the use of nozzles and part feeder mechanisms and at the same time increase throughput. The optimization of these machines is a large NP-complete problem, and therefore, a heuristic search method is needed to solve the problem in reasonable time. Four knowledge-based systems are introduced to solve this problem. These systems were designed to emulate human experts, who have optimized these types of machines manually. Benchmarks were performed for 18 industrial test cases. The results show that overall, the knowledge-based systems outperformed software supplied by the vendor of the machine in both feeder slot savings and throughput. This performance represents a key improvement, and a prototype system has been implemented in our industrial partner's factory.","1094-6977;1558-2442","","10.1109/5326.897068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897068","","Throughput;Knowledge based systems;Legged locomotion;Optimization methods;NP-complete problem;Search methods;Humans;Design optimization;Benchmark testing;Performance evaluation","knowledge based systems;manufacturing resources planning;surface mount technology;electronic engineering computing","automated high-speed modular placement machines;knowledge-based systems;high-speed chip mounters;nozzles;part feeder mechanisms;NP-complete problem;heuristic search method;prototype system","","9","14","","","","","","IEEE","IEEE Journals & Magazines"
"Managing dynamic reconfiguration overhead in systems-on-a-chip design using reconfigurable datapaths and optimized interconnection networks","Zhining Huang; S. Malik","Dept. of Electr. Eng., Princeton Univ., NJ, USA; NA","Proceedings Design, Automation and Test in Europe. Conference and Exhibition 2001","","2001","","","735","740","This research examines the role of dynamically reconfigurable logic in systems-on-a-chip (SoC) design. Specifically we study the overhead of storing and downloading the configuration code bits for different parts of an application in a dynamically reconfigurable coprocessor environment. For SoC designs the different configuration bit-streams will likely need to be stored on chip, thus it becomes crucial to reduce the storage overhead. In addition, reducing the reconfiguration time overhead is crucial in realizing performance benefits. This study provides insight into the granularity of the reconfigurable logic that is appropriate for the SoC context. Our initial study is in the domain of multimedia and communication systems. We first present profiling results for these using the MESCAL compiler infrastructure. These results are used to derive an architecture template that consists of dynamically reconfigurable datapaths using coarse grain logic blocks and a reconfigurable interconnection network. We justify this template based on the constraints of SoC design. We then describe a design flow where we start from an application, derive the kernel loops via profiling and then map the application using the dynamically reconfigurable datapath and the simplest interconnection network. As part of this flow we have developed a mapping algorithm that minimizes the size of the interconnection network and thus the overhead of reconfiguration, which is key for systems-on-a-chip. We provide some initial results that validate our approach.","1530-1591","0-7695-0993","10.1109/DATE.2001.915110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915110","","Reconfigurable logic;Multiprocessor interconnection networks;System-on-a-chip;Coprocessors;Programmable logic arrays;Programmable logic devices;Kernel;Hardware;Field programmable gate arrays;Design methodology","hardware-software codesign;reconfigurable architectures;coprocessors;embedded systems;program compilers;integrated circuit design","systems-on-a-chip design;dynamic reconfiguration overhead management;reconfigurable datapaths;optimized interconnection networks;dynamically reconfigurable logic;configuration code bits;coprocessor environment;logic granularity;MESCAL compiler infrastructure;architecture template;coarse grain logic blocks;design flow;kernel loops;mapping algorithm","","16","11","","","","","","IEEE","IEEE Conferences"
"eBlocks - an enabling technology for basic sensor based systems","S. Cotterell; R. Mannion; F. Vahid; H. Hsieh","Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA","IPSN 2005. Fourth International Symposium on Information Processing in Sensor Networks, 2005.","","2005","","","422","427","We describe the development of a set of embedded system building blocks, known as eBlocks. An eBlock network can be viewed as a basic form of sensor network that can be developed by non-programming engineers, scientists, and others. Each eBlock has a defined function, either one of a few predefined combinational or sequential functions, a custom-programmed function defined by an automated tool, or by user with programming skills. A user creates an application simply by connecting blocks, and possibly performing simple configuration via dials and switches. We have built over 100 physical eBlock prototypes, and tested their usability with over 100 non-programming users to date. We will describe the architecture of the blocks, including design tradeoffs we considered and the benefit of an exploration tool that we developed to help optimize the power and performance of the design. We have also built a graphical eBlock simulator that users can utilize to quickly build and test systems before deployment, and that we have used in experiments with over 300 non-programming users to help us define intuitive block functions and interfaces. We will describe the simulator architecture, as well as a tool that automatically converts a user's eBlock network into a much smaller network of programmable blocks with accompanying automatically generated programs.","","0-7803-9201","10.1109/IPSN.2005.1440960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1440960","","Sensor systems;Embedded system;Automatic programming;Functional programming;Joining processes;Switches;Prototypes;Testing;Usability;Design optimization","embedded systems;actuators;graphical user interfaces;computer graphics;optimisation;combinatorial mathematics;software tools;wireless sensor networks","embedded system building block;graphical eBlocks simulator;sensor network;nonprogramming engineers;combinational function;sequential function;custom-programmed function;automated tool;programming skills;optimization","","3","30","","","","","","IEEE","IEEE Conferences"
"Generation of minimal size code for schedule graphs","C. Passerone; Y. Watanabe; L. Lavagno","Politecnico di Torino, Italy; NA; NA","Proceedings Design, Automation and Test in Europe. Conference and Exhibition 2001","","2001","","","668","673","This paper proposes a procedure for minimizing the code size of sequential programs for reactive systems. It identifies repeated code segments (a generalization of basic blocks to directed rooted trees) and finds a minimal covering of the input control flow graphs with code segments. The segments are disjunct, i.e. no two segments have the same code in common. The program is minimal in the sense that the number of code segments is minimum under the property of disjunction for the given control flow specification. The procedure makes no assumption on the target processor architecture, and is meant to be used between task synthesis algorithms from a concurrent specification and a standard compiler for the target architecture. It is aimed at optimizing the size of very large, automatically generated flat code, and extends dramatically the scope of classical common sub-expression identification techniques. The potential effectiveness of the proposed approach is demonstrated through preliminary experiments.","1530-1591","0-7695-0993","10.1109/DATE.2001.915096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915096","","Productivity;Embedded system;Tree graphs;Automatic control;Hardware;Software performance;Software quality;Application software;Design methodology;Programming","data flow graphs;scheduling;software engineering;minimisation","minimal size code;schedule graphs;sequential programs;reactive systems;repeated code segments;directed rooted trees;input control flow graphs;code segments;disjunction;control flow specification;concurrent specification;standard compiler;target architectur;automatically generated flat code","","","8","","","","","","IEEE","IEEE Conferences"
"Performance results of ickp-a consistent checkpointer on the iPSC/860","J. S. Plank; Kai Li","Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA; NA","Proceedings of IEEE Scalable High Performance Computing Conference","","1994","","","686","693","Presents performance results of ickp, a program that checkpoints applications written on the Intel iPSC/860. ickp is the first implementation of general-purpose checkpointing on a multicomputer. ickp implements three consistent checkpointing algorithms, as well as two optimizations, and recovery. This paper displays the results of testing ickp on many benchmark programs. These benchmarks are useful iPSC/860 programs written by other scientists, and not toy benchmarks used to test the checkpointer. An alpha release of ickp has been made to the Intel community.<<ETX>>","","0-8186-5680","10.1109/SHPCC.1994.296708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296708","","Checkpointing;Benchmark testing;Fault tolerance;Libraries;Computer science;Application software;Displays;Parallel processing;Supercomputers;Registers","parallel programming;optimisation;performance evaluation;system recovery;program testing","performance results;ickp;consistent checkpointing algorithm;Intel iPSC/860;multicomputer;optimizations;recovery;benchmark programs;alpha release","","5","32","","","","","","IEEE","IEEE Conferences"
"Testing and Evaluation of the Defense Data Network","T. R. Dalton; J. Downey; T. L. Hahler","Intermetrics, Inc., 733 Concord Avenue, Cambridge, MA 02138; Intermetrics, Inc., 733 Concord Avenue, Cambridge, MA 02138; Intermetrics, Inc., 733 Concord Avenue, Cambridge, MA 02138","MILCOM 1987 - IEEE Military Communications Conference - Crisis Communications: The Promise and Reality","","1987","2","","0398","0403","The purpose of this document is to establish a concept for test and evaluation (T&E) for the Defense Data Network (DDN) as a whole. This concept is based on the requirements of the DDN in its evolving configurations (e.g., MILNET, SACDIN, DODIIS, ..., DDN); the goals and priorities of the DDN DCS DSD; the needs of the various DDN user communities; knowledge of the ARPANET technology; and experience in T&E of systems and independent verification, validation, and test (IVV&T) of software and hardware. Since 1983 Intermetrics has been a contractor for Independent Validation and Verification (IV&V) support to the Testing branch of the Defense Data Network Defense Communications System Data Systems Directorate (DDN DCS DSD).","","","10.1109/MILCOM.1987.4795240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795240","","System testing;Software testing;Distributed control;Performance evaluation;Hardware;Data systems;ARPANET;Time factors;Government;Feedback","","","","","","","","","","","IEEE","IEEE Conferences"
"NASA battery testbed: lessons learned applications to orbiting spacecraft","F. Deligiannis; J. O. Blosiu; D. Perrone; S. Di Stefano","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA; NA; NA","IECEC 96. Proceedings of the 31st Intersociety Energy Conversion Engineering Conference","","1996","1","","405","409 vol.1","A spacecraft battery testbed has been operational for the last two years. The objective of the testbed is the verification (on the ground) of proposed operational battery management techniques on spacecraft. The testbed consists of a computer controlled system which has the capability to simulate power/orbit profiles with varying occultation periods and charge control methodologies (V/T levels, CC. etc.). Currently the testbed is deployed to simulate the NASA Modular Power System (MPS). This system consists of three NASA standard 50 Ah 22 cell nickel-cadmium batteries. Several orbital profiles have been implemented on the testbed. Since spacecraft telemetry channels are at a premium, battery data from spacecraft is usually very sparse. On the testbed, data on the performance of each of the cells within the batteries can be obtained, and hence a very accurate assessment of the battery performance is available. This data can subsequently be compared with telemetered data for analysis. A 'Taguchi' orthogonal array was utilized to determine the effect of five factors (at four levels) on battery characteristics. These factors are inrush-current, depth-of-discharge, temperature, orbit duration, and VT level. Sixteen independent experiments consisting of 60 orbits each were performed. Based on the results of this experiment, verification experiments were proposed. To date the verification experiments confirm the optimization of parameters yield better performance. Details of the experiment and recommendations based on the data are presented.","1089-3547","0-7803-3547","10.1109/IECEC.1996.552916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=552916","","NASA;Space vehicles;Power system simulation;System testing;Computational modeling;Application software;Battery management systems;Power system management;Control system synthesis;Computer simulation","space vehicle power plants;secondary cells;battery testers;test facilities;nickel;cadmium;automatic test equipment","NASA battery testbed;orbiting spacecraft;operational battery management techniques;computer controlled system;power/orbit profiles simulation;occultation periods;charge control methodologies;NASA Modular Power System;nickel-cadmium batteries;battery data;Taguchi orthogonal array;inrush-current;depth-of-discharge;orbit duration;VT level;Ni-Cd","","1","","","","","","","IEEE","IEEE Conferences"
"Discrete fitness values for improving the human interface in an interactive GA","H. Takagi; K. Ohya","Dept. of Acoust. Design, Kyushu Inst. of Design, Fukuoka, Japan; Dept. of Acoust. Design, Kyushu Inst. of Design, Fukuoka, Japan","Proceedings of IEEE International Conference on Evolutionary Computation","","1996","","","109","112","Proposes two methods of inputting fitness values that reduce the psychological burden of interactive genetic algorithm (GA) operators. The first proposal is an input method that allows discrete fitness values which require less attention from operators in scoring slightly different individuals. The second proposal is an input method that allows both discrete and continuous fitness values. These proposed methods are evaluated using two subjective tests for the task of drawing a face. The results of the subjective tests show that: (1) the first method is significantly better than the method which forces operators to input only continuous fitness values (P<0.01), and (2) the second method is significantly better than the first method (p<0.01).","","0-7803-2902","10.1109/ICEC.1996.542343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=542343","","Humans;Psychology;Testing;Design optimization;Computer graphics;Proposals;Application software;Face;Displays;Genetics","genetic algorithms;interactive systems;human factors;user interfaces;psychology","discrete fitness value input methods;human interface;interactive genetic algorithm;operator psychological burden;scoring;continuous fitness values;subjective tests;face drawing","","24","14","","","","","","IEEE","IEEE Conferences"
"MetalP - a new approach to combinatorial optimization: case studies","Yanzhi Li; Andrew Lim","Dept. of Ind. Eng. & Eng. Manage., Hong Kong Univ. of Sci. & Technol., Kowloon, China; NA","16th IEEE International Conference on Tools with Artificial Intelligence","","2004","","","56","62","We propose a new approach to solve combinatorial optimization problems. Our approach is simple to implement but powerful in terms of performance and speed. We combine the strengths of a meta-heuristic approach with the integer programming method by partitioning the problem into two interrelated subproblems, where the higher level problem is solved by the metahueristic and the lower level problem is solved by integer programming. We discuss the selection of key variables to facilitate an effective partitioning, and test our approach on two real world crossdocking problems, which is very popular in this part of the world. Our experimental results indicate that our new approach is very promising.","1082-3409","0-7695-2236","10.1109/ICTAI.2004.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374170","","Computer aided software engineering;Linear programming;Computer science;Operations research;Application software;Artificial intelligence;Space technology;Industrial engineering;Research and development management;Testing","genetic algorithms;integer programming;heuristic programming;computational complexity;problem solving;search problems","combinatorial optimization;meta-heuristic approach;integer programming;crossdocking problem;genetic algorithm;NP-hard problems","","","11","","","","","","IEEE","IEEE Conferences"
"On the optimization of a task-farm model for the parallel integration of a two-dimensional Schrodinger equation","R. Baraglia; R. Ferrini; D. Laforenza; A. Lagana","CNUCE-Inst., Italian Nat. Res. Council, Pisa, Italy; NA; NA; NA","Proceedings of 3rd International Conference on Algorithms and Architectures for Parallel Processing","","1997","","","543","556","As a case study of computational problems difficult to parallelize we discuss the parallelization of a quantum reactive scattering application integrating a two dimensional Schrodinger equation. From a detailed analysis of the computational procedure, sources for inefficiency causing a performance degradation when running the application on a MIMD-DM machine were singled out and the parallel model was structured accordingly. The suitability of the model adopted was checked by running some test calculations.","","0-7803-4229","10.1109/ICAPP.1997.651521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=651521","","Application software;Particle scattering;Schrodinger equation;Concurrent computing;Quantum computing;Boundary conditions;Differential equations;Councils;Parallel processing;Chemistry","optimisation;parallel machines;Schrodinger equation;physics computing;mathematics computing;performance evaluation;differential equations;integration","optimization;task-farm model;two-dimensional Schrodinger equation;computational problems;parallel integration;quantum reactive scattering application;performance degradation;MIMD-DM machine;parallel model;test calculations","","1","15","","","","","","IEEE","IEEE Conferences"
"Trade-offs in overhead vs effectiveness of causality inconsistency tracking for preemptive rollback in optimistic simulation","F. Quaglia; A. Santoro; B. Ciciani","Dipt. di Informatica e Sistemistica, Universita di Roma "La Sapienza", Rome, Italy; Dipt. di Informatica e Sistemistica, Universita di Roma "La Sapienza", Rome, Italy; Dipt. di Informatica e Sistemistica, Universita di Roma "La Sapienza", Rome, Italy","Proceedings. Sixth IEEE International Workshop on Distributed Simulation and Real-Time Applications","","2002","","","63","70","We discuss and compare three different causality inconsistency tracking mechanisms in support of preemptive rollback in optimistic parallel simulation on myrinet clusters. These mechanisms exhibit different communication/processing overhead and also different effectiveness in revealing causality inconsistency of the currently executed simulation event. By the results of an empirical study on a classical simulation benchmark we have found some trade-offs between these mechanisms, pointing out indications of application contexts for which each mechanism is expected to be well tailored.","1530-1990","0-7695-1853","10.1109/DISRTA.2002.1166890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166890","","Discrete event simulation;Context modeling;Synchronization;Remuneration;Clocks;Processor scheduling;Context awareness;Testing","digital simulation;workstation clusters;parallel processing;software performance evaluation","causality inconsistency tracking;preemptive rollback;software performance;overhead;myrinet clusters;simulation benchmark;optimistic parallel simulation","","","21","","","","","","IEEE","IEEE Conferences"
"Feasibility of SMES devices based on the developed technology of superconducting magnets for tokamak fusion reactors","V. Glukhikh; O. Filatov; V. Belyakov; S. Egorov; V. Kuchinsky; V. Sytnikov; A. Shikov","Efremov (D.V.) Sci. Res. Inst. of Electrophys. Apparatus, St. Petersburg, Russia; NA; NA; NA; NA; NA; NA","IEEE Transactions on Applied Superconductivity","","2000","10","1","771","776","Achievements in the technology of superconducting magnets for the tokamak fusion devices contribute to the applicability of GJ-range SMES for the utilities by means of using already developed components and constructed production lines. The paper presents the technologies developed by the NIIEFA, VNIIKP and VNIINM, institutes, covering most of the critical items needed to construct the GJ-range SMES devices for the utility needs. That includes software packages for the supporting analysis and SMES optimization, heavy current CICC production lines with the length of pull-through jacketing tooling of up to 1 km, 30 kV 40 kA-range cryogenic current leads and 30 kV high-pressure insulating breaks for ""cold"" and ""warm"" helium communications, various types of heavy-current 30 kV range commutating equipment, including superconducting switches, and cryogenic test facilities for the acceptance tests of the produced conductors, current leads and insulation breaks.","1051-8223;1558-2515;2378-7074","","10.1109/77.828345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=828345","","Samarium;Superconducting magnets;Cryogenics;Insulation;Tokamaks;Optimized production technology;Paper technology;Software packages;Superconducting magnetic energy storage","superconducting magnet energy storage;superconducting cables;cryogenics;superconducting magnets;superconducting switches","SMES devices;superconducting magnets;tokamak fusion reactors;software packages;VNIINM;VNIIKP;NIIEFA;supporting analysis;SMES optimization;heavy current CICC production lines;pull-through jacketing tooling;cryogenic current leads;high-pressure insulating breaks;warm helium communications;cold helium communications;heavy-current range commutating equipment;superconducting switches;cryogenic test facilities;acceptance tests;insulation breaks;current leads;1 km;30 kV;40 kA","","5","25","","","","","","IEEE","IEEE Journals & Magazines"
"State oriented programming","H. Nomoto","Aeronaut. & Astronaut. Eng., Massachusetts Inst. of Technol., Cambridge, MA, USA","Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.","","2004","","","304","305","This paper describes a methodology called ""state oriented programming"" to generate code from state-machine based formal design specification. The methodology is focused on the safety critical systems. In order to achieve highly reliable system development, the following several key techniques are presented: 1) State-of-interest based design technique which seamlessly incorporates system/hazard analysis into the design and executable code; 2) Automated transparent code generation technique that mitigates unexpected behaviors of the code; and 3) Fully deterministic backward execution mechanism of the generated code.","1530-2059","0-7695-2094","10.1109/HASE.2004.1281771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281771","","Safety;Attitude control;Global Positioning System;Timing;Hazards;Space technology;Aerospace engineering;Control systems;State-space methods;Concurrent engineering","software reliability;safety-critical software;formal specification;optimising compilers","state oriented programming;state-machine;formal design specification;safety critical systems;highly reliable system development;automated transparent code generation;backward execution mechanism","","1","2","","","","","","IEEE","IEEE Conferences"
"A real-time simulation of a 200 MW thermal power plant for optimising combustion control","D. J. Geddes; A. P. H. Bailie; M. Cregan; E. Swidenbank","Dept. of Electr. & Electron. Eng., Queen's Univ., Belfast, UK; NA; NA; NA","UKACC International Conference on Control '98 (Conf. Publ. No. 455)","","1998","1","","859","864 vol.1","This paper gave an overview of the work being carried out by The Queen's University of Belfast TOPGEN project. The development implementation of a data server for monitoring and storing data from the Ballylumford power plant in Northern Ireland was presented. A computer power plant model was designed and validated using data collected by the data server. This plant model formed the basis of a virtual plant system that included an industrial standard distributed control system with multiple operator screens on a local area network. An FTP server was also incorporated for connection over the Internet with the software package CORAGE. The virtual plant system was then used as a test-bed for CORAGE to be tuned and tested remotely over the Internet before being applied to the Ballylumford power plant.","0537-9989","0-85296-708","10.1049/cp:19980341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=728047","","","digital simulation","real-time simulation;thermal power plant;combustion control optimisation;TOPGEN project;data server;data monitoring;data storage;Ballylumford power plant;Northern Ireland;UK;virtual plant system;industrial standard distributed control system;multiple operator screens;local area network;LAN;FTP server;Internet;software package CORAGE;200 MW","","","","","","","","","IET","IET Conferences"
"TOSCA: a user-friendly behavioural simulator for oversampling A/D converters","V. F. Dias; V. Liberali; F. Maloberti","Dipartimento di Elettronica, Pavia Univ., Italy; NA; NA","1991., IEEE International Sympoisum on Circuits and Systems","","1991","","","2677","2680 vol.5","The potential and flexibility of use of the software tool TOSCA is presented. The tool proved to be fully user-friendly for the design of Sigma Delta A/D converters. Modulator and decimator structures can easily be designed simply by using a set of basic building blocks and applying adequate analysis commands. Simulation results were presented for the example of a two-stage MASH Sigma Delta A/D converter.<<ETX>>","","0-7803-0050","10.1109/ISCAS.1991.176097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=176097","","Circuit simulation;Analytical models;Signal resolution;Circuit optimization;Circuit analysis;Software tools;Performance analysis;Circuit testing;Multi-stage noise shaping;Noise shaping","analogue-digital conversion;circuit analysis computing;circuit CAD;digital simulation;modulators;switched capacitor networks;user interfaces","MASH sigma-delta ADC;modulator structures;SC integrators;CAD;TOSCA;user-friendly;behavioural simulator;oversampling A/D converters;software tool;decimator structures","","12","6","","","","","","IEEE","IEEE Conferences"
"Performance benefits of optimistic programming: a measure of HOPE","C. Cowan; H. L. Lutfiyya; M. A. Bauer","Dept. of Comput. Sci. & Eng., Oregon Graduate Inst., Portland, OR, USA; NA; NA","Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing","","1995","","","197","204","Optimism is a powerful technique for avoiding latency by increasing concurrency. By optimistically assuming the results of some computation, other computations can be executed in parallel, even when they depend on the assumed result. Optimistic techniques can be particularly beneficial to parallel and distributed systems because of the critical impact of inter-node communications latency. This paper describes how optimism can be used to enhance the performance of distributed programs by avoiding remote communications delay. We then present a new programming model that automates many of the difficulties of using optimistic techniques in a general programming environment, and describe a prototype implementation. Finally, we present performance measurements showing how optimism improved the performance of a test application in this environment.","1082-8907","0-8186-7088","10.1109/HPDC.1995.518710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=518710","","Delay;Concurrent computing;Prototypes;Computer science;Power engineering computing;Programming profession;Power engineering and energy;Automatic programming;Programming environments;Testing","programming environments;parallel programming;concurrency control;software performance evaluation","performance benefits;optimistic programming;latency;concurrency;inter-node communications latency;programming model;programming environment;prototype implementation","","1","24","","","","","","IEEE","IEEE Conferences"
"Modeling tumor growth and irradiation response in vitro-a combination of high-performance computing and Web-based technologies including VRML visualization","G. S. Starnatakos; E. I. Zacharaki; M. Makropoulou; N. A. Mouravliansky; A. Marsh; K. S. Nikita; N. K. Uzunoglu","Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece; NA; NA; NA; NA; NA; NA","IEEE Transactions on Information Technology in Biomedicine","","2001","5","4","279","289","A simplified three-dimensional Monte Carlo simulation model of in vitro tumor growth and response to fractionated radiotherapeutic schemes is presented in this paper. The paper aims at both the optimization of radiotherapy and the provision of insight into the biological mechanisms involved in tumor development. The basics of the modeling philosophy of Duechting (1968, 1981, 1992, 1995) have been adopted and substantially extended. The main processes taken into account by the model are the transitions between the cell cycle phases, the diffusion of oxygen and glucose, and the cell survival probabilities following irradiation. Specific algorithms satisfactorily describing tumor expansion and shrinkage have been applied, whereas a novel approach to the modeling of the tumor response to irradiation has been proposed and implemented. High-performance computing systems in conjunction with Web technologies have coped with the particularly high computer memory and processing demands. A visualization system based on the MATLAB software package and the virtual-reality modeling language has been employed. Its utilization has led to a spectacular representation of both the external surface and the internal structure of the developing tumor. The simulation model has been applied to the special case of small cell lung carcinoma in vitro irradiated according to both the standard and accelerated fractionation schemes. A good qualitative agreement with laboratory experience has been observed in all cases. Accordingly, the hypothesis that advanced simulation models for the in silico testing of tumor irradiation schemes could substantially enhance the radiotherapy optimization process is further strengthened. Currently, our group is investigating extensions of the presented algorithms so that efficient descriptions of the corresponding clinical (in vivo) cases are achieved.","1089-7771;1558-0032","","10.1109/4233.966103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966103","","Neoplasms;Visualization;Mathematical model;Biological system modeling;In vitro;Fractionation;Biology computing;Sugar;MATLAB;Software packages","tumours;virtual reality languages;data visualisation;radiation therapy;digital simulation;Monte Carlo methods;cellular transport;biodiffusion;Internet;medical computing;cancer;lung","3D Monte Carlo simulation model;in vitro tumor growth;modeling;irradiation response;high-performance computing;Web-based technologies;VRML visualization;biological mechanisms;cell cycle phases;oxygen diffusion;glucose diffusion;cell survival probabilities;tumor expansion;tumor shrinkage;MATLAB software package;external surface;internal structure;simulation model;small cell lung carcinoma;in silico testing;radiotherapy optimization","Carcinoma, Small Cell;Carcinoma, Small Cell;Cell Division;Computer Simulation;Humans;Internet;Lung Neoplasms;Lung Neoplasms;Models, Biological;Monte Carlo Method;Neoplasms;Neoplasms;Radiotherapy Planning, Computer-Assisted;Software Design;Spheroids, Cellular;Spheroids, Cellular","17","31","","","","","","IEEE","IEEE Journals & Magazines"
"An on-line, business-oriented optimization of performance and availability for utility computing","J. L. Hellerstein; K. Katircioglu; M. Surendra","IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Journal on Selected Areas in Communications","","2005","23","10","2013","2021","Utility computing provides a pay-as-you-go approach to information systems in which application providers (e.g., web sites) can better manage their costs by adding capacity in response to increased demands and shedding capacity when it is no longer needed. This paper addresses application providers who use clusters of servers. Our work develops a framework to determine the number of servers that minimizes the sum of quality-of-service (QoS) costs resulting from service level penalties and server holding costs for the server cluster. The server characteristics considered are service rate, failure rates, repair rates, and costs. The contributions of this paper are: 1) a model for the performance and availability of an e-Commerce system that is consistent with data from a multisystem testbed with an e-Commerce workload; 2) a business-oriented cost model for resource allocation for application providers; 3) a closed form approximation for the optimal allocation of servers for an application provider based on the performance model in 1) and the cost model in 2); and 4) a simple criteria for utility owners and server manufacturers to make tradeoffs between server characteristics.","0733-8716;1558-0008","","10.1109/JSAC.2005.854125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1514530","Optimal server allocation;performability;service level agreement (SLA)","Availability;Cost function;Hardware;Quality of service;Resource management;Application software;Delay;Licenses;Management information systems;System testing","optimisation;resource allocation;electronic commerce;business communication;utility programs;costing;quality of service","on-line business-oriented optimization;utility computing;information system;application provider;quality-of-service;QoS;server holding cost;server cluster;e-commerce system;multisystem testbed;business-oriented cost model;resource allocation;closed form approximation;optimal server allocation;service level agreement;SLA","","9","19","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing register and phase requirements for synchronous circuits derived using software pipelining techniques","N. Chabini; E. M. Aboulhamid; Y. Savaria","Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; NA; NA","Proceedings IEEE Computer Society Workshop on VLSI 2001. Emerging Technologies for VLSI Systems","","2001","","","71","77","A method based on a modulo scheduling algorithm for software pipelining has been recently proposed to optimize clocked circuits. The resulting circuits are multi-phase clocked circuits, where all clocks have the same period. To preserve the functionality of the original circuit, registers must be placed after minimizing the clock period. The placement of these registers is derived from an arbitrary schedule determined during a clock period minimization step. A good schedule may allow one to decrease the number of registers and the number of phases needed in the final circuit. Decreasing the number of registers contributes to minimizing the area occupied by the circuit and reduces its power consumption; while decreasing the number of phases reduces the complexity of the clock generation and distribution task. In this paper, we propose polynomial-time-solvable methods to choose a good schedule once the clock period is minimized. The methods have been tested on a subject of the ISCAS89 benchmarks. Experimental results show that the number of registers which must be inserted in the final circuit, and the number of phases, have been significantly decreased compared to the case where an arbitrary schedule is chosen.","","0-7695-1056","10.1109/IWV.2001.923142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923142","","Registers;Circuits;Clocks;Scheduling algorithm;Pipeline processing;Optimization methods;Minimization;Energy consumption;Power generation;Polynomials","clocks;pipeline processing;scheduling;shift registers;sequential circuits;minimisation of switching nets;low-power electronics;circuit optimisation;logic CAD;graph theory","phase requirements;synchronous circuits;software pipelining techniques;modulo scheduling algorithm;multi-phase clocked circuits;arbitrary schedule;clock period minimization step;registers;power consumption;clock generation;polynomial-time-solvable methods;ISCAS89 benchmarks","","6","15","","","","","","IEEE","IEEE Conferences"
"Robust tuning of PSS in power systems with different operating conditions","A. Mendonca; J. A. P. Lopes","INESC, Porto, Portugal; INESC, Porto, Portugal","2003 IEEE Bologna Power Tech Conference Proceedings,","","2003","1","","7 pp. Vol.1","","This paper presents a new approach to deal with the problem of robust tuning of PSS in multimachine power systems with different operating conditions of load, topology and generation scheduling. The proposed method is based on the formulation of an optimization problem, solved by an evolutionary programming algorithm. The effectiveness of the proposed approach is demonstrated through modal analysis and nonlinear simulations of two test systems, carried out using Matlab and PSS/E software package.","","0-7803-7967","10.1109/PTC.2003.1304129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304129","","Robustness;Power systems;Power system simulation;Power system analysis computing;Topology;Power generation;Optimization methods;Genetic programming;Modal analysis;Analytical models","power system stability;power generation scheduling;optimisation;evolutionary computation;modal analysis;power system simulation;software packages","robust tuning;PSS;operating conditions;multimachine power systems;generation scheduling;optimization problem;evolutionary programming algorithm;modal analysis;nonlinear simulations;Matlab;PSS/E software package","","6","12","","","","","","IEEE","IEEE Conferences"
"Transport protocol optimization for energy efficient wireless embedded systems","D. Bertozzi; A. Raghunathan; L. Benini; S. Ravi","DEIS, Bologna Univ., Italy; NA; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","706","711","For wireless embedded systems, the power consumption in the network interface (radio) plays a dominant role in determining battery life. In this paper, we explore transport protocol optimizations for reducing the energy consumption of wireless LAN interfaces. Our work is based on the observation that, the transport protocol, which implements flow control to regulate the network traffic, plays a significant role in determining the workload of the network interface. Hence, by monitoring run-time parameters in the transport protocol, coarse-granularity idle periods, which present the best opportunities for network interface power reduction, can be accurately identified. We further show that, by tuning parameters in the protocol software implementation, we can shape the activity profile of the network interface, making it more energy efficient while remaining compliant to the TCP standard. We have performed extensive current measurements using an experimental testbed that consists of a Compaq iPAQ PDA with a Cisco Aironet wireless network adapter, to validate the proposed techniques. Our measurements indicate energy savings ranging from 28% to 69% compared to the use of state-of-the-art MAC layer power reduction techniques, with little or no impact on performance.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253690","","Transport protocols;Energy efficiency;Embedded system;Network interfaces;Energy consumption;Batteries;Wireless LAN;Communication system traffic control;Condition monitoring;Runtime","transport protocols;embedded systems;network interfaces;wireless LAN","transport protocol optimization;energy efficient wireless embedded systems;network interface;wireless LAN interfaces;flow control;workload;run-time parameters;coarse-granularity idle periods;protocol software implementation;activity profile;current measurements;Compaq iPAQ PDA;Cisco Aironet wireless network adapter","","10","28","","","","","","IEEE","IEEE Conferences"
"Dynamic multiresolution level of detail mesh simplification for real-time rendering of large digital terrain models","A. Agrawal; M. Radhakrishna; R. C. Joshi","Indian Inst. of Inf. Technol., Allahabad, India; Indian Inst. of Inf. Technol., Allahabad, India; NA","Proceedings of the IEEE INDICON 2004. First India Annual Conference, 2004.","","2004","","","278","282","There is an increasing demand for high quality interactive 3D visualization of geographical datasets. Because of the inherent geometric complexity and data size, this goal is often unachievable even with high performance graphics workstations, unless the original terrain height field mesh is simplified and approximated in order to reduce the number of geometric primitives that need to be rendered without compromising visual quality. This paper discusses the methodology and implementation aspects of our research work to improve the quality and speed of rendering general purpose desktop PCs by enhancing the geometrical and topological techniques for handing the very large terrain data sets. The proposed algorithm uses a compact and efficient multi-resolution grid representation and employs a variable screen-space threshold to bound the maximum error of the projected image. The appropriate level of detail is computed and generated dynamically in real-time, allowing for smooth changes of resolution across area of the surface. The method is different from the triangle-based LOD algorithms and is optimized for modern, low-end consumer 3D graphics cards and minimizes CPU usage during rendering. The software has been tested on real-world height maps as well as satellite phototextures of size up to 16K/spl middot/16K.The experimental result shows that the proposed algorithm can dynamically generate view-dependent multi-resolution LOD terrain model and real-time rendering can be attained.","","0-7803-8909","10.1109/INDICO.2004.1497755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1497755","","Digital elevation models;Rendering (computer graphics);Graphics;Multiresolution analysis;Data visualization;Workstations;Personal communication networks;Optimization methods;Software testing;Satellites","image resolution;image texture;image representation;real-time systems;mesh generation;data visualisation;rendering (computer graphics);terrain mapping","3D visualization;geographical dataset;geometrical-topological technique;digital terrain model;multiresolution grid representation;image projection;triangle-based LOD algorithm;optimization;graphics card;software testing;satellite phototexture;real-time rendering","","1","10","","","","","","IEEE","IEEE Conferences"
"Uni-planar CPW-fed slot launchers for efficient TM/sub 0/ surface-wave excitation","H. F. Hammad; Y. M. M. Antar; A. P. Freundorfer; S. F. Mahmoud","Electr. & Comput. Eng. Dept., Queen's Univ., Kingston, Ont., Canada; NA; NA; NA","IEEE Transactions on Microwave Theory and Techniques","","2003","51","4","1234","1240","Several new uni-planar coplanar-waveguide-fed slot surface-wave launchers are presented in this paper. The launchers are used to efficiently excite the dominant transverse-magnetic surface-wave mode inside a grounded dielectric slab for possible use in power combiners and other applications. Analysis of the surface waves and optimization of the slab thickness and other needed parameters are presented. Next, the launchers design and optimization using two commercially available software are described. The launchers were fabricated and tested. Good agreement is obtained between the numerical and experimental results.","0018-9480;1557-9670","","10.1109/TMTT.2003.809668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1193135","","Slabs;Power combiners;Surface waves;Military computing;Application software;Antennas and propagation;Microstrip antenna arrays;Design optimization;Testing;Slot antennas","surface electromagnetic waves;power combiners;slot antennas;coplanar waveguides;antenna feeds;optimisation","TM/sub 0/ surface-wave excitation;uni-planar CPW-fed slot launchers;uni-planar surface-wave launchers;coplanar-waveguide-fed slot launchers;dominant TM surface-wave mode;transverse-magnetic surface-wave mode;grounded dielectric slab;power combiner applications;slab thickness optimization;Yagi-Uda coplanar slot antennas;30 GHz;12 GHz","","31","16","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient evaluation of alternatives for assembly of services","N. Barthwal; M. Woodside","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","19th IEEE International Parallel and Distributed Processing Symposium","","2005","","","8 pp.","","Component based software engineering (CBSE) provides rapid development using well-tested components with established properties. Performance and other nonfunctional properties can also be analyzed by building models from sub-models, calibrated for the components. Further there can be many choices of components to build-systems, which can provide alternatives. The choice can be governed by goal functions which evaluate the predicted performance. This paper describes a systematic approach to find the feasible combinations of alternatives, and to rank them based on predicted performance. It extends the CBML (component based modeling language) for defining components in layered queuing models for software performance.","1530-2075","0-7695-2312","10.1109/IPDPS.2005.192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420232","","Assembly;Software engineering;Software performance;Delay;Libraries;Systems engineering and theory;Performance analysis;Cost function;System performance;Operating systems","object-oriented programming;software performance evaluation;queueing theory;specification languages","component based software engineering;component based modeling language;layered queuing models;software performance","","1","16","","","","","","IEEE","IEEE Conferences"
"Call path profiling","R. J. Hall","AT&T Bell Laboratories","International Conference on Software Engineering","","1992","","","296","306","Practical performance improvement of a complex program must be guided by empirical measurements of its resource usage in order to avoid wasting programmer time and to avoid needlessly destroying the original, clear structure. Previous approaches to measuring programs, while very useful, have shortcomings in that they provide either too little or too much information. In this paper, I classify previous approaches, explain their strengths and weaknesses, and describe a new approach, call path profiling, that reports resource usage of subroutine calls in their full lexical contexts. This relates resource usage directly to design decisions, providing better guidance to the optimizer. I also discuss the implementation of cpp, a working prototype that operates on Common Lisp programs.","0270-5257","0-89791-504","10.1145/143062.143147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=753507","","Algorithms;Design optimization;Prototypes;Performance analysis;Time measurement;Debugging;Testing;Computer bugs;Distributed computing","","","","2","12","","","","","","IEEE","IEEE Conferences"
"Design of inductors in organic substrates for 1-3 GHz wireless applications","S. Dalmia; F. Ayazi; M. Swaminathan; Sung Hwan Min; Seock Hee Lee; Woopoung Kim; Dongsu Kim; S. Bhattacharya; V. Sundaram; G. White; R. Tummala","Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA","2002 IEEE MTT-S International Microwave Symposium Digest (Cat. No.02CH37278)","","2002","3","","1405","1408 vol.3","High Q inductors with maximum quality factors in the range of 180-60 have been obtained at frequencies in the 1-3 GHz band for inductances in the range of 1 nH to 20 nH using a low-temperature organic laminate build-up process. This is the first time such high Q inductors have been demonstrated in this technology. The different inductor designs, optimization schemes, and trade-offs between different topologies, have been discussed in this paper.","0149-645X","0-7803-7239","10.1109/MWSYM.2002.1012118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012118","","Inductors;Topology;Microstrip;Testing;Design optimization;Application software;Q factor;Frequency;Inductance;Spirals","Q-factor;inductors;substrates;microstrip components;coplanar waveguide components;multichip modules;UHF devices;UHF integrated circuits","high-Q inductors;quality factors;low-temperature organic laminate build-up process;optimization schemes;wireless applications;organic substrates;microstrip spiral inductors;CPW loop inductors;1 to 3 GHz","","21","7","","","","","","IEEE","IEEE Conferences"
"Improving RBF-DDA performance on optical character recognition through parameter selection","A. L. I. Oliveira; F. B. L. Neto; S. R. L. Meira","Polytech Sch., Pernambuco Univ., Magdalena Recife, Brazil; Polytech Sch., Pernambuco Univ., Magdalena Recife, Brazil; NA","Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.","","2004","4","","625","628 Vol.4","The dynamic decay adjustment (DDA) algorithm is a fast constructive algorithm for training RBF neural networks. In previous works it has been shown that for some datasets the generalization performance of RBF-DDA depends only weakly on the algorithm parameters /spl theta//sup +/ and /spl theta//sup -/. However, we have observed experimentally that for some problems performance is considerably dependent on the value of /spl theta//sup -/. In this work we propose a method for selecting the value of /spl theta//sup -/ for performance optimization. The proposed method has been evaluated on three optical recognition datasets from the UCI repository. The results show that the proposed method considerably improves the performance of RBF-DDA with default parameters on these tasks. The results are compared to MLP and k-NN results obtained in previous works. It is shown that the method proposed in this paper outperforms MLPs and obtains results comparable to k-NN on these tasks.","1051-4651","0-7695-2128","10.1109/ICPR.2004.1333850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333850","","Optical character recognition software;Character recognition;Radial basis function networks;Testing;Neural networks;Gaussian processes;Informatics;Multilayer perceptrons;Optimization methods;Training data","radial basis function networks;learning (artificial intelligence);generalisation (artificial intelligence);image recognition","performance improvement;optical character recognition;parameter selection;dynamic decay adjustment algorithm;RBF neural network training;fast constructive algorithm;generalization performance;performance optimization;optical recognition datasets;UCI repository","","7","6","","","","","","IEEE","IEEE Conferences"
"Load distribution in a CORBA environment","T. Barth; G. Flender; B. Freisleben; F. Thilo","Siegen Univ., Germany; NA; NA; NA","Proceedings of the International Symposium on Distributed Objects and Applications","","1999","","","158","166","The design and implementation of a CORBA load distribution service for distributed scientific computing applications running in a network of workstations is described. The proposed approach is based on integrating load distribution into the CORBA naming service which in turn relies on information provided by the underlying WINNER resource management system developed for typical networked Unix workstation environments. The necessary extensions to the naming service, the WINNER features for collecting load information and the placement decisions are described. A prototypical implementation of the complete system is presented, and performance results obtained for the parallel optimization of a mathematical test function are discussed.","","0-7695-0182","10.1109/DOA.1999.794010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=794010","","Application software;Scientific computing;Concurrent computing;Workstations;Resource management;Message passing;Computer architecture;Parallel processing;Software engineering;Software performance","distributed object management;workstation clusters;naming services;resource allocation;natural sciences computing","CORBA environment;CORBA load distribution service;distributed scientific computing applications;network of workstations;CORBA naming service;WINNER resource management system;networked Unix workstation environments;WINNER features;load information;placement decisions;prototypical implementation;performance results;parallel optimization;mathematical test function","","7","22","","","","","","IEEE","IEEE Conferences"
"On a new class of codes for identifying vertices in graphs","M. G. Karpovsky; K. Chakrabarty; L. B. Levitin","Dept. of Electr. & Comput. Eng., Boston Univ., MA, USA; NA; NA","IEEE Transactions on Information Theory","","1998","44","2","599","611","We investigate a new class of codes for the optimal covering of vertices in an undirected graph G such that any vertex in G can be uniquely identified by examining the vertices that cover it. We define a ball of radius t centered on a vertex /spl upsi/ to be the set of vertices in G that are at distance at most t from /spl upsi/. The vertex /spl upsi/ is then said to cover itself and every other vertex in the ball with center /spl upsi/. Our formal problem statement is as follows: given an undirected graph G and an integer t/spl ges/1, find a (minimal) set C of vertices such that every vertex in G belongs to a unique set of balls of radius t centered at the vertices in C. The set of vertices thus obtained constitutes a code for vertex identification. We first develop topology-independent bounds on the size of C. We then develop methods for constructing C for several specific topologies such as binary cubes, nonbinary cubes, and trees. We also describe the identification of sets of vertices using covering codes that uniquely identify single vertices. We develop methods for constructing optimal topologies that yield identifying codes with a minimum number of codewords. Finally, we describe an application of the theory developed in this paper to fault diagnosis of multiprocessor systems.","0018-9448;1557-9654","","10.1109/18.661507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=661507","","Fault diagnosis;Multiprocessing systems;Topology;Error correction codes;Graph theory;Reliability engineering;Application software;System testing;Software systems;Sections","codes;graph theory;optimisation","radius;optimal vertices covering;undirected graph;balls;vertex identification;topology-independent bounds;binary cubes;nonbinary cubes;trees;covering codes;optimal topologies;codewords","","185","26","","","","","","IEEE","IEEE Journals & Magazines"
"DDP-a tool for life-cycle risk management","S. L. Cornford; M. S. Feather; K. A. Hicks","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA; NA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","1","","1/441","1/451 vol.1","At JPL we have developed, and implemented, a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called Defect Detection and Prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The 'determine where we want to be' is captured as trees of requirements and the 'what could get in the way' is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of PACTs (Preventative measures, Analyses, process Controls and Tests) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs/spl Dagger/ which minimizes the residual risk subject to the project resource constraints.","","0-7803-6599","10.1109/AERO.2001.931736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931736","","Risk management;Software tools;Propulsion;Laboratories;Application software;Testing;Tree graphs;Feathers;Quality assurance;Failure analysis","life cycle costing;risk management;failure analysis;project management;software tools","DDP;life-cycle risk management;JPL;software tool;defect detection and prevention;trees;failure modes;PACTs;residual risk;project resource constraints","","27","8","","","","","","IEEE","IEEE Conferences"
"Global positioning system receiver tracking-loop optimisation via an H/sub /spl infin// approach","H. -. Wang","Dept. of Commun. & Guidance Eng., Nat. Taiwan Ocean Univ., Keelung, Taiwan","IEE Proceedings - Control Theory and Applications","","2005","152","6","715","722","The application of H/sub /spl infin// optimal control theory to global positioning system receiver tracking loops is investigated. The design of an H/sub /spl infin// controller for the receiver tracking-loop is presented. The H/sub /spl infin// controller is particularly attractive because it is a robust design in the sense that small disturbances lead to small tracking errors. Furthermore, it easily accommodates the inclusion of plant uncertainties as part of the plant model. By adding unstructured or structured perturbations to the plant model, it is possible to design controllers that ensure stability robustness and performance robustness of the closed-loop system. In order to apply the H/sub /spl infin// optimal design, the GPS receiver tracking loop is rewritten into a two-input two-output generalised plant model. Various levels of disturbances are derived using a software-based GPS L5 signal generator which are then used as test inputs to evaluate the performance of the proposed tracking loop.","1350-2379","","10.1049/ip-cta:20045292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512683","","","Global Positioning System;H/sup /spl infin// control;control system synthesis;satellite tracking;artificial satellites;stability;closed loop systems;MIMO systems;radio receivers","Global Positioning System receiver;tracking-loop optimisation;H/sub /spl infin// optimal control theory;controller design;receiver tracking-loop;plant uncertainties;stability robustness;performance robustness;closed-loop system;two-input two-output generalised plant model;disturbances;software-based GPS L5 signal generator","","","","","","","","","IET","IET Journals & Magazines"
"Agent-based evolutionary multiobjective optimisation","K. Socha; M. Kisiel-Dorohinicki","Free Univ. of Brussles, Belgium; NA","Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)","","2002","1","","109","114 vol.1","This work presents a new evolutionary approach to searching for a global solution (in the Pareto sense) to a multiobjective optimisation problem. The novelty of the method proposed consists in the application of an evolutionary multi-agent system (EMAS) instead of classical evolutionary algorithms. Decentralisation of the evolution process in EMAS allows for intensive exploration of the search space, and the introduced mechanism of crowd allows for effective approximation of the whole Pareto frontier. In the paper the technique is described as well as preliminary experimental results are reported.","","0-7803-7282","10.1109/CEC.2002.1006218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006218","","Evolutionary computation;Multiagent systems;Pareto optimization;Computer science;Electronic mail;Application software;Sampling methods;System testing;Decision making;Humans","multi-agent systems;evolutionary computation;search problems","agent-based evolutionary multiobjective optimisation;searching;global solution;evolutionary multi-agent system;EMAS;search space;crowd;Pareto frontier;experimental results","","12","14","","","","","","IEEE","IEEE Conferences"
"Efficient optimization of all-terminal reliable networks, using an evolutionary approach","B. Dengiz; F. Altiparmak; A. E. Smith","Dept. of Industrial Eng., Gazi Univ., Ankara, Turkey; NA; NA","IEEE Transactions on Reliability","","1997","46","1","18","26","The use of computer communication networks has been rapidly increasing in order to: (1) share expensive hardware and software resources, and (2) provide access to main system from distant locations. The reliability and cost of these systems are important and are largely determined by network topology. Network topology consists of nodes and the links between nodes. The selection of optimal network topology is an NP-hard combinatorial problem so that the classical enumeration-based methods grow exponentially with network size. In this study, a heuristic search algorithm inspired by evolutionary methods is presented to solve the all-terminal network design problem when considering cost and reliability. The genetic algorithm heuristic is considerably enhanced over conventional implementations to improve effectiveness and efficiency. This general optimization approach is computationally efficient and highly effective on a large suite of test problems with search spaces up to 2/spl middot/10/sup 90/.","0018-9529;1558-1721","","10.1109/24.589921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589921","","Network topology;Computer network reliability;Telecommunication network reliability;Costs;Computer networks;Communication networks;Hardware;Communication system software;Heuristic algorithms;Algorithm design and analysis","genetic algorithms;communication complexity;Monte Carlo methods;network topology;computer network reliability","all-terminal reliable networks;evolutionary approach;computer communication networks;software resources sharing;hardware resources sharing;reliability;network topology;optimal network topology;NP-hard combinatorial problem;heuristic search algorithm;genetic algorithm heuristic;heuristic optimization;network reliability;combinatorial optimisation;Monte Carlo simulation","","34","","","","","","","IEEE","IEEE Journals & Magazines"
"A compiler design for IEC 1131-3 standard languages of programmable logic controllers","Hyung Seok Kim; Jae Young Lee; Wook Hyun Kwon","Sch. of Electr. Eng., Seoul Nat. Univ., South Korea; NA; NA","SICE '99. Proceedings of the 38th SICE Annual Conference. International Session Papers (IEEE Cat. No.99TH8456)","","1999","","","1155","1160","This paper proposes a compiler design for IEC 1131-3 standard languages of PLCs (programmable logic controllers). It describes the structure of the front end of the compiler and the optimization phase of the intermediate representation of the back end. The paper also proposes optimization methods using several characteristics of the PLC. A software-based optimization uses the flow of the program and a hardware-based optimization is implemented by a programmable logic device to accelerate the logical operations that are most part of the PLC instructions. A benchmark test shows the proposed compiler speeds up the execution of the PLC.","","4-907764-13","10.1109/SICE.1999.788715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=788715","","IEC standards;Program processors;Logic design;Programmable logic arrays;Programmable logic devices;Programmable control;Optimizing compilers;Manufacturing automation;Optimization methods;Computer languages","programming languages;program compilers;high level languages","compiler design;IEC 1131-3 standard languages;programmable logic controllers;PLC;compiler front end structure;optimization phase;back end intermediate representation;software-based optimization;hardware-based optimization","","","12","","","","","","IEEE","IEEE Conferences"
"Wide-band high-efficiency printed loop antenna design for wireless communication systems","T. A. Denidni; Hyeonjin Lee; Yeongseog Lim; Qinjiang Rao","Inst. Nat. de la Recherche Scientifique, Univ. du Quebec, Montreal, Que., Canada; NA; NA; NA","IEEE Transactions on Vehicular Technology","","2005","54","3","873","878","A new wide-band high-efficiency coplanar waveguide-fed printed loop antenna is presented for wireless communication systems in this paper. By adjusting geometrical parameters, the proposed antenna can easily achieve a wide bandwidth. To optimize the antenna performances, a parametric study was conducted with the aid of a commercial software, and based on the optimized geometry, a prototype was designed, fabricated, and tested. The simulated and measured results confirmed that the proposed antenna can operate at (1.68-2.68 GHz) band and at (1.46-2.6 GHz) band with bandwidth of 1 and 1.14 GHz, respectively. Moreover, the antenna has a nearly omnidirectional radiation pattern with a reasonable gain and high efficiency. Due to the above characteristics, the proposed antenna is very suitable for applications in PCS and IMT2000 systems.","0018-9545;1939-9359","","10.1109/TVT.2005.844687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1433233","Loop antenna;PCS and IMT2000 systems;wide-band and two-frequency operation","Broadband antennas;Wireless communication;Bandwidth;Design optimization;Coplanar waveguides;Performance evaluation;Parametric study;Software performance;Geometry;Software prototyping","broadband antennas;coplanar waveguides;microstrip antennas;antenna radiation patterns;UHF antennas;3G mobile communication;loop antennas","wideband coplanar waveguide-fed printed loop antenna;wireless communication systems;commercial software;omnidirectional radiation pattern;IMT2000 systems;mobile handsets;1.68 to 2.68 GHz;1 GHz;1.14 GHz","","13","11","","","","","","IEEE","IEEE Journals & Magazines"
"Iterative optimization of convex divergence: applications to independent component analysis","Y. Matsuyama","Waseda University","IEEE International Symposium on Information Theory, 2003. Proceedings.","","2003","","","214","214","","","0-7803-7728","10.1109/ISIT.2003.1228228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1228228","","Independent component analysis;Equations;Iterative algorithms;Application software;Brain modeling;Computer science;Computational modeling;Information theory;Testing;Magnetic resonance imaging","","","","","4","","","","","","IEEE","IEEE Conferences"
"Optimization Procedures For Scattering Matrices In The Coherent And Partially Coherent Cases","W. Boerner; Wei-ling Yan; A. B. Kostinski","University Of Illinois At Chicago; NA; NA","12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,","","1989","2","","941","941","","","","10.1109/IGARSS.1989.579043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=579043","","Computer aided software engineering;Radar scattering;Polarization;Covariance matrix;Radar polarimetry;Laser radar;Physics;Testing;Measurement errors","","","","","","","","","","","IEEE","IEEE Conferences"
"Reconfigurable parallel approximate string matching on FPGAs","J. H. Park","Dept. of Comput. Sci., State Univ. of New York, New Paltz, NY, USA","8th Euromicro Conference on Digital System Design (DSD'05)","","2005","","","214","217","This paper presents a design and implementation of a reconfigurable parallel approximate string matching hardware on FPGAs. The design is based on a linear systolic dataflow algorithm, and control logic is added to reconfigure the resulting hardware. For the k-differences version of the approximate string matching problem, the proposed approach finds all approximate occurrences of a pattern in the reference string, with the time complexity O(n+m) where n and m are lengths of the reference string and the pattern, respectively. Unlike other hardware approaches found in the literature, the design is size optimized since it uses only m PEs that are independent on the reference string length. Also the design is flexible for handling arbitrary size pattern strings within the maximum bound. The design is implemented and tested on the target device Xilinx Spartan 2S XC2S200EPQ208.","","0-7695-2433","10.1109/DSD.2005.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559803","","Field programmable gate arrays;Hardware;Pattern matching;Concurrent computing;Reconfigurable logic;Software algorithms;Computer science;Algorithm design and analysis;Logic design;Design optimization","field programmable gate arrays;systolic arrays;parallel algorithms;string matching;computational complexity;logic design;logic testing","reconfigurable parallel approximate string matching;FPGA;linear systolic dataflow algorithm;control logic;k-differences version;reference string;Xilinx Spartan 2S XC2S200EPQ208","","","15","","","","","","IEEE","IEEE Conferences"
"Lagrangian relaxation neural networks for job shop scheduling","P. B. Luh; Xing Zhao; Yajun Wang; L. S. Thakur","Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; NA; NA; NA","Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146)","","1998","2","","1799","1804 vol.2","Manufacturing scheduling is an important but difficult task. Building on our previous success in developing optimization-based scheduling methods using Lagrangian relaxation for practical applications, this paper presents a novel Lagrangian relaxation neural network (LRNN) optimization technique. The convergence of LRNN for separable convex programming problems is established. For separable integer programming problems, LRNN is constructed to obtain near optimal solution in an efficient manner. When applying LRNN to separable job shop scheduling, a new neural dynamic programming method is developed to solve subproblems making innovative use of the dynamic programming structure. The synergy of Lagrangian relaxation and neural dynamic programming leads to a powerful neural optimization method for job shop scheduling. Testing results obtained by software simulation demonstrate that the performance is superior to what has been reported in the neural network literature. Results are also very close to what were obtained by a state-of-the-art optimization algorithm, and should be much improved when the method is refined and implemented in hardware.","1050-4729","0-7803-4300","10.1109/ROBOT.1998.677428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=677428","","Lagrangian functions;Neural networks;Job shop scheduling;Optimization methods;Dynamic programming;Manufacturing;Buildings;Linear programming;Software testing;Software performance","integer programming;convex programming;convergence;production control;neural nets","Lagrangian relaxation neural networks;job shop scheduling;optimization-based scheduling methods;separable convex programming problems;separable integer programming problems;neural dynamic programming method","","2","16","","","","","","IEEE","IEEE Conferences"
"Case study of principal component inverse and cross spectral metric for low rank interference adaptation","B. E. Freburger; D. W. Tufts","Dept. of Comput. & Electr. Eng., Rhode Island Univ., Kingston, RI, USA; NA","Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)","","1998","4","","1977","1980 vol.4","This paper presents a review of the principal component inverse (PCI) method of rapid adaptive signal detection and contrasts the use of principal components with the cross spectral metric (CSM) method for the generalized sidelobe canceller. The CSM method is optimal with known statistics and has been shown to outperform the PCI method in many cases of unknown covariance. This paper describes a scenario which represents a class of covariances where the PCI method can be expected to outperform the CSM method. The choice of method is therefore more subtle than previously thought.","1520-6149","0-7803-4428","10.1109/ICASSP.1998.681528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=681528","","Computer aided software engineering;Testing;Statistical analysis;Eigenvalues and eigenfunctions;Signal to noise ratio;Interference cancellation;Covariance matrix;Detectors;Statistical distributions;Integrated circuit noise","adaptive signal detection;interference suppression;statistical analysis;inverse problems;covariance matrices","cross spectral metric;principal component inverse;low rank interference adaptation;review;rapid adaptive signal detection;generalized sidelobe canceller;CSM method;PCI method;covariance","","4","16","","","","","","IEEE","IEEE Conferences"
"The antennas on a mobile board and their electromagnetic compatibility","E. A. Ibatoulline","Dept. of Radiophys., Kazan State Univ., Russia","IEEE Transactions on Electromagnetic Compatibility","","2003","45","1","119","124","This paper presents the solutions of a problem of optimization in the decrease of antenna-coupling coefficients on a mobile board in case of limitations on coverage for each antenna. To determine a minimum value of an antenna-coupling coefficient for one pair of antennas, we use the Gauss-Zeidel optimization method or search method. When two or several antenna pairs have a common antenna, the coordinates of this antenna are determined by the compromise. The algorithms of optimum antenna placement are supported by the computer software and are tested for a case of four antennas.","0018-9375;1558-187X","","10.1109/TEMC.2002.808071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180401","","Antenna theory;Optimization methods;Electromagnetic shielding;Electromagnetic compatibility;Mobile antennas","antenna theory;optimisation;electromagnetic shielding;electromagnetic compatibility;mobile antennas","antenna-coupling coefficients;search method;antenna pairs;antenna coordinates;electromagnetic compatibility;mobile board;coverage;Gauss-Zeidel optimization method","","4","6","","","","","","IEEE","IEEE Journals & Magazines"
"Adapting a rule-base for optimising combustion on a double burner boiler","T. C. Fogarty","Bristol Polytech., UK","Second International Conference on Software Engineering for Real Time Systems, 1989.","","1989","","","106","110","Rules for adjusting, individually, the air-intake to each of a number of burners, depending the concentration of oxygen and of carbon monoxide monitored in their common flue were elicited from the experts. The rules, along with routines to allow them to interact with a multiple channel analog/digital converter via a personal computer's communications port, were coded into PROLOG and tested on a multiple burner furnace. Electrically actuated louvres were fitted over the air-inlet aperture of each of the burners in a double burner boiler allowing individual control of the amount of air reaching each burner. A zirconia probe and an infrared analyser were fitted into the boiler's common flue providing on-line concentrations of oxygen and carbon monoxide in the mixed waste gases of the boiler's burners. The original rule-base was adapted to cope with the modulation of firing level on the boiler.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=51731","","Boilers;Digital control;Monitoring;Knowledge based systems","boilers;combustion;computerised control;computerised monitoring;knowledge based systems","electrically actuated louvres;O/sub 2/ monitoring;steam generating plants;CO/sub 2/ monitoring;double burner boiler;air-intake;flue;multiple channel analog/digital converter;multiple burner furnace;air-inlet aperture;double burner boiler;zirconia probe;infrared analyser;rule-base","","","","","","","","","IET","IET Conferences"
"Agent services-based infrastructure for online assessment of trading strategies","Longbing Cao; Jiaqi Wang; Li Lin; Chengqi Zhang","Fac. of Inf. Technol., Sydney Univ. of Technol., NSW, Australia; Fac. of Inf. Technol., Sydney Univ. of Technol., NSW, Australia; Fac. of Inf. Technol., Sydney Univ. of Technol., NSW, Australia; Fac. of Inf. Technol., Sydney Univ. of Technol., NSW, Australia","Proceedings. IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2004. (IAT 2004).","","2004","","","345","348","Traders and researchers in stock marketing often hold some private trading strategies. Evaluation and optimization of their strategies is a great benefit to them before they take any risk in realistic trading. We build an agent services-driven infrastructure: F-TRADE. It supports online plug in, iterative back-test, and recommendation of trading strategies. We propose agent services-driven approach for building the above automated enterprise infrastructure. Description, directory and mediation of agent services are discussed. System structure of the agent services-based F-TRADE is also discussed. F-TRADE has been an online test platform for research and application of multi-agent technology, and data mining in stock markets.","","0-7695-2101","10.1109/IAT.2004.1342967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342967","","Plugs;Data mining;Buildings;Iterative algorithms;Mediation;Testing;Subscriptions;Information technology;Australia;Stock markets","software agents;virtual enterprises;stock markets;multi-agent systems;data mining;electronic commerce;electronic trading","agent services-based infrastructure;online assessment;stock marketing;private trading strategy;online plug in;iterative back-test;automated enterprise infrastructure;agent services-based F-TRADE;online test platform;multiagent technology;data mining","","2","13","","","","","","IEEE","IEEE Conferences"
"Local minima free neural network learning","I. N. Jordanov; T. A. Rafik","Dept. of Comput. Sci. & Software Eng., Portsmouth Univ., UK; NA","2004 2nd International IEEE Conference on 'Intelligent Systems'. Proceedings (IEEE Cat. No.04EX791)","","2004","1","","34","39 Vol.1","Global optimization algorithm applied for feedforward neural networks (NN) supervised learning is investigated. The network weights are determined by minimizing the traditional backpropagation error function. The difference is that the optimization based learning algorithm utilizes stochastic technique, based on the use of low discrepancy sequences. This technique searches the parameter space, defined by the network weights, to define initial regions of attraction with candidates for local minima, and then exploits each region to locate the minima, and to determine a global minimum. The proposed technique is initially tested on multimodal mathematical functions and subsequently applied for training NN with moderate size for solving simple benchmark problems. Finally, the results are analysed, discussed, and compared with others.","","0-7803-8278","10.1109/IS.2004.1344633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1344633","","Neural networks;Stochastic processes;Convergence;Backpropagation algorithms;Supervised learning;Benchmark testing;Pattern recognition;Function approximation;Speech recognition;Robot control","feedforward neural nets;backpropagation;optimisation;stochastic processes","local minima;free neural network learning;global optimization algorithm;feedforward neural networks;supervised learning;network weights;backpropagation error function;stochastic technique;low-discrepancy sequences;parameter space;multimodal mathematical functions;stochastic global optimisation","","","15","","","","","","IEEE","IEEE Conferences"
"Refinements of a L2F anemometry technique for inter-blade flow field investigations in high-speed turbomachinery","I. Trebinjac; A. Vouillarmet; I. Claudin","Lab. de Mecanique des Fluides et d'Acoust., Ecole Centrale de Lyon, Ecully, France; Lab. de Mecanique des Fluides et d'Acoust., Ecole Centrale de Lyon, Ecully, France; Lab. de Mecanique des Fluides et d'Acoust., Ecole Centrale de Lyon, Ecully, France","ICIASF '95 Record. International Congress on Instrumentation in Aerospace Simulation Facilities","","1995","","","54/1","54/6","The need for detailed experimental measurements in small, high rotational speed compressors leads us to refine the L2F measurement technique, in particular the data acquisition procedures for the case of investigations within the rotating blade passages. The extent of shadow regions (due to blade twist) is predicted prior to making any measurement using a modelling software which is outlined in this paper. Such prediction allows us to assess the feasibility of creating the probe volume and collecting the scattered light, and can help choose the orientation of the lines of sight and the window locations. Due to reflections from the blade surfaces, there are usually regions where measurement is impossible. The analysis of these zero data zones is presented and leads to an optimized blade pitch partition for decreasing the acquisition time and better defining the azimuthal location of the acquired data. Two examples of the benefits which can be obtained using such a procedure are presented.","","0-7803-2088","10.1109/ICIASF.1995.519485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=519485","","Blades;Rotation measurement;Velocity measurement;Particle measurements;Compressors;Measurement techniques;Data acquisition;Software measurement;Predictive models;Probes","compressors;anemometry;laser velocimetry;gas turbines;flow measurement;data acquisition;computerised instrumentation;automatic test software","L2F anemometry technique;inter-blade flow field;high-speed turbomachinery;high rotational speed compressors;data acquisition;rotating blade passages;shadow regions;blade twist;modelling software;zero data zones;optimized blade pitch partition;acquisition time;azimuthal location;laser anemometry;centrifugal compressors;impellers","","","16","","","","","","IEEE","IEEE Conferences"
"On the impact of naming methods for heap-oriented pointers in C programs","Tong Chen; Jin Lin; Wei-Chung Hsu; Pen-Chung Yew","Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA","Proceedings International Symposium on Parallel Architectures, Algorithms and Networks. I-SPAN'02","","2002","","","251","256","Many applications written in C allocate memory blocks for their major data structures from the heap space at run time. The analysis of heap-oriented pointers in such programs is critical for compilers to generate high-performance code. However, most previous research on pointer analysis mostly focuses on pointers pointing to global or local variables. In this paper, we study points-to analysis of heap-oriented pointers using profiling information. An instrumentation tool and a set of library routines are developed to measure points-to sets of memory references at run time. Different naming methods for heap-oriented pointers are studied. We found that it is very important to adopt appropriate naming methods to recognize wrapper functions for memory allocation and memory management functions defined by users. Based on these naming methods, the approaches in pointer analysis, such as flow sensitivity and context sensitivity, are examined with the run-time tool. The program characteristics are observed at run time to evaluate what kind of compiler analysis is needed. Experiments are conducted on SPEC CPU2000 integer benchmarks. We found that flow sensitivity and context sensitivity have little impact on the analysis of heap-oriented pointers.","1087-4089","0-7695-1579","10.1109/ISPAN.2002.1004290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004290","","Runtime;Memory management;Computer science;Read only memory;Instruments;Optimized production technology;Program processors;Optimizing compilers;Testing;Shape","naming services;data structures;C language;program compilers;storage allocation;storage management;software libraries;software performance evaluation","naming methods;heap-oriented pointer analysis;C programs;run-time memory block allocation;data structures;compilers;high-performance code generation;global variables;local variables;points-to analysis;profiling information;instrumentation tool;library routines;memory references;wrapper functions;memory management;flow sensitivity;context sensitivity;program characteristics;SPEC CPU2000 integer benchmarks","","","15","","","","","","IEEE","IEEE Conferences"
"A connectionist model for diagnostic problem solving","Y. Peng; J. A. Reggia","Inst. for Software, Acad. Sinica, Beijing, China; NA","IEEE Transactions on Systems, Man, and Cybernetics","","1989","19","2","285","298","A competition-based connectionist model for solving diagnostic problems is described. The problems under consideration are computationally difficult in that multiple disorders may occur simultaneously and a global optimum in the space exponential to the total number of possible disorders is sought as a solution. To solve this problem, global optimization criteria are decomposed into local optimization criteria that are used to govern node activation updating in the connectionist model. Nodes representing disorders compete with each other to account for each 'individual' present manifestation, yet complement each other to account for all present manifestation, yet complement each other to account for all present manifestations through parallel node interactions. When equilibrium is reached, the network settles into a locally optimal state in which some disorder nodes (winners) are fully activated and compose the diagnosis for the given case, while all other disorder nodes are fully deactivated. A resettling process is proposed to improve accuracy. Three randomly generated examples of diagnostic problems, each of which has 1024 cases, were tested.<<ETX>>","0018-9472;2168-2909","","10.1109/21.31034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=31034","","Problem-solving;Artificial intelligence;Testing;NASA;Information systems;Computer science;Nervous system;Polynomials;Computer networks;Concurrent computing","artificial intelligence;optimisation;problem solving","probabilistic causal model;artificial intelligence;connectionist model;diagnostic problem solving;global optimization;local optimization;parallel node interactions","","76","33","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of software faults preventable at a personal level in a very large software development environment","W. D. Yu; A. Barshefsky; S. T. Huang","Systems Management and Quality Architecture Department at Lucent's Network Systems in Naperville, Illinois; 5ESS Call Processing and Japan Wireless Application Offers and Development Department at Lucent's Network Systems in Naperville, Illinois; University of North Carolina in Chapel Hill","Bell Labs Technical Journal","","1997","2","3","221","232","The Capability Maturity Model of the Software Engineering Institute (described by Watts S. Humphrey in Managing the Software Process, by Addison-Wesley, 1989) defines software fault prevention as a key process area required at Level 5  the optimizing level. We conducted a detailed study on a number of software faults found during low-level design, coding, and unit testing in the 5ESS-2000 switch software development environment. We discovered that the faults found during those phases were typically limited in their scopes of impact, but they usually account for more than 60% of the total software faults found in a software product before its delivery. We also found that the corrective and preventive tasks of each fault identified during those phases were very likely manageable by one development engineer. The results of the study of frequent software faults served as a base of knowledge that enabled engineers to strengthen their design and programming capabilities and prevent faults from occurring again.","1538-7305;1089-7089","","10.1002/bltj.2076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770937","","","","","","1","","","","","","","Nokia Bell Labs",""
"Fine pitch package mounting with TAB tape","G. Dehaine; J. Joly","Bull SA, Paris, France; Bull SA, Paris, France","Proceedings. Japan IEMT Symposium, Sixth IEEE/CHMT International Electronic Manufacturing Technology Symposium","","1989","","","73","78","The authors describe the basic concepts, development, assembly, and qualification testing of a ceramic leaded package for high-pin-count VLSI (>200 I/Os). The design has been aimed at achieving high-speed logic, testability, and reliability for a small pitch (<25 mil) package. To improve the electrical performance, ground and voltage distributions are separated from the logic circuitry connected by the surface mounting process. At the design level, thermal exchange and mechanical strength of the package were simulated using ANSYS thermomechanical software. Testability and availability of connections for burn-in before mounting are provided by the use of 50-mil-pitch test pads interconnected to the leads. The preferred solution for external connection and test pads is provided by the use of a TAB (tab automated bonding) lead frame bonded to the ceramic package. After burn-in and test, outer test pads are separated and the package is surface mounted. The TAB lead frame provides an optimized solution. Mounting to the printed circuit board is achieved using a thermode tool or laser soldering. Qualification results and mountability tests are satisfactory.<<ETX>>","","","10.1109/IEMTS.1989.76113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=76113","","Packaging;Circuit testing;Lead;Qualifications;Ceramics;Logic testing;Bonding;Assembly;Very large scale integration;Logic design","circuit reliability;lead bonding;packaging;printed circuit manufacture;surface mount technology;VLSI","small pitch package;digital simulation;PCB mounting;microassembly;SMT;TAB tape;assembly;qualification testing;ceramic leaded package;high-pin-count VLSI;high-speed logic;testability;reliability;electrical performance;surface mounting process;thermal exchange;mechanical strength;ANSYS thermomechanical software;tab automated bonding;burn-in;TAB lead frame;printed circuit board;25 mil","","3","5","","","","","","IEEE","IEEE Conferences"
"IF-sampling fourth-order bandpass /spl Delta//spl Sigma/ modulator for digital receiver applications","A. E. Cosand; J. F. Jensen; H. C. Choe; C. H. Fields","HRL Labs. LLC, Malibu, CA, USA; HRL Labs. LLC, Malibu, CA, USA; NA; NA","IEEE Journal of Solid-State Circuits","","2004","39","10","1633","1639","Bandpass modulators sampling at high IFs (/spl sim/200 MHz) allow direct sampling of an IF signal, reducing analog hardware, and make it easier to realize completely software-programmable receivers. This paper presents the circuit design of and test results from a continuous-time tunable IF-sampling fourth-order bandpass /spl Delta//spl Sigma/ modulator implemented in InP HBT IC technology for use in a multimode digital receiver application. The bandpass /spl Delta//spl Sigma/ modulator is fabricated in AlInAs-GaInAs heterojunction bipolar technology with a peak unity current gain cutoff frequency (f/sub T/) of 130 GHz and a maximum frequency of oscillation (f/sub MAX/) of 130 GHz. The fourth-order bandpass /spl Delta//spl Sigma/ modulator consists of two bandpass resonators that can be tuned to optimize both wide-band and narrow-band operation. The IF is tunable from 140 to 210 MHz in this /spl Delta//spl Sigma/ modulator for use in multiple platform applications. Operating from /spl plusmn/5-V power supplies, the fabricated fourth-order /spl Delta//spl Sigma/ modulator sampling at 4 GSPS demonstrates stable behavior and achieves a signal-to-(noise + distortion) ratio (SNDR) of 78 dB at 1 MHz BW and 50 dB at 60 MHz BW. The average SNDR performance measured on over 250 parts is 72.5 dB at 1 MHz BW and 47.7 dB at 60 MHz BW.","0018-9200;1558-173X","","10.1109/JSSC.2004.833544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336991","","Delta modulation;Digital modulation;Sampling methods;Application software;Circuit testing;Tunable circuits and devices;Cutoff frequency;Hardware;Circuit synthesis;Integrated circuit testing","delta-sigma modulation;radio receivers;digital radio;bipolar MIMIC;circuit tuning;microwave amplifiers;heterojunction bipolar transistors;integrated circuit design;integrated circuit testing;indium compounds;gallium arsenide;III-V semiconductors","bandpass modulator sampling;direct IF signal sampling;analog hardware;software-programmable receiver;circuit design;continuous-time IF-sampling bandpass modulators;tunable IF-sampling bandpass modulators;InP HBT IC technology;multimode digital receiver;heterojunction bipolar technology;peak unity current gain cutoff frequency;oscillation frequency;bandpass resonator;AS modulator;power supplies;signal-to-noise ratio;SNDR performance;analog-digital conversion;bandpass delta-sigma modulator;heterojunction bipolar transistor circuits;Q tuning;tunable delta-sigma modulator;fourth-order bandpass modulators;130 GHz;140 to 210 MHz;InP;AlInAs;GaInAs","","12","7","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient support of java RMI over heterogeneous wireless networks","Cheng-Wei Chen; Chung-Kai Chen; Jyh-Cheng Chen; Chien-Tan Ko; Jenq-Kuen Lee; Hong-Wei Lin; Wang-Jer Wu","Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan","2004 IEEE International Conference on Communications (IEEE Cat. No.04CH37577)","","2004","3","","1391","1395 Vol.3","Distributed object-oriented platforms are increasingly important over wireless environments to provide frameworks for collaborative computations and for managing a large pool of distributed resources. For beyond 3G environments, distributed object-oriented platforms can provide the framework and toolkits for application developments with heterogeneous wireless environments. In this paper, we present our support for Java RMI over Bluetooth, GPRS, and WLAN environments. We propose a software mechanism which can be used to dynamically adapt RMI over different networks with optimization-related strategies. This is an important middleware for component communications. We will show how to employ Java Dynamic Proxy and exception handling techniques to help perform roaming and resource scheduling among heterogeneous wireless environments. Java Grande benchmarks are used to demonstrate that our RMI implementations over GPRS, WLAN, and Bluetooth environments are effective in supporting parallel and distributed control of Java layers over heterogeneous wireless environments.","","0-7803-8533","10.1109/ICC.2004.1312740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1312740","","Java;Wireless networks;Bluetooth;Ground penetrating radar;Wireless LAN;Collaboration;Distributed computing;Environmental management;Resource management;Application software","4G mobile communication;packet radio networks;wireless LAN;Java;parallel programming;data handling;Bluetooth;distributed control;benchmark testing;middleware;optimisation;object-oriented programming;remote procedure calls;groupware","java RMI heterogeneous wireless network;distributed object-oriented platform;collaborative computation;distributed resource;toolkit;Bluetooth;GPRS;WLAN environment;software mechanism;optimization;middleware;component communication;Java dynamic proxy;handling technique;roaming resource scheduling;Java Grande benchmark;parallel-distributed control","","3","16","","","","","","IEEE","IEEE Conferences"
"Cooperation of synthesis, retargetable code generation and test generation in the MSS","P. Marwedel; W. Schenk","Inf. XII, Univ. of Dortmund, Germany; Inf. XII, Univ. of Dortmund, Germany","1993 European Conference on Design Automation with the European Event in ASIC Design","","1993","","","63","69","The authors report how the different tools in the MIMOLA hardware design system MSS are used during a typical design process. Typical design processes are partly automatic and partly manual. They include high-level synthesis, manual post-optimization, retargetable code generation, testability evaluation and simulation. It is shown how consistent tools can help to solve a variety of related design tasks. There is no other system with an equivalent set of consistent tools. A key point of this work is to show how current high-level synthesis systems can be extended by retargetable code generators which map algorithms to predefine structures. This extension is necessary in order to support manual design modifications.<<ETX>>","","0-8186-3410","10.1109/EDAC.1993.386499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=386499","","Testing;High level synthesis;Hardware;Process design;Reactive power;Counting circuits;Computer aided software engineering;Virtual colonoscopy","high level synthesis;design for testability;hardware description languages","MSS system;retargetable code generation;test generation;MIMOLA hardware design system;high-level synthesis;manual post-optimization","","4","19","","","","","","IEEE","IEEE Conferences"
"Information optimization for decision making","R. J. Martel; J. J. Sudano","Lockheed Martin Gov. Electron. Syst., Moorestown, NJ, USA; NA","Proceedings of the IEEE 1997 National Aerospace and Electronics Conference. NAECON 1997","","1997","1","","454","461 vol.1","Information optimization for decision-making is a system architecture definition problem as well as cognitive engineering or human factors issue. Human information processing, decision making and control are top-level functions of system architectures, but human information processing and decision making are not currently defined at that level. Present system design principles do not articulate a philosophy nor a body of practice about placing man in the system at the system architecture level. Since these functions do not appear in the top level architecture, they do not appear at the intermediate and lower levels of the design specifications. Having not been defined in the specifications, human decision making and control performance are generally not tested during the subsequent test and verification cycles. Decision making is no longer a uniquely human function in complex systems. Indeed, the speed and complexity of many system processes often preclude the human from decision and control functions. Several types of decision making processes can and should be automated. Optimized information processing for decision-making requires that cost allocations be made to provide for information generation, software and computer hardware architectures at the system-requirements levels.","","0-7803-3725","10.1109/NAECON.1997.618119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618119","","Decision making;Information processing;Control systems;Testing;Human factors;Automatic control;Cost function;Software;Hardware;Computer architecture","decision support systems;man-machine systems;hierarchical systems;aerospace computing;reliability","decision making;system architecture definition;human factors;human information processing;decision making processes;optimized information processing;cost allocation;human computer interaction;time error resolution functions;NASA","","3","22","","","","","","IEEE","IEEE Conferences"
"Efficient path profiling","T. Ball; J. R. Larus","Bell labs., Lucent Technol., Murray Hill, NJ, USA; NA","Proceedings of the 29th Annual IEEE/ACM International Symposium on Microarchitecture. MICRO 29","","1996","","","46","57","A path profile determines how many times each acyclic path in a routine executes. This type of profiling subsumes the more common basic block and edge profiling, which only approximate path frequencies. Path profiles have many potential uses in program performance tuning, profile-directed compilation, and software test coverage. This paper describes a new algorithm for path profiling. This simple, fast algorithm selects and places profile instrumentation to minimize run-time overhead. Instrumented programs run with overhead comparable to the best previous profiling techniques. On the SPEC95 benchmarks, path profiling overhead averaged 31%, as compared to 16% for efficient edge profiling. Path profiling also identifies longer paths than a previous technique, which predicted paths from edge profiles (average of 88, versus 34 instructions). Moreover, profiling shows that the SPEC95 train input datasets covered most of the paths executed in the ref datasets.","","0-8186-7641","10.1109/MICRO.1996.566449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566449","","Frequency;Force measurement;Aerospace electronics;Government;Tuning;Runtime;Instruments;Benchmark testing;Force control;Sun","optimising compilers;program testing","path profiling;program performance tuning;profile-directed compilation;software test coverage;run-time overhead;SPEC95 benchmarks;path profiling overhead;ref datasets","","147","11","","","","","","IEEE","IEEE Conferences"
"Dynamic process initial conditions in repetitive processes. Controllability and stability analysis","K. Galkowski; E. Rogers; A. Gramacki; J. Gramacki; D. H. Owens","Inst. of Robotics & Software Eng., Tech. Univ. of Zielona Gora, Poland; NA; NA; NA; NA","1999 Information, Decision and Control. Data and Information Fusion Symposium, Signal Processing and Communications Symposium and Decision and Control Symposium. Proceedings (Cat. No.99EX251)","","1999","","","271","276","Repetitive, or multipass, processes are a class of 2D systems of both practical and algorithmic/theoretical interest whose dynamics cannot be analysed or controlled using standard (1D) systems theory. Recently it has been shown that the modelling of the boundary conditions, also known as the process initial conditions, is a crucial feature in the analysis and control of these processes. This paper presents some further results on the effects of so-called 'dynamic' process initial conditions on the controllability and stability properties of discrete linear repetitive processes. Previous work has shown that these dynamic process initial conditions alone can destroy the stability properties of these processes. Hence their effects must be 'adequately' accounted for the process modelling stage in order to ensure that subsequent analysis does not lead to incorrect results/conclusions. The main results developed in this paper can be summarised as follows. (i) Computationally efficient stability tests which can, in effect, be applied using standard, or 1D, linear systems tests. (ii) Characterisation of so-called pass controllability in the form of matrix rank based conditions. (iii) Conditions under which the dynamic process initial conditions can be selected to ensure stability and pass controllability.","","0-7803-5256","10.1109/IDC.1999.754169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=754169","","Stability;Controllability;System testing;Control system analysis;Algorithm design and analysis;Control systems;Boundary conditions;Process control;Standards development;Linear systems","controllability;stability criteria;discrete systems","dynamic process initial conditions;stability analysis;multipass processes;2D systems;boundary conditions;discrete linear repetitive processes;computationally efficient stability tests;1D linear systems tests;pass controllability;matrix rank based conditions","","","10","","","","","","IEEE","IEEE Conferences"
"A New Synthesis Algorithm for the MIMOLA Software System","P. Marwedel","Institut fur Informatik und Prakt. Math., University of Kiel, Kiel, W. Germany","23rd ACM/IEEE Design Automation Conference","","1986","","","271","277","The MIMOLA software system is a system for the design of digital processors. The system includes subsystems for retargetable microcode generation, automatic generation of self-test programs and a synthesis subsystem. This paper describes the synthesis part of the system, which accepts a PASCAL-like, high-level program as specification and produces a register transfer structure. Because of the complexity of this design process, a set of sub-problems is identified and algorithms for their solution are indicated. These algorithms include a flexible statement decomposition, statement scheduling, register assignment, module selection and optimizations of interconnections and instruction word length.","0738-100X","0-8186-0702","10.1109/DAC.1986.1586100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1586100","","Software algorithms;Software systems;Algorithm design and analysis;Process design;Registers;Hardware;Built-in self-test;Design methodology;Delay;Contracts","","","","44","23","","","","","","IEEE","IEEE Conferences"
"Automating circuit simulation and evaluation for the semi-custom IC design process","J. T. Chang; L. L. Werner; R. S. Tepper","Applied Micro Circuits Corp., San Diego, CA, USA; Applied Micro Circuits Corp., San Diego, CA, USA; Applied Micro Circuits Corp., San Diego, CA, USA","[1992] Proceedings. Fifth Annual IEEE International ASIC Conference and Exhibit","","1992","","","257","260","A data-driven software system called ASE (automatic simulation environment) that automatically creates, simulates, evaluates, and stores the results in a database is described. ASE significantly reduces the engineering design cycle time. By automating the creation and analysis of SPICE input and output files, this tool provides designers with more time to focus on designing and optimizing circuits, rather than manually creating correct and complete simulation files.<<ETX>>","","0-7803-0768","10.1109/ASIC.1992.270239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=270239","","Circuit simulation;Process design;Circuit testing;SPICE;Automatic testing;Runtime;Integrated circuit testing;Software systems;Databases;Design automation","application specific integrated circuits;circuit analysis computing;circuit CAD;integrated circuit technology","SPICE input files;SPICE output files;semicustom IC;ASIC;semi-custom IC design process;data-driven software system;ASE;automatic simulation environment;database;engineering design cycle time","","","","","","","","","IEEE","IEEE Conferences"
"Data integration by describing sources with constraint databases","X. Cheng; G. Dong; T. Lau; J. Su","Dept. of Comput. Sci., California Univ., Santa Barbara, CA, USA; NA; NA; NA","Proceedings 15th International Conference on Data Engineering (Cat. No.99CB36337)","","1999","","","374","381","We develop a data integration approach for the efficient evaluation of queries over autonomous source databases. The approach is based on some novel applications and extensions of constraint database techniques. We assume the existence of a global database schema. The contents of each data source are described using a set of constraint tuples over the global schema; each such tuple indicates possible contributions from the source. The ""source description catalog"" (SDC) of a global relation consists of its associated constraint tuples. Such a method of description is advantageous since it is flexible to add new sources and to modify existing ones. In our framework, to evaluate a conjunctive query over the global schema, a plan generator first identifies relevant data sources by ""evaluating"" the query against the SDCs using techniques of constraint query evaluation; it then formulates an evaluation plan, consisting of some specialized queries over different paths. The evaluation of a query associated with a path is done by a sequence of partial evaluations at data sources along the path, similar to sideways information passing of Datalog; the partially evaluated queries travel along their associated paths. Our SDC based query planning is efficient since it avoids the NP-complete query rewriting process. We can achieve further optimization using techniques such as emptiness test.","1063-6382","0-7695-0071","10.1109/ICDE.1999.754953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=754953","","Databases;Computer science;Query processing;Warehousing;Data warehouses;Data engineering;Application software;Process planning;Testing;Software libraries","query processing;deductive databases;constraint handling","constraint databases;data integration approach;autonomous source databases;global database schema;data source;constraint tuples;global schema;source description catalog;global relation;conjunctive query;plan generator;data sources;constraint query evaluation;evaluation plan;specialized queries;partial evaluations;sideways information passing;Datalog;partially evaluated queries;SDC based query planning;NP-complete query rewriting process;emptiness test","","","31","","","","","","IEEE","IEEE Conferences"
"Product integrity assessment using fatigue synthesis for avionics programs","M. Rassaian; D. A. Pietila","Defense & Space Group, Boeing Co., Seattle, WA, USA; Defense & Space Group, Boeing Co., Seattle, WA, USA","Seventeenth IEEE/CPMT International Electronics Manufacturing Technology Symposium. 'Manufacturing Technologies - Present and Future'","","1995","","","133","140","The thermo-mechanical integrity of an electronics assembly is strongly affected by the attachment method used between functional components and the printed wiring board. The use of solder as an interconnect medium is the most common method in use today. In most applications, solder is required to serve multiple functions including electrical, thermal, and structural connection. Repetitive loads caused by thermal cycling and vibrations result in cyclical thermo-mechanical stresses that often lead to pre-mature fatigue cracking in the solder. Continued exposure to these environments leads to degradation or failure of the solder thereby reducing the hardware's useful life. This paper presents a novel method for predicting the fatigue life of an electronics assembly by analyzing every interconnect using an automated process modeling approach. Modular fatigue based routines, incorporated through a user friendly software tool known as FSAP (Fatigue Synthesis for Avionics Programs), facilitate concurrent design development and optimization relative to the hardware's thermo-mechanical fatigue response. FSAP integrates the global effects from specific usage environments with the detailed design and process features associated with the electronics assembly. An application of FSAP along with comprehensive model validation results from accelerated fatigue life testing is presented for a variety of common interconnect configurations including leaded and non-leaded surface mount components and hybrid/MCM packages.","","0-7803-2996","10.1109/IEMT.1995.526105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526105","","Fatigue;Aerospace electronics;Thermomechanical processes;Assembly;Lead;Thermal stresses;Hardware;Wiring;Thermal loading;Thermal degradation","production testing;fatigue;printed circuit testing;soldering;cracks;integrated circuit interconnections;concurrent engineering;life testing;fatigue testing;avionics","product integrity assessment;fatigue synthesis;avionics programs;thermo-mechanical integrity;electronics assembly;attachment method;functional components;printed wiring board;pre-mature fatigue cracking;automated process modeling approach;FSAP;concurrent design development;specific usage environments;model validation results;accelerated fatigue life testing;interconnect configurations;surface mount components;solder interconnect medium","","2","12","","","","","","IEEE","IEEE Conferences"
"Model-based human motion analysis in monocular video","W. W. Lok; K. L. Chan","Dept. of Comput. Eng. & Inf. Technol., City Univ. of Hong Kong, Kowloon, China; NA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","","2005","2","","ii/697","ii/700 Vol. 2","Tracking human motion in monocular video is a challenging problem in computer vision. It has found a wide range of applications, such as visual surveillance, virtual reality, sports science, etc. This project aims to develop a model-based human motion analysis system that can track human movement in a monocular image sequence with minimum constraint. No markers or sensors are attached to the subject. Given a video clip, the first step is to fit the 3D human model manually to the subject in the first frame of the video. Then background subtraction is used to extract the human silhouette. We propose the silhouette chamfer as the main matching feature. A chamfer distance measure is carried out on the extracted subject silhouette. The silhouette chamfer contains both the chamfer distance and region information. Finally, we use a discrete Kalman filter to predict the pose of the subject in each image frame. The updating step uses Broydent's method to optimize the predicted pose to fit the person's silhouette by using the cost function. We use the gait database SOTON to test our system. The image sequences contain human walking in both indoor and outdoor environments. The motion tracking results demonstrate that our system has an encouraging performance.","1520-6149;2379-190X","0-7803-8874","10.1109/ICASSP.2005.1415500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415500","","Humans;Motion analysis;Tracking;Image sequences;Data mining;Computer vision;Application software;Surveillance;Virtual reality;Optimization methods","optical tracking;computer vision;image motion analysis;video signal processing;image sequences;feature extraction;edge detection;Kalman filters;prediction theory;optimisation;image matching","human motion analysis;monocular video;human motion tracking;computer vision;visual surveillance;virtual reality;sports science;3D human model;image sequence;background subtraction;human silhouette extraction;silhouette chamfer distance measure;matching feature;discrete Kalman filter;Broydent method;cost function","","1","8","","","","","","IEEE","IEEE Conferences"
"Design, simulation and measurement of temperature optimized power hybrids","H. Khakzar","Fachhochschule fur Technik Esslingen","12th International Electronic Manufacturing Technology Symposium","","1992","","","346","351","By the end of the 90's it is predicted that the 1 Gbit memory chip will be available, Yhe super high density chips will dissipate more power, espaspecially because the operating freuency will be much higher. The most difficult problem of future packaging will be that of controlling the temperature of the transistor junctions to ensure high manufacturing reliability. In addition, more electronic devices am being used under the hood of automobiles, where high temperature is also a problem. This paper describes several methods for placing chips on a substrate to optimize the temperature. The thermal properties of a few test circuits were simulated usinig commercially available and custom software. The test circuits were then fabricated, and a commercially available system was used to measure the temperature at 10 points of the substrate and calculate the junction temperature. A large discrepancy was found between the simulated and the measured results. This shows that the two-dimensional thermal simulation packages for electronics cirsuits are insufficient and should be improved to take the third dimension into consideration. However the two-dimensional programs did show general trends. To obtain more accurate predictions of the thermal properties which are reguired for reliable manufacturing, actual measurements are still needed which we costly and time consurning.","","0-7803-0629","10.1109/IEMT.1992.763429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=763429","","Temperature measurement;Power measurement;Design optimization;Volume measurement;Microelectronics;Laboratories","","","","","4","","","","","","IEEE","IEEE Conferences"
"Optimum design of parallel coupled-line filters","H. Oraizi; M. Moradian; K. Hirasawa","NA; NA; NA","The Ninth International Conference onCommunications Systems, 2004. ICCS 2004.","","2004","","","340","344","In this paper, a design and optimization procedure based on the method of least squares is presented for edge-coupled or parallel coupled-line filters, with impedance matching of arbitrary source and load impedances. An error function is constructed for the implementation of specified insertion loss in the passband and return loss in the stopband, based on the even- and odd-mode theory using the impedance, transmission and scattering matrices. The minimization of error function is performed with respect to the lengths, widths and gap spacings of lines, which determines the filter configuration. Computer implementation of the proposed method, comparison with the available full-wave analysis softwares, fabrication, and testing of sample filters, indicate the ease of application and advantages of the proposed method over the other methods (such as impedance matching, increasing bandwidth, and ease of computer implementation)","","0-7803-8549","10.1109/ICCS.2004.1359395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359395","","Matched filters;Impedance matching;Propagation losses;Design optimization;Least squares methods;Couplings;Insertion loss;Passband;Scattering;Computer errors","band-pass filters;impedance matching;least squares approximations;matrix algebra;network synthesis;optimisation","parallel coupled-line filter;optimization procedure;least squares method;edge-coupled-line filter;impedance matching;error function;odd-mode theory;even-mode theory;scattering matrix;full-wave analysis;band-pass filter","","1","14","","","","","","IEEE","IEEE Conferences"
"Matrix formulation: fast filter bank","Lim Yong Ching; Lee Jun Wei","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore; NA","2004 IEEE International Conference on Acoustics, Speech, and Signal Processing","","2004","5","","V","133","The fast filter bank (FFB) describes a class of tree-structured filter banks that operate on a frequency response masking principle. Although the structure is highly regular and conveniently implemented in hardware designs, real-time software implementations lead to inefficiencies due to its branching structure. In this paper, an alternative formulation of the FFB is proposed in terms of matrix computations. This allows an efficient approach in its implementation, and significantly reduces the overall buffer memory size required. The matrix operations can be carried out using easily available highly-optimized mathematical software packages, resulting in improvements in computational speed. Savings of up to a factor of 3 in the computer time have been observed during tests on a Pentium 4 computational platform workstation.","1520-6149","0-7803-8484","10.1109/ICASSP.2004.1327065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1327065","","Filter bank;Packaging;Hardware;Computer buffers;Frequency response;Software packages;Channel bank filters;Software design;Testing;Workstations","channel bank filters;digital filters;frequency response;matrix algebra","frequency-selective channels;single-rate filter bank;fast filter banks;tree-structured filter banks;frequency response masking principle;real-time FFB software implementations;matrix computations;buffer memory size reduction;matrix operations","","","11","","","","","","IEEE","IEEE Conferences"
"Region based variable quantization for JPEG image compression","M. A. Golner; W. B. Mikhael; V. Krishnan; A. Ramaswamy","Sch. of Electr. Eng. & Comput. Sci., Univ. of Central Florida, Orlando, FL, USA; NA; NA; NA","Proceedings of the 43rd IEEE Midwest Symposium on Circuits and Systems (Cat.No.CH37144)","","2000","2","","604","607 vol.2","Introduces concepts that optimize image compression ratio by utilizing the information about a signal's properties and their uses. This additional information about the image is used to achieve further gains in image compression. The techniques developed in this work are on the ubiquitous JPEG still image compression standard [IS094] for compression of continuous tone grayscale and color images. This paper is based on a region based variable quantization JPEG software codec that was developed tested and compared with other image compression techniques. The application, named JPEGTool, has a graphical user interface (GUI) and runs under Microsoft Windows (R) 95. This paper discusses briefly the standard JPEG implementation and software extensions to the standard. region selection techniques and algorithms that complement variable quantization techniques are presented in addition to a brief discussion on the theory and implementation of variable quantization schemes. The paper includes a presentation of generalized criteria for image compression performance and specific results obtained with JPEGTool.","","0-7803-6475","10.1109/MWSCAS.2000.952829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952829","","Quantization;Transform coding;Image coding;Graphical user interfaces;Software standards;Standards development;Gray-scale;Color;Codecs;Software testing","quantisation (signal);data compression;image coding;image colour analysis;codecs","region based variable quantization;JPEG image compression;image compression ratio;still image compression standard;continuous tone grayscale;color images;software codec;variable quantization techniques;JPEGTool","","7","6","","","","","","IEEE","IEEE Conferences"
"A unifying framework for iteration reordering transformations","W. Kelly; W. Pugh","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","Proceedings 1st International Conference on Algorithms and Architectures for Parallel Processing","","1995","1","","153","162 vol.1","We present a framework for unifying iteration reordering transformations such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. The framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. The framework is designed to provide a uniform way to represent and reason about transformations. We also provide algorithms to test the legality of mappings, and to generate optimized code for mappings.<<ETX>>","","0-7803-2018","10.1109/ICAPP.1995.472180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472180","","Optimizing compilers;Performance analysis;Programming environments","optimising compilers;program compilers;parallel programming;optimisation","iteration reordering transformations;loop interchange;loop distribution;skewing;tiling;index set splitting;statement reordering;optimized code","","16","19","","","","","","IEEE","IEEE Conferences"
"Declassification: transforming Java programs to remove intermediate classes","B. Power; G. W. Hamilton","Dept. of Comput. & Networking, Carlow Inst. of Technol., Ireland; NA","Fifth IEEE International Workshop on Source Code Analysis and Manipulation (SCAM'05)","","2005","","","183","192","This paper presents an optimisation technique which automatically inlines certain classes within their enclosing class. Inlining a class involves inserting the fields and methods of this class into the body of its enclosing class. The enclosing class is the class which declared an instance of the class. The declaration of the inlined class can then be removed from the program. This technique transforms Java programs into an equivalent form, which may be less readable, but is more efficient. The results of the empirical study showed that few classes were found suitable for inlining and that the declassification was not overly successful when optimizing the test programs. One of the advantages of declassification is that it does not result in code bloating. It is thought that further extensions to the declassification technique and an intrinsically object-oriented set of test programs could greatly improve its effectiveness.","","0-7695-2292","10.1109/SCAM.2005.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541170","","Java;Testing;Object oriented programming;Object oriented modeling;Dynamic programming;Data structures;Algorithm design and analysis;Computer networks;Computer applications;Application software","program interpreters;Java;optimisation","Java program transformation;intermediate classes;program optimisation;class inlining;declassification technique","","1","15","","","","","","IEEE","IEEE Conferences"
"Scheduling of soft real-time systems for context-aware applications","J. L. Wong; Weiping Liao; Fei Li; Lei He; M. Potkonjak","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; NA; NA; NA; NA","Design, Automation and Test in Europe","","2005","","","318","323 Vol. 1","Context-aware applications pose new challenges, including a need for new computational models, uncertainty management, and efficient optimization under uncertainty. Uncertainty can arise at two levels: multiple and single tasks. When a mobile user changes environments, the context changes resulting in the possibility of the user requesting tasks which are specific for the new environment. However as the user moves these requested tasks may no longer be context relevant. Additionally, the runtime of each task is often highly dependent on the input data. We introduce an hierarchical multi-resolution statistical task model that captures relevant aspects at the task and intertask levels, and captures not only uncertainty, but also introduces the notion of utility for the user. We have developed a system of nonparametric statistical techniques for modeling the runtime of a specific task. This model is a framework where we define problems of design and optimization of statistical soft real-time systems (SSRTS). The main algorithmic novelty is a cumulative potential-based task scheduling heuristic for maximizing utility. The heuristic conducts global optimization and induces low runtime overhead. We demonstrate the effectiveness of the scheduling heuristic using a Trimaran-based evaluation platform.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395579","","Real time systems;Uncertainty;Processor scheduling;Computational modeling;Runtime;Application software;Pervasive computing;Multimedia databases;Computer science;Context modeling","real-time systems;scheduling;mobile computing;optimisation;nonparametric statistics;uncertain systems","scheduling;context-aware applications;cumulative potential-based task scheduling;global optimization;Trimaran-based evaluation platform;mobile user;hierarchical task model;multi-resolution statistical task model;uncertainty;utility;nonparametric statistical techniques;runtime modeling;statistical soft real-time systems","","","13","","","","","","IEEE","IEEE Conferences"
"A new tool for multijunction cell diagnostics-QE measurements under AM1 bias light","Jingya Hou; S. J. Fonash; Liangfan Chen","Electron. Mater. & Processing Res. Lab., Pennsylvania State Univ., University Park, PA, USA; Electron. Mater. & Processing Res. Lab., Pennsylvania State Univ., University Park, PA, USA; NA","Conference Record of the Twenty Third IEEE Photovoltaic Specialists Conference - 1993 (Cat. No.93CH3283-9)","","1993","","","891","895","The authors attempt to obtain a relatively simple diagnostic tool that can be used to optimize the efficiency of multijunction solar cells. This measurement should be able to pinpoint which sub-cell most needs improvement. In this report, they explore such a tool: the quantum efficiency at the maximum power point under AM1 light (AM1QE). They demonstrate the application of AM1QE-based guidelines with AMPS computer program simulations and show that the most substantial improvement from sub-cell thickness adjustments comes from the sub-cell which causes a peak in the AM1QE. They also show that this proposed AM1QE quantum efficiency measurement for multijunctions is a much more sensitive monitor of changes in a multijunction than the so-called color light bias quantum efficiency measurement.<<ETX>>","","0-7803-1220","10.1109/PVSC.1993.347103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=347103","","Wavelength measurement;Shape;Thickness measurement;Monitoring;Color;Current measurement;Electric variables measurement;Identity-based encryption;Power measurement;Power conversion","solar cells;semiconductor device testing;semiconductor device models;p-n heterojunctions;electronic engineering computing;power engineering computing;digital simulation;software packages","multijunction solar cells;diagnostic tool;efficiency;optimisation;measurement;quantum efficiency;maximum power point;AM1 light;guidelines;AM1QE;computer program;simulation;AMPS;sub-cell thickness","","","1","","","","","","IEEE","IEEE Conferences"
"Maximizing performance by retiming and clock skew scheduling","Xun Liu; M. C. Papaefthymiou; E. G. Friedman","Dept. of Electr. Eng., Michigan Univ., Ann Arbor, MI, USA; NA; NA","Proceedings 1999 Design Automation Conference (Cat. No. 99CH36361)","","1999","","","231","236","The application of retiming and clock skew scheduling for improving the operating speed of synchronous circuits under setup and hold constraints is investigated in this paper. It is shown that when both long and short paths are considered, circuits optimized by the simultaneous application of retiming and clock scheduling can achieve shorter clock periods than optimized circuits generated by applying either of the two techniques separately. A mixed-integer linear programming formulation and an efficient heuristic are given for the problem of simultaneous retiming and clock skew scheduling under setup and hold constraints. Experiments with benchmark circuits demonstrate the efficiency of this heuristic and the effectiveness of the combined optimization. All of the test circuits show improvement. For more than half of them, the maximum operating speed increases by more than 21% over the optimized circuits obtained by applying retiming or clock skew scheduling separately.","","1-58113-092","10.1109/DAC.1999.781317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781317","","Clocks;Circuit testing;Processor scheduling;Linear programming;Digital circuits;Timing;Permission;Delay;Computer science;Application software","logic CAD;circuit optimisation;timing;scheduling;linear programming;integer programming;directed graphs","synchronous circuits;performance maximization;retiming;clock skew scheduling;setup constraints;hold constraints;circuit optimization;clock period;mixed-integer linear programming formulation;heuristic;benchmark circuits;combined optimization;maximum operating speed","","11","18","","","","","","IEEE","IEEE Conferences"
"Sparse kernel density construction using orthogonal forward regression with leave-one-out test score and local regularization","Sheng Chen; Xia Hong; C. J. Harris","Sch. of Electron. & Comput. Sci., Univ. of Southampton, UK; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2004","34","4","1708","1717","This paper presents an efficient construction algorithm for obtaining sparse kernel density estimates based on a regression approach that directly optimizes model generalization capability. Computational efficiency of the density construction is ensured using an orthogonal forward regression, and the algorithm incrementally minimizes the leave-one-out test score. A local regularization method is incorporated naturally into the density construction process to further enforce sparsity. An additional advantage of the proposed algorithm is that it is fully automatic and the user is not required to specify any criterion to terminate the density construction procedure. This is in contrast to an existing state-of-art kernel density estimation method using the support vector machine (SVM), where the user is required to specify some critical algorithm parameter. Several examples are included to demonstrate the ability of the proposed algorithm to effectively construct a very sparse kernel density estimate with comparable accuracy to that of the full sample optimized Parzen window density estimate. Our experimental results also demonstrate that the proposed algorithm compares favorably with the SVM method, in terms of both test accuracy and sparsity, for constructing kernel density estimates.","1083-4419;1941-0492","","10.1109/TSMCB.2004.828199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315754","","Kernel;Testing;Support vector machines;Support vector machine classification;Computational efficiency;Probability density function;Machine learning;State estimation;Least squares approximation;Application software","regression analysis;support vector machines;least mean squares methods;estimation theory;probability;minimisation","construction algorithm;sparse kernel density construction;model generalization;orthogonal forward regression;leave-one-out test score;local regularization method;support vector machine;SVM;Parzen window density estimate;orthogonal least square;probability density function","Algorithms;Artificial Intelligence;Computer Simulation;Least-Squares Analysis;Models, Statistical;Regression Analysis;Statistical Distributions","41","34","","","","","","IEEE","IEEE Journals & Magazines"
"Testing autonomous systems for deep space exploration","K. Reinholtz; K. Patel","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA","1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)","","1998","2","","283","288 vol.2","NASA is moving into an era of increasing spacecraft autonomy. However, before autonomy can be routinely utilized, we must provide techniques for providing assurance that the system will perform correctly in flight. We describe why autonomous systems require advanced verification techniques, and offer some management and technical techniques for addressing the differences. Autonomous goal-driven spacecraft require advances in verification techniques because optimization (e.g. planning and scheduling) algorithms are at the core of much of autonomy. It is the nature of such algorithms that over much of the input space an intuitively ""small"" change in the input results in a correspondingly ""small"" change in the output: This type of response typically leads one to conclude, quite reasonably, that if the two responses are correct, those responses ""between"" them will probably be correct. However, there are certain regions in the input space where a ""small"" change in the input will result in a radically different output: One is not so inclined to conclude that all responses in these transition zones are likely to be correct. We believe, for two reasons, that these transition zones are one place where autonomous systems are likely to fail. First, boundary conditions, often a rich source of faults, are highly exercised in the transition zones, and so increase the likelihood of faults. Second, within the transition zone the algorithm outputs are likely to appear unusual, and, since the outputs of the algorithm become inputs to the remainder of the system, the whole system is probably pushed outside of its nominal usage profile: historically shown to be another good source of faults. We close with a discussion of risk management. Autonomous systems have many well-known management risk factors. Risk management and quality concerns must be pervasive, throughout all team members and the whole life-cycle of the project.","1095-323X","0-7803-4311","10.1109/AERO.1998.687915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=687915","","System testing;Space exploration;Space vehicles;Risk management;Software testing;Propulsion;Space technology;Aerospace testing;Formal specifications;Costs","space vehicles;aerospace control;risk management","autonomous systems;deep space exploration;NASA;spacecraft autonomy;verification techniques;goal-driven spacecraft;transition zones;boundary conditions;nominal usage profile;risk management","","1","14","","","","","","IEEE","IEEE Conferences"
"An algorithm for diagnostic reasoning using TFPG models in embedded real-time applications","L. Howard","Inst. for Software Integrated Syst., Vanderbilt Univ., Nashville, TN, USA","2001 IEEE Autotestcon Proceedings. IEEE Systems Readiness Technology Conference. (Cat. No.01CH37237)","","2001","","","978","987","Embedded diagnostic reasoners require compact modeling representations and efficient reasoning algorithms given limited available computational resources. Timed failure propagation graphs (TFPG) are compact representations used to model failure causes and progressions of conditions that are symptoms of failure occurrence, together with the temporality and likelihood of these symptom progressions and the observation of some of the aberrant conditions. Algorithms for design-time diagnosability analysis using TFPG models have previously been reported, but these algorithms have different design objectives leading to different computational strategies and optimization criteria. This paper presents and discusses an algorithm specifically designed for efficient failure isolation based on reported observations of abnormal conditions that is suitable for use in embedded real-time applications.","1080-7225","0-7803-7094","10.1109/AUTEST.2001.949477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949477","","Algorithm design and analysis;Application software;Embedded computing;Real time systems;Embedded software;Software systems;Software algorithms;Design optimization;Failure analysis","diagnostic reasoning;embedded systems;failure analysis;graph theory;engineering computing","online diagnosis;embedded diagnostic reasoners;TFPG models;embedded real-time applications;embedded diagnostics;diagnostic reasoning algorithms;compact modeling representations;timed failure propagation graphs;failure occurrence;efficient failure isolation;reported observations;abnormal conditions","","2","5","","","","","","IEEE","IEEE Conferences"
"A unified System C-based framework for simulation, optimization and synthesis of complex systems implementing DSP algorithms","G. C. Cardarilli; A. Malatesta; M. Re; L. Arnone; A. Rosti","Dept. of Electron. Eng., Tor Vergata Univ., Rome, Italy; Dept. of Electron. Eng., Tor Vergata Univ., Rome, Italy; Dept. of Electron. Eng., Tor Vergata Univ., Rome, Italy; NA; NA","Proceedings of the Fourth IEEE International Symposium on Signal Processing and Information Technology, 2004.","","2004","","","90","94","This paper proposes a methodology for fast design-to-synthesis flow. We want to promote an improvement to the currently used methodology, which is based on functional design, refinement, HDL implementation and logic synthesis. Our new approach is based on three principles. One is the choice of a unique modeling language that spans all the needed abstraction levels for hardware system design, from functional verification down to system implementation. The second is the possibility to allow fast system design analysis and exploration avoiding to recompile the model every time the design is changed, by dynamic loading parameters from XML files. Finally we want to support automatic generation of the hardware model from which the final system logic synthesis can be performed.","","0-7803-8689","10.1109/ISSPIT.2004.1433695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1433695","","Digital signal processing;Mathematical model;Hardware design languages;Computational modeling;Computer languages;System testing;Writing;Design methodology;Logic design;Automatic logic units","simulation languages;hardware description languages;hardware-software codesign;XML;optimisation;signal processing","unified System C;complex system synthesis;DSP algorithm;fast design-to-synthesis flow;hardware description language;HDL implementation;logic synthesis;modeling language;hardware system design;functional verification;dynamic loading parameter;XML file","","","8","","","","","","IEEE","IEEE Conferences"
"Promoting the application of expert systems in short-term unit commitment","S. Li; S. M. Shahidehpour; C. Wang","Dept. of Electr. & Comput. Eng., Illinois Inst. of Technol., Chicago, IL, USA; Dept. of Electr. & Comput. Eng., Illinois Inst. of Technol., Chicago, IL, USA; Dept. of Electr. & Comput. Eng., Illinois Inst. of Technol., Chicago, IL, USA","IEEE Transactions on Power Systems","","1993","8","1","286","292","A heuristic approach to the short-term unit commitment problem is presented. The generating unit's capacities are divided into thee categories, and then a rough 24 h-generation schedule is established. The global optimization is performed as an inference process. A prototype system is developed and tested on an IBM-386 personal computer using the C language. The ToolBook software is used to enhance the graphics capability of the package. The primary advantage of the proposed method is its short processing time for each schedule, as it schedules the generation unit commitment for a typical system in less than 2 s. The same system is examined by dynamic programming with an average running time of 20 min on an IBM 386 personal computer.<<ETX>>","0885-8950;1558-0679","","10.1109/59.221229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=221229","","Expert systems;Processor scheduling;Microcomputers;Software prototyping;Prototypes;System testing;Software tools;Computer graphics;Software packages;Packaging","engineering graphics;expert systems;microcomputer applications;power system analysis computing;user interfaces","expert systems;short-term unit commitment;heuristic approach;24 h-generation schedule;global optimization;inference process;IBM-386 personal computer;C language;ToolBook software;graphics;dynamic programming","","20","22","","","","","","IEEE","IEEE Journals & Magazines"
"Precision and error analysis of MATLAB applications during automated hardware synthesis for FPGAs","A. Nayak; M. Haldar; A. Choudhary; P. Banerjee","Center for Parallel & Distributed Comput., Northwestern Univ., Evanston, IL, USA; NA; NA; NA","Proceedings Design, Automation and Test in Europe. Conference and Exhibition 2001","","2001","","","722","728","We present a compiler that takes high level signal and image processing algorithms described in MATLAB and generates an optimized hardware for an FPGA with external memory. We propose a precision analysis algorithm to determine the minimum number of bits required by an integer variable and a combined precision and error analysis algorithm to infer the minimum number of bits required by a floating point variable. Our results show that on average, our algorithms generate hardware requiring a factor of 5 less FPGA resources in terms of the configurable logic blocks (CLBs) consumed as compared to the hardware generated without these optimizations. We show that our analysis results in the reduction in the size of lookup tables for functions like sin, cos, sqrt, exp etc. Our precision analysis also enables us to pack various array elements into a single memory location to reduce the number external memory accesses. We show that such a technique improves the performance of the generated hardware by an average of 35%.","1530-1591","0-7695-0993","10.1109/DATE.2001.915108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915108","","Error analysis;MATLAB;Hardware;Field programmable gate arrays;Signal processing;Image processing;Signal generators;Optimizing compilers;Algorithm design and analysis;Logic","hardware-software codesign;field programmable gate arrays;hardware description languages;table lookup;image processing;digital signal processing chips;error analysis;program compilers;integer programming;floating point arithmetic","MATLAB applications;automated hardware synthesis;FPGA;precision and error analysis;compiler;high level algorithms;image processing algorithms;signal processing algorithms;optimized hardware;external memory;precision analysis algorithm;minimum number of bits;integer variable;floating point variable;configurable logic blocks;reduced lookup table size;various array elements;single memory location;value range propagation algorithm;memory packing algorithm;MATCH compiler;VHDL","","40","28","","","","","","IEEE","IEEE Conferences"
"IFS avionics bus communication system","Shikui Wang; Jingyan Cui","Aeronautical Comput. Tech. Inst., Xi'an, China; Aeronautical Comput. Tech. Inst., Xi'an, China","Proceedings of TENCON '93. IEEE Region 10 International Conference on Computers, Communications and Automation","","1993","1","","531","534 vol.1","Based on GJB 289-87, the IFS avionics bus communication system is a distributed communication simulation system. It provides a real-time environment and experimental data for the engineering implementation, optimization and testing of avionics integrated tasks. It's architecture and design are presented.<<ETX>>","","0-7803-1233","10.1109/TENCON.1993.320043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=320043","","Aerospace electronics;Computer architecture;Protocols;Random access memory;Logic;Computational modeling;Hardware;Application software;Data engineering;Testing","aerospace computing;aircraft instrumentation;network interfaces;computer communications software;open systems;digital simulation;aerospace simulation;real-time systems","IFS avionics bus communication system;GJB 289-87;distributed communication simulation system;real-time environment;experimental data;avionics integrated tasks","","","5","","","","","","IEEE","IEEE Conferences"
"Impacts of load models on solutions of Newton optimal power flow","Ying-Yi Hong","Dept. of Electr. Eng., Chung Yuan Christian Univ., Chung Li, Taiwan","Proceedings of TENCON '93. IEEE Region 10 International Conference on Computers, Communications and Automation","","1993","5","","398","401 vol.5","The optimal power flow (OPF) package is a useful tool in a modern energy management system. Generally, the constant power load model is used in the OPF study. However, various models would lead to different conclusions. In this paper, the constant power and constant impedance load models are adopted to investigate the impacts of various load models on Newton OPF solutions. Several significances, based on the test results, are presented in this paper. A practical 251-bus system, namely the Taiwan power system, is used to serve as a sample to illustrate the above significant points.<<ETX>>","","0-7803-1233","10.1109/TENCON.1993.320666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=320666","","Load modeling;Load flow;Power system modeling;Voltage;Packaging;Impedance;Energy management;Power system security;Equations;Lagrangian functions","power system analysis computing;digital simulation;load flow;optimisation;software packages","Newton optimal power flow;load models;energy management system;constant power load model;constant impedance load model;power system;EMS;software package","","","13","","","","","","IEEE","IEEE Conferences"
"Synthesis and applications of lattice image operators based on fuzzy norms","P. Maragos; V. Tzouvaras; G. Stamou","Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece; NA; NA","Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)","","2001","1","","521","524 vol.1","We use concepts from the lattice-based theory of morphological operators and fuzzy sets to develop generalized lattice image operators that can be expressed as nonlinear convolutions that are suprema or infima of fuzzy intersection or union norms. Our emphasis (different from previous works) is the construction of pairs of fuzzy dilation and erosion operators that form lattice adjunctions. This guarantees that their composition will be a valid algebraic opening or closing. The power, but also the difficulty, in applying these fuzzy operators to image analysis is the large variety of fuzzy norms and the absence of systematic ways in selecting them. Towards this goal, we have performed extensive experiments in applying these fuzzy operators to various nonlinear filtering and image analysis tasks, attempting first to understand the effect that the type of fuzzy norm and the shape/size of the structuring function has on the resulting new image operators. Further, we have developed some new fuzzy edge gradients and optimized their usage for edge detection on test problems via a parametric fuzzy norm.","","0-7803-6725","10.1109/ICIP.2001.959068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959068","","Lattices;Image edge detection;Fuzzy sets;Fuzzy systems;Filtering;Convolution;Signal mapping;Application software;Fuzzy set theory;Testing","edge detection;lattice theory;fuzzy set theory;mathematical morphology;convolution;nonlinear filters;optimisation","lattice image operators;fuzzy intersection norms;lattice theory;morphological operators;fuzzy sets;nonlinear convolutions;suprema;infima;fuzzy union norms;fuzzy dilation operators;fuzzy erosion operators;lattice adjunctions;image analysis;nonlinear filtering;edge detection;mathematical morphology","","2","12","","","","","","IEEE","IEEE Conferences"
"Enhanced fault management for future IMA systems","G. W. Wilcock","Sensor & Avionic Syst., Defence Evaluation & Res. Agency, Farnborough, UK","16th DASC. AIAA/IEEE Digital Avionics Systems Conference. Reflections to the Future. Proceedings","","1997","1","","3.2","32","Integrated Modular Avionic (IMA) concepts offer the capability to enhance the reliability and availability of future avionic systems whilst supporting high performance, flexible deployment and affordability. Fault management is the key to achieving many of the projected benefits but involves significant technical risk: design entails complex trade-offs whilst validation of solutions through testing is only practicable for a small proportion of the possible failure set. System modelling as part of requirements definition and design is essential to achieve optimised solutions with low risk. This paper describes a modelling programme initially aimed at exploring the tradeoffs in system architecture and fault management. The approach being taken to extend it to allow for software reliability and fault tolerance is described.","","0-7803-4150","10.1109/DASC.1997.635050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=635050","","Aerospace electronics;Fault tolerance;Hardware;Availability;Software reliability;Mission critical systems;Mars;Application software;Sensor systems;Project management","software reliability;military avionics;aircraft computers;aerospace computing;military computing;reliability theory","enhanced fault management;integrated modular avionics;availability;system modelling;optimised solutions;modelling programme;system architecture;software reliability;fault tolerance","","1","5","","","","","","IEEE","IEEE Conferences"
"A minimum cut based re-synthesis approach","M. Welling; S. Tragoudas; H. Wang","Electr. & Comput. Eng. Dept., Southern Illinois Univ., Carbondale, IL, USA; Electr. & Comput. Eng. Dept., Southern Illinois Univ., Carbondale, IL, USA; Electr. & Comput. Eng. Dept., Southern Illinois Univ., Carbondale, IL, USA","Sixth international symposium on quality electronic design (isqed'05)","","2005","","","202","207","A new re-synthesis approach that benefits from min-cut based partitioning is proposed. This divide and conquer approach is shown to improve the performance of existing synthesis tools on a variety of benchmarks.","1948-3287;1948-3295","0-7695-2301","10.1109/ISQED.2005.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410584","","Redundancy;Circuit synthesis;Packaging;Automatic test pattern generation;Circuit faults;Network synthesis;Kernel;Design optimization;Optimization methods;Polynomials","circuit CAD;integrated circuit design;integrated circuit modelling;divide and conquer methods;software tools","minimum cut based resynthesis approach;min-cut based partitioning;divide and conquer approach;synthesis tools;multiple-level networks;circuit design modeling;integrated circuits","","","12","","","","","","IEEE","IEEE Conferences"
"Power-aware network swapping for wireless palmtop PCs","A. Acquaviva; E. Lattanzi; A. Bogliolo","Univ. di Urbino, Italy; Univ. di Urbino, Italy; Univ. di Urbino, Italy","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","2","","858","863 Vol.2","Virtual memory is considered to be an unlimited resource in desktop or notebook computers with high storage memory capabilities. However, in wireless mobile devices like palmtops and personal digital assistants (PDA), storage memory is limited or absent due to weight, size and power constraints. As a consequence, swapping over remote memory devices can be considered as a viable alternative. Nevertheless, power hungry wireless network interface cards (WNIC) may limit the battery lifetime and application performance if not efficiently exploited. In this work we explore performance and energy of network swapping in comparison with swapping on local micro-drives and flash memories. Our study points out that remote swapping over power-manageable WNICs can be more efficient than local swapping and that both energy and performance can be optimized through power-aware reshaping of data requests. Experimental results show that our optimization technique can save up to 60% of communication energy while improving performance.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268993","","Personal digital assistants;Personal communication networks;Wireless networks;Batteries;Mobile communication;File servers;Network servers;File systems;Application software;Linux","notebook computers;network interfaces;optimisation;radio access networks","network swapping;wireless palmtop PC;virtual memory;notebook computers;wireless mobile device;personal digital assistants;storage memory;power constraints;remote memory devices;wireless network interface cards;microdrives;flash memory;power aware reshaping;optimization technique","","1","15","","","","","","IEEE","IEEE Conferences"
"Static-priority scheduling of multiframe tasks","S. K. Baruah; Deji Chen; A. Mok","Vermont Univ., Burlington, VT, USA; NA; NA","Proceedings of 11th Euromicro Conference on Real-Time Systems. Euromicro RTS'99","","1999","","","38","45","The multiframe model of hard-real-time tasks is a generalization of the well-known periodic task model of C. Liu and J. Layland (1973). The feasibility analysis of systems of multiframe tasks which are assigned priorities according to the rate-monotonic priority assignment scheme is studied. An efficient sufficient feasibility test for such systems of multiframe tasks is presented and proved correct-this generalizes a result of A.K. Mok and D. Chen (1997).","1068-3070","0-7695-0240","10.1109/EMRTS.1999.777448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777448","","Power system modeling;Scheduling algorithm;Real time systems;Runtime;System testing;Character generation;Processor scheduling","software engineering;multiprogramming;processor scheduling","static-priority scheduling;multiframe tasks;multiframe model;hard-real-time tasks;feasibility analysis;rate-monotonic priority assignment scheme;feasibility test","","8","11","","","","","","IEEE","IEEE Conferences"
"Jumbo: run-time code generation for Java and its applications","S. Kamin; L. Clausen; A. Jarvis","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA","International Symposium on Code Generation and Optimization, 2003. CGO 2003.","","2003","","","48","56","Run-time code generation is a well-known technique for improving the efficiency of programs by exploiting dynamic information. Unfortunately, the difficulty of constructing run-time code-generators has hampered their widespread use. We describe Jumbo, a tool for easily creating run-time code generators for Java. Jumbo is a compiler for a two-level version of Java, where programs can contain quoted code fragments. The Jumbo API allows the code fragments to be combined at run-time and then executed. We illustrate Jumbo with several examples that show significant speedups over similar code written in plain Java, and argue further that Jumbo is a generalized software component system.","","0-7695-1913","10.1109/CGO.2003.1191532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191532","","Runtime;Java;Program processors;Automatic programming;Application software;Computer science;Software systems;Dynamic compiler;Assembly;Testing","program compilers;Java;application program interfaces","code generation;Jumbo;compiler;Java;run-time code generators","","6","11","","","","","","IEEE","IEEE Conferences"
"Classical sorting embedded in genetic algorithms for improved permutation search","V. Estivill-Castro; R. Torres-Velazquez","Dept. of Comput. Sci. & Software Eng., Newcastle Univ., Callaghan, NSW, Australia; NA","Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)","","2001","2","","941","948 vol. 2","A sorting algorithm defines a path in the search space of n! permutations based on the information provided by a comparison predicate. Our generic mutation operator for hybridization is a hill-climber and follows the path traced by any sorting algorithm. Our proposal adds exploitation capability to the mutation operator. Mutation requests swaps to construct and test new permutations, while the sorting algorithm supplies suggestions for swapping pairs as comparison to perform. The need to compare pairs of items in sorting is fulfilled by evaluating a guiding function. This novel HGA-sorting hybrid, instantiated with Insertion Sort, dramatically improves previous results for a benchmark of experiments of the Error-Correcting Graph Isomorphism.","","0-7803-6657","10.1109/CEC.2001.934291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934291","","Sorting;Genetic algorithms;Proposals;Genetic mutations;Computer science;Software engineering;Drives;Optimization methods;Software algorithms;Testing","genetic algorithms;sorting;search problems;combinatorial mathematics","classical sorting;genetic algorithms;improved permutation search;sorting algorithm;search space;permutations;comparison predicate;generic mutation operator;hybridization;hill-climber;exploitation capability;mutation operator;pair swapping;guiding function;HGA-sorting hybrid;Insertion Sort;Error-Correcting Graph Isomorphism","","1","17","","","","","","IEEE","IEEE Conferences"
"Efficient four-standard calibration procedure for six-port reflectometers","L. Qiao; S. P. Yeo","Dept. of Electr. Eng., Nat. Univ. of Singapore, Singapore; Dept. of Electr. Eng., Nat. Univ. of Singapore, Singapore","Conference Proceedings. 10th Anniversary. IMTC/94. Advanced Technologies in I & M. 1994 IEEE Instrumentation and Measurement Technolgy Conference (Cat. No.94CH3424-9)","","1994","","","994","997 vol.2","A more efficient implementation of an earlier four-standard calibration procedure for six-port reflectometers (utilizing simple iteration computations instead of other more cumbersome optimization techniques) is presented. Test results show that the calibration software that has been developed is capable of yielding reasonably good accuracies even in the presence of measurement noise.<<ETX>>","","0-7803-1880","10.1109/IMTC.1994.351938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=351938","","Calibration;Power measurement;Instruments;Hardware;Measurement standards;Software testing;Noise measurement;Software measurement;Acoustic reflection;Equations","microwave reflectometry;reflectometers;calibration;iterative methods;computerised instrumentation;measurement standards","four-standard calibration;six-port reflectometers;iteration;optimization;measurement noise","","1","9","","","","","","IEEE","IEEE Conferences"
"A GRASP algorithm for the multi-objective knapsack problem","D. S. Vianna; J. E. C. Arroyo","Nucleo de Pesquisa e Desenvolvimento em Informatica, Univ. Candido Mendes, Brazil; Nucleo de Pesquisa e Desenvolvimento em Informatica, Univ. Candido Mendes, Brazil","XXIV International Conference of the Chilean Computer Science Society","","2004","","","69","75","In this article, we propose a greedy randomized adaptive search procedure (GRASP) to generate a good approximation of the efficient or Pareto optimal set of a multi-objective combinatorial optimization problem. The algorithm is based on the optimization of all weighted linear utility functions. In each iteration, a preference vector is defined and a solution is built considering the preferences of each objective. The found solution is submitted to a local search trying to improve the value of the utility function. In order to find a variety of efficient solutions, we use different preference vectors, which are distributed uniformly on the Pareto frontier. The proposed algorithm is applied for the 0/1 knapsack problem with r = 2, 3, 4 objectives and n = 250, 500, 750 items. The quality of the approximated solutions is evaluated comparing with the solutions given by three genetic algorithms from the literature.","","0-7695-2185","10.1109/QEST.2004.2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1372106","","Genetic algorithms;Simulated annealing;Pareto optimization;Vectors;Hardware;Software systems;Costs;Large-scale systems;Bibliographies;Testing","knapsack problems;combinatorial mathematics;Pareto optimisation;search problems;greedy algorithms;randomised algorithms","GRASP algorithm;multiobjective knapsack problem;greedy randomized adaptive search procedure;Pareto optimal set;multiobjective combinatorial optimization;weighted linear utility functions;preference vector;Pareto frontier;0/1 knapsack problem;genetic algorithm","","15","18","","","","","","IEEE","IEEE Conferences"
"Measuring the Performance and Behavior of Icon Programs","C. A. Coutant; R. E. Griswold; D. R. Hanson","Information Systems Laboratory; NA; NA","IEEE Transactions on Software Engineering","","1983","SE-9","1","93","103","The importance of the ability to measure the performance of programs written in high-level languages is well known. Performance measurement enables users to locate and correct program inefficiencies where automatic optimizations fail and provides a tool for understanding program behavior. This paper describes performance measurement facilities for the Icon programming language, and shows not only how these facilities provided insight into program behavior, but also how they were used to improve the implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1983.236299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703016","Icon;program measurement;storage management","High level languages;Computer languages;Programming profession;Measurement;Program processors;Optimizing compilers;Computer science;Instruments;Testing;Performance evaluation","","Icon;program measurement;storage management","","8","22","","","","","","IEEE","IEEE Journals & Magazines"
"On the solution of the bilevel programming formulation of the terrorist threat problem","J. M. Arroyo; F. D. Galiana","Dept. de Ingenieria Electr.a, Univ. de CastillaLa Mancha, Ciudad Real, Spain; NA","IEEE Transactions on Power Systems","","2005","20","2","789","797","This paper generalizes the ""terrorist threat problem"" first defined by Salmero/spl acute/n, Wood, and Baldick by formulating it as a bilevel programming problem. Specifically, the bilevel model allows one to define different objective functions for the terrorist and the system operator as well as permitting the imposition of constraints on the outer optimization that are functions of both the inner and outer variables. This degree of flexibility is not possible through existing max-min models. The bilevel formulation is investigated through a problem in which the goal of the destructive agent is to minimize the number of power system components that must be destroyed in order to cause a loss of load greater than or equal to a specified level. This goal is tempered by the logical assumption that, following a deliberate outage, the system operator will implement all feasible corrective actions to minimize the level of system load shed. The resulting nonlinear mixed-integer bilevel programming formulation is transformed into an equivalent single-level mixed-integer linear program by replacing the inner optimization by its Karush-Kuhn-Tucker optimality conditions and converting a number of nonlinearities to linear equivalents using some well-known integer algebra results. The equivalent formulation has been tested on two case studies, including the 24-bus IEEE Reliability Test System, through the use of commercially available software.","0885-8950;1558-0679","","10.1109/TPWRS.2005.846198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1425574","Bilevel programming;deliberate outages;load shedding;mixed-integer linear programming (MILP);power system security and vulnerability;terrorist threat","Lagrangian functions;Upper bound;Power system modeling;Integer linear programming;Software testing;System testing;Linear programming;Power transmission lines;Load flow;Power generation","minimisation;power system reliability;power system security;load shedding;integer programming;terrorism","terrorist threat problem;optimization;max-min models;power system components;mixed-integer bilevel programming formulation;linear equivalents;integer algebra;IEEE reliability test system;load shedding;power system security","","99","23","","","","","","IEEE","IEEE Journals & Magazines"
"The life cycle of a fuzzy knowledge-based classifier","P. P. Bonissone","Gen. Electr. Global Res. Center, Schenectady, NY, USA","22nd International Conference of the North American Fuzzy Information Processing Society, NAFIPS 2003","","2003","","","488","494","We describe the life cycle of a fuzzy knowledge-based classifier with special emphasis on one of its most neglected steps: the maintenance of its knowledge base. First, we analyze the process of underwriting Insurance applications, which is the classification problem used to illustrate the life cycle of a classifier. After discussing some design tradeoffs that must be addressed for the on-line and off-line use of a classifier, we describe the design and implementation of a fuzzy rule-based (FRB) and a fuzzy case-based (FCB) classifier. We establish a standard reference dataset (SRD), consisting of 3,000 insurance applications with their corresponding decisions. The SRD exemplifies the results achieved by an ideal, optimal classifier, and represents the target for our design. We apply evolutionary algorithms to perform an off-line optimization of the design parameters of each classifier, modifying their behavior to approximate this target. The SRD is also used as a reference for testing and performing a five-fold cross-validation of the classifiers. Finally, we focus on the monitoring and maintenance of the FRB classifier. We describe a fusion architecture that supports an off-line quality assurance process of the on-line FRB classifier. The fusion module takes the outputs of multiple classifiers, determines their degree of consensus, and compares their overall agreement with that of the FRB classifier. From this analysis, we can identify the most suitable cases to update the SRD, to audit, or to be reviewed by senior underwriters.","","0-7803-7918","10.1109/NAFIPS.2003.1226834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226834","","Insurance;Production;Monitoring;Costs;Software maintenance;Design optimization;Algorithm design and analysis;Testing;Performance evaluation;Quality assurance","evolutionary computation;fuzzy logic;optimisation;knowledge based systems;insurance;life cycle costing","fuzzy knowledge based classifier;underwriting insurance application;fuzzy rule based classifier;fuzzy case based classifier;standard reference dataset;optimal classifier;evolutionary algorithms;offline optimization;design parameters;five fold cross validation;fusion architecture;offline quality assurance;online classifier;multiple classifiers","","5","16","","","","","","IEEE","IEEE Conferences"
"An improved AHP method in performance assessment","Yidan Bao; Yanping Wu; Yong He; Xiaofeng Ge","Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China; Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China; Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China; Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","","2004","1","","177","180 Vol.1","In order to reduce subjective errors in the traditional analytic hierarchy process (AHP), an improved AHP method integrated with orthogonal experimental design was presented in this paper. The new method combined both AHP and orthogonal design principles to make decisions, objectives in assessment system and values computed through mapping and satisfying function respectively as the corresponding factors, levels and results in orthogonal experimental design. With software SAS analysis, priorities on performance were to be ranked out accordingly. Application has showed that the method can improve the accuracy of AHP in performance assessment.","","0-7803-8273","10.1109/WCICA.2004.1340551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1340551","","Design for experiments;Decision making;Performance analysis;Analysis of variance;Testing;Helium;Educational institutions;Design engineering;Software performance;Synthetic aperture sonar","operations research;decision making;design of experiments;decision theory","AHP method;analytic hierarchy process;performance assessment system;error reduction;orthogonal experimental design;orthogonal design principles;decision making;SAS software analysis","","2","6","","","","","","IEEE","IEEE Conferences"
"Formal approaches to intelligent swarms","C. Rouff; W. Truszkowski; J. Rash; M. Hinchey","SAIC, McLean, VA, USA; NA; NA; NA","28th Annual NASA Goddard Software Engineering Workshop, 2003. Proceedings.","","2003","","","51","57","Autonomous intelligent swarms of satellites are being proposed for future space missions. These types of missions provide greater flexibility and the chance to perform more and different kinds of science than traditional single satellite/vehicle missions, but also have complex interactions and behaviors. The emergent properties of swarms make these missions powerful, but at the same time are more difficult to design and assure that the proper behaviors will emerge due to their complexity. We are currently investigating formal methods and techniques for verification and validation of swarm-based missions. The advantage of using formal methods is their ability to mathematically assure the behavior of a swarm, emergent or otherwise. The autonomous nanotechnology swarm (ANTS) mission is being used as an example and case study for swarm-based missions for which to experiment and test current formal methods with intelligent swarms.","","0-7695-2064","10.1109/SEW.2003.1270725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270725","","Particle swarm optimization;NASA;Satellites;Space technology;Space missions;Remotely operated vehicles;Testing;Earth;Intelligent agent;Nanotechnology","formal verification;intelligent control;aerospace expert systems;artificial satellites","formal methods;autonomous intelligent swarms;satellites;space missions;formal verification;swarm-based missions;autonomous nanotechnology swarm mission","","7","22","","","","","","IEEE","IEEE Conferences"
"Methods for observing global properties in distributed systems","V. K. Garg","Texas Univ., TX, USA","IEEE Concurrency","","1997","5","4","69","77","The author surveys algorithms that focus on special classes of predicates to observe global properties in distributed systems. This abstraction is key to efficient debugging and fault tolerance. The algorithms are also flexible, allowing for optimizations to fit application needs. For example, algorithms to detect general predicates can be optimized by exploiting specific properties of the channel predicate. In checking if a channel is empty, it may be sufficient to deal with the number of messages only, rather than the message themselves.","1092-3063;1558-0849","","10.1109/4434.641629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=641629","","Fault tolerance;Lattices;Distributed computing;System recovery;Functional programming;Software debugging;Software testing;Monitoring;Clocks;Registers","program debugging;distributed algorithms;software fault tolerance;message passing","distributed systems;global properties;algorithms;predicates;debugging;fault tolerance;optimization;general predicate detection;channel predicate;channel emptiness checking;messages","","6","19","","","","","","IEEE","IEEE Journals & Magazines"
"Type infeasible call chains","A. L. Souter; L. L. Pollock","Dept. of Comput. & Inf. Sci., Delaware Univ., Newark, DE, USA; NA","Proceedings First IEEE International Workshop on Source Code Analysis and Manipulation","","2001","","","194","203","While some software engineering applications perform static analysis over the whole program call graph, others are more interested in specific call chains within a program's call graph. It is thus important to identify when a particular static call chain for an object-oriented program may not be executable, or feasible, such that there is no input for which the chain will be taken. The paper examines type infeasibility of call chains, which is the infeasibility caused by inherently polymorphic call sites and sometimes also due to imprecision in call graphs. The problem of determining whether a call chain is type infeasible is defined and exemplified, a key property characterizing type in infeasible call chains is described, empirical results from examining the call graphs for a set of Java programs are described, and two approaches to automatically deciding the type infeasibility of a call chain due to object parameters are presented.","","0-7695-1387","10.1109/SCAM.2001.972681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972681","","Information analysis;Application software;Runtime;Performance analysis;Java;Software engineering;Computer applications;Debugging;Software testing;Optimizing compilers","program diagnostics;graph theory;object-oriented programming;type theory;Java","type infeasible call chains;software engineering applications;static analysis;program call graph;static call chain;object-oriented program;inherently polymorphic call sites;call graph imprecision;Java programs;object parameters","","1","24","","","","","","IEEE","IEEE Conferences"
"An elliptical aperture dual shaped reflector antenna for SNG applications","S. A. Skyttemyr","Telenor R&D, Kjeller, Norway","IEEE Antennas and Propagation Society International Symposium. Transmitting Waves of Progress to the Next Millennium. 2000 Digest. Held in conjunction with: USNC/URSI National Radio Science Meeting (C","","2000","2","","558","561 vol.2","A method for synthesis of shaped dual reflector antennas with circular apertures was earlier developed at Telnor R&D. Software for synthesis and analysis of such antennas was developed in connection with this. Later on the concept was extended to also include antennas with elliptical apertures. A new elliptical beam Ku-band antenna based on this concept, was designed, analysed and measured. The antenna was optimised to suit the specifications as a dedicated SNO (Satellite News Gathering) antenna. It has an aperture of 1.5/spl times/1.2 m making it possible to have FCC compliance.","","0-7803-6369","10.1109/APS.2000.875207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=875207","","Aperture antennas;Reflector antennas;FCC;Satellite antennas;Physical optics;Application software;Research and development;Azimuth;Geometrical optics;Shape","microwave antennas;offset reflector antennas;aperture antennas;satellite communication;electronic news gathering;antenna radiation patterns;antenna testing","elliptical aperture dual shaped reflector antenna;SNG applications;shaped dual reflector antennas;circular apertures;Telnor R&D;software;antenna synthesis;antenna analysis;Ku-band antenna;satellite news gathering;FCC compliance;SHF;14.25 GHz","","1","3","","","","","","IEEE","IEEE Conferences"
"High-Tc SQUID systems for practical use","P. Seidel; F. Schmidl; S. Wunderlich; L. Dorrer; T. Vogt; H. Schneidewind; R. Weidl; S. Losche; U. Leder; O. Solbig; H. Nowak","Inst. fur Festkorperphys., Friedrich-Schiller-Univ., Jena, Germany; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Applied Superconductivity","","1999","9","2","4077","4080","Planar thin film DC-SQUID gradiometers with small base length (4...6 mm) were optimized for application in different measurement systems. The field gradient noise of these planar DC-SQUID gradiometers in unshielded environment is better than 5 pT/cm/spl radic/(Hz) (at 1 Hz, 77 K). Other components of these systems like electronics, cryostats, scanning equipment, and data analysis software are discussed. An industrial system for nondestructive evaluation and a clinical system for bedside investigations on cardiac infarction are demonstrated as examples of starting cooperation between university and small companies interested in application of superconductivity.","1051-8223;1558-2515;2378-7074","","10.1109/77.783922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=783922","","SQUIDs;Magnetic field measurement;Biomagnetics;Magnetic sensors;Cardiology;Hospitals;Transistors;Application software;Cooling;Geometry","SQUID magnetometers;high-temperature superconductors;magnetocardiography;biomedical equipment;inspection;superconducting device noise;eddy current testing","HTSC SQUID systems;planar thin film DC-SQUID gradiometers;field gradient noise;unshielded environment;cryostats;scanning equipment;data analysis software;industrial system;nondestructive evaluation;clinical system;cardiac infarction investigation;bedside investigation;inspection;quality control;computer controlled table;MCG;4 to 6 mm;77 K","","16","9","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing decision aids for technology transfers","T. H. Murray","NA","International Conference on Information-Decision-Action Systems in Complex Organisations, 1992.","","1992","","","100","104","Proponents of various decision aid systems loudly proclaim the merits of 'their' system over all competition, especially 'brand X,' and justify their own process because it does a 'better job of decision making' than that done by competing products. In order to compare such decision aids independently, a systematic process was conceived, designed, constructed, tested and implemented. The purpose was to use automation to standardize a decision process among diverse adversaries in a dynamic, complex, technological environment. The process first step structured the factors needed to arrive at an equitable decision on technology transfer from domestic to foreign ownership, control, or influence. The factors were identified by participants in the decentralized decision process. Using group problem solving techniques, the most important factors were identified and ranked. Factor weights were determined with automated aids. Four decision processes were evaluated to determine their accuracy in arriving at a decision dealing with proposed technology transfers. The author deals with the process of identifying the factors and testing the decision aids.<<ETX>>","","0-85296-541","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=140394","","Decision support systems;Government data processing;Software testing;Technology transfer","decision support systems;government data processing;program testing;software selection;technology transfer","technology transfers;decision aid systems;decision making;competing products;systematic process;decision process;diverse adversaries;technological environment;equitable decision;foreign ownership;decentralized decision process;group problem solving techniques;automated aids","","1","","","","","","","IET","IET Conferences"
"Properties of binary statistical morphology","C. S. Regazzoni; G. L. Foresti","Dept. of Biophys. & Electron. Eng., Genoa Univ., Italy; NA","Proceedings of 13th International Conference on Pattern Recognition","","1996","2","","631","635 vol.2","The properties and applications of a class of statistical morphological operators, i.e. binary statistical morphology (BSM) operators, for binary image processing are described. The proposed operators are based on quantization of the output of a statistical morphological operator, modeled as a binary probabilistic hypothesis-testing step. The operator obtained is shown to be equivalent to a rank-order filter. Relationships are established between the quantization threshold, rank of the equivalent rank-order filter and parameters of the model. It is also shown that basic BSM operators, i.e. binary statistical dilation and binary statistical erosion can be used as the basis for defining more complex filters. In this paper, attention is paid to describe specific properties of BSM operators which are useful for different applications, e.g. shape description.","1051-4651","0-8186-7282","10.1109/ICPR.1996.546900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=546900","","Morphology;Filters;Samarium;Probability;Quantization;Shape;Mathematics;Computer science;Application software;Image processing","computer vision","binary statistical morphology;binary image processing;quantization;binary probabilistic hypothesis-testing;rank-order filter;binary statistical dilation;binary statistical erosion;shape description","","3","12","","","","","","IEEE","IEEE Conferences"
"Energy optimization of subthreshold-voltage sensor network processors","L. Nazhandali; B. Zhai; A. Olson; A. Reeves; M. Minuth; R. Helfand; Sanjay Pant; T. Austin; D. Blaauw","Michigan Univ., USA; Michigan Univ., USA; Michigan Univ., USA; Michigan Univ., USA; Michigan Univ., USA; Michigan Univ., USA; Michigan Univ., USA; Michigan Univ., USA; Michigan Univ., USA","32nd International Symposium on Computer Architecture (ISCA'05)","","2005","","","197","207","Sensor network processors and their applications are a growing area of focus in computer system research and design. Inherent to this design space is a reduced processing performance requirement and extremely high energy constraints, such that sensor network processors must execute low-performance tasks for long durations on small energy supplies. In this paper, we demonstrate that subthreshold-voltage circuit design (400 mV and below) lends itself well to the performance and energy demands of sensor network processors. Moreover, we show that the landscape for microarchitectural energy optimization dramatically changes in the subthreshold domain. The dominance of leakage power in the subthreshold regime demands architectures that i) reduce overall area; ii) increase the utility of transistors; while iii) maintaining acceptable CPI efficiency. We confirm these observations by performing SPICE-level analysis of 21 sensor network processors and memory architectures. Our best sensor platform, implemented in 130nm CMOS and operating at 235 mV, only consumes 1.38 pJ/instruction, nearly an order of magnitude less energy than previously published sensor network processor results. This design, accompanied by bulk-silicon solar cells for energy scavenging, has been manufactured by IBM and is currently being tested.","1063-6897","0-7695-2270","10.1109/ISCA.2005.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431557","","Sensor systems and applications;Application software;Computer networks;Circuit synthesis;Microarchitecture;Performance analysis;Memory architecture;CMOS process;Photovoltaic cells;Manufacturing","program processors;optimisation;SPICE;memory architecture;solar cells;CMOS integrated circuits;sensor fusion;microprocessor chips","microarchitectural energy optimization;sensor network processor;subthreshold-voltage circuit design;leakage power;CPI efficiency;SPICE-level analysis;memory architecture;CMOS;bulk-silicon solar cell;130 nm;235 mV","","36","17","","","","","","IEEE","IEEE Conferences"
"Making benchmarks uncheatable","Jin-Yi Cai; A. Nerurkar; Min-You Wu","Dept. of Comput. Sci., State Univ. of New York, Buffalo, NY, USA; NA; NA","Proceedings. IEEE International Computer Performance and Dependability Symposium. IPDS'98 (Cat. No.98TB100248)","","1998","","","216","226","The industry is full of anecdotes of benchmark ""cheating"" of various degrees ranging from innocent mistakes to flagrant misuse. Based on the tremendous progress made in recent years in theoretical computer science, it becomes possible to make a wide variety of existing benchmarks more accurate, resistant to tampering and more trustworthy. We propose two methods, the Secret Key method and the result verification method, to take computational advantage in judging performance claims. A realtime measurement method is also proposed to prevent cheating on performance reporting. The design and implementation of uncheatable benchmarks are presented in this paper.","1087-2191","0-8186-8679","10.1109/IPDS.1998.707724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=707724","","Benchmark testing;Optimizing compilers;Computer science;High performance computing;Kernel;Assembly;Computer industry;Performance evaluation;Software performance;Software systems","performance evaluation","benchmarks;tampering;trustworthy;secret key method;result verification method;performance claims","","1","21","","","","","","IEEE","IEEE Conferences"
"Data windows: a data-centric approach for query execution in memory-resident databases","J. Pisharath; A. Choudhary; M. Kandemir","Dept. of Electr. & Comput. Eng., Northwestern Univ., Evanston, IL, USA; Dept. of Electr. & Comput. Eng., Northwestern Univ., Evanston, IL, USA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","2","","1352","1353 Vol.2","Structured embedded databases are currently becoming an integrated part of embedded systems, thus, enabling higher standards in system automation. These embedded databases are typically memory resident. In this paper, we present a data-centric approach called data windowing that optimizes multiple queries issued to an embedded database. Traditional approaches improve the performance by optimizing the control flow of operations, whereas we target performance improvements based on the data that is brought into the system.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269083","","Databases;Embedded system;Data engineering;Embedded computing;Computer science;Automation;Automatic control;Control systems;Embedded software;System software","embedded systems;query processing;database management systems;storage management","data windows;data centric approach;query execution;memory resident databases;structured embedded databases;embedded systems;system automation","","1","9","","","","","","IEEE","IEEE Conferences"
"Reliability and failure analysis of voting circuits in hardware redundant design","M. Radu; D. Pitica; C. Posteuca","Fac. of Electron. & Telecommun., Tech. Univ. Cluj-Napoca, Romania; NA; NA","International Symposium on Electronic Materials and Packaging (EMAP2000) (Cat. No.00EX458)","","2000","","","421","423","This paper presents some aspects of fault-tolerant design using hardware redundancy. The voter is the key element in N-modular redundant design. Hardware voters are bit voters that compute a majority of n input bits. An m-out-of-n hardware bit voter is a circuit with n bit inputs, and 1 bit output y, such that y=1 if at least m-out-of-n input bits have the value 1. A hardware voter can be implemented with logic gates in CMOS VLSI technology. Designers are looking for optimal designs with respect to the following criteria: circuit complexity, number of logic levels, fan-in and fan-out, power dissipation, testability, or any combinations of the previous requirements in order to obtain high reliability for the voting circuits. A detailed reliability analysis, failure modes and effects analysis of voters at the transistor level, is performed using CARE tools. CARE (computer aided reliability engineering) is a powerful software tool that can be used concurrently in the phases of R&D for complex reliability analysis of electronic circuits. The main goal of these analyses is to identify the best designs of voting circuits, in terms of reliability parameters and to identify the possible technological failures that can affect them.","","0-7803-6654","10.1109/EMAP.2000.904192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904192","","Failure analysis;Voting;Hardware;CMOS logic circuits;CMOS technology;Logic testing;Circuit testing;Performance analysis;Fault tolerance;Redundancy","failure analysis;majority logic;logic design;redundancy;circuit complexity;VLSI;CMOS logic circuits;circuit optimisation;integrated circuit design;design for testability;computer aided engineering;circuit CAD;integrated circuit reliability","reliability;failure analysis;voting circuits;hardware redundant design;fault-tolerant design;hardware redundancy;N-modular redundant design;hardware voters;bit voters;input bit majority;m-out-of-n hardware bit voter;hardware voter;logic gates;CMOS VLSI technology;optimal design;circuit complexity;logic levels;fan-in;fan-out;power dissipation;testability;reliability analysis;failure modes/effects analysis;transistor level FMEA;CARE tools;computer aided reliability engineering;software tool;electronic circuits;voting circuit design;reliability parameters;technological failures","","23","7","","","","","","IEEE","IEEE Conferences"
"Design tools for GNSS","V. Ashkenazi; W. Chen; W. Y. Ochieng; C. J. Hill; T. Moore","Inst. of Eng. Surveying & Space Geodesy, Nottingham Univ., UK; Inst. of Eng. Surveying & Space Geodesy, Nottingham Univ., UK; Inst. of Eng. Surveying & Space Geodesy, Nottingham Univ., UK; Inst. of Eng. Surveying & Space Geodesy, Nottingham Univ., UK; Inst. of Eng. Surveying & Space Geodesy, Nottingham Univ., UK","IEE Colloquium on Implementation of GNSS","","1995","","","2/1","2/8","The process of GNSS constellation design can be divided into two parts. Firstly, it is possible to design a new configuration of satellites, given specific criteria which it must satisfy. Secondly, it is possible to analyse a given constellation, either resulting from the design process, or based on current satellites, to assess the degree to which it meets the specified requirements. A thorough description of the basic theoretical principles of the design of optimal satellite orbits, in terms of satellite availability, coverage, accuracy and integrity, as implemented in the IESSG design software, is given in this paper. As a demonstration of the analysis principles, a number of GNSS constellations have been tested. GNSSI is represented by the two scenarios of GPS plus augmentations.","","","10.1049/ic:19951306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494805","","Global Positioning System;Optimization methods","Global Positioning System;optimisation","GNSS constellation design;configuration of satellites;optimal satellite orbits;satellite availability;coverage;accuracy;integrity monitoring;IESSG design software;navigation;GNSSI;GPS","","1","","","","","","","IET","IET Conferences"
"Parallelising calculations in electric power distribution networks","Y. Neumann; B. Dwolatzky","Sch. of Electr. & Inf. Eng., Univ. of the Witwatersrand, Johannesburg, South Africa; Sch. of Electr. & Inf. Eng., Univ. of the Witwatersrand, Johannesburg, South Africa","IEEE AFRICON. 6th Africon Conference in Africa,","","2002","2","","957","960 vol.2","CART (Computer Aided Reticulation of Townships) is a software package used for the design of radial, electrical power distribution networks. Having many transformers and large numbers of consumers in the distribution network significantly increases computation time. A distributed, service-based platform is being developed, with parallelisation functionality for CART as the flagship service. The system will be used to parallelise CART's voltage and current calculations for use in the design of much larger networks, possibly of metropolitan scale. Eventually, more complex scenarios will be addressed, including real-time calculation and simulation of lightning and transient faults and performance testing and optimisation of existing power distribution networks.","","0-7803-7570","10.1109/AFRCON.2002.1160042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1160042","","Intelligent networks;Transformers;Computer networks;Distributed computing;Substations;Conductors;Network servers;Power engineering computing;Power systems;Packaging","power distribution planning;power system CAD;power system analysis computing;parallel processing;software packages","electric power distribution networks;CART software package;Computer Aided Reticulation of Townships;calculations parallelisation;South Africa;transformers;consumers;computation time;voltage calculations;current calculations;lightning faults;transient faults;performance testing;computer simulation","","","8","","","","","","IEEE","IEEE Conferences"
"READS: a prototyping environment for real-time active applications","Kam-Yiu Lam; T. S. H. Lee; S. H. Son","Dept. of Comput. Sci., City Univ. of Hong Kong, Kowloon, Hong Kong; NA; NA","Database and Expert Systems Applications. 8th International Conference, DEXA '97. Proceedings","","1997","","","265","270","We present our Real-time Active Database System, READS, which is a prototyping environment for real-time active database applications on a conventional Unix environment, e.g., Solaris 2.4. In READS, transactions are associated with deadlines and priorities. Priority scheduling is supported by the real-time extensions provided in the underlying operating system. READS can be served as a testbed for evaluating different issues in the design of real-time active database systems (RTADBS). Different approaches for assigning deadlines and priorities to triggered transactions have been suggested and discussed. An application, the programmed trading database system, is implemented with READS and experiments have been performed to study the impact of the deadline constraints on the performance of the deferred and immediate coupling modes for triggering.","","0-8186-8147","10.1109/DEXA.1997.617285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617285","","Prototypes;Database systems;Real time systems;Application software;Transaction databases;Timing;Computer science;Operating systems;System testing;Algorithm design and analysis","real-time systems;active databases;deductive databases;software prototyping;Unix;transaction processing;scheduling;software performance evaluation;concurrency control","READS;prototyping environment;real-time active database;Unix environment;Solaris 2.4;transactions;deadlines;priority scheduling;operating system;triggered transactions;trading database system;experiments;deadline constraints;performance;concurrency control","","","13","","","","","","IEEE","IEEE Conferences"
"Exploiting processor workload heterogeneity for reducing energy consumption in chip multiprocessors","I. Kadayif; M. Kandemir; I. Kolcu","Dept. of Comput. Eng., Canakkale Onsekiz Mart Univ., Turkey; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","2","","1158","1163 Vol.2","Advances in semiconductor technology are enabling designs with several hundred million transistors. Since building sophisticated single processor based systems is a complex process from design, verification, and software development perspectives, the use of chip multiprocessing is inevitable in future microprocessors. In fact, the abundance of explicit loop-level parallelism in many embedded applications helps us identify chip multiprocessing as one of the most promising directions in designing systems for embedded applications. Another architectural trend that we observe in embedded systems, namely, multi-voltage processors, is driven by the need of reducing energy consumption during program execution. Practical implementations such as Transmeta's Crusoe and Intel's XScale tune processor voltage/frequency depending on current execution load. Considering these two trends, chip multiprocessing and voltage/frequency scaling, this paper presents an optimization strategy for an architecture that makes use of both chip parallelism and voltage scaling. In our proposal, the compiler takes advantage of heterogeneity in parallel execution between the loads of different processors and assigns different voltages/frequencies to different processors if doing so reduces energy consumption without increasing overall execution cycles significantly. Our experiments with a set of applications show that this optimization can bring large energy benefits without much performance loss.","1530-1591","0-7695-2085","10.1109/DATE.2004.1269048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269048","","Energy consumption;Voltage;Frequency;Application software;Transistors;Buildings;Software design;Process design;Programming;Microprocessors","multiprocessing systems;parallel processing;microprocessor chips;parallelising compilers;power consumption;embedded systems","processor workload heterogeneity;energy consumption reduction;chip multiprocessors;semiconductor technology;single processor based systems;loop level parallelism;embedded systems;multivoltage processors;Transmetas Crusoe;Intels XScale tune processor;voltage/frequency scaling;optimization strategy;compiler support;chip parallelism;parallel execution","","21","21","","","","","","IEEE","IEEE Conferences"
"A background-thinning based algorithm for separating connected handwritten digit strings","Zhongkang Lu; Zheru Chi; Pengfei Shi","Dept. of Electron. Eng., Hong Kong Polytech., Kowloon, Hong Kong; NA; NA","Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)","","1998","2","","1065","1068 vol.2","Most algorithms for segmenting connected handwritten digit strings are based on the analysis of the foreground pixel distributions and the features on the upper/lower contours of the image. A new approach is presented to segment connected handwritten two-digit strings based on the thinning of background regions. The algorithm first locates several feature points on the background skeleton of the digit image. Possible segmentation paths are then constructed by matching these feature points. With geometric property measures, these segmentation paths are ranked using fuzzy rules generated from a decision-tree approach. Finally, the top ranked segmentation paths are tested one by one by an optimized nearest neighbor classifier until one of these candidates is accepted based on an acceptance criterion. Experimental results on NIST special database 3 show that our approach can achieve a correct classification rate of 92.4% with only 4.7% of digit strings rejected, which compares favorably with the other techniques tested.","1520-6149","0-7803-4428","10.1109/ICASSP.1998.675452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675452","","Image segmentation;Skeleton;Flowcharts;Testing;Image analysis;Nearest neighbor searches;Character recognition;Optical character recognition software;Feature extraction;Table lookup","handwriting recognition;image classification;image segmentation;image matching;feature extraction;fuzzy systems","background-thinning based algorithm;connected handwritten digit strings separation;foreground pixel distributions;image upper/lower contours;background skeleton;digit image;segmentation paths;feature points matching;geometric property measures;fuzzy rules;decision-tree approach;optimized nearest neighbor classifier;NIST special database 3;acceptance criterion;experimental results;correct classification rate;digit segmentation","","","5","","","","","","IEEE","IEEE Conferences"
"Modeling And Optimization Of A Regenerative Fuel Call System Using The ASPEN Process Simulator","T. M. Maloney; H. F. Leibecki","Sverdrup Technology, Inc.; NA","Proceedings of the 25th Intersociety Energy Conversion Engineering Conference","","1990","6","","114","118","The Hydrogen-Oxygen Regenerative Fuel Cell System has been identified as a key component for energy storage In support of future lunar missions. Since the H/sub 2/-O/sub 2/ regenerative electrochemical conversion technology has not yet been tested In space applications, It Is necessary to implement predictive techniques to develop initial feasible system designs. The ASPEN simulation software furnishes a constructive medium for analyzing and for optimizing such systems. A rudimentary regenerative fuel cell system design has been examined using the ASPEN simulator and this modular approach allows for easy addition of supplementary ancillary components and easy Integration with life support systems. The modules Included in the preliminary analyses may serve as the fundamental structure for more complicated energy storage systems.","","0-8169-0490","10.1109/IECEC.1990.748040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=748040","","Fuel cells;Moon;Energy storage;Space technology;Hydrogen;NASA;Application software;Analytical models;Photovoltaic systems;Solar power generation","","","","1","9","","","","","","IEEE","IEEE Conferences"
"Yemanja-a layered event correlation engine for multi-domain server farms","K. Appleby; G. Goldszmidt; M. Steinder","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","2001 IEEE/IFIP International Symposium on Integrated Network Management Proceedings. Integrated Network Management VII. Integrated Management Strategies for the New Millennium (Cat. No.01EX470)","","2001","","","329","344","Yemanja is a model-based event correlation engine for multi-layer fault diagnosis. It targets complex propagating fault scenarios, and can smoothly correlate low-level network events with high-level application performance alerts related to quality of service violations. Entity-models that represent devices or abstract components encapsulate entity behavior. Distantly associated entities are not explicitly aware of each other, and communicate through event propagation chains. Yemanja's state-based engine supports generic scenario definitions, prioritization of alternate solutions, integrated problem-state and device testing, and simultaneous analysis of overlapping problems. The system of correlation rules was developed based on device, layer, and dependency analysis, and reveals the layered structure of computer networks. The primary objectives of this research include the development of reusable, configuration independent, correlation scenarios; adaptability and the extensibility of the engine to match the constantly changing topology of a multi-domain server farm; and the development of a concise specification language that is relatively simple yet powerful.","","0-7803-6719","10.1109/INM.2001.918051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918051","","Engines;Fault diagnosis;Testing;Network servers;Resource management;Milling machines;Rivers;Application software;Quality of service;Independent component analysis","quality of service;computer network management;network servers;correlation methods;computer network reliability;specification languages","Yemanja;layered event correlation engine;multi-domain server farms;multi-layer fault diagnosis;complex propagating fault scenarios;low-level network events;high-level application performance alerts;quality of service violations;entity-models;abstract components;entity behavior;event propagation chains;state-based engine;generic scenario definitions;prioritization;problem-state testing;device testing;overlapping problem;correlation rules;dependency analysis;layered structure;computer networks;adaptability;extensibility;topology;specification language","","16","","","","","","","IEEE","IEEE Conferences"
"An efficient compiled simulation system for VLIW code verification","Jae-Woo Ahn; Soo-Mook Moon; Wonyong Sung","Dept. of Electr. Eng., Seoul Nat. Univ., South Korea; NA; NA","Proceedings 31st Annual Simulation Symposium","","1998","","","91","95","We present an efficient compiled simulation system for the verification of a VLIW instruction set architecture and its assembly code. Our existing compiled simulation system is made to be faster by adopting incremental recompilation and C-assembly cosimulation techniques to improve the conventional compiled simulation. As a part of SPARC-based VLIW testbed, the efficiency and validity of our compiled simulation system are verified with three SPEC '89 integer benchmarks and several UNIX utilities.","1080-241X","0-8186-8418","10.1109/SIMSYM.1998.668452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668452","","VLIW;Instruction sets;Moon;Assembly systems;System testing;Benchmark testing;Process design;Hardware;Software performance;Optimizing compilers","instruction sets;parallel architectures;digital simulation;performance evaluation","compiled simulation system;VLIW code verification;instruction set architecture;assembly code;incremental recompilation;C-assembly cosimulation;SPEC '89 integer benchmarks;UNIX utilities","","2","6","","","","","","IEEE","IEEE Conferences"
"Repair level analysis model enhancement","R. A. Douglas","Modern Technol. Corp., Dayton, OH, USA","IEEE Conference on Aerospace and Electronics","","1990","","","1220","1224 vol.3","An ongoing three-phased government-sponsored research and development effort directed at developing a more flexible, more adaptive repair level analysis (RLA) model is described. Phase 1 is the research phase to compile, rank, and assess available methodologies, and to study how the current software may be upgraded, particularly with respect to improving the user interface on the current model, network repair level analysis (NRLA). The Phase 1 effort is described. The topics dealt with include: terminology review, repair/discard decision sequence, repair-level analysis (RLA), complicated RLA problems, current RLA model capabilities, Air Force task order languages, and phase 1 results.<<ETX>>","","","10.1109/NAECON.1990.112944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=112944","","Weapons;Terminology;Test equipment;Electronic equipment testing;Aerospace electronics;Research and development;User interfaces;Software prototyping;Prototypes;Government","aerospace computing;maintenance engineering;military computing;military systems;research and development management;user interfaces","military systems;aerospace;weapons;government-sponsored research and development;repair level analysis;software;user interface;network repair level analysis;repair/discard decision sequence;Air Force task order languages","","","","","","","","","IEEE","IEEE Conferences"
"Run-time energy estimation in system-on-a-chip designs","J. Haid; G. Kaefer; C. Steger; R. Weiss","Inst. for Tech. Informatics, Graz Univ. of Technol., Austria; Inst. for Tech. Informatics, Graz Univ. of Technol., Austria; Inst. for Tech. Informatics, Graz Univ. of Technol., Austria; Inst. for Tech. Informatics, Graz Univ. of Technol., Austria","Proceedings of the ASP-DAC Asia and South Pacific Design Automation Conference, 2003.","","2003","","","595","599","In this paper, a co-processor for run-time energy estimation in system-on-a-chip (SOC) designs is proposed. The estimation process is done by using power macro-models, thus making analogue measurement equipment obsolete to the software engineer once the SOC design is characterized. Compared to sampling-based profiling systems, the performance overhead of energy profiling is less, because the energy estimation is done completely parallel to the functional units residing on the SOC. The proposed methodology can be used for run-time power optimization and in-system energy profiling. The co-processor was evaluated on an SOC for MPEG layer III audio decoding and the experimental results show a maximum relative error of <5%.","","0-7803-7659","10.1109/ASPDAC.2003.1195094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1195094","","Runtime;System-on-a-chip;Coprocessors;Circuit simulation;Energy consumption;Energy management;Power measurement;Power engineering and energy;Decoding;Embedded system","system-on-chip;coprocessors;integrated circuit design;logic design;integrated circuit testing;logic testing;optimisation;audio coding","run-time energy estimation;system-on-a-chip;co-processor;power macro-models;SOC characterization;performance overhead;in-system energy profiling;SOC functional units;run-time power optimization;MPEG layer III audio decoding;estimation error","","3","21","","","","","","IEEE","IEEE Conferences"
"Heeding more than the top template","P. Sarkar; G. Nagy","Dept. of Electr. Comput. & Syst. Eng., Rensselaer Polytech. Inst., Troy, NY, USA; NA","Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)","","1999","","","382","385","We present a method of classifying a pattern using information furnished by a ranked list of templates, rather than just the best matching template. We propose a parsimonious model to compute the class-conditional likelihood of a list of templates ranked on the basis of their match scores. We discuss the estimation of parameters used in the model. The results of maximum likelihood classification on isolated digit patterns consistently show a 10-20% relative gain in recognition accuracy when we use more than one top-template.","","0-7695-0318","10.1109/ICDAR.1999.791804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791804","","Character recognition;Pattern recognition;Shape;Optical character recognition software;Testing;Systems engineering and theory;Radio access networks;Parameter estimation;Maximum likelihood estimation;Buildings","character recognition;pattern classification;maximum likelihood estimation;pattern matching","pattern classification;ranked template list;best matching template;class-conditional likelihood;match scores;parameter estimation;maximum likelihood classification;isolated digit patterns;recognition accuracy","","2","8","","","","","","IEEE","IEEE Conferences"
"Computer-assisted experimental design for the optimization of electrostatic separation processes","M. Mihailescu; A. Samuila; A. Urs; R. Morar; A. Iuga; L. Dascalescu","High-Intensity Electr. Fields Lab., Tech. Univ., Cluj-Napoca, Romania; NA; NA; NA; NA; NA","Conference Record of the 2000 IEEE Industry Applications Conference. Thirty-Fifth IAS Annual Meeting and World Conference on Industrial Applications of Electrical Energy (Cat. No.00CH37129)","","2000","1","","687","694 vol.1","Electrostatic separation is a typical multi-factorial process. Its efficiency depends on the characteristics of the granular mixtures to be sorted, the feed rate, the configuration of the electrode system, the applied high-voltage, the environmental conditions, and so on. The aim of the present work is to demonstrate the usefulness of computer-assisted experimental design in the optimization of such a process. The example analyzed in the paper was suggested by a typical application of electrostatic separation technique in the recycling industry: the selective sorting of metals and insulating materials from chopped wire and cable waste. The objective was to maximize the benefits from the recycling of both constituents of a binary copper-PVC granular mixture. A preliminary set of electrostatic separation tests, performed on a custom-designed laboratory unit, guided the choice of the starting values of the parameters considered in the computer-assisted experimental procedure of process optimization. The results of a first experiment, carried out in conformity with a fractional factorial scheme, were used for the computation of the coefficients of a linear mathematical model of the electrostatic process. The model was then employed to predict the values of the operating variables for which the optimum of the process is attained. A second experiment was performed in order to confirm the accuracy of the prediction. The procedure presented in this paper and the accompanying computer programs can be easily adapted to other electrostatic process applications.","0197-2618","0-7803-6401","10.1109/IAS.2000.881189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881189","","Design for experiments;Design optimization;Application software;Recycling;Electrostatic processes;Feeds;Electrodes;Electrostatic analysis;Metals industry;Sorting","electrostatics;separation;design of experiments;recycling;electrodes;electrical engineering computing","computer-assisted experimental design;electrostatic separation processes;multi-factorial process;granular mixtures;feed rate;electrode system;high-voltage;environmental conditions;process optimization;electrostatic separation;recycling industry;selective sorting;insulating materials;chopped wire waste;chopped cable waste;binary copper-PVC granular mixture;linear mathematical model","","","23","","","","","","IEEE","IEEE Conferences"
"Shortening the design cycle for programmable logic","S. H. Kelem; J. P. Seidel","Xilinx, San Jose, CA, USA; Xilinx, San Jose, CA, USA","IEEE Design & Test of Computers","","1992","9","4","40","50","X-BLOX, a software tool for mapping architecture-independent designs to field-programmable gate arrays (FPGAs), is described. X-BLOX synthesizes a delay- and area-efficient logic-level design from an input specification consisting of a network of generic modules. The tool automatically propagates partial data type specification, performs architecture-specific design optimization, and performs context-dependent module synthesis.<<ETX>>","0740-7475;1558-1918","","10.1109/54.173332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=173332","","Logic design;Programmable logic arrays;Programmable logic devices;Logic devices;Application specific integrated circuits;Circuit synthesis;Design methodology;Hardware design languages;Packaging machines;Logic circuits","delays;logic arrays;logic CAD;software tools","delays;design cycle;programmable logic;X-BLOX;software tool;mapping architecture-independent designs;field-programmable gate arrays;logic-level design;input specification;generic modules;partial data type specification;architecture-specific design optimization;context-dependent module synthesis","","4","11","","","","","","IEEE","IEEE Journals & Magazines"
"Object Management In A Case Environment","E. W. Adams; M. Honda; T. C. Miller","Sun Microsystems, Inc.; NA; NA","11th International Conference on Software Engineering","","1989","","","154","163","The Sun Network Software Environment (NSE) is a net- work-based object manager for software development. The NSE supports parallel development through an optimistic concurrency control mechanism, in which developers do not acquire locks before modifying objects, Instead, developers copy objects, modify the copies, and merge the modified objects with the originals. Objects managed by the NSE are typed, and the set of types can be extended by tool builders. The NSE is designed to work with heterogeneous implementations and poor communication.","0270-5257","0-8186-8941","10.1109/ICSE.1989.714416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=714416","","Environmental management;Computer aided software engineering;Neutron spin echo;Permission;Sun;Concurrency control;Testing;Costs;Intelligent networks;Software development management","","","","13","13","","","","","","IEEE","IEEE Conferences"
"Towards autonomic computing middleware via reflection","Gang Huang; Tiancheng Liu; Hong Mei; Zizhan Zheng; Zhao Liu; Gang Fan","Sch. of Electron. Eng. & Comput. Sci., Peking Univ., China; Sch. of Electron. Eng. & Comput. Sci., Peking Univ., China; Sch. of Electron. Eng. & Comput. Sci., Peking Univ., China; Sch. of Electron. Eng. & Comput. Sci., Peking Univ., China; Sch. of Electron. Eng. & Comput. Sci., Peking Univ., China; Sch. of Electron. Eng. & Comput. Sci., Peking Univ., China","Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.","","2004","","","135","140 vol.1","Autonomic computing middleware is a promising way to enable middleware based systems to cope with the rapid and continuous changes in the era of Internet. Technically, there have been three fundamental and challenging capabilities to an autonomic computing middleware, including how to monitor, reason and control middleware platform and applications. This position paper presents a reflection-based approach to autonomic computing middleware, which shows the philosophy that autonomic computing should focus on how to reason while reflective computing supports how to monitor and control. In this approach, the states and behaviors of middleware-based systems can be observed and changed through reflective mechanisms embedded in middleware platform at runtime. On the basis of reflection, some autonomic computing facilities could be constructed to reason and decide when and what to change. The approach is demonstrated on a reflective J2EE application server, which can automatically optimize itself in the standard J2EE benchmark testing","0730-3157","0-7695-2209","10.1109/CMPSAC.2004.1342817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342817","","Middleware;Reflection;Runtime;Java;Monitoring;Internet;Costs;Technology management;Application software;Computer science","benchmark testing;middleware","autonomic computing middleware;reflection-based approach;reflective J2EE application server;J2EE benchmark testing","","10","15","","","","","","IEEE","IEEE Conferences"
"Increasing energy efficiency of embedded systems by application-specific memory hierarchy generation","L. Benini; A. Macii; E. Macii; M. Poncino","Bologna Univ., Italy; NA; NA; NA","IEEE Design & Test of Computers","","2000","17","2","74","85","This article presents a methodology for automatic memory hierarchy generation that exploits memory access locality of embedded software. The methodology is successfully applied to the design of an MP3 decoder.","0740-7475;1558-1918","","10.1109/54.844336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844336","","Energy efficiency;Embedded system;Decoding;Costs;Cache memory;Logic;Memory architecture;Hardware;Buffer storage;Embedded software","embedded systems;optimisation;systems analysis;decoding","energy efficiency;embedded systems;application-specific memory hierarchy generation;automatic memory hierarchy generation;memory access locality;embedded software;MP3 decoder","","42","10","","","","","","IEEE","IEEE Journals & Magazines"
"Data clustering by ant colony on a digraph","Ling Chen; Li Tu; Hong-Jian Chen","Dept. of Comput. Sci., Yangzhou Univ., China; Dept. of Comput. Sci., Yangzhou Univ., China; Dept. of Comput. Sci., Yangzhou Univ., China","2005 International Conference on Machine Learning and Cybernetics","","2005","3","","1686","1692 Vol. 3","An adaptive data clustering algorithm based on ant colony (ant-cluster) is presented. Enlightened by the self-organizing behavior of ant society, we assign acceptance rates on the directed edges of a pheromone digraph in ant-cluster system. The pheromone on the edges of the digraph is adaptively updated by the ants passing it. Some edges with less pheromone are progressively removed under a list of certain thresholds in the process. Strong connected components of the final digraph are extracted as clusters. The performance of ant-cluster is compared with classical K-means clustering algorithm and ACO clustering algorithm LF in terms of clustering quality and efficiency on several real datasets and clustering benchmarks. Experimental results indicate that the ant-cluster is able to find clusters faster with better clustering quality and is easier to implement than K-means and LF.","2160-133X;2160-1348","0-7803-9091","10.1109/ICMLC.2005.1527216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1527216","Digraph;clustering;ant colony;K-means;LF","Clustering algorithms;Ant colony optimization;Particle swarm optimization;Cadaver;Computer science;Software algorithms;Testing;DNA;Birds;Marine animals","directed graphs;pattern clustering;adaptive systems;particle swarm optimisation","ant colony;K-means;adaptive data clustering algorithm;ant-cluster;self-organizing behavior;pheromone digraph","","4","15","","","","","","IEEE","IEEE Conferences"
"Design and performance of the Dawning Cluster File System","Jin Xiong; Sining Wu; Dan Meng; Ninghui Sun; Guojie Li","National Res. Center for Intelligent Comput. Syst., Chinese Acad. of Sci., China; National Res. Center for Intelligent Comput. Syst., Chinese Acad. of Sci., China; National Res. Center for Intelligent Comput. Syst., Chinese Acad. of Sci., China; National Res. Center for Intelligent Comput. Syst., Chinese Acad. of Sci., China; National Res. Center for Intelligent Comput. Syst., Chinese Acad. of Sci., China","2003 Proceedings IEEE International Conference on Cluster Computing","","2003","","","232","239","Cluster file system is a key component of system software of clusters. It attracts more and more attention in recent years. In this paper, we introduce the design and implementation of DCFS (the Dawning Cluster File System) - a cluster file system developed for Dawning4000-L. DCFS is a global file system sharing among all cluster nodes. Applications see a single uniform name space, and can use system calls to access DCFS files. The features of DCFS include its scalable architecture, metadata policy, server-side optimization, flexible communication mechanism and easy management. Performance tests of DCFS on Dawning4000-L show that DCFS can provide high aggregate bandwidth and throughput.","","0-7695-2066","10.1109/CLUSTR.2003.1253320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253320","","File systems;Network operating systems;Client-server systems;Software performance","file organisation;network operating systems;workstation clusters;meta data;client-server systems;software performance evaluation","Dawning Cluster File System;system software;Dawning4000-L;global file system sharing;cluster nodes;uniform name space;file access;metadata policy;server-side optimization;communication mechanism;performance tests;bandwidth;throughput","","6","14","","","","","","IEEE","IEEE Conferences"
"COSIMA: a self-testable simulated annealing processor for universal cost functions","B. Eschermann; O. Haberl; O. Bringmann; O. Seitz","Inst. fur Rechnerentwurf und Fehlertoleranz, Karlsruhe, Univ., Germany; Inst. fur Rechnerentwurf und Fehlertoleranz, Karlsruhe, Univ., Germany; Inst. fur Rechnerentwurf und Fehlertoleranz, Karlsruhe, Univ., Germany; Inst. fur Rechnerentwurf und Fehlertoleranz, Karlsruhe, Univ., Germany","Proceedings Euro ASIC '92","","1992","","","374","377","Presents a chip forming the heart of a special purpose coprocessing unit, which accelerates simulated annealing algorithms to solve combinatorial optimization problems. The chip includes about 26000 transistors and runs at an expected clock frequency of 20 MHz. Compared with a software solution it leads to a speedup of about 500.<<ETX>>","","0-8186-2845","10.1109/EUASIC.1992.227998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=227998","","Built-in self-test;Simulated annealing;Cost function;Computational modeling;Clocks;Frequency;Hardware;Heart;Acceleration;Computer science","application specific integrated circuits;built-in self test;microprocessor chips;simulated annealing","self-testable simulated annealing processor;universal cost functions;special purpose coprocessing unit;combinatorial optimization;clock frequency;20 MHz","","2","6","","","","","","IEEE","IEEE Conferences"
"Disassembly simulation for an effective recycling of electrical scrap","J. Hesselbach; K. von Westernhagen","Inst. of Production Autom. & Handling Technol., Tech. Univ. Braunschweig, Germany; NA","Proceedings First International Symposium on Environmentally Conscious Design and Inverse Manufacturing","","1999","","","582","585","Disassembly of worn-out products, e.g. electronic products, without a careful planning of the processes may make recycling of such electrical scrap costly and inefficient. Computer assisted methods to support systematic planning of disassembly processes can help to increase the productivity and to improve the environmental-friendly removal of this kind of worn-out product. The paper introduces a software-tool, so-called LaySiD (Lay out Simulation for Disassembly), that includes a few modules to support the phases of planning disassembly. Methods of classifying electronic devices into disassembly-families with respect to their dismantling-related attributes are outlined. Further methods include the generation of flexible disassembly systems and the simulation of disassembly processes. The methods are applied to a spectrum of electronic devices that were analyzed in a dismantling experiment. The depicted tool may allow companies involved in dismantling and recycling of electrical scrap to optimize their disassembly processes and to improve productivity in recycling electrical scrap.","","0-7695-0007","10.1109/ECODIM.1999.747679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=747679","","Recycling;Consumer electronics;Process planning;Productivity;System testing;Hazardous materials;Automation;Telephony;Production planning;Technology planning","recycling;planning;electronic engineering computing;software packages;digital simulation","disassembly simulation;electrical scrap recycling;worn-out products disassembly;computer assisted methods;systematic disassembly planning;productivity increase;software-tool;LaySiD;Lay out Simulation for Disassembly;electronic devices classification;disassembly-families;dismantling-related attributes;electronic device;electronic scrap","","","8","","","","","","IEEE","IEEE Conferences"
"An agent-based approach to the control of flexible production systems","S. Bussmann; K. Schild","Res. & Technol., DaimlerChrysler AG, Berlin, Germany; NA","ETFA 2001. 8th International Conference on Emerging Technologies and Factory Automation. Proceedings (Cat. No.01TH8597)","","2001","2","","481","488 vol.2","We present an agent-based approach to the control of flexible production systems. This approach flexibly adapts to changing production conditions and is suitable for high volume production. In this approach, workpiece agents auction off their current task, while machine agents bid for tasks. When awarding a machine, a workpiece agent takes into account not only the machine's current workload, but also the workpieces leaving the machine. If a machine's outgoing stream is blocked, the machine agent eventually ceases to accept new workpieces, thus blocking its input stream as well. As a result, a capacity bottleneck is automatically propagated in the opposite direction of the material flow. A unique feature of this mechanism is that it does not pre-suppose any specific material flow; the current capacity bottleneck is always propagated in the opposite direction of the actual flow, no matter how this flow looks like. Daimler Chrysler has evaluated the new control approach as a bypass to an existing manufacturing line. A suite of performance tests demonstrated the industrial feasibility and benefits of the approach.","","0-7803-7241","10.1109/ETFA.2001.997722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=997722","","Control systems;Production systems;Manufacturing industries;Manufacturing processes;Software agents;Testing;Investments;Costs;Robustness;Optimized production technology","flexible manufacturing systems;manufacturing data processing;software agents;computer aided production planning","flexible production systems;workpiece agents;machine agents;machine workload;capacity bottleneck;material flow;product-life cycles;software agent","","46","14","","","","","","IEEE","IEEE Conferences"
"Deliver 100,000 Modules a Month? of Course, We use Cam!","A. M. Pavio","Texas Instruments Incorporated, Dallas, Texas","1988 18th European Microwave Conference","","1988","","","38","48","The application of computer-aided manufacturing (CAM) techniques to the assembly and test of a low-cost high-volume X-band Radar T/R module will be investigated by highlighting the differences between low frequency and microwave CAM methods, focusing on the difficulties of automated testing, evaluation and circuit tuning. The problems encountered when implementing high-precision automated assembly methods, which include wire-bonding, soldering and piece-part placement will also be discussed. The assembly methods, and a novel implementation of a microwave circuit Laser tuning technique, which were used to manufacture the module's five-bit phase shifter, aids in illustrating the above CAM concepts.","","","10.1109/EUMA.1988.333795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4132483","","Computer aided manufacturing;Circuit testing;CADCAM;Assembly;Automatic testing;Microwave theory and techniques;Circuit optimization;Laser tuning;Application software;Radar applications","","","","1","4","","","","","","IEEE","IEEE Conferences"
"Validation of the GSM radio interface signalling protocols-selected studies","A. Eizenhofer; W. F. Detecon","NA; NA","[1991 Proceedings] 41st IEEE Vehicular Technology Conference","","1991","","","333","338","Selected results of the validation of the GSM (Groupe Special Mobile) radio interface signaling protocol are presented. During this process, specification errors have been detected and corrected. The simulations also contribute to the optimization of the signaling protocols. Points of interest are the random access procedures on common control channels, transitions among the various types of signaling channels, the signaling delay of signaling procedures due to varying transmission conditions and traffic flow. Selected aspects of overload control are studied. Experience showed that simulation is a very valuable supplement for the specification process.<<ETX>>","1090-3038","0-87942-582","10.1109/VETEC.1991.140505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=140505","","GSM;Hardware;Physical layer;Software measurement;Software prototyping;Prototypes;Access protocols;Computer architecture;Software testing;Telecommunications","cellular radio;digital radio systems;protocols;signalling (telecommunication networks);telecommunications control","error detection;error correction;TDMA;digit cellular radio;GSM;Groupe Special Mobile;radio interface signaling protocol;specification errors;simulations;optimization;random access procedures;control channels;signaling channels;signaling delay;transmission conditions;traffic flow;overload control","","","8","","","","","","IEEE","IEEE Conferences"
"Timing and power issues in wireless sensor networks - an industrial test case","N. Aakvaag; M. Mathiesen; G. Thonet","ABB Corporate Res., Billingstad, Norway; ABB Corporate Res., Billingstad, Norway; ABB Corporate Res., Billingstad, Norway","2005 International Conference on Parallel Processing Workshops (ICPPW'05)","","2005","","","419","426","This paper reports on experimental results of a wireless sensor network set up in an industrial automation facility at a Swedish mining company. It illustrates that the new ZigBee standard performs well even in heavy industrial environments. The main focus is on timing issues and achieving synchronized duty cycling. A simple synchronization algorithm is devised and shown to be sufficient to meet strict power consumption requirements.","0190-3918;2332-5690","0-7695-2381","10.1109/ICPPW.2005.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488724","","Timing;Intelligent networks;Wireless sensor networks;Testing;Computer aided software engineering;Automation;Energy consumption;Costs;Mining industry;ZigBee","wireless sensor networks;telecommunication standards;optimisation;synchronisation;timing","wireless sensor networks;ZigBee standard;heavy industrial environment;timing;duty cycling synchronization;power consumption;telecommunication standards;telecommunication power supplies","","24","10","","","","","","IEEE","IEEE Conferences"
"Distributed virtual environment collaborative simulator for underwater robots","S. K. Choi; S. A. Menor; J. Yuh","Dept. of Mech. Eng., Hawaii Univ., Honolulu, HI, USA; NA; NA","Proceedings. 2000 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2000) (Cat. No.00CH37113)","","2000","2","","861","866 vol.2","This paper describes a distributed virtual environment collaborative simulator (DVECS) for testing of unmanned underwater vehicles (UUV) where both real and simulated vehicles-remotely operated vehicles (ROV) and autonomous underwater vehicles (AUV)-can interact and cooperate in a hybrid, synthetic, virtual environment containing various types of real and simulated vehicles, obstacles, conditions and disturbances. This virtual system can be used to determine (1) the optimal performance criteria for the vehicles and its application, (2) the advantages and disadvantages of collaborative application tasks between multiple UUV, and (3) the optimal communication links between remote locations. The current DVECS is being tested between the University of Hawaii in Honolulu and the University of Tokyo in Tokyo as a joint, international research program. It also utilizes virtual reality projection system, which immerses the user for optimal visualization.","","0-7803-6348","10.1109/IROS.2000.893127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=893127","","Virtual environment;Collaboration;Underwater vehicles;System testing;Remotely operated vehicles;Mobile robots;Graphics;Virtual reality;Software maintenance;Hardware","underwater vehicles;remotely operated vehicles;telerobotics;mobile robots;virtual reality;distributed processing;optimisation;multi-robot systems;telecommunication links;digital simulation","distributed virtual environment collaborative simulator;underwater robots;DVECS;unmanned underwater vehicles;UUV;ROV;autonomous underwater vehicles;AUV;optimal performance criteria;collaborative application tasks;optimal communication links;virtual reality projection system;optimal visualization","","7","5","","","","","","IEEE","IEEE Conferences"
"Loss reduction in distribution networks by network reconfiguration: a two stage solution approach","V. C. Veera Reddy; N. Perumal; Y. Rajasekharareddy","Sch. of Electr. & Electron. Engg., USM, Malaysia; NA; NA","PECon 2004. Proceedings. National Power and Energy Conference, 2004.","","2004","","","241","246","In this paper, a simplified load flow method is developed for solving radial distribution networks that involves only the evaluation of a simple algebraic expression of voltage magnitudes. A general formulation of the network reconfiguration for loss minimization is given for optimization of distribution loss reduction and a solution approach is presented. The solution algorithm for loss minimization has been developed based on a two-stage solution methodology. The first stage of this solution algorithm finds a loop, which gives the maximum loss reduction in the network. For this purpose a simple-to-use formula, called loop loss reduction formula has been developed. To find a branch exchange, which results in the maximum loss reduction in the loop, the second stage applies a proposed technique called distance-center technique. Therefore, the solution algorithm of the proposed method can identify the most effective branch exchange operations for loss reduction, with minimum computational efforts. The software for proposed solution algorithm is capable of performing optimum reconfiguration for a radial distribution network for any number of buses and laterals. The proposed feeder reconfiguration is tested with 33-bus IEEE test system and the results are compared with Baran and Wu's method.","","0-7803-8724","10.1109/PECON.2004.1461651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1461651","","Intelligent networks;Voltage;Minimization methods;Load flow;System testing;Niobium;Equations;Software performance;Software algorithms;Impedance","distribution networks;losses;minimisation;load flow","network reconfiguration;loss reduction;load flow method;radial distribution networks;simple algebraic expression;optimization;loss minimization;two-stage solution methodology;loop loss reduction formula;distance-center technique;33-bus IEEE test system","","1","8","","","","","","IEEE","IEEE Conferences"
"An efficient video rendering system for real-time Adaptive Playout based on physical motion field estimation","L. Piccarreta; A. Sarti; S. Tubaro","Dipartimento di Elettronica e Informazione, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133, ITALY; Dipartimento di Elettronica e Informazione, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133, ITALY; Dipartimento di Elettronica e Informazione, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133, ITALY","2005 13th European Signal Processing Conference","","2005","","","1","4","In this paper we present an implementation of an Adaptive Playout System for video rendering. This system can be used for rendering multimedia material that is delivered (in single/multi-cast fashion) to the final user(s) over a &#x201C;besteffort&#x201D; network that is unable to guarantee a constant delay in the delivery of the data packets. The proposed solution is an alternative to the &#x201C;traditional&#x201D; pre-buffering at the decoder side, which &#x201C;cushions&#x201D; the variations of delivery delay, but forces the final user to wait a great deal before the rendering starts and may easily generate annoying freezing of the video in case of pre-buffer underflow. The system can be quite effective in enabling the &#x201C;zapping&#x201D; between channels broadcasted over the network by using streaming technology. The system is currently able to run in real time on commercial PCs for the decoding and adaptive playout of CIF sequences, but there is still a great deal of room for further software optimization. A series of informal subjective tests have been conducted to demonstrate the potential of the system.","","978-160-4238-21","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7078017","","Streaming media;Delays;Adaptive systems;Standards;Media;Real-time systems;Decoding","motion estimation;optimisation;video signal processing","efficient video rendering system;real-time adaptive playout;physical motion field estimation;video rendering;multimedia material;streaming technology;CIF sequences;software optimization","","","6","","","","","","IEEE","IEEE Conferences"
"Global convergence analysis of non-crossover genetic algorithm and its application to optimization","D. Xiaoming; S. Rong; Z. Runmin; X. Chao; S. Huihe","Dept. of Auto., School of Electric and Information, Shanghai Jiaotong University, Shanghai 200030, P. R. China; Dept. of Auto., School of Electric and Information, Shanghai Jiaotong University, Shanghai 200030, P. R. China; College of Information Science and Enginereing, Central South University, Changsha 410083, P. R. China; Dept. of Auto., School of Electric and Information, Shanghai Jiaotong University, Shanghai 200030, P. R. China; Dept. of Auto., School of Electric and Information, Shanghai Jiaotong University, Shanghai 200030, P. R. China","Journal of Systems Engineering and Electronics","","2002","13","2","84","91","Selection, crossover, and mutation are three main operators of the canonical genetic algorithm (CGA). This paper presents a new approach to the genetic algorithm. This new approach applies only to mutation and selection operators. The paper proves that the search process of the non-crossover genetic algorithm (NCGA) is an ergodic homogeneous Markov chain. The proof of its convergence to global optimum is presented. Some nonlinear multi-modal optimization problems are applied to test the efficacy of the NCGA. NP-hard traveling salesman problem (TSP) is cited here as the benchmark problem to test the efficiency of the algorithm. The simulation result shows that NCGA achieves much faster convergence speed than CGA in terms of CPU time. The convergence speed per epoch of NCGA is also faster than that of CGA.","1004-4132","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6077452","Canonical;Genetic algorithm;Ergodic homogeneous Markov chain;Global convergence","Biological cells;Genetic algorithms;Markov processes;Convergence;Optimization;Matrix decomposition;Educational institutions","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"System level design of embedded controllers: knock detection, a case study in the automotive domain","L. Mangeruca; A. Ferrari; A. Sangiovanni-Vincentelli; A. Pierantoni; M. Pennese","PARADES, Roma, Italy; PARADES, Roma, Italy; NA; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","232","237 suppl.","We present a case study in the design of automotive engine controllers: the development of a knock detection algorithm and its implementation in an optimized platform. The design problem is complicated by the need of using heterogeneous models of computation and different design environments. The use of different design environments, one for functional design and one for architectural design space exploration, requires to transform a model of computation into another We describe how we solved this problem and we present the final design with the trade-offs explored.","1530-1591","0-7695-1870","10.1109/DATE.2003.1186700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1186700","","System-level design;Control systems;Computer aided software engineering;Automotive engineering;Computational modeling;Sparks;Internal combustion engines;Pistons;Algorithm design and analysis;Computer architecture","embedded systems;automotive electronics;microcontrollers;circuit CAD;high level synthesis;circuit optimisation","system level design;embedded controllers;automotive engine controllers;knock detection algorithm;optimized platform;heterogeneous models;architectural design space exploration;functional design","","","12","","","","","","IEEE","IEEE Conferences"
"Neural network based on dynamic tunneling technique for weather forecast","Zheng Qin; Haoliang Wang; Jinmin Yang; Bin Wang; Jianjun Zou","Coll. of Software, Hunan Univ., Changsha, China; NA; NA; NA; NA","11th Pacific Rim International Symposium on Dependable Computing (PRDC'05)","","2005","","","5 pp.","","In this paper, a method of short-term temperature forecasting based on artificial neural networks is presented. An improved learning algorithm of neural network, RPROP, combined with a new efficient computational technique, dynamic tunneling technique is used to train neural network, for short, GDT. These two techniques are repeated alternatively processed to avoid local minima and result into a global optimization. The proposed networks are trained with actual data of the past 24 months (1999-2000) and are tested with data of 6 months (2001.1/spl sim/2001.3,2001.7/spl sim/2001.9), which come from several meteorological stations around or in Chongqing, China. Since the average prediction error of network on the test set equals 1.4, the obtained results demonstrated the efficiency of proposed method and show that the scheme reaches global minimum soon and converges at high rate.","","0-7695-2492","10.1109/PRDC.2005.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607542","","Weather forecasting;Neural networks;Tunneling;Meteorology;Temperature;Artificial neural networks;Testing;Technology forecasting;Load forecasting;Educational institutions","geophysics computing;weather forecasting;neural nets;learning (artificial intelligence)","dynamic tunneling;weather forecast;short-term temperature forecasting;artificial neural networks;neural network training;global optimization;meteorology;Chongqing;China","","","16","","","","","","IEEE","IEEE Conferences"
"SVD-based portable device for real-time hoarse voice denoising","C. Manfredi; L. Landini; F. Faita; V. Gemignani","Dept. of Electron. & Telecommun., Univ. of Florence, Italy; NA; NA; NA","2002 14th International Conference on Digital Signal Processing Proceedings. DSP 2002 (Cat. No.02TH8628)","","2002","2","","857","860 vol.2","For pathological voices, hoarseness is mainly due to airflow turbulence in the vocal tract and is often referred to as noise. This paper focuses on enhancement of speech signals, assumed to be degraded by additive white noise. Speech enhancement is performed in the time-domain, by means of a low-order singular value decomposition that allows removal of the noise contribution while preserving the harmonic structure of the original speech signal. The approach was tested on synthetic data and applied to real data, concerning hoarse voices of cordectomised patients. Its simple structure allows a real-time implementation, based on C and assembler code, properly optimised on a DSP board. Hence, the method is suitable for portable device realisation, as an aid to disphonic speakers. It could be of help for diminishing the effort in speaking, which is closely related to social problems due to the awkwardness of voice.","","0-7803-7503","10.1109/ICDSP.2002.1028225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028225","","Noise reduction;Speech enhancement;Pathology;Degradation;Additive white noise;Time domain analysis;Singular value decomposition;Testing;Assembly;Digital signal processing","singular value decomposition;real-time systems;speech enhancement;handicapped aids;C language;digital signal processing chips;white noise;software tools;program assemblers","SVD-based portable device;real-time hoarse voice denoising;pathological voices;hoarseness;airflow turbulence;vocal tract;noise;speech signals;speech enhancement;additive white noise;low-order singular value decomposition;speech signal harmonic structure;synthetic data;real data;cordectomised patients;real-time implementation;C code;assembler code;DSP board;disphonic speakers;dysphonia;software development tool","","2","15","","","","","","IEEE","IEEE Conferences"
"Determination of worst case line energization overvoltages","O. Castellanos; L. Rugeles; A. J. Urdaneta; A. Brillembourg; A. E. Hernandez; B. Nouel","NA; NA; NA; NA; NA; NA","Proceedings of First International Caracas Conference on Devices, Circuits and Systems","","1995","","","46","51","An optimization methodology, is presented, for the automatic determination of the maximum transient overvoltage due to transmission line energization. The approach is based upon digital simulations of the electromagnetic phenomena, and uses a modified version of the Hooke and Jeeves direct search nonlinear optimization method, specially adapted to the particular characteristics of the problem, for the determination of the closing times of the circuit breaker poles which cause the maximum transient line energization overvoltage. Results are presented for the application of the proposed technique to different test cases, including transmission lines from 115 kV to 765 kV from the Venezuelan electric system. The overall method is shown to be accurate and reliable.","","0-7803-2672","10.1109/ICCDCS.1995.499116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=499116","","Computer aided software engineering;Surges;Circuit breakers;Power system transients;Insulation;Voltage;Transient analysis;Electromagnetic transients;Analytical models;Optimization methods","overvoltage;digital simulation;power system analysis computing;power system transients;power system planning;insulation co-ordination;power overhead lines;power transmission lines","line energization overvoltages;optimization methodology;maximum transient overvoltage;digital simulations;electromagnetic phenomena;Hooke and Jeeves direct search nonlinear optimization;closing times;circuit breaker poles;Venezuelan electric system;115 to 765 kV","","1","9","","","","","","IEEE","IEEE Conferences"
"Monitoring networks using ntop","L. Deri; R. Carbone; S. Suin","Netikos S.p.A, Pisa, Italy; NA; NA","2001 IEEE/IFIP International Symposium on Integrated Network Management Proceedings. Integrated Network Management VII. Integrated Management Strategies for the New Millennium (Cat. No.01EX470)","","2001","","","199","212","Today's networks present several management challenges due to the variety of network types and the integration of different network media. Network administrators need automated tools to support human effort, gathering information about the status and behaviour of network elements. Network monitoring is probably the most fundamental aspect of automated network management. The goal of this paper is to describe the design and implementation of ntop a simple, open-source, portable traffic measurement and monitoring tool, which supports various management activities, including network optimisation and planning, and detection of network security violations. In addition, it describes some scenarios where ntop can be effectively used for identifying common network problems as well as detecting intruders and security violations.","","0-7803-6719","10.1109/INM.2001.918032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918032","","Telecommunication traffic;Humans;Computerized monitoring;Open source software;IP networks;Testing;Data security;Information security;Design optimization;Electronic commerce","telecommunication traffic recording;computer network management;telecommunication security;telecommunication network planning;optimisation","ntop;network media;automated tools;network monitoring;automated network management;design;implementation;open-source tool;portable traffic measurement tool;monitoring tool;network optimisation;planning;network security violations;intruders;security violation","","10","14","","","","","","IEEE","IEEE Conferences"
"Optimal selection of secondary indexes","E. Barcucci; R. Pinzani","Dipartimento di Sistemi e Inf., Firenze Univ., Italy; Dipartimento di Sistemi e Inf., Firenze Univ., Italy","IEEE Transactions on Software Engineering","","1990","16","1","32","38","When planning a database, the problem of index selection is of particular interest. The authors examine a transaction model that includes queries, updates, insertions, and deletions, and they define a function that calculates the transaction's total cost when an index set is used. Their aim is to minimize the function cost in order to identify the optimal set. The algorithms proposed in other studies require an exponential time in the number of attributes in order to solve the problem. The authors propose a heuristic algorithm based on some properties of the cost function that produces an almost optimal set in polynomial time. In many cases, the cost function properties make it possible to prove that the solution obtained is the optimal one.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=44361","","Cost function;Indexes;Transaction databases;Heuristic algorithms;Optimization;Relational databases;Algorithm design and analysis;Testing;Read-write memory","database management systems;heuristic programming;indexing;transaction processing","optimal selection;secondary indexes;database;transaction model;queries;updates;insertions;deletions;heuristic algorithm;polynomial time;cost function properties","","10","15","","","","","","IEEE","IEEE Journals & Magazines"
"A vision system for landing an unmanned aerial vehicle","C. S. Sharp; O. Shakernia; S. S. Sastry","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; NA; NA","Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)","","2001","2","","1720","1727 vol.2","We present the design and implementation of a real-time computer vision system for a rotorcraft unmanned aerial vehicle to land onto a known landing target. This vision system consists of customized software and off-the-shelf hardware which perform image processing, segmentation, feature point extraction, camera pan/tilt control, and motion estimation. We introduce the design of a landing target which significantly simplifies the computer vision tasks such as corner detection and correspondence matching. Customized algorithms are developed to allow for realtime computation at a frame rate of 30 Hz. Such algorithms include certain linear and nonlinear optimization schemes for model-based camera pose estimation. We present results from an actual flight test which show the vision-based state estimates are accurate to within 5 cm in each axis of translation, and 5 degrees in each axis of rotation, making vision a viable sensor to be placed in the control loop of a hierarchical flight management system.","1050-4729","0-7803-6576","10.1109/ROBOT.2001.932859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932859","","Machine vision;Unmanned aerial vehicles;Computer vision;Cameras;Control systems;Real time systems;Software performance;Hardware;Image processing;Image segmentation","aerospace robotics;mobile robots;robot vision;aircraft landing guidance;helicopters;real-time systems;image segmentation;feature extraction;motion estimation;optimisation;state estimation","vision system;unmanned aerial vehicle landing;real-time computer vision system;rotorcraft unmanned aerial vehicle;landing target;customized software;off-the-shelf hardware;image processing;image segmentation;feature point extraction;camera pan/tilt control;motion estimation;computer vision tasks;corner detection;correspondence matching;linear optimization;nonlinear optimization;model-based camera pose estimation;vision-based state estimates;hierarchical flight management system;30 Hz","","125","15","","","","","","IEEE","IEEE Conferences"
"Investigation of observer-performance in MAP-EM reconstruction with anatomical priors and scatter correction for lesion detection in /sup 67/Ga images","P. P. Bruyant; H. C. Gifford; G. Gindi; M. A. King","Massachusetts Univ., Worcester, MA, USA; Massachusetts Univ., Worcester, MA, USA; NA; NA","2003 IEEE Nuclear Science Symposium. Conference Record (IEEE Cat. No.03CH37515)","","2003","5","","3166","3169 Vol.5","In a previous work, we showed that anatomical priors can improve lesion detection in simulated Ga67 images of the chest. We herein expand and enhance our previous investigations by adding scatter in the projections, by using the triple energy window scatter compensation method and by implementing a new scheme for image reconstruction. Phantom images are created using the SIMIND Monte Carlo simulation software and the mathematical cardiac-torso (MCAT) phantom. The anatomical data are the original, noise-free slices of the MCAT phantom. Images are reconstructed using the DePierro algorithm. Two weights for the prior are tested (0.005 and 0.02). The following reconstruction scheme is used to reach convergence: The 120 projections are reconstructed successively with 4, 8, 24, 60, and 120 projections per subset with 1,1,1,1, and 50 iterations respectively; the result of each reconstruction is used as an initial estimate for the next reconstruction. Several strategies were investigated: no anatomical prior information, and anatomical information for organs and/or lesion. Lesion detection was performed by a numerical observer with an LROC task. Strategies including anatomical priors yield better results in terms of lesion detection, as compared to the strategy using no prior and only post-reconstruction Gaussian smoothing.","1082-3654","0-7803-8257","10.1109/NSSMIC.2003.1352568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1352568","","Image reconstruction;Lesions;Electromagnetic scattering;Particle scattering;Imaging phantoms;Neoplasms;Convergence;Iterative algorithms;Testing;Smoothing methods","biological organs;phantoms;radioisotope imaging;maximum likelihood estimation;optimisation;image reconstruction;medical image processing;Monte Carlo methods;smoothing methods;cardiology","observer performance;MAP-EM image reconstruction;maximum a posteriori method;expectation maximization method;anatomical priors;scatter correction;lesion detection;/sup 67/Ga images;chest;triple energy window scatter compensation method;phantom images;SIMIND Monte Carlo simulation software;mathematical cardiac-torso phantom;DePierro algorithm;post-reconstruction Gaussian smoothing","","2","4","","","","","","IEEE","IEEE Conferences"
"Symbolic Semantics and Program Reduction","V. Ambriola; F. Giannotti; D. Pedreschi; F. Turini","Dipartimento di Informatica, Universit&#224; di Pisa; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","8","784","794","A class of transformations of functional programs based on symbolic execution and simplification of conditionals is presented. The operational symbolic semantics of a family of functional languages is defined exploiting a set-theoretic notion of symbolic constants. An effective transformation able to simplify a functional program via removal of conditionals is discussed. Finally, it is shown that a structural approach, based on abstract data type specifications, provides a suitable representation for symbolic constants.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702087","Abstract data type specifications;functional programming languages;program reduction;program transformations;rewriting systems;symbolic evaluation","Constraint optimization;Input variables;Functional programming;Proposals;Testing;Performance analysis;Constraint theory;Concrete;Robustness","","Abstract data type specifications;functional programming languages;program reduction;program transformations;rewriting systems;symbolic evaluation","","1","17","","","","","","IEEE","IEEE Journals & Magazines"
"Qualified Data Flow Problems","L. H. Holley; B. K. Rosen","IBM Cambridge Scientific Center; NA","IEEE Transactions on Software Engineering","","1981","SE-7","1","60","78","It is known that not aU paths are possible in the run time control flow of many programs. It is also known that data flow analysis cannot restrict attention to exactly those paths that are possible. It is, therefore, usual for analytic methods to consider aU paths. Sharper information can be obtained by considering a recursive set of paths that is large enough to include aUl possible paths, but smaU enough to exclude many of the impossible ones. This paper presents a simple uniform methodology for sharpening data flow information by considering certain recursive path sets of practical importance. Associated with each control flow arc there is a relation on a finite set Q. The paths that qualify to be considered are (essentially) those for which the composition of the relations encountered is nonempty. For example, Q might be the set of all assignments of values to each of several bit variables used by a program to remember some facts about the past and branch accordingly in the future. Given any data-flow problem together with qualifying relations on Q associated with the control flow arcs, we construct a new problem. Considering all paths in the new problem is equivalent to considering only qualifying paths in the old one. Preliminary experiments (with a smaUl set of real programs) indicate that qualified analysis is feasible and substantialy more informative than ordinary analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1981.234509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702803","Data flow analysis;global variable;interprocedural analysis;label variable;symbolic execution","Testing;Data analysis;Computer languages;Feedback;Assembly;Flowcharts;Constraint optimization;Operating systems;Algorithm design and analysis;Availability","","Data flow analysis;global variable;interprocedural analysis;label variable;symbolic execution","","10","25","","","","","","IEEE","IEEE Journals & Magazines"
"Real-time adaptive control of knowledge based avionics tasks","R. Cowin; H. F. Krikorian","Northrop Corp. Hawthorne, CA, USA; Northrop Corp. Hawthorne, CA, USA","Proceedings of the IEEE National Aerospace and Electronics Conference","","1989","","","1175","1184 vol.3","Advanced decision-making capabilities are being developed to aid the pilots of the next generation of tactical fighters. Due to the limited processing resources available in an avionics suite, efforts have focused on developing a distributed fault-tolerant software architecture that permits the real-time prioritization and scheduling of these tasks. The authors outline the design details of an architecture under development to meet these performance requirements. The system has been tested with a threat-avoidance system, implemented on a testbed of five internetted LISP workstations, to evaluate overall system capabilities including scheduling, task operations, and database accesses. It has a simulation cycle of 50 ms and synchronization between distributed nodes can be achieved within 2 ms. This test case has nine knowledge tasks, one of which is defined as a simulation cycle that drives the test case. This system has been evaluated with the current trace capabilities and runs with a peak of 16 task instances active at any time.<<ETX>>","","","10.1109/NAECON.1989.40357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=40357","","Adaptive control;Aerospace electronics;System testing;Decision making;Fault tolerance;Software architecture;Computer architecture;Internet;Workstations;Databases","adaptive control;aerospace computer control;aerospace testing;fault tolerant computing;knowledge based systems;military systems;real-time systems;scheduling","real-time adaptive control;knowledge based avionics tasks;distributed fault-tolerant software architecture;threat-avoidance system;scheduling;task operations;database accesses;simulation cycle","","1","5","","","","","","IEEE","IEEE Conferences"
"TAB inner-lead bond process characterization for single-point laser bonding","A. Emamjomeh; P. Wesling; J. Doong; A. Chi; K. Ling; S. Chiao","Tandem Comput. Inc., Cupertino, CA, USA; Tandem Comput. Inc., Cupertino, CA, USA; NA; NA; NA; NA","[1991 Proceedings] Eleventh IEEE/CHMT International Electronics Manufacturing Technology Symposium","","1991","","","21","26","Results are presented for a set of screening experiments used to characterize a process for performing inner-lead bonding (ILB) using a continuous-wave (CW) laser. Three key parameters were selected for initial analysis: the force exerted by the bonding stylus; the laser power; and the on-time of the laser. Using a Box-Behnken statistical design for the experiment, an efficient set of tests was derived. Four sets of experimental samples were fabricated, to be used for the as-fabricated testing, high-temperature aging, and thermal cycling. Only the as-fabricated results are reported here. Using a design-of-experiments software package for the PC, a quadratic equation was derived from the data explicitly defining the relationships among the three parameters and relating them to bond strength. Using the techniques of surface response methodology, a response surface was created, showing the optimum operating region for the ILB process based on the parameter range chosen. The initial screening experiments show that good bond strength occurs in a range of constant energy (power times on-time) for the CW laser bonder; in the range of the three parameters tested, bond strength was optimized at forces of 20 grams, a power of 4.3 W, and laser on-time of 150 ms.<<ETX>>","","0-7803-0155","10.1109/IEMT.1991.279738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=279738","","Bonding;Power lasers;Response surface methodology;Optical design;Equations;Calibration;Laser stability;Application software;Circuit testing;Sun","laser beam applications;tape automated bonding","process window;TAB;process characterization;laser bonding;screening experiments;inner-lead bonding;ILB;force;bonding stylus;laser power;on-time;Box-Behnken statistical design;set of tests;experimental samples;as-fabricated testing;high-temperature aging;thermal cycling;design-of-experiments software package;quadratic equation;bond strength;surface response methodology;response surface;optimum operating region;CW laser bonder;4.3 W;150 ms","","","17","","","","","","IEEE","IEEE Conferences"
"Grey-box modelling of a motorcycle shock absorber","M. Liberati; A. Beghi; S. Mezzalira; S. Peron","Aprilia Racing Dept., Italy; NA; NA; NA","2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601)","","2004","1","","755","760 Vol.1","There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software is essentially multi body simulation software that requires the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modelling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior. In this paper, a grey-box model of a racing motorcycle mono tube shock absorber is proposed. It consists of a nonlinear parametric model and a black-box, neural network based model. The absorber model has been implemented in a numerical simulation environment, and it has been validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.","0191-2216","0-7803-8682","10.1109/CDC.2004.1428748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428748","","Motorcycles;Shock absorbers;Virtual prototyping;Testing;Optimization;Availability;Vehicles;MONOS devices;Parametric statistics;Neural networks","motorcycles;shock absorbers;neural nets;software prototyping;vehicle dynamics","motorcycle shock absorber;grey-box modelling;virtual prototyping tools;multi body simulation software;complex nonlinear components;motorcycle suspension systems;neural network based model","","6","16","","","","","","IEEE","IEEE Conferences"
"A method for optimal pricing of electric supply including transmission system considerations","M. Muchayi; M. E. El-Hawary","Dept. of Electr. Eng., Tech. Univ. Nova Scotia, Halifax, NS, Canada; NA","CCECE '97. Canadian Conference on Electrical and Computer Engineering. Engineering Innovation: Voyage of Discovery. Conference Proceedings","","1997","1","","293","296 vol.1","Determining real-time electricity rate structures is currently receiving a great deal of attention. In this paper, a strategy for pricing electricity supply is formulated and evaluated. Unlike other methods, which use only the variation of fuel cost for generation to estimate the rate structures, the proposed pricing algorithm incorporates the optimal allocation of transmission system operating costs based on time-of-use pricing. The transmission costs are obtained by assigning a price to each unit of power flow in the network. The assignment does not discriminate between participants located at differing parts of the network. The real-time pricing reflects the instantaneous cost of production and functions as a load management tool because this interacts with consumer behavior. The demand for power flows and transmission on an electricity supply system, like the demand for any bundle of economic goods, depends upon the assigned transmission prices, together with the economic benefit to the consumer. It is assumed that there are no privately owned generating plants and that all plants and transmission lines are operated by the utility. The modeling scheme is applied to the IEEE standard 5, 14, 30 and 57 bus power systems and involves solving a modified optimal power flow problem iteratively using the MINOS package. It is concluded that the method has wide potential application in electricity supply pricing.","0840-7789","0-7803-3716","10.1109/CCECE.1997.614847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614847","","Pricing;Cost function;Load flow;Power system economics;Power generation economics;Fuel economy;Power system modeling;Production;Load management;Consumer behavior","economics;transmission network calculations;tariffs;power system analysis computing;load flow;optimisation;software packages","optimal electricity pricing method;transmission system considerations;real-time electricity rate structures;pricing algorithm;operating costs;time-of-use pricing;transmission costs;economic benefit;IEEE-test systems;MINOS package;modified optimal power flow problem;electricity supply pricing;computer simulation","","2","11","","","","","","IEEE","IEEE Conferences"
"New contrasts for blind separation of non iid sources in the convolutive case","M. Castella; J. Pesquet; A. P. Petropulu","IGM and URA-CNRS 820, Universit&#x00E9; Marne-la-Vall&#x00E9;e, 5 bd Descartes, 77454 CEDEX 2, France; IGM and URA-CNRS 820, Universit&#x00E9; Marne-la-Vall&#x00E9;e, 5 bd Descartes, 77454 CEDEX 2, France; ECE Department, Drexel University, Philadelphia, PA 19104, USA","2002 11th European Signal Processing Conference","","2002","","","1","4","This paper deals with the separation of convolutive mixtures of independent source signals. Starting from a frequency point of view, we consider a joint diagonalization criterion. Integration of the latter over the frequency domain leads to contrasts which are valid for both iid and non iid sources. A generalization of these contrasts is proposed: it aims at obtaining contrasts with improved statistical performances. A link with existing time-domain contrasts is established. A simple gradient based method for the optimization of the contrasts is proposed and evaluated through simulation tests.","2219-5491","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072060","","Computer aided software engineering;Vectors;Deconvolution","blind source separation;convolution;FIR filters;frequency-domain analysis;gradient methods;independent component analysis;optimisation;time-domain analysis","nonlid source blind separation;convolutive mixture;independent source signals;joint diagonalization criterion;frequency domain;improved statistical performance;time-domain contrast optimization;gradient method;independent component analysis;finite impulse response filter","","","12","","","","","","IEEE","IEEE Conferences"
"Requirements and design specification of distributed systems: the lift problem","M. Broy","Fac. of Math. & Inf., Passau Univ., West Germany","[1988] Proceedings. Workshop on the Future Trends of Distributed Computing Systems in the 1990s","","1988","","","164","173","The author proposes a stepwise refinement through several levels of abstracting the design of distribution programs in a formal setting. A requirement specification is formulated, and a design specification is given and proved to guarantee the properties formulated in the requirement specification. On the basis of the design specification an abstract program is derived and proved. More machine-oriented optimized versions are obtained.<<ETX>>","","0-8186-0897","10.1109/FTDCS.1988.26695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=26695","","Control systems;Interleaved codes;Concurrent computing;Application software;Operating systems;Concrete;System testing;Production control;Software systems;Process control","distributed processing;software engineering","design specification;distributed systems;lift problem;stepwise refinement;distribution programs;requirement specification;abstract program;machine-oriented optimized versions","","","14","","","","","","IEEE","IEEE Conferences"
"IGBT modelling using HSPICE","Li Zhang; C. Watthanasarn; W. Shepherd","Dept. of Electr. & Electron. Eng., Bradford Univ., UK; NA; NA","V IEEE International Power Electronics Congress Technical Proceedings, CIEP 96","","1996","","","160","169","This paper discusses a simple method for IGBT modelling, which requires only circuit element parameters and does not involve the user in implementing mathematical equations. The structure of the model is comprised of a bipolar junction transistor (BJT) combined with a metal oxide field effect/transistor (MOSFET). The parameters are estimated using an optimization algorithm in HSPICE and the device test data. Results showing a close fit of model output to the experimental data are presented.","","0-7803-3633","10.1109/CIEP.1996.618532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618532","","Insulated gate bipolar transistors;MOSFET circuits;Power MOSFET;Circuit simulation;SPICE;Equations;Semiconductor process modeling;FETs;Switching frequency;Application software","insulated gate bipolar transistors;semiconductor device models;circuit analysis computing;SPICE;parameter estimation;optimisation","IGBT modelling;HSPICE;circuit element parameters;bipolar junction transistor;BJT;metal oxide field effect/transistor;MOSFET;optimization algorithm;parameters estimation","","1","8","","","","","","IEEE","IEEE Conferences"
"Systematic design of a 200 MS/s 8-bit interpolating/averaging A/D converter","J. Vandenbussche; K. Uyttenhove; E. Lauwers; M. Steyaert; G. Gielen","Dept. of Electr. Eng., Katholieke Univ., Leuven, Heverlee, Belgium; Dept. of Electr. Eng., Katholieke Univ., Leuven, Heverlee, Belgium; Dept. of Electr. Eng., Katholieke Univ., Leuven, Heverlee, Belgium; Dept. of Electr. Eng., Katholieke Univ., Leuven, Heverlee, Belgium; Dept. of Electr. Eng., Katholieke Univ., Leuven, Heverlee, Belgium","Proceedings 2002 Design Automation Conference (IEEE Cat. No.02CH37324)","","2002","","","449","454","The systematic design of a high-speed, high-accuracy Nyquist-rate A/D converter is proposed. The presented design methodology covers the complete flow and is supported by software tools. A generic behavioral model is used to explore the A/D converter's specifications during high-level design and exploration. The inputs to the flow are the specifications of the AID converter and the technology process. The result is a generated layout and the corresponding extracted behavioral model. The approach has been applied to a real-life test case, where a Nyquist-rate 8-bit 200 MS/s 4-2 interpolating/averaging A/D converter was developed for a WLAN application.","0738-100X","1-58113-461","10.1109/DAC.2002.1012667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012667","","Design methodology;Preamplifiers;Circuit simulation;Hardware design languages;Permission;Error correction codes;Software tools;Circuit testing;Wireless LAN;Simulated annealing","analogue-digital conversion;CMOS integrated circuits;integrated circuit design;integrated circuit layout;circuit CAD;interpolation;circuit optimisation;simulated annealing","200 MS/s 8-bit interpolating/averaging A/D converter;systematic design;high-speed high-accuracy Nyquist-rate A/D converter;design methodology;software tools;generic behavioral model;high-level design;generated layout;extracted behavioral model;WLAN application;CMOS process;global optimization;simulated annealing;0.35 micron;3.3 V;2.5 V;8 bit","","1","14","","","","","","IEEE","IEEE Conferences"
"A testbed for the experimental study of a cylindrical shape robot on rough-terrain","M. Foglia; A. Gentile; G. Reina","Dipt. di Ingegneria Meccanica e Gestionale, Politecnico di Bari, Italy; NA; NA","2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.","","2002","1","","163","168 vol.1","A testbed for the characterization of the dynamic behavior of a cylindrical robot in motion on deformable soil is described. The mobile robot consists of a cylindrical rigid shell which can roll on rough terrain thanks to an internal device. The test-bench was designed and optimized to experimentally test the interaction between the external shell and the soil with different mechanical characteristics. A LabView based software to lead and acquire the necessary data for the experiments was developed. Pc-based boards with general purposes A/D I/O ports were used for the rotative and linear encoders and the internal device d.c. motor. The goal is to build a big sensor to estimate the forces system arising from the interaction between a rigid cylindrical shell and rough terrain. Experimental results show the testbed can give a good measure of the travelling resistance that the mini-vehicle meets on rough terrain.","","0-7803-7657","10.1109/ICIT.2002.1189883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189883","","Testing;Shape;Mobile robots;Wheels;Soil;Stress;Motion control;Design optimization;Mechanical sensors;Sensor phenomena and characterization","mobile robots;robot dynamics;integration","cylindrical shape robot;rough terrain;testbed;dynamic behavior;deformable soil;cylindrical rigid shell;mechanical characteristics;LabView based software;rotative encoders;linear encoders;forces system estimation;travelling resistance;mini-vehicle;terramechanics","","","14","","","","","","IEEE","IEEE Conferences"
"Microarchitecture development via metropolis successive platform refinement","D. Densmore; S. Rekhi; A. Sangiovanni-Vincentelli","California Univ., Berkeley, CA, USA; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","346","351 Vol.1","Productivity data for IC designs indicates an exponential increase in design time and cost with the number of elements that are to be included in a device. Present applications require the development of complex systems to support novel functionality. To cope with these difficulties, we need to change radically the present design methodology to allow for extensive re-use, early verification in the design cycle, pervasive use of software, and architecture-level optimization. Platform-based design as defined in A. Sangiovanni-Vincentelli (2002), has these characteristics. We present the application of this methodology to a complex industrial application provided by Cypress Semiconductor. In this case study, we focus on a particular aspect of this methodology that eases considerably the verification process: successive refinement. We compare this approach versus a parallel team of designers who developed the IC using standard design approaches.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268871","","Microarchitecture;Application software;Design methodology;Refining;Computational modeling;Space exploration;Mechanical factors;Productivity;Costs;Design optimization","integrated circuit design;memory architecture;circuit simulation","microarchitecture development;metropolis successive platform refinement;IC designs;architecture-level optimization;platform-based design;Cypress Semiconductor;verification process","","3","8","","","","","","IEEE","IEEE Conferences"
"Hardware implementation of genetic algorithms using FPGA","Wallace Tang; Leslie Yip","Dept. of Electron. Eng., City Univ. of Hong Kong, Kowloon, China; Dept. of Electron. Eng., City Univ. of Hong Kong, Kowloon, China","The 2004 47th Midwest Symposium on Circuits and Systems, 2004. MWSCAS '04.","","2004","1","","I","549","In this paper, a hardware implementation of genetic algorithm using field-programmable gate arrays (FPGAs) is described and implemented. Such development can greatly improve the speed of genetic algorithm by the hardware parallel and pipelined architectures. In our design, various configurations of parallelization are available with a PCI board based design, which further helps in forming a fast optimization tool for real-world applications.","","0-7803-8346","10.1109/MWSCAS.2004.1354049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1354049","","Hardware;Genetic algorithms;Field programmable gate arrays;Application specific integrated circuits;Algorithm design and analysis;Application software;Prototypes;Testing;Random access memory;Read-write memory","field programmable gate arrays;genetic algorithms;circuit optimisation;parallel architectures;pipeline processing;logic design;peripheral interfaces","hardware implementation;genetic algorithms;FPGA;field programmable gate arrays;hardware parallel architectures;pipelined architectures;PCI board based design","","12","5","","","","","","IEEE","IEEE Conferences"
"Assignment of orthologous genes via genome rearrangement","Xin Chen; Jie Zheng; Zheng Fu; Peng Nan; Yang Zhong; S. Lonardi; Tao Jiang","Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; NA; NA; NA; NA","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2005","2","4","302","315","The assignment of orthologous genes between a pair of genomes is a fundamental and challenging problem in comparative genomics. Existing methods that assign orthologs based on the similarity between DNA or protein sequences may make erroneous assignments when sequence similarity does not clearly delineate the evolutionary relationship among genes of the same families. In this paper, we present a new approach to ortholog assignment that takes into account both sequence similarity and evolutionary events at a genome level, where orthologous genes are assumed to correspond to each other in the most parsimonious evolving scenario under genome rearrangement. First, the problem is formulated as that of computing the signed reversal distance with duplicates between the two genomes of interest. Then, the problem is decomposed into two new optimization problems, called minimum common partition and maximum cycle decomposition, for which efficient heuristic algorithms are given. Following this approach, we have implemented a high-throughput system for assigning orthologs on a genome scale, called SOAR, and tested it on both simulated data and real genome sequence data. Compared to a recent ortholog assignment method based entirely on homology search (called INPARANOID), SOAR shows a marginally better performance in terms of sensitivity on the real data set because it is able to identify several correct orthologous pairs that are missed by INPARANOID. The simulation results demonstrate that SOAR, in general, performs better than the iterated exemplar algorithm in terms of computing the reversal distance and assigning correct orthologs.","1545-5963;1557-9964;2374-0043","","10.1109/TCBB.2005.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541983","Ortholog;paralog;gene duplication;genome rearrangement;reversal;comparative genomics.","Genomics;Bioinformatics;Sequences;Genetic mutations;DNA;Proteins;Heuristic algorithms;Partitioning algorithms;System testing;Computational modeling","genetics;DNA;proteins;molecular biophysics;molecular configurations;optimisation;biology computing","orthologous genes;genome rearrangement;comparative genomics;DNA sequences;protein sequences;sequence similarity;evolutionary events;optimization;minimum common partition;maximum cycle decomposition;efficient heuristic algorithms;SOAR;signed reversal distance","Algorithms;Animals;Chromosomes, Human, X;Computational Biology;Databases, Genetic;Evolution, Molecular;Gene Rearrangement;Genome;Genomics;Humans;Mice;Rats;Sequence Alignment;Sequence Analysis, DNA;Sequence Homology, Nucleic Acid;Software;X Chromosome","60","28","","","","","","IEEE","IEEE Journals & Magazines"
"Constrained mixture modeling of intrinsically low-dimensional distributions","J. P. Zwart; B. Krose","Dept. of Comput. Syst., Amsterdam Univ., Netherlands; NA","Proceedings 15th International Conference on Pattern Recognition. ICPR-2000","","2000","2","","610","613 vol.2","We introduce a way of modeling distributions with a low latent dimensionality our method allows for a strict control of the properties of the mapping between the latent and the feature space. Usually, as in for example generative topographic mapping, this mapping is constructed through the maximization of the log likelihood of the data set. However, if the data set is supervised, in the sense that we know the corresponding latent vector value for each feature vector; it is more sensible to use same regression method for finding the mapping in advance. The mapping is then fixed during optimization of the log likelihood of the data set. It is concluded that in terms of log likelihood the methods are comparable. The advantages however lie in the better understanding of the properties of the mapping and a clear interpretation of the latent variables.","1051-4651","0-7695-0750","10.1109/ICPR.2000.906148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=906148","","Computer science;Equations;Physics;Laboratories;Pattern recognition;Probability density function;Testing;Training data;Application software;Neural networks","optimisation;probability;pattern recognition;radial basis function networks;learning (artificial intelligence)","constrained mixture modeling;intrinsically low-dimensional distributions;GTM;log likelihood;latent vector value;feature vector;regression method;generative topographic mapping","","","6","","","","","","IEEE","IEEE Conferences"
"Pickup and delivery with time windows: algorithms and test case generation","H. C. Lau; Z. Liang","Sch. of Comput., Nat. Univ. of Singapore, Singapore; NA","Proceedings 13th IEEE International Conference on Tools with Artificial Intelligence. ICTAI 2001","","2001","","","333","340","In the pickup and delivery problem with time windows (PDPTW), vehicles have to transport loads from origins to destinations respecting capacity and time constraints. In this paper, we present a two-phase method to solve the PDPTW. In the first phase, we apply a novel construction heuristics to generate an initial solution. In the second phase, a tabu search method is proposed to improve the solution. Another contribution of this paper is a strategy to generate good problem instances and benchmarking solutions for PDPTW, based on Solomon's benchmark test cases for VRPTW. Experimental results show that our approach yields very good solutions when compared with the benchmarking solutions.","","0-7695-1417","10.1109/ICTAI.2001.974481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=974481","","Computer aided software engineering;Vehicles;Benchmark testing;Transportation;Neural networks;Time factors;Search methods;Constraint optimization;Logistics;Routing","search problems;transportation","Pickup and Delivery Problem;time windows;fleet of vehicles;transportation requests;pickup and delivery;Vehicle Routing Problem with Time Window;tabu search;construction heuristics","","13","17","","","","","","IEEE","IEEE Conferences"
"Incorporating compiler feedback into the design of ASIPs","F. Onion; A. Nicolau; N. Dutt","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA","Proceedings the European Design and Test Conference. ED&TC 1995","","1995","","","508","513","This paper presents a framework for providing feedback from an optimizing compiler into the design of an ASIP (Application Specific Instruction-set Processor). The optimizing compiler is used to assess the hardware needs of a suite of applications to which the ASIP is to be tuned. By incorporating the compiler into the design process, the design space is increased as more information is provided at an earlier stage during the design process. Our initial study involves detecting potentially chainable operation sequences using scheduling techniques developed for exploiting instruction-level parallelism. Results of this study are included.<<ETX>>","","0-8186-7039","10.1109/EDTC.1995.470353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470353","","Feedback;Application specific processors;Hardware;Process design;Optimizing compilers;Application software;Processor scheduling;Computer science;Parallel processing;Application specific integrated circuits","processor scheduling;scheduling;feedback;circuit layout CAD;digital signal processing chips;microprocessor chips;high level synthesis","compiler feedback;ASIP design;optimizing compiler;application specific instruction-set processor;chainable operation sequences;scheduling techniques;DSP chips;CAD","","4","12","","","","","","IEEE","IEEE Conferences"
"Effective use of customized AutoSAT templates in a foundry environment for better recipe set-up","S. W. Choi-Yoke; M. Small; R. Ghaskadvi; S. Jawaharlal; G. Yin","NA; NA; NA; NA; NA","2001 IEEE/SEMI Advanced Semiconductor Manufacturing Conference (IEEE Cat. No.01CH37160)","","2001","","","145","149","Efficient defect detection and analysis is critical to the success of any fab. For a yield management group, it is essential to capture defects of interest and to eliminate defects that do not impact yield. In a foundry where multiple devices are manufactured, it is crucial to have an inspection strategy that is transferable to many devices. Such a strategy also needs to be effective in capturing the same defect types by process layer. This paper discusses the challenges involved in creating such a strategy and steps taken to successfully combat it through a collaboration between Chartered Silicon Partnership (CSP) and KLA-Tencor (K-T). A KLA-Tencor 2139 brightfield inspection tool was used for defect image capture. Segmented Auto Threshold (SAT), a tool available on the 2139, is used to eliminate nuisance defects during inspection. SAT is a powerful tool, but the inspection recipes can be complex. Another tool on the 2139, AutoSAT, may be used to facilitate creation of such sophisticated inspections. AutoSAT samples defects on the wafer and prompts the user to classify them as real, killer or nuisance. Once this is done, AutoSAT uses a predefined SAT template set to optimize the thresholds to suppress nuisance and enhance critical defect capture. This approach is particularly useful in a foundry environment where the yield management group is faced with a variety of devices, requiring many inspections.","1078-8743","0-7803-6555","10.1109/ASMC.2001.925638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925638","","Foundries;Inspection;Etching;Silicon;Optical scattering;Pixel;Semiconductor device noise;Manufacturing;Collaboration;Delay","inspection;integrated circuit yield;integrated circuit testing;production testing;fault location;software tools;production engineering computing","customized AutoSAT templates;foundry environment;recipe set-up;defect detection;defect analysis;yield management;defect capture;defect elimination;defect yield impact;multiple device manufacture;transferable inspection strategy;defect types;process layer;brightfield inspection tool;defect image capture;Segmented Auto Threshold tool;SAT tool;nuisance defects;inspection;inspection recipe complexity;AutoSAT;AutoSAT defect sampling;defect classification;real defects;killer defects;critical defect capture","","","","","","","","","IEEE","IEEE Conferences"
"Optimal resource distribution to manufactory model","V. P. Grigorjev; V. N. Kaljuta; K. A. Kiselev","Tomsk Polytech. Univ., Russia; Tomsk Polytech. Univ., Russia; Tomsk Polytech. Univ., Russia","Proceedings. The 9th Russian-Korean International Symposium on Science and Technology, 2005. KORUS 2005.","","2005","","","871","873","Represented mathematical model of optimal resource distribution to manufactory based on modification of inter-industry balance model. Testing has been performed on food manufactory data.","","0-7803-8943","10.1109/KORUS.2005.1507925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507925","","Resource management;Mathematical model;Production;Testing;Performance evaluation;Mathematics;Cybernetics;Electronic mail;Performance analysis;Software standards","food manufacturing;manufacturing resources planning;mathematical analysis;production management;resource allocation;optimisation","optimal resource distribution;manufactory model;mathematical model;interindustry balance model;food manufactory","","","4","","","","","","IEEE","IEEE Conferences"
"PCB assembly sequence and feeder assignment problem for the case of Tchebyshev robot arm motion. I. Basic problem","Nguyen Van Hop; M. T. Tabucanon; Do Quang Minh","Fac. of Sci. & Technol., Assumption Univ., Bangkok, Thailand; NA; NA","Proceedings of the 2000 IEEE International Conference on Management of Innovation and Technology. ICMIT 2000. 'Management in the 21st Century' (Cat. No.00EX457)","","2000","2","","919","924 vol.2","This paper considers a basic problem in printed circuit board (PCB) assembly planning for the case of Tchebyshev robot arm motion in which it will be used for the other problems. In this problem, the assembly sequence is determined when feeder rack travel with infinite speed and the board is fixed. The problem is analyzed and converted into a shortest path problem of a corresponding interval graph that is proved to be NP-complete. Therefore, a heuristic procedure is developed based on network theory and interval graph. This heuristic is tested and found to give the closed optimal solution.","","0-7803-6652","10.1109/ICMIT.2000.916829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916829","","Robotic assembly;Computer aided software engineering;Robot kinematics;Motion planning;Shortest path problem;Robot motion;Printed circuits;Path planning;Network theory (graphs);Testing","printed circuit manufacture;industrial robots;computational complexity;optimisation;assembly planning","PCB assembly sequence;feeder assignment problem;Tchebyshev robot arm motion;PCB assembly planning;feeder rack travel;infinite speed;shortest path problem;interval graph;NP-complete;heuristic procedure;network theory;closed optimal solution;dynamic pick and place;fixed pick and place","","","17","","","","","","IEEE","IEEE Conferences"
"Gauss-Newton approximation to Bayesian learning","F. Dan Foresee; M. T. Hagan","Lucent Technol., Oklahoma City, OK, USA; NA","Proceedings of International Conference on Neural Networks (ICNN'97)","","1997","3","","1930","1935 vol.3","This paper describes the application of Bayesian regularization to the training of feedforward neural networks. A Gauss-Newton approximation to the Hessian matrix, which can be conveniently implemented within the framework of the Levenberg-Marquardt algorithm, is used to reduce the computational overhead. The resulting algorithm is demonstrated on a simple test problem and is then applied to three practical problems. The results demonstrate that the algorithm produces networks which have excellent generalization capabilities.","","0-7803-4122","10.1109/ICNN.1997.614194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614194","","Newton method;Least squares methods;Recursive estimation;Bayesian methods;Neural networks;Feedforward neural networks;Cities and towns;Computer networks;Application software;Testing","feedforward neural nets;generalisation (artificial intelligence);approximation theory;optimisation;Hessian matrices;Bayes methods;learning (artificial intelligence)","Gauss-Newton approximation;Bayesian learning;feedforward neural networks;Hessian matrix;Levenberg-Marquardt algorithm;generalization","","211","9","","","","","","IEEE","IEEE Conferences"
"A novel metric for nearest-neighbor classification of hand-written digits","Z. M. Kovacs-V; R. Guerrieri; G. Baccarani","Dipartimento di Elettronica, Inf. e Sistemistica, Bologna Univ., Italy; Dipartimento di Elettronica, Inf. e Sistemistica, Bologna Univ., Italy; Dipartimento di Elettronica, Inf. e Sistemistica, Bologna Univ., Italy","Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems","","1992","","","96","100","Classifiers based on the k-nearest neighbors (k-NN) approach have recently received an increasing attention because of their simple implementation and absence of training. In this technique, the similarity measure used to compute the distance between the stored patterns and the test element is the most crucial part of the method. The paper addresses this issue within the context of recognition of hand-written digits. A novel similarity measure is proposed and used to associate a number to each pair of samples in a suitable N-dimensional space in order to define the distance between two handwritten characters. The proposed similarity measure has been parameterized and the best values of these parameters have been evaluated using suitable statistical approaches. Finally, some results obtained from the classification of digits extracted from a ZIP code database are provided.<<ETX>>","","0-8186-2915","10.1109/ICPR.1992.201730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=201730","","Optical character recognition software;Extraterrestrial measurements;Performance evaluation;Testing;Hardware;Classification algorithms;Character recognition;Neural networks;Spatial databases;Voting","character recognition;learning systems;optimisation;statistical analysis","handwritten digit recognition;character recognition;statistical analysis;nearest-neighbor classification;similarity measure;ZIP code database","","3","15","","","","","","IEEE","IEEE Conferences"
"Design of optimal gains for the proportional integral Kalman filter with application to single particle tracking","O. Y. Bas; B. Shafai; S. P. Linder","Dept. of Electr. & Comput. Eng., Northeastern Univ., Boston, MA, USA; NA; NA","Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304)","","1999","5","","4567","4571 vol.5","In this paper, we introduce an optimal design strategy for the proportional-integral (PI) Kalman filter. The design of the PI Kalman filter involves the design of four matrices: The proportional and integral gains, the fading constant and the integral effect coefficient. The method in this paper provides optimal proportional and integral gains. Guidelines for the design of the fading constant and the integral effect coefficient are discussed as well. The suggested algorithm was tested as a single particle tracking system, for a maneuvering target.","0191-2216","0-7803-5250","10.1109/CDC.1999.833262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=833262","","Fading;Particle tracking;Kalman filters;Integrated circuit noise;Vectors;Application software;System testing;Design engineering;Filtering algorithms;Stochastic processes","state estimation;Kalman filters;filtering theory;optimisation","optimal gain design;proportional integral Kalman filter;single particle tracking;PI Kalman filter;fading constant;integral effect coefficient;optimal state estimation process","","2","10","","","","","","IEEE","IEEE Conferences"
"Design and performance comparison of fuzzy logic based tracking controllers","R. N. Lea; Y. Jani","NASA Johnson Space Center, Houston, TX, USA; NA","NAFIPS/IFIS/NASA '94. Proceedings of the First International Joint Conference of The North American Fuzzy Information Processing Society Biannual Conference. The Industrial Fuzzy Control and Intellige","","1994","","","340","344","Several camera tracking controllers based on fuzzy logic principles have been designed and tested in software simulation in the software technology at the Johnson Space Center. The fuzzy logic based controllers utilize range measurement and pixel positions from the image as input parameters and provide pan and tilt gimble rate commands as output. Two designs of the rule base and tuning process applied to the membership functions are discussed in light of optimizing performance for a v-bar approach testcase. The two controller designs are compared in terms of responsiveness, and ability to maintain the object in the field-of-view of the camera.<<ETX>>","","0-7803-2125","10.1109/IJCF.1994.375090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=375090","","Fuzzy logic;Cameras;Radar tracking;Space technology;Extraterrestrial measurements;Fuzzy control;Space stations;Logic testing;Pixel;Control systems","fuzzy logic;controllers;fuzzy control","performance comparison;fuzzy logic based tracking controllers;camera tracking controllers;software simulation;software technology;input parameters;gimble rate commands;rule base;tuning process;membership functions;optimizing performance;v-bar approach;responsiveness","","1","9","","","","","","IEEE","IEEE Conferences"
"A hybrid model for invariant and perceptual texture mapping","Huizhong Long; Wee Kheng Leow","Dept. of Comput. Sci., Nat. Univ. of Singapore, Singapore; Dept. of Comput. Sci., Nat. Univ. of Singapore, Singapore","Object recognition supported by user interaction for service robots","","2002","1","","135","138 vol.1","Texture is an important visual feature for computer vision tasks. In applications such as image retrieval and computer image understanding, texture similarity should be measured in a manner that is invariant to texture scale and orientation, as well as consistent with human perception. However, most existing computational features and similarity measures are not perceptually consistent. A solution is to map textures into an invariant and perceptual space such that similarity measured in the space is perceptually consistent. The paper presents a hybrid method, using a convolutional neural network and SVM, to perform the invariant and perceptual mapping. Test results show that it's overall performance is better than that of an individual neural network. and a SVM.","1051-4651","0-7695-1695","10.1109/ICPR.2002.1044631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044631","","Support vector machines;Extraterrestrial measurements;Performance evaluation;Neural networks;Image retrieval;Testing;Kernel;Application software;Computer science;Drives","image texture;computer vision;learning automata;multilayer perceptrons;iterative methods;optimisation","hybrid model;visual feature;invariant texture mapping;perceptual texture mapping;three-layer network;support vector machines;computer vision;image retrieval;computer image understanding;texture similarity;convolutional neural network;SVM","","1","14","","","","","","IEEE","IEEE Conferences"
"To reject or not to reject: that is the question-an answer in case of neural classifiers","C. De Stefano; C. Sansone; M. Vento","Facolta di Ingegneria, Sannio Univ., Benevento, Italy; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","","2000","30","1","84","94","A method defining a reject option that is applicable to a given 0-reject classifier is proposed. The reject option is based on an estimate of the classification reliability, measured by a reliability evaluator /spl Psi/. Trivially, once a reject threshold /spl sigma/ has been fixed, a sample is rejected if the corresponding value of /spl Psi/ is below /spl sigma/. Obviously, as /spl sigma/ represents the least tolerable classification reliability level, when its value varies the reject option becomes more or less severe. In order to adapt the behavior of the reject option to the requirements of the considered application domain, a function P characterizing the reject option's adequacy to the domain has been introduced. It is shown that P can be expressed as a function of /spl sigma/ and, consequently, the optimal value for /spl sigma/ is defined as the one which maximizes the function P. The method for determining the optimal threshold value is independent of the specific 0-reject classifier, while the definition of the reliability evaluators is related to the classifier's architecture. General criteria for defining appropriate reliability evaluators within a classification paradigm are illustrated in the paper and are based on the localization, in the feature space, of the samples that could be classified with a low reliability. The definition of the reliability evaluators for three popular architectures of neural networks (backpropagation, learning vector quantization and probabilistic network) is presented. Finally, the method has been tested with reference to a complex classification problem with data generated according to a distribution-of-distributions model.","1094-6977;1558-2442","","10.1109/5326.827457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=827457","","Computer aided software engineering;Neural networks;Error analysis;Vector quantization;Testing;Costs;Error correction","neural net architecture;pattern classification;reliability;optimisation","neural classifiers;sample reject option;0-reject classifier;classification reliability estimation;reliability evaluators;reject threshold;adequacy characterization;optimal value;function maximization;optimal threshold value;classifier architecture;sample localization;feature space;neural network architectures;backpropagation;learning vector quantization;probabilistic neural net;distribution-of-distributions model","","66","28","","","","","","IEEE","IEEE Journals & Magazines"
"WIND-FLEX: developing a novel testbed for exploring flexible radio concepts in an indoor environment","A. Polydros; J. Rautio; G. Razzano; H. Bogucka; D. Ragazzi; A. Mammela; M. Benedix; M. Lobeira; L. Agarossi","Athens Univ., Greece; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Communications Magazine","","2003","41","7","116","122","We describe the essential features of a novel testbed, code-named WIND-FLEX and developed under the auspices of the EU IST research program, whose primary goal has been to define and explore concepts related to radio flexibility, with application to an OFDM-based short-range indoor radio transceiver design. The emphasis has been on the profitable use of software and DSP tools for the purpose of demonstrating adaptivity and reconfigurability features, primarily for the lower layers (i.e., PHY and DLC/MAC layers). The topics covered briefly herein include the meaning of the terms in the present context, their instantiation in the design, their contribution to an optimized low-transmit-power scheme, the novel elements required in order to achieve it, the contribution to a guaranteed QoS philosophy, and the relationship of the testbed to broader software-defined-radio concepts.","0163-6804;1558-1896","","10.1109/MCOM.2003.1215648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1215648","","Testing;Indoor environments;Modems;Adaptive systems;Concatenated codes;Turbo codes;Runtime;Transceivers;Hardware;Frequency division multiplexing","indoor radio;telecommunication equipment testing;OFDM modulation;transceivers;quality of service;software radio","flexible radio concepts;indoor environment;testbed;WIND-FLEX;OFDM-based short-range indoor radio transceiver;adaptivity;reconfigurability;low-transmit-power scheme;QoS philosophy;software-defined-radio concepts","","21","9","","","","","","IEEE","IEEE Journals & Magazines"
"Force controlled and teleoperated endoscopic grasper for minimally invasive surgery-experimental performance evaluation","J. Rosen; B. Hannaford; M. P. MacFarlane; M. N. Sinanan","Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; NA; NA; NA","IEEE Transactions on Biomedical Engineering","","1999","46","10","1212","1221","Minimally invasive surgery generates new user interfaces which create visual and haptic distortion when compared to traditional surgery. In order to regain the tactile and kinesthetic information that is lost, a computerized force feedback endoscopic surgical grasper (FREG) was developed with computer control and a haptic user interface. The system uses standard unmodified grasper shafts and tips. The FREG can control grasping forces either by surgeon teleoperation control, or under software control. The FREG performance was evaluated using an automated palpation function (programmed series of compressions) in which the grasper measures mechanical properties of the grasped materials. The material parameters obtained from measurements showed the ability of the FREG to discriminate between different types of normal soft tissues (small bowel, lung, spleen, liver, colon, and stomach) and different kinds of artificial soft tissue replication materials (latex/silicone) for simulation purposes. In addition, subjective tests of ranking stiffness of silicone materials using the FREG teleoperation mode showed significant improvement in the performance compared to the standard endoscopic grasper. Moreover, the FREG performance was closer to the performance of the human hand than the standard endoscopic grasper. The FREG as a tool incorporating the force feedback teleoperation technology may provide the basis for application in telesurgery, clinical endoscopic surgery, surgical training, and research.","0018-9294;1558-2531","","10.1109/10.790498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790498","","Force control;Minimally invasive surgery;Surges;Automatic control;Biological materials;User interfaces;Haptic interfaces;Computer interfaces;Force feedback;Biological tissues","haptic interfaces;surgery;biomedical equipment;force feedback;telecontrol;touch (physiological);biocontrol;computerised control","force controlled teleoperated endoscopic grasper;minimally invasive surgery;experimental performance evaluation;haptic distortion;visual distortion;kinesthetic information;tactile information;computer control;haptic user interface;software control;automated palpation function;mechanical properties measurement;normal soft tissues discrimination;small bowel;lung;spleen;liver;colon;stomach;artificial soft tissue replication materials;latex;silicone;force feedback teleoperation technology","Algorithms;Analysis of Variance;Animals;Digestive Physiology;Elasticity;Endoscopes;Equipment Design;Feedback;Humans;Latex;Liver;Lung;Materials Testing;Palpation;Pilot Projects;Robotics;Silicones;Spleen;Stress, Mechanical;Surgical Procedures, Minimally Invasive;Swine;Therapy, Computer-Assisted;User-Computer Interface","211","27","","","","","","IEEE","IEEE Journals & Magazines"
"Cryogenic dual-mode resonator for a fly-wheel oscillator for a caesium frequency standard","M. E. Tobar; J. G. Hartnett; E. N. Ivanov; D. Cros; P. Bilski","Univ. of Western Australia, Crawley, WA, Australia; Univ. of Western Australia, Crawley, WA, Australia; Univ. of Western Australia, Crawley, WA, Australia; NA; NA","IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control","","2002","49","10","1349","1355","A dual-mode, sapphire-loaded cavity (SLC) resonator has been designed and optimized with the aid of finite element software. The resonance frequency was designed to be near the frequency of a Cs atomic frequency standard. Experimental tests are shown to agree very well with calculations. The difference frequency of two differently polarized modes is shown to be a highly sensitive temperature sensor in the 50 to 80 K temperature range. We show that an oscillator based on this resonator has the potential to operate with fractional frequency instability below 10/sup -14/ for measurement times of 1 to 100 seconds. This is sufficient to operate an atomic clock at the quantum projection noise limit.","0885-3010;1525-8955","","10.1109/TUFFC.2002.1041076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041076","","Cryogenics;Oscillators;Atomic clocks;Temperature sensors;Design optimization;Finite element methods;Resonance;Resonant frequency;Testing;Polarization","caesium;frequency stability;frequency standards;atomic clocks;sapphire;cavity resonators;finite element analysis;microwave oscillators","dual-mode resonator;sapphire-loaded cavity resonator;cryogenic resonator;fly-wheel oscillator;finite element software;resonance frequency;Cs atomic frequency standard;polarized modes;highly sensitive temperature sensor;frequency instability;atomic clock;quantum projection noise limit;50 to 80 K;1 to 100 sec;Cs","Cesium;Cold;Equipment Design;Equipment Failure Analysis;Equipment and Supplies;Models, Theoretical;Radio Waves;Reference Standards;Reproducibility of Results;Sensitivity and Specificity;Software;Time","7","13","","","","","","IEEE","IEEE Journals & Magazines"
"Blackboard architecture for intelligent control system","D. A. Linkens; M. F. Abbod; A. Browne","Dept. of Autom. Control & Syst. Eng., Sheffield Univ., UK; NA; NA","1999 7th IEEE International Conference on Emerging Technologies and Factory Automation. Proceedings ETFA '99 (Cat. No.99TH8467)","","1999","2","","1185","1192 vol.2","A blackboard for integrated intelligent control systems (BIICS) software architecture has been developed. The system is designed to simultaneously support multiple heterogeneous intelligent paradigms, such as neural networks, expert systems, fuzzy logic and genetic algorithms. It is shown how such paradigms are assimilated into the software architecture. This paper describes the BIICS system as it utilises intelligent control techniques (neuro-fuzzy and genetic optimisation) for controlling a cryogenic plant used for superconductor testing by cooling the test samples to temperatures below 100 K.","","0-7803-5670","10.1109/ETFA.1999.813123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=813123","","Intelligent control;Software architecture;System testing;Computer architecture;Algorithm design and analysis;Intelligent networks;Neural networks;Expert systems;Fuzzy logic;Genetic algorithms","intelligent control;fuzzy neural nets;expert systems;blackboard architecture;software architecture;genetic algorithms;cryogenics;temperature control","blackboard architecture;intelligent control systems;software architecture;fuzzy neural networks;expert systems;genetic algorithms;cryogenic plant;cooling","","","11","","","","","","IEEE","IEEE Conferences"
"DFY/DFM - design for yield and manufacturability (industrial tutorial)","A. Ripp; R. Sommer; E. Hennig; M. Pronath","MunEDA GmbH, DE; Infineon Technologies AG, DE; Infineon Technologies AG, DE; MunEDA GmbH, DE","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","xxxi","xxxi","Summary form only given, as follows. The tutorial presents an introduction into ""DfY/DfM - Design for Yield and Manufacturability"" covering basics of analogue circuit simulation, statistical analysis and design centering from both methodology/implementation as well as from the industrial application side. The tutorial presents the following six topics: introduction into DfY/DfM, basics of analogue circuit simulation, methodology for statistical circuit analysis and yield optimisation, software solutions and design flow integration, design flow specific industrial applications and use cases closing with an outlook on actual and future challenges in the DfY/DfM area regarding a global design environment. Intended audience: analogue- and mixed-signal circuit designers, CAD- and design-support engineers (library management, technology migration and design reuse, process characterisation)","1530-1591","0-7695-2085","10.1109/DATE.2004.1268807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268807","","Tutorial;Design for manufacture;Manufacturing industries;Algorithm design and analysis;Hardware;Application software;Design automation;Silicon;Circuit simulation","","","","","","","","","","","IEEE","IEEE Conferences"
"Teaching the manufacturing design cycle in a project course","J. C. Anderson","Purdue Univ., West Lafayette, IN, USA","32nd Annual Frontiers in Education","","2002","2","","F4D","F4D","One of problems associated with project courses is the short time available to complete the course. This typically limits the project to a segment of the total manufacturing design cycle. For a manufacturing project such as that to produce a plastic part, this translates to doing the product design or the tooling design, but not both. This paper describes an innovative approach to the problem using the injection molding process as an example of a manufacturing process. Students use commercial solid modeling software to do the product design of a fairly simple part, for example a golf divot tool. The product design is then used to develop tooling (molds), to simulate structural performance and molding characteristics, and generate the NC code to machine the tooling. Once the tooling is finished it is placed on a lab injection molder and the process is tested and optimized. The entire lab experience is completed in approximately 30 hours of laboratory time, The equipment and software used to complete the project are typical of that available in undergraduate engineering facilities.","0190-5848","0-7803-7444","10.1109/FIE.2002.1158228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1158228","","Education;Product design;Plastics;Injection molding;Manufacturing processes;Solid modeling;Software tools;Character generation;Testing;Laboratories","product development;project engineering;manufacturing processes;engineering education;educational courses;student experiments","manufacturing design cycle teaching;project course;manufacturing project;plastic product development;injection molding process;manufacturing process;commercial solid modeling software;golf divot tool;structural performance;molding characteristics;NC code;machine tooling;student laboratory experiments;undergraduate engineering facilities","","","7","","","","","","IEEE","IEEE Conferences"
"Fuzzy control structures in multiple parameter systems: an application in handwritten address interpretation systems","K. Inakiev; V. Govindaraju","Dept. of Comput. Sci. & Eng., Univ. at Buffalo, Amherst, NY, USA; NA","18th International Conference of the North American Fuzzy Information Processing Society - NAFIPS (Cat. No.99TH8397)","","1999","","","918","922","Most practical software systems are a conglomerate of many modules, where each module has its own parameters that control the accuracy of the module. While each individual module can be optimized by tuning the relevant parameters, it is a non-trivial task to optimize the entire system. When the number of modules and the parameters are few, manual choosing of all parameters is possible by a ""trial and error"" mechanism. However, when the modules are many, other methods have to be adopted. We use fuzzy set technology for the purpose in the particular application of a Handwritten Address Interpretation System. We use a fuzzy measure based on the topology of the ""blind"" set of handwritten addresses and a predetermined neighborhood to construct the fuzzy membership function. This allows a nonlinear partitioning of the results to maximize the correct rate. On a test set of 10000 images, the fuzzy methodology accomplishes 5% higher accuracy.","","0-7803-5211","10.1109/NAFIPS.1999.781828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781828","","Fuzzy control;Fuzzy sets;Character recognition;Postal services;Application software;Venus;Computer science;Particle measurements;Marine vehicles;Testing","handwriting recognition;handwritten character recognition;fuzzy set theory;uncertainty handling;fuzzy control","fuzzy control structures;multiple parameter systems;handwritten address interpretation systems;practical software systems;fuzzy set technology;fuzzy measure;blind set;handwritten addresses;predetermined neighborhood;fuzzy membership function;nonlinear partitioning;fuzzy methodology","","","3","","","","","","IEEE","IEEE Conferences"
"Symbolic computation of digital filter transfer function using MATLAB","M. D. Lutovac; D. V. Tosic","Telecommun. & Electron. Inst., Belgrade Univ., Yugoslavia; NA","2002 23rd International Conference on Microelectronics. Proceedings (Cat. No.02TH8595)","","2002","2","","651","654 vol.2","We present an original software DFSYM for symbolic computation of transfer function of digital filters and linear time-invariant discrete-time Systems. The software uses standard MATLAB commands and MATLAB SYMBOLIC TOOLBOX. The transfer function is derived directly from the filter schematic created by DrawFilt. DFSYM finds transfer function of filters with arbitrary real or complex coefficients. Filter designers, practitioners, students, researchers, educators and scientists can use DFSYM to evaluate, validate, document or verify the existing filter realizations or to explore, get insight to and optimize the filters they work on.","","0-7803-7235","10.1109/MIEL.2002.1003342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003342","","Digital filters;Transfer functions;MATLAB;Circuits;Software systems;Software tools;Software standards;Electronic equipment testing;Delay;Adders","symbol manipulation;digital filters;transfer functions;discrete time filters;circuit CAD","symbolic computation;digital filter;transfer function;linear time-invariant discrete-time systems;filter realizations;SYMBOLIC TOOLBOX;DrawFilt;arbitrary real coefficients;arbitrary complex coefficients;MATLAB","","1","3","","","","","","IEEE","IEEE Conferences"
"Automatic beam steering in the CERN PS Complex","B. Autin; G. H. Hemelsoet; M. Martini; E. Wildner","CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland","Proceedings Particle Accelerator Conference","","1995","4","","2178","2180 vol.4","The recombination, transfer and injection of the four beams from the PS Booster to the PS Main Ring, have a high level of intricacy and are a subject of permanent concern for the operation of the PS Injector Complex. These tasks were thus selected as a test bench for the implementation of a prototype of an automatic beam steering system. The core of the system is based on a generic trajectory optimizer, robust enough to cope with imperfect observations. The algorithmic engine is connected to pick-up monitors and corrector magnets and its decision can be validated by the operator through a graphics user interface. Automatic beam steering can only be efficient if the beam optics is fully confirmed by experimental observations, a condition which forces the systematic elimination of errors both in hardware and software.","","0-7803-2934","10.1109/PAC.1995.505490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=505490","","Beam steering;Optical beams;Automatic testing;System testing;Prototypes;Magnetic cores;Robustness;Engines;Magnets;Graphics","proton accelerators;storage rings;synchrotrons;colliding beam accelerators;beam handling techniques;particle beam dynamics;particle optics;accelerator control systems;computerised control","Proton Synchrotron Complex;Booster;Main Ring;automatic beam steering;generic trajectory optimizer;algorithmic engine;beam optics;hardware errors;software errors","","","8","","","","","","IEEE","IEEE Conferences"
"A HW/SW partitioning algorithm for dynamically reconfigurable architectures","J. Noguera; R. M. Badia","Dept. of Comput. Archit., Univ. Politecnica de Catalunya, Barcelona, Spain; NA","Proceedings Design, Automation and Test in Europe. Conference and Exhibition 2001","","2001","","","729","734","""System-On-Chip"" has become a reality, and recently new reconfigurable devices have appeared. However, few efforts have been carried out in order to define HW/SW codesign methodologies and algorithms which address the challenges presented by new reconfigurable devices. In this paper we address this open problem and present a novel HW/SW partitioning algorithm for dynamically reconfigurable architectures. The algorithm is a constructive algorithm, which obtains an initial solution and afterwards tries to optimize it. The HW/SW partitioning is done taking into account the features of the dynamically reconfigurable devices, and its final goal is to minimize the reconfiguration latency. The partitioning algorithm has been implemented and integrated into our developed codesign environment, where several experiments have been carried out. The results obtained demonstrate the benefits of the algorithm.","1530-1591","0-7695-0993","10.1109/DATE.2001.915109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915109","","Heuristic algorithms;Partitioning algorithms;Reconfigurable architectures;Delay;Prefetching;Reconfigurable logic;Scheduling;Logic devices;Runtime;Minimization","reconfigurable architectures;hardware-software codesign;embedded systems;logic partitioning;microprocessor chips","dynamically reconfigurable architectures;hardware-software partitioning algorithm;system-on-chip;hardware-software codesign;constructive algorithm;initial solution;reconfiguration latency;discrete event system specification;design constraints;scheduling;class packing;embedded system","","18","21","","","","","","IEEE","IEEE Conferences"
"Labyrinth: a homogeneous computational medium","F. Furtek; G. Stone; I. Jones","Concurrent Logic Inc., Arlington, MA, USA; NA; NA","IEEE Proceedings of the Custom Integrated Circuits Conference","","1990","","","31.1/1","31.1/4","As a RAM-based reconfigurable logic array, Labyrinth provides the flexibility and malleability of software with the performance of a dedicated circuit. With a single bit register and a half adder per cell, the architecture is optimized for register intensive, massively parallel algorithms. The fine-grained, highly-symmetric architecture scales very naturally and facilitates compact circuit layouts. A 64-cell test chip has been successfully built and tested, and a 4096-cell chip is in the final stages of preparation for fabrication.<<ETX>>","","","10.1109/CICC.1990.124839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=124839","","Registers;Computer architecture;Circuit testing;Reconfigurable logic;Logic arrays;Software performance;Flexible printed circuits;Adders;Parallel algorithms;Fabrication","CMOS integrated circuits;logic arrays;microprocessor chips;parallel architectures","PLD;programmable logic devices;homogeneous computational medium;RAM-based reconfigurable logic array;Labyrinth;single bit register;half adder per cell;massively parallel algorithms;highly-symmetric architecture;compact circuit layouts;64-cell test chip;4096-cell chip","","5","6","","","","","","IEEE","IEEE Conferences"
"Printed straight F antennas for WLAN and Bluetooth","H. Y. D. Yang","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Antennas and Propagation Society International Symposium. Digest. Held in conjunction with: USNC/CNC/URSI North American Radio Sci. Meeting (Cat. No.03CH37450)","","2003","2","","918","921 vol.2","Miniaturized printed-circuit-board antennas are proposed in PCMCIA cards for WLAN and Bluetooth applications. The proposed antenna is a straight F in shape printed on a FR4 substrate together with the rest of the circuit components, providing a low-cost antenna solution. The straight F antenna resembles a printed inverted F antenna, but the inductive tuning arm is in the same side of the capacitive arm, resulting in further reduced overall antenna area. The proposed antenna occupies an area of about 9 mm by 7 mm. Several prototype antennas are designed and fabricated. Reasonable impedance bandwidth and good range of coverage are found.","","0-7803-7846","10.1109/APS.2003.1219384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1219384","","Wireless LAN;Bluetooth;Bandwidth;Antenna accessories;Conducting materials;Resonance;Application software;Shape;Circuit optimization;Prototypes","antenna testing;antenna theory;Bluetooth;wireless LAN;design engineering;antenna radiation patterns;printed circuits;memory cards;mobile antennas;tuning","printed straight F antennas;WLAN;Bluetooth;miniaturized printed-circuit-board antennas;PCMCIA cards;antenna straight F shape;FR4 substrate;circuit components;low-cost antenna;printed inverted F antenna;inductive tuning arm;capacitive arm;antenna area;prototype antenna design;prototype antenna fabrication;impedance bandwidth;antenna coverage range;9 mm;7 mm","","2","3","","","","","","IEEE","IEEE Conferences"
"FPGA implementation of a microcoded elliptic curve cryptographic processor","K. H. Leung; K. W. Ma; W. K. Wong; P. H. W. Leong","Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, China; NA; NA; NA","Proceedings 2000 IEEE Symposium on Field-Programmable Custom Computing Machines (Cat. No.PR00871)","","2000","","","68","76","Elliptic curve cryptography (ECC) has been the focus of much recent attention since it offers the highest security per bit of any known public key cryptosystem. This benefit of smaller key sizes makes ECC particularly attractive for embedded applications since its implementation requires less memory and processing power. In this paper a microcoded Xilinx Virtex based elliptic curve processor is described. In contrast to previous implementations, it implements curve operations as well as optimal normal basis field operations in F(2/sup n/); the design is parameterized for arbitrary n; and it is microcoded to allow for rapid development of the control part of the processor. The design was successfully tested on a Xilinx Virtex XCV300-4 and, for n=113 bits, utilized 1290 slices at a maximum frequency of 45 MHz and achieved a thirty-fold speedup over an optimized software implementation.","","0-7695-0871","10.1109/FPGA.2000.903394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=903394","","Field programmable gate arrays;Elliptic curve cryptography;Security;Public key cryptography;Elliptic curves;Optimal control;Process design;Software testing;Frequency;Design optimization","field programmable gate arrays;firmware;cryptography;microprocessor chips","FPGA implementation;microcoded elliptic curve cryptographic processor;security;public key cryptosystem;microcoded Xilinx Virtex based elliptic curve processor;curve operations;optimal normal basis field operations;Xilinx Virtex XCV300-4;optimized software implementation","","30","20","","","","","","IEEE","IEEE Conferences"
"Experimental results on an inner/outer loop controller for a two-link flexible manipulator","F. Khorami; S. Jain","Sch. of Electr. Eng. & Comput. Sci., Polytech. Univ., Brooklyn, NY, USA; Sch. of Electr. Eng. & Comput. Sci., Polytech. Univ., Brooklyn, NY, USA","Proceedings 1992 IEEE International Conference on Robotics and Automation","","1992","","","742","747 vol.1","Experimental results for end-point positioning of multilink flexible manipulators are considered. This control strategy has been implemented on an experimental test-bed developed in the authors' Laboratory. The advocated approach is based on a two-stage control design. The first stage is an inner-loop nonlinear-based controller corresponding to the rigid body motion of the manipulator. The second stage is an outer control loop based on linear output LQR design. The outer-loop control design results in a nonconvex nonlinear optimization problem. A software package has been developed to solve this problem. The outer-loop controller enhances vibration damping and robustness of the closed-loop dynamics to parameter variations. The measurement utilized for vibration suppression is through an accelerometer attached at the end point of the manipulator.<<ETX>>","","0-8186-2720","10.1109/ROBOT.1992.220280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=220280","","Control design;Manipulator dynamics;Vibration measurement;Testing;Motion control;Design optimization;Software packages;Vibration control;Damping;Robust control","control system synthesis;distributed parameter systems;large-scale systems;robots","inner/outer loop controller;two-link flexible manipulator;end-point positioning;two-stage control design;inner-loop nonlinear-based controller;outer control loop;linear output LQR design;nonconvex nonlinear optimization;software package;vibration damping;robustness;closed-loop dynamics;vibration suppression;accelerometer","","9","12","","","","","","IEEE","IEEE Conferences"
"Design of a cryogenic dual-mode resonator for a fly-wheel oscillator for a cesium frequency standard","M. E. Tobar; J. G. Hartnett; E. N. Ivanov; D. Cros; P. Bilski","Western Australia Univ., Crawley, WA, Australia; NA; NA; NA; NA","Proceedings of the 2001 IEEE International Frequncy Control Symposium and PDA Exhibition (Cat. No.01CH37218)","","2001","","","715","719","A dual-mode Sapphire Loaded Cavity (SLC) resonator has been designed and optimized with the aid of finite element software. The resonance frequency was designed to be near the frequency of a Cs atomic frequency standard. Experimental tests are shown to agree very well with calculations. The difference frequency of two differently polarized modes is shown to be a highly sensitive temperature sensor in the 50 to 80 K temperature range. We show that an oscillator based on this resonator has the potential to operate with fractional frequency instability below 10/sup -14/ for measurement times of 1 to 1000 seconds. This is sufficient to operate an atomic clock at the quantum projection noise limit.","1075-6787","0-7803-7028","10.1109/FREQ.2001.956369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=956369","","Cryogenics;Atomic clocks;Temperature sensors;Design optimization;Finite element methods;Resonance;Resonant frequency;Testing;Polarization;Temperature distribution","frequency standards;frequency stability;atomic clocks;dielectric resonator oscillators;cavity resonators;finite element analysis;quantum noise;low-temperature techniques","cryogenic dual-mode resonator;flywheel oscillator;atomic frequency standard;sapphire loaded cavity resonator;finite element software;resonance frequency;differently polarized modes;fractional frequency instability;atomic clock;quantum projection noise limit;atomic transition;high-Q whispering gallery sapphire resonator;optimum Q-factor;Allan deviation;50 to 80 K;1 to 1000 sec;Al/sub 2/O/sub 3/;Cs","","","9","","","","","","IEEE","IEEE Conferences"
"Low energy data management for different on-chip memory levels in multi-context reconfigurable architectures","M. Sanchez-Elez; M. Fernandez; M. Anido; H. Du; N. Bagherzadeh; R. Hermida","Dept. de Arquitectura de Computadores y Automatica, Univ. Complutense de Madrid, Spain; Dept. de Arquitectura de Computadores y Automatica, Univ. Complutense de Madrid, Spain; NA; NA; NA; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","36","41","This paper presents a new technique to improve the efficiency of data scheduling for multi-context reconfigurable architectures targeting multimedia and DSP applications. The main goal is to improve application energy consumption. Two levels of on-chip data storage are assumed in the reconfigurable architecture. The data scheduler attempts to optimally exploit this storage, by deciding in which on-chip memory the data have to be stored in order to reduce energy consumption. We also show that a suitable data scheduling could decrease the energy required to implement the dynamic reconfiguration of the system.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253584","","Energy management;Memory management;Reconfigurable architectures;Application software;Energy consumption;Processor scheduling;Kernel;Digital signal processing;Dynamic scheduling;Field programmable gate arrays","reconfigurable architectures;microprocessor chips;circuit optimisation;logic design;logic simulation;low-power electronics;processor scheduling;memory architecture","software programmable processor;low energy data management;on-chip memory levels;multicontext reconfigurable architectures;data scheduling efficiency;multimedia applications;DSP applications;application energy consumption;on-chip data storage;data scheduler;energy consumption reduction;dynamic reconfiguration","","4","18","","","","","","IEEE","IEEE Conferences"
"An Orthogonal Multi-objective Evolutionary Algorithm for Multi-objective Optimization Problems with Constraints","S. Y. Zeng; L. S. Kang; L. X. Ding","Dept. of Computer Science and Technology, China University of GeoSciences, Wuhan 430074, Hubei, P.R.China, <email xlink:href="mailto:sanyou-zeng@263.net" xlink:type="simple">sanyou-zeng@263.net</email>; Dept. of Computer Science and Technology, China University of GeoSciences, Wuhan 430074, Hubei, P.R.China, <email xlink:href="mailto:kang@whu.edu.cn" xlink:type="simple">kang@whu.edu.cn</email>; State Key Laboratory of Software Engineering, Wuhan University, Wuhan 430072, Hubei, P.R.China","Evolutionary Computation","","2004","12","1","77","98","In this paper, an orthogonal multi-objective evolutionary algorithm (OMOEA) is proposed for multi-objective optimization problems (MOPs) with constraints. Firstly, these constraints are taken into account when determining Pareto dominance. As a result, a strict partial-ordered relation is obtained, and feasibility is not considered later in the selection process. Then, the orthogonal design and the statistical optimal method are generalized to MOPs, and a new type of multi-objective evolutionary algorithm (MOEA) is constructed. In this framework, an original niche evolves first, and splits into a group of sub-niches. Then every sub-niche repeats the above process. Due to the uniformity of the search, the optimality of the statistics, and the exponential increase of the splitting frequency of the niches, OMOEA uses a deterministic search without blindness or stochasticity. It can soon yield a large set of solutions which converges to the Pareto-optimal set with high precision and uniform distribution. We take six test problems designed by Deb, Zitzler et al., and an engineering problem (W) with constraints provided by Ray et al. to test the new technique. The numerical experiments show that our algorithm is superior to other MOGAS and MOEAs, such as FFGA, NSGAII, SPEA2, and so on, in terms of the precision, quantity and distribution of solutions. Notably, for the engineering problem W, it finds the Pareto-optimal set, which was previously unknown.","1063-6560","","10.1162/evco.2004.12.1.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6792987","Evolutionary algorithms;orthogonal design;multi-objective optimization;Paretooptimal set;strict partial ordered relation","","","","","13","","","","","","","MITP",""
"StatCache: a probabilistic approach to efficient and accurate data locality analysis","E. Berg; E. Hagersten","Dept. of Inf. Technol., Uppsala Univ., Sweden; Dept. of Inf. Technol., Uppsala Univ., Sweden","IEEE International Symposium on - ISPASS Performance Analysis of Systems and Software, 2004","","2004","","","20","27","The widening memory gap reduces performance of applications with poor data locality. Therefore, there is a need for methods to analyze data locality and help application optimization. In this paper we present StatCache, a novel sampling-based method for performing data-locality analysis on realistic workloads. StatCache is based on a probabilistic model of the cache, rather than a functional cache simulator. It uses statistics from a single run to accurately estimate miss ratios of fully-associative caches of arbitrary sizes and generate working-set graphs. We evaluate StatCache using the SPEC CPU2000 benchmarks and show that StatCache gives accurate results with a sampling rate as low as 10/sup -4/. We also provide a proof-of-concept implementation, and discuss potentially very fast implementation alternatives.","","0-7803-8385","10.1109/ISPASS.2004.1291352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1291352","","Data analysis;Sampling methods;Statistics;Runtime;Application software;Information analysis;Information technology;Modems;High performance computing;Cache memory","cache storage;probability;sampling methods;data structures;performance evaluation;benchmark testing","StatCache;data locality analysis;optimization;sampling-based method;probabilistic model;statistics;working-set graphs;SPEC CPU2000 benchmarks;cache memories","","46","22","","","","","","IEEE","IEEE Conferences"
"On-line condition monitoring of power transformers","S. Tenbohlen; F. Figel","NA; NA","2000 IEEE Power Engineering Society Winter Meeting. Conference Proceedings (Cat. No.00CH37077)","","2000","3","","2211","2216 vol.3","The development in sensor and computer technology allows the realisation of on-line monitoring systems for application to power transformers, in order to use this most expensive transmission equipment in the optimum technical and economical manner. This means the controlled utilisation of overload and life capacity of the transformer, as well as early warning in case of an oncoming insulation fault and condition-based maintenance. In this contribution a state-of-the-art monitoring system for power transformers based on field bus technology and process control software is described. The presentation of experiences in operation shows considerable possibilities regarding optimisation of service and early fault recognition.","","0-7803-5935","10.1109/PESW.2000.847699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847699","","Condition monitoring;Power transformers;Power transformer insulation;Petroleum;Oil insulation;Insulators;Surges;Process control;Power system reliability;Gas detectors","power transformer testing;computerised monitoring;field buses;power transformer insulation;fault location;condition monitoring","on-line condition monitoring;power transformers;transmission equipment;life capacity;overload;early warning;insulation fault;condition-based maintenance;state-of-the-art monitoring system;field bus technology;process control software;service optimisation;early fault recognition","","10","6","","","","","","IEEE","IEEE Conferences"
"Exploitation of low-cost microcontroller potentialities to develop fully digital rotor flux oriented induction motor drives","F. Giacobbe; M. Marchesoni; P. Segarich; A. Taffone","Dept. of Electr. Eng., Genoa Univ., Italy; Dept. of Electr. Eng., Genoa Univ., Italy; Dept. of Electr. Eng., Genoa Univ., Italy; Dept. of Electr. Eng., Genoa Univ., Italy","Proceedings of IECON '95 - 21st Annual Conference on IEEE Industrial Electronics","","1995","1","","506","511 vol.1","The authors have developed a high dynamics induction motor control scheme using a low-cost, 16-bit, single chip microcontroller. The control algorithm, running on an evaluation board of the microcontroller, implements indirect vector control on a 1.5 kW servomotor. The required digital processing has been performed by means of a multitasking software architecture and careful code optimization. Experimental results confirm the expectations of excellent dynamic behaviour from the theory of flux orientation prediction, without using expensive 32-bit microprocessors or DSPs.","","0-7803-3026","10.1109/IECON.1995.483460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=483460","","Microcontrollers;Rotors;Machine vector control;Regulators;Induction motors;Stators;Couplings;Multitasking;Digital signal processing;Equations","induction motor drives;servomotors;machine theory;machine control;machine testing;digital control;power engineering computing;microcontrollers;multiprogramming;rotors","induction motor drives;digital rotor flux oriented control;single chip microcontroller;control algorithm;evaluation board;indirect vector control;servomotor;digital signal processing;multitasking software architecture;code optimization;flux orientation prediction theory;dynamic behaviour;16 bit;1.5 kW","","","5","","","","","","IEEE","IEEE Conferences"
"The Use of Analytical Methods to Maximize the Performance of Divers' Breathing Apparatus","M. Nuckols; P. Sexton","U.S. Naval Coastal System Center, Panama City, Florida, USA; NA","OCEANS 1984","","1984","","","527","531","The use of computer simulation in the design, development, and testing of complex military hardware systems has become increasingly widespread throughout the research and development community. A breathing system simulator for diver life support equipment has been developed which makes possible the modular ""construction"" of any conceivable breathing gas system from a bank of computer memory-resident components. This paper discusses the application of this analytical tool to optimize the conceptual design of an advanced diver underwater breathing apparatus. Design and environmental parameters are evaluated to determine their effects on the overall system performance. A ""best"" design is proposed based on this analysis method for a lung-powered breathing apparatus.","","","10.1109/OCEANS.1984.1152307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1152307","","Performance analysis;Computer simulation;Military computing;System testing;Hardware;Research and development;Computational modeling;Modular construction;Application software;Design optimization","","","","","2","","","","","","IEEE","IEEE Conferences"
"Modern control valves with failure diagnostics","J. Kiesbauer; H. Hoffmann","SAMSON AG, WEISMLLERSTR. 3, 60314 FRANKFURT/MAIN (GERMANY); SAMSON AG, WEISMLLERSTR. 3, 60314 FRANKFURT/MAIN (GERMANY)","1999 European Control Conference (ECC)","","1999","","","350","355","In the modern process automation industry control valves with digital positioners have the ability to communicate with the process control system. Additional important functions (automatic start-up and control loop self-optimization, alarm signals, detailed fault diagnosis) lead to a process integrated maintenance and more process plant reliability. In this paper, these possibilities and a new diagnostic software for control valves are presented. This tool only uses the sensors required for valve positioning anyway, and, in particular, by recording the control loop dynamics through evaluation of small diagnostic test signals with no mean value at short and little process disturbances so that all important control valve parameters can be monitored and determined on-line. Remarkable are the clear statements without the necessity for consulting an expert.","","978-3-9524173-5","10.23919/ECC.1999.7099327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7099327","Control valves;control loop self-optimization;on-line failure diagnosis","Valves;Process control;Actuators;Friction;Monitoring;Sensors;Maintenance engineering","control engineering computing;fault diagnosis;maintenance engineering;position control;reliability;valves","control valve;failure diagnostic;process automation industry;process control system;automatic start-up;control loop self-optimization;alarm signal;process integrated maintenance;process plant reliability;diagnostic software;valve positioning;control loop dynamics","","","2","","","","","","IEEE","IEEE Conferences"
"Load balancing and communication optimization for parallel adaptive finite element methods","J. E. Flaherty; R. M. Loy; P. C. Scully; M. S. Shephard; B. K. Szymanski; J. D. Teresco; L. H. Ziantz","Dept. of Comput. Sci., Rensselaer Polytech. Inst., Troy, NY, USA; NA; NA; NA; NA; NA; NA","Proceedings 17th International Conference of the Chilean Computer Science Society","","1997","","","246","255","The paper describes predictive load balancing schemes designed for use with parallel adaptive finite element methods. We provide an overview of data structures suitable for distributed storage of finite element mesh data as well as software designed for mesh adaptation and load balancing. During the course of a parallel computation, processor load imbalances are introduced at adaptive enrichment steps. The predictive load balancing methods described here use a priori estimates of work load for adaptive refinement and subsequent computation to improve enrichment efficiency and reduce total balancing time. An analysis code developed with these components for solving compressible flow problems is used to obtain predictive load balancing results on an IBM SP2 computer. Our test problem involves the transient solution of the three dimensional Euler equations of compressible flow inside a perforated shock tube. We also present a message passing library extension in development which allows for automated packing of messages to improve communication efficiency.","","0-8186-8052","10.1109/SCCC.1997.637097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637097","","Load management;Finite element methods;Data structures;Software design;Concurrent computing;Load flow analysis;Testing;Equations;Electric shock;Message passing","parallel algorithms;processor scheduling;resource allocation;finite element analysis;data structures;communication complexity;compressible flow;mechanical engineering computing;message passing","predictive load balancing schemes;communication optimization;parallel adaptive finite element methods;data structures;distributed storage;finite element mesh data;mesh adaptation;load balancing;parallel computation;processor load imbalances;adaptive enrichment steps;message passing library extension;a priori estimates;work load;adaptive refinement;enrichment efficiency;balancing time;compressible flow problems;predictive load balancing;IBM SP2 computer;transient solution;three dimensional Euler equations","","","22","","","","","","IEEE","IEEE Conferences"
"Transparent mobile IP: an approach and implementation","A. Giovanardi; G. Mazzini","Ferrara Univ., Italy; NA","GLOBECOM 97. IEEE Global Telecommunications Conference. Conference Record","","1997","3","","1861","1865 vol.3","The problem of Internet terminal mobility without reconfigurations and host software modifications (transparent) is presented. A solution based on the introduction of network agents, with proxy address resolution protocol and IP tunneling features, has been introduced, discussed and implemented. Some strategies for routing optimization, in order to avoid triangular paths, are investigated and tested. Implementation results demonstrate the value of this approach in terms of both functionality and performance and are of interest in indicating a simple way of supporting the increasing request for mobility.","","0-7803-4198","10.1109/GLOCOM.1997.644594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=644594","","Web and internet services;Transport protocols;Electronic mail;Tunneling;Routing;Testing;IP networks;Portable computers;Ethernet networks;LAN interconnection","Internet;telecommunication network routing;software agents;optimisation;mobile radio;protocols","transparent mobile IP;Internet terminal mobility;network agents;proxy address resolution protocol;IP tunneling features;routing optimization;implementation;functionality;performance","","6","7","","","","","","IEEE","IEEE Conferences"
"Improving instruction-level parallelism by loop unrolling and dynamic memory disambiguation","J. W. Davidson; S. Jinturkar","Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA","Proceedings of the 28th Annual International Symposium on Microarchitecture","","1995","","","125","132","Exploitation of instruction-level parallelism is an effective mechanism for improving the performance of modern super-scalar/VLIW processors. Various software techniques can be applied to increase instruction-level parallelism. This paper describes and evaluates a software technique, dynamic memory disambiguation, that permits loops containing loads and stores to be scheduled more aggressively, thereby exposing more instruction-level parallelism. The results of our evaluation show that when dynamic memory disambiguation is applied in conjunction with loop unrolling, register renaming, and static memory disambiguation, the ILP of memory-intensive benchmarks can be increased by as much as 300 percent over loops where dynamic memory disambiguation is not performed. Our measurements also indicate that for the programs that benefit the most from these optimizations, the register usage does not exceed the number of registers on mast high-performance processors.","1072-4451","0-8186-7349","10.1109/MICRO.1995.476820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=476820","","Parallel processing;Registers;Hardware;Computer science;VLIW;Processor scheduling;Dynamic scheduling;Performance evaluation;Pipelines;Program processors","instruction sets;software performance evaluation;program testing;parallel architectures","instruction-level parallelism;loop unrolling;dynamic memory disambiguation;performance improvement;VLIW processors;superscalar processors;software techniques;software technique evaluations;register renaming;static memory disambiguation;memory-intensive benchmarks","","18","18","","","","","","IEEE","IEEE Conferences"
"Reluctance motor control for fault-tolerant capability","Dinyu Qin; Xiaogang Luo; T. A. Lipo","Dept. of Electr. & Comput. Eng., Wisconsin Univ., Madison, WI, USA; NA; NA","1997 IEEE International Electric Machines and Drives Conference Record","","1997","","","WA1/1.1","WA1/1.6","While it is not difficult to operate a synchronous reluctance motor (SynRM) with a one phase open-circuit, it is not a trivial task to operate a SynRM under a short-circuit fault condition. SynRM fault-tolerant operation, especially in the situation of one phase short-circuit, is investigated in this paper. This work suggests that for a normal SynRM, fault-tolerant operation, which is important for high-performance applications, can be achieved by proper control of the currents fed to the machine to produce a satisfactory average torque. Both computer simulation and hardware experiment are carried out based upon a theoretical analysis. Furthermore, a control strategy with torque optimization is also explored.","","0-7803-3946","10.1109/IEMDC.1997.604282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=604282","","Reluctance motors;Fault tolerance;Reluctance machines;Circuit faults;Application software;Torque control;Acoustic noise;Permanent magnets;Electromagnetic coupling;Drives","reluctance motors;machine control;machine theory;machine testing;control system analysis computing;electric machine analysis computing;short-circuit currents;optimal control;torque control;control system synthesis","synchronous reluctance motor;fault-tolerant capability;single-phase short-circuit;short-circuit fault condition;control strategy;torque optimization;control design;hardware experiments;high-performance applications;fault-tolerant operation;control simulation;computer simulation","","9","6","","","","","","IEEE","IEEE Conferences"
"Optimizing the pulsing denial-of-service attacks","Xiapu Luo; R. K. C. Chang","Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, CHINA; Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, CHINA","2005 International Conference on Dependable Systems and Networks (DSN'05)","","2005","","","582","591","In this paper we consider how to optimize a new generation of pulsing denial-of-service (PDoS) attacks from the attackers' points of views. The PDoS attacks are 'smarter' than the traditional attacks in several aspects. The most obvious one is that they require fewer attack packets to cause a similar damage. Another is that the PDoS attacks can be tuned to achieve different effects. This paper concentrates on the attack tuning part. In particular, we consider two conflicting goals involved in launching a PDoS attack: (1) maximizing the throughput degradation and (2) minimizing the risk of being detected. To address this problem, we first analyze the TCP throughput and quasi-global synchronization phenomenon caused by the PDoS attack. We then propose a family of objective functions to incorporate the two conflicting goals, and obtain the optimal attack settings. To validate the analytical results, we have carried out extensive experiments using both ns-2 simulation and a test-bed. The overall experimental results match well with the analytical results.","1530-0889;2158-3927","0-7695-2282","10.1109/DSN.2005.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467832","","Computer crime;Degradation;Throughput;Analytical models;Floods;Computer vision;Computational modeling;Testing;Computer security;Application software","telecommunication security;computer crime;transport protocols","pulsing denial-of-service attack optimization;pulsing PDoS attacks;throughput degradation maximization;risk minimization;TCP throughput;quasiglobal synchronization phenomenon;ns-2 simulation","","8","24","","","","","","IEEE","IEEE Conferences"
"On-line optimization and monitoring of power plant performance through machine learning techniques","E. Swidenbank; J. A. Garcia; D. Flynn; J. Rapun","Queen's Univ., Belfast, UK; NA; NA; NA","UKACC International Conference on Control '98 (Conf. Publ. No. 455)","","1998","1","","257","262 vol.1","This paper describes TOPGEN, a collaborative project for the demonstration of research results in the field of advanced control and optimization of a power plant. The project is partially funded by the European ESPRIT programme. In TOPGEN, several related technologies are being jointly tested on a 200 MW generating unit of Ballylumford Power Station, Northern Ireland. The objective is to provide operational and engineering recommendations for plant adjustments, in order to achieve performance improvements, during steady-state and fault-free conditions, given a set of exploitation constraints. Thus, the expected result is a software system, fully installed at Ballylumford Power Plant, which will improve the performance of the plant, reducing the amount of fuel necessary to obtain the electrical power required. This system will work online, continuously receiving data from the process.","0537-9989","0-85296-708","10.1049/cp:19980237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=727918","","","power station control","online optimization;online monitoring;power plant performance;machine learning techniques;TOPGEN;collaborative project;ESPRIT programme;generating unit;Ballylumford Power Station;Northern Ireland;UK;plant adjustments;steady-state conditions;fault-free conditions;exploitation constraints;200 MW","","1","","","","","","","IET","IET Conferences"
"PSCP: A scalable parallel ASIP architecture for reactive systems","A. Pyttel; A. Sedlmeier; C. Veith","Corp. Technol., Siemens AG, Munich, Germany; NA; NA","Proceedings Design, Automation and Test in Europe","","1998","","","370","376","We describe a codesign approach based on a parallel and scalable ASIP architecture, which is suitable for the implementation of reactive systems. The specification language of our approach is extended statecharts. Our ASIP architecture is scalable with respect to the number of processing elements as well as parameters such as bus widths and register file sizes. Instruction sets are generated from a library of components covering a spectrum of space/time trade-off alternatives. Our approach features a heuristic static timing analysis step for statecharts. An industrial example requiring the real-time control of several stepper motors illustrates the benefits of our approach.","","0-8186-8359","10.1109/DATE.1998.655884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655884","","Application specific processors;Timing;Hardware;Application software;Instruction sets;Specification languages;Software systems;Optimization;Program processors;Libraries","parallel architectures;application specific integrated circuits;instruction sets;microprocessor chips;high level synthesis;specification languages;timing;real-time systems","scalable parallel ASIP architecture;reactive systems;codesign approach;specification language;extended statecharts;processing elements;bus widths;register file sizes;instruction sets;space/time trade-off alternatives;heuristic static timing analysis step;real-time control;stepper motors","","5","18","","","","","","IEEE","IEEE Conferences"
"An augmented pattern matcher as a tool to synthesize conceptual descriptions of programs","F. Balmas","Univ. Paris VIII, France","Proceedings KBSE '94. Ninth Knowledge-Based Software Engineering Conference","","1994","","","150","157","We present a pattern matcher specially developed far the parsing of LISP functions. Its main features are patterns defined by context-free grammars, parametrized patterns, multi-step matching, as well as dynamic synthesis of new patterns. We have designed a model far the conceptual description of functions, which abstracts computations performed by a function, but is still complete enough to be executable. It constitutes a key for the understanding of the function. The augmented pattern matcher is used by our system PRISME to analyze LISP functions and to synthesize conceptual descriptions. We are currently working on the extension of the system in order to handle programs and to use the conceptual descriptions as guide to program verification and optimization.<<ETX>>","1068-3062","0-8186-6380","10.1109/KBSE.1994.342667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=342667","","Pattern matching;Kernel;Abstracts;Pattern analysis;Feature extraction;Testing;Embedded computing","LISP;context-free grammars;program verification;program compilers;program diagnostics;knowledge based systems;software tools","augmented pattern matcher;conceptual descriptions;pattern matcher;parsing;LISP functions;context-free grammars;PRISME;program verification;program optimization","","1","21","","","","","","IEEE","IEEE Conferences"
"Optimal Controller Placements in Large Scale Linear Systems","H. Chiang; J. S. Thorp; J. Wang; J. Lu","School of Electrical Engineering, Cornell University, Ithaca, NY 14853; School of Electrical Engineering, Cornell University, Ithaca, NY 14853; School of Electrical Engineering, Cornell University, Ithaca, NY 14853; School of Electrical Engineering, Cornell University, Ithaca, NY 14853","1989 American Control Conference","","1989","","","1615","1620","The problem of controller placements in large scale linear systems is considered in this paper. Specifically, given a large scale linear system, it is desired to find the optimal number of linear controllers, the optimal locations in the system to place these controllers and the optimal feedback gain of each contoller so that an objective function is optimized, and also so that other desirable properties of the controlled system, such as eigenstructure assignment, occur. A formulation of the controller placement problem is presented. A general solution algorithm based on simulated annealing is developed for the controller placement problem. The solution algorithm can arrive at the global optimal solution. The solution algorithm has been implemented into a software package and tested on a 132-dimension linear system with very promising results.","","","10.23919/ACC.1989.4790446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4790446","","Optimal control;Control systems;Large-scale systems;Linear systems;Linear feedback control systems;Simulated annealing;Software algorithms;Software packages;Software testing;System testing","","","","","7","","","","","","IEEE","IEEE Conferences"
"Low-cost register-pressure prediction for scalar replacement using pseudo-schedules","Y. Ma; S. Carr; R. Ge","Dept. of Comput. Sci., Michigan Technol. Univ., Houghton, MI, USA; Dept. of Comput. Sci., Michigan Technol. Univ., Houghton, MI, USA; NA","International Conference on Parallel Processing, 2004. ICPP 2004.","","2004","","","116","124 vol.1","Scalar replacement is an effective optimization for removing memory accesses. However, exposing all possible array reuse with scalars may cause a significant increase in register pressure, resulting in register spilling and performance degradation. We present a low cost method to predict the register pressure of a loop before applying scalar replacement on high-level source code, called pseudo-schedule register prediction (PRP), that takes into account the effects of both software pipelining and register allocation. PRP attempts to eliminate the possibility of degradation from scalar replacement due to register spilling while providing opportunities for a good speedup. PRP uses three approximation algorithms: one for constructing a data dependence graph, one for computing the recurrence constraints of a software pipelined loop, and one for building a pseudo-schedule. Our experiments show that PRP predicts the floating-point register pressure within 2 registers and the integer register pressure within 2.7 registers on average with a time complexity of O(n/sup 2/) in practice. PRP achieves similar performance to the best previous approach, having O(n/sup 3/) complexity, with less than one-fourth of the compilation time on our test suite.","0190-3918","0-7695-2197","10.1109/ICPP.2004.1327911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1327911","","Registers;Pipeline processing;Degradation;Software algorithms;Software performance;Filling;Computer science;Costs;Approximation algorithms;Buildings","optimising compilers;computational complexity;pipeline processing;program control structures","register-pressure prediction;scalar replacement;memory access;pseudo-schedule register prediction;software pipelining;register allocation;data dependence graph;floating-point register;time complexity","","","13","","","","","","IEEE","IEEE Conferences"
"X-ray inspection for electronic packaging latest developments","F. Maur","feinfocus Rontgen-Syst. GmbH, Germany","Fifth International Conference onElectronic Packaging Technology Proceedings, 2003. ICEPT2003.","","2003","","","235","239","During the past few years, advances have been made in both in X-ray tube and detector technologies. The field of microfocus radioscopy has been established as an important testing process and has expanded into many new industrial applications that require quality control or process optimization. The first nanofocus and multifocus X-ray systems have become available with a focal spot of 5 micron. In the existing range of microfocus X-ray tubes, further improvements have been achieved as well, such as increased long term stability of intensity position constancy. Software, image processing and manipulation techniques have all progressed as well, allowing X-ray to become a formidable non-destructive inspection method for manufacturers in virtually every industry, especially those involved with Electronic Packaging.","","0-7803-8168","10.1109/EPTC.2003.1298731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298731","","Inspection;Electronics packaging;X-ray imaging;X-ray detection;X-ray detectors;Testing;Electrical equipment industry;Industrial control;Application software;Quality control","inspection;electronics packaging;X-ray imaging;X-ray tubes;radiography;focusing","X-ray inspection;electronic packaging;microfocus radioscopy;quality control;process optimization;multifocus X-ray systems;nanofocus X-ray systems;X-ray tubes;long term stability;intensity position constancy;image processing;nondestructive inspection method;bond wire inspection;die attach void analysis;wafer bump inspection;X-ray imaging;3D capability;volume rendering;axial computed tomography","","2","","","","","","","IEEE","IEEE Conferences"
"A timing-driven synthesis of arithmetic circuits using carry-save-adders","Taewhan Kim; Junhyung Um","Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Taejon, South Korea; NA","Proceedings 2000. Design Automation Conference. (IEEE Cat. No.00CH37106)","","2000","","","313","316","Carry-save-adder (CSA) is one of the most widely used types of operation in implementing a fast computation of arithmetics. An inherent limitation of the conventional CSA applications is that the applications are confined to the sections of arithmetic circuit that can be directly translated into addition expressions. To overcome this limitation, from the analysis of the structures of arithmetic circuits found in industry, we derive a set of simple, but effective CSA transformation techniques. Those are (1) optimization across multiplexors, (2) optimization across design boundaries (restricted notion of [3]), and (3) optimization across multiplications. Based on the techniques, we develop a new timing-driven CSA transformation algorithm that is able to utilize CSAs extensively throughout the whole circuit. Experimental data for practical testcases are provided to show the effectiveness of our algorithm.","","0-7803-5973","10.1109/ASPDAC.2000.835116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=835116","","Circuit synthesis;Design optimization;Timing;Adders;Algorithm design and analysis;Digital arithmetic;Application software;Computer science;Information technology;Circuit testing","timing;carry logic;adders;digital arithmetic;high level synthesis","timing-driven synthesis;arithmetic circuits;carry-save-adders;CSA transformation techniques;multiplexors;design boundaries;RTL synthesis","","","5","","","","","","IEEE","IEEE Conferences"
"Concurrent program archetypes","K. M. Chandy","Dept. of Comput. Sci., California Inst. of Technol., Pasadena, CA, USA","Proceedings Scalable Parallel Libraries Conference","","1994","","","1","9","A program archetype is a program design strategy appropriate for a restricted class of problems, and a collection of program designs with implementations of examplar problems in one or more programming languages and optimized for a collection of target machines. The program design strategy includes: archetype specific information about methods of deriving a program from a specification; methods of parallelizing sequential programs; the program structure; methods of reasoning about correctness and performance; empirical data on performance measurements and tuning for different kinds of machines; and suggestions for test suites. The paper reports on a project at Caltech which is exploring the question: can a library of parallel program archetypes be used to reduce the effort required to produce correct efficient programs?.<<ETX>>","","0-8186-6895","10.1109/SPLC.1994.377010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=377010","","Parallel programming;Productivity;Programming profession;Standards development;Runtime library;Concurrent computing;Yarn;Parallel machines;Computer science;Design optimization","parallel programming;software reusability;software libraries;subroutines","concurrent program archetypes;program design strategy;restricted class;examplar problems;programming languages;target machines;archetype specific information;parallelizing;program structure;reasoning;correctness;empirical data;performance measurements;test suites;parallel program archetype library;correct efficient programs","","5","","","","","","","IEEE","IEEE Conferences"
"Measuring transfers of control in program execution: input sensitivity for numerical programs","C. Z. Loboz","Australian Centre for UNISYS Software, North Ryde, NSW, Australia","TENCON'92 - Technology Enabling Tomorrow","","1992","","","744","748 vol.2","The mean number of instructions executed between transfers of control is of interest in designing pipelines, instruction buffers and management strategies for them. The analysis of three nontrivial numerical programs shows that the range of values received for a program may vary widely with the program's input. This confirms previous findings and suggests that caution should be exercised when measuring and reporting this characteristic. The results also suggest that program type can influence the sensitivity of this characteristic to program input.<<ETX>>","","0-7803-0849","10.1109/TENCON.1992.271871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=271871","","Program processors;Australia;Pipelines;Computer architecture;High level languages;Monitoring;Testing;Numerical analysis;Optimizing compilers;Programming profession","program compilers","instructions transfer;program execution;input sensitivity;numerical programs;pipelines;instruction buffers;management strategies;program type;sensitivity","","1","9","","","","","","IEEE","IEEE Conferences"
"Whole word recognition in facsimile images","N. Sherkat; T. J. Allen","Dept. of Comput., Nottingham Trent Univ., UK; NA","Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)","","1999","","","547","550","This paper presents the research carried out in producing a whole recognizor for cursive handwritten words in facsimile images. Two sets of handwritten data samples are collected and converted into facsimile images. The first set comprises approximately 1600 word images from 8 writers and is used for development purposes. The second set consists of approximately 2000 word images from 10 writers. This set is used for testing only. The algorithms for extraction of holistic features namely, vertical bars, holes and cups used in the recognizor are described. A series of test are carried out and the results are presented using a 200 word lexicon. The holistic recognizor produced 62% top rank and 82% in top 5 alternatives. When a lexicon of 1000 words was used these values reduced to 49% and 70% respectively. The future directions of the research for improvement of recognition rate are proposed. It is envisaged that definition of further features would improve the overall accuracy.","","0-7695-0318","10.1109/ICDAR.1999.791846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791846","","Image recognition;Facsimile;Bars;Handwriting recognition;Engines;Testing;Image databases;Writing;Image resolution;Optical character recognition software","facsimile;document image processing;handwritten character recognition","whole word recognition;facsimile images;cursive handwritten words;word images;testing;algorithms;holistic feature extraction;vertical bars;holes;cups;word lexicon;recognition rate","","3","8","","","","","","IEEE","IEEE Conferences"
"Computing budget allocation for simulation experiments with different system structures","Chun-Hung Chen; Yu Yuan; Hsiao-Chang Chen; E. Yucesan; L. Dai","Dept. of Syst. Eng., Pennsylvania Univ., Philadelphia, PA, USA; NA; NA; NA; NA","1998 Winter Simulation Conference. Proceedings (Cat. No.98CH36274)","","1998","1","","735","741 vol.1","Simulation plays a vital role in analyzing discrete event systems, particularly in comparing alternative system designs with a view to optimize system performance. Using simulation to analyze complex systems, however, can be both prohibitively expensive and time consuming. We present effective algorithms to intelligently allocate computing budget for discrete event simulation experiments with different system structures. These algorithms dynamically determine the best simulation lengths for all simulation experiments and thus significantly reduce the total computation cost for a desired confidence level. Numerical illustrations are included. We also compare our algorithms with our earlier approach in which different system structures are not considered. Numerical testing shows that we can further improve simulation efficiency.","","0-7803-5133","10.1109/WSC.1998.745058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=745058","","Computational modeling;Analytical models;Discrete event simulation;Performance analysis;Discrete event systems;Design optimization;System performance;Intelligent structures;Heuristic algorithms;Computational efficiency","discrete event simulation;software cost estimation;resource allocation;computational complexity;software performance evaluation","budget allocation;simulation experiments;system structures;discrete event systems;alternative system designs;system performance optimization;complex systems;discrete event simulation experiments;best simulation lengths;total computation cost;confidence level;simulation efficiency","","9","24","","","","","","IEEE","IEEE Conferences"
"Tuning parallel programs with computational steering and controlled execution","M. Oberhuber; S. Rathmayer; A. Bode","LRR-TUM, Tech. Univ. Munchen, Germany; NA; NA","Proceedings of the Thirty-First Hawaii International Conference on System Sciences","","1998","7","","157","166 vol.7","Online visualization and computational steering of parallel scientific applications has been widely recognized as the key to better insight and understanding of the observed simulation. From the parallel program developer's point of view further problems arise and need to be solved. The behavior and performance of parallel programs does not only depend on the input data but also on inter-process communication. To reflect this fact we propose a novel combination of online visualization, computational steering of parallel high performance computing applications and controlled deterministic execution. Both the visualization and the classical part of steering is based on the VIPER tool. For the control of the communication we rely on a tool called codex, which was developed to test and control communication by the use of control patterns. Finally, VIPER and codex form an environment for tuning, steering and testing based on VIPER's extended programming model.","","0-8186-8255","10.1109/HICSS.1998.649209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649209","","Concurrent computing;High performance computing;Communication system control;Data visualization;Testing;Parallel processing;Application software;Runtime;Wide area networks;Optimization","parallel programming","parallel program tuning;computational steering;controlled execution;online visualization;parallel scientific applications;simulation;software performance;input data;interprocess communication;parallel high performance computing;deterministic execution;VIPER tool;codex;control patterns;program testing","","3","12","","","","","","IEEE","IEEE Conferences"
"Energy-efficient mapping and scheduling for DVS enabled distributed embedded systems","M. T. Schmitz; B. M. Al-Hashimi; P. Eles","Dept. of Electron. & Comput. Sci., Southampton Univ., UK; NA; NA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","514","521","In this paper, we present an efficient two-step iterative synthesis approach for distributed embedded systems containing dynamic voltage scalable processing elements (DVS-PEs), based on genetic algorithms. The approach partitions, schedules, and voltage scales multi-rate specifications given as task graphs with multiple deadlines. A distinguishing feature of the proposed synthesis is the utilisation of a generalised DVS method. In contrast to previous techniques, which ""simply"" exploit available slack time, this generalised technique additionally considers the PE power profile during a refined voltage selection to further increase the energy savings. Extensive experiments are conducted to demonstrate the efficiency of the proposed approach. We report up to 43.2% higher energy reductions compared to previous DVS scheduling approaches based on constructive techniques and total energy savings of up to 82.9% for mapping and scheduling optimised DVS systems.","1530-1591","0-7695-1471","10.1109/DATE.2002.998349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998349","","Energy efficiency;Voltage control;Embedded system;Dynamic voltage scaling;Energy consumption;Processor scheduling;Microprocessors;Timing;Switches;Dynamic scheduling","embedded systems;genetic algorithms;circuit optimisation;processor scheduling;hardware-software codesign;timing;logic partitioning;low-power electronics","DVS enabled distributed embedded systems;energy-efficient mapping;scheduling;two-step iterative synthesis;dynamic voltage scalable processing elements;genetic algorithms;partitioning;voltage scaling;multi-rate specifications;task graphs;multiple deadlines;processing element power profile;refined voltage selection;energy savings;energy reduction;low-energy task mapping algorithm;system level co-design","","48","26","","","","","","IEEE","IEEE Conferences"
"Error-tolerance","M. A. Breuer","NA","IEEE International Conference on Computer Design: VLSI in Computers and Processors, 2004. ICCD 2004. Proceedings.","","2004","","","3","","Summary form only given. Because of trends in scaling, in the near future every high performance dice can contain a massive number of defects and process aggravated noise and performance problems. In an attempt to obtain useful yields, designers and test engineers need to adopt a qualitatively different approach to their work. They need to learn, enhance and deploy techniques such as fault- and defect-tolerance. For some applications, they may even apply error-tolerance, a somewhat controversial emerging paradigm. A circuit is error-tolerant (ET) with respect to an application, if (1) it contains defects that cause internal and may cause external errors, and (2) the system that incorporates this circuit produces acceptable results. In this presentation we illustrate and give quantitative bounds on several factors that shape the future of digital design. We compare and contrast defect and fault-tolerant schemes with that of error-tolerance. We discuss how yield can be optimized by appropriately selecting the granularity of spares in light of defect densities and interconnect complexity. Finally, we show that several large classes of consumer electronic applications are resilient to errors, and how error-tolerance can then be used to significantly enhance effective yield.","1063-6404","0-7695-2231","10.1109/ICCD.2004.1347888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347888","","Digital systems;Circuit testing;Computer science;Design automation;Automatic testing;System testing;Application software;Computer errors;Design engineering;Circuit faults","integrated circuit reliability;integrated circuit yield;integrated circuit interconnections;circuit complexity;consumer electronics","error tolerance;digital design;integrated circuit yield;defect density;interconnect complexity;consumer electronics","","1","","","","","","","IEEE","IEEE Conferences"
"Efficient on-the-fly data race detection in multithreaded C++ programs","E. Pozniansky; A. Schuster","Comput. Sci. Dept., Technion-Israel Inst. of Technol., Haifa, Israel; NA","Proceedings International Parallel and Distributed Processing Symposium","","2003","","","8 pp.","","Data race detection is highly essential for debugging multithreaded programs and assuring their correctness. Nevertheless, there is no single universal technique capable of handling the task efficiently, since the data race detection problem is computationally hard in the general case. Thus, all currently available tools, when applied to some general case program, usually result in excessive false alarms or in a large number of undetected races. Another major drawback of currently available tools is that they are restricted, for performance reasons, to detection units of fixed size. Thus, they all suffer from the same problem - choosing a small unit might result in missing some of the data races, while choosing a large one might lead to false detection. We present a novel testing tool, called MultiRace, which combines improved versions of Djit and Lockset - two very powerful on-the-fly algorithms for dynamic detection of apparent data races. Both extended algorithms detect races in multithreaded programs that may execute on weak consistency systems, and may use two-way as well as global synchronization primitives. By employing novel technologies, MultiRace adjusts its detection to the native granularity of objects and variables in the program under examination. In order to monitor all accesses to each of the shared locations, MultiRace instruments the C++ source code of the program. It lets the user fine-tune the detection process, but otherwise is completely automatic and transparent. This paper describes the algorithms employed in MultiRace, gives highlights of its implementation issues, and suggests some optimizations. It shows that the overheads imposed by MultiRace are often much smaller (orders of magnitude) than those obtained by other existing tools.","1530-2075","0-7695-1926","10.1109/IPDPS.2003.1213513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213513","","Multithreading;Computer science;Debugging;Testing;Heuristic algorithms;Object detection;Monitoring;Instruments;Parallel programming;Parallel processing","multi-threading;C++ language;program debugging;program verification;program testing;software tools","on-the-fly data race detection;multithreaded C++ programs;debugging;correctness;MultiRace;testing tool;Djit;Lockset;weak consistency systems;source code;fine tuning","","15","16","","","","","","IEEE","IEEE Conferences"
"Modern design techniques with systemC [Tutorial]","M. Speitel; B. Niemann; A. Braun; K. Einwich; C. Haubelt; F. Mayer","Fraunhofer IIS; NA; NA; NA; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","xxxiv","xxxv","Summary form only given, as follows. Even with new design languages coming up, SystemC is widely accepted by EDA companies and used in many design teams. The tutorial gives an extensive overview of the application of SystemC for various aspects of system-on-chip design. It starts with an introduction to SystemC 2.0. Modelling at different levels of abstraction - from system development down to a synthesisable ASIC implementation - are covered. The tutorial is extended by HW/SW partitioning methodologies using SystemC, and includes analogue and mixed analogue/digital modelling with SystemC AMS. The verification of hardware dependent software and the novel SystemC verification library and its usage are also presented. Intended audience: This master course is targeted to designers, who want to acquire basic knowledge of SystemC and its applications as well as design managers, searching for an inside view on the usage of SystemC in a C/C++ based design flow.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268817","","Tutorial;Linear programming;Electronic design automation and methodology;Design optimization;Power system modeling;Integer linear programming;Digital systems;Hardware;Logic programming;Mathematical programming","","","","","","","","","","","IEEE","IEEE Conferences"
"Vision-model-based impairment metric to evaluate blocking artifacts in digital video","Zhenghua Yu; Hong Ren Wu; S. Winkler; Tao Chen","Sch. of Comput. Sci. & Software Eng., Monash Univ., Melbourne, Vic., Australia; NA; NA; NA","Proceedings of the IEEE","","2002","90","1","154","169","In this paper investigations are conducted to simplify and refine a vision-model-based video quality metric without compromising its prediction accuracy. Unlike other vision-model-based quality metrics, the proposed metric is parameterized using subjective quality assessment data recently provided by the Video Quality Experts Group. The quality metric is able to generate a perceptual distortion map for each and every video frame. A perceptual blocking distortion metric (PBDM) is introduced which utilizes this simplified quality metric. The PBDM is formulated based on the observation that blocking artifacts are noticeable only in certain regions of a picture. A method to segment blocking dominant regions is devised, and perceptual distortions in these regions are summed up to form an objective measure of blocking artifacts. Subjective and objective tests are conducted and the performance of the PBDM is assessed by a number of measures such as the Spearman rank-order correlation, the Pearson correlation, and the average absolute error The results show a strong correlation between the objective blocking ratings and the mean opinion scores on blocking artifacts.","0018-9219;1558-2256","","10.1109/5.982412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982412","","Australia;PSNR;Computer science;Software engineering;Discrete cosine transforms;Quality assessment;Distortion measurement;Testing;Standardization;Pulse modulation","video coding;brightness;image segmentation;visual perception;image resolution;physiological models;edge detection","digital-video quality;blocking artifacts;vision-model-based impairment metric;prediction accuracy;perceptual distortion map;perceptual blocking distortion metric;segmentation method;Spearman rank-order correlation;Pearson correlation;average absolute error;psychovisual experiments;spatio-temporal contrast sensitivity;contrast gain control;edge detection;MPEG-2","","84","59","","","","","","IEEE","IEEE Journals & Magazines"
"Battery lifetime prediction for energy-aware computing","D. Rakhmatov; S. Vrudhula; D. A. Wallach","Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA; Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA; NA","Proceedings of the International Symposium on Low Power Electronics and Design","","2002","","","154","159","Predicting the time of full discharge of a finite-capacity energy source, such as a battery, is important for the design of portable electronic systems and applications. In this paper we present a novel analytical model of a battery that not only can be used to predict battery lifetime, but also can serve as a cost function for optimization of the energy usage in battery-powered systems. The model is physically justified, and involves only two parameters, which are easily estimated. The paper includes the results of extensive experimental evaluation of the model with respect to numerical simulations of the electrochemical cell, as well as measurements taken on a real battery. The model was tested using constant, interrupted, periodic and non-periodic discharge profiles, which were derived from standard applications run on a pocket computer.","","1-5811-3475","10.1109/LPE.2002.146729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029584","","Predictive models;Analytical models;Battery charge measurement;Circuit simulation;Low power electronics;Cost function;Numerical simulation;Application software;Permission;Computational modeling","low-power electronics;CAD;cells (electric);power supplies to apparatus;portable computers;life testing;circuit simulation;optimisation","low power applications;battery lifetime prediction;energy-aware computing;finite-capacity energy source full discharge prediction;portable electronic systems/applications;battery analytical model;energy usage optimization cost function;battery-powered systems;electrochemical cell numerical simulations;constant/interrupted/periodic/non-periodic discharge profiles;pocket computer application running","","14","16","","","","","","IEEE","IEEE Conferences"
"How many solutions does a SAT instance have?","P. R. Pari; Lin Yuan; Gang Qu","Dept. of Electr. & Comput. Eng., Maryland Univ., College Park, MD, USA; Dept. of Electr. & Comput. Eng., Maryland Univ., College Park, MD, USA; Dept. of Electr. & Comput. Eng., Maryland Univ., College Park, MD, USA","2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)","","2004","5","","V","V","Our goal is to investigate the solution space of a given Boolean satisfiability (SAT) instance. In particular, we are interested in determining the size of the solution space - the number of truth assignments that make the SAT instance true - and finding all such truth assignments, if possible. This apparently hard problem has both theoretical and practical values. We propose an exact algorithm based on exhaustive search that solves the instance once and finds all solutions (SOFAS) and several sampling techniques that estimate the size of the solution space. SOFAS works better for SAT instances of small size with a 5X-100X speed-up over the brute force search algorithm. The sampling techniques estimate the solution space reasonably well for standard SAT benchmarks.","","0-7803-8251","10.1109/ISCAS.2004.1329499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1329499","","Sampling methods;Very large scale integration;Logic design;Logic testing;Space exploration;Design optimization;Educational institutions;Computer science;Application software;Automatic test pattern generation","computability;Boolean algebra;computational complexity;search problems","solution space;Boolean satisfiability;truth assignments;sampling techniques;brute force search algorithm;SOFAS","","1","8","","","","","","IEEE","IEEE Conferences"
"A high-level fault modeling technique using CONES","J. -. Jou; T. Lin","AT&T Bell Lab., Murray Hill, NJ, USA; AT&T Bell Lab., Murray Hill, NJ, USA","Proceedings of the 32nd Midwest Symposium on Circuits and Systems,","","1989","","","287","289 vol.1","A technique to automatically derive and inject meaningful internal faults of C-modeled primitives using a high-level synthesis tool, called CONES, is presented. The realistic internal faults are derived from an optimized two-level implementation of the functional primitives synthesized by CONES. The concept of a parameterized C-model is developed to introduce fault effects for fault simulation. This technique has been verified using AT&T's in-house CAD tools. The experiments show fairly promising and interesting results.<<ETX>>","","","10.1109/MWSCAS.1989.101848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=101848","","Circuit faults;Circuit simulation;High level synthesis;Circuit testing;Central Processing Unit;Very large scale integration;Power system modeling;Equations;Sampling methods;Probability","C language;CAD;digital simulation;fault location;software tools","high-level fault modeling technique;CONES;internal faults;C-modeled primitives;high-level synthesis tool;optimized two-level implementation;functional primitives;fault effects;fault simulation;CAD tools","","","10","","","","","","IEEE","IEEE Conferences"
"Self-assembling electrical networks: an application of micromachining technology","M. B. Cohn; C. J. Kim; A. P. Pisano","California Univ., Berkeley, CA, USA; California Univ., Berkeley, CA, USA; California Univ., Berkeley, CA, USA","TRANSDUCERS '91: 1991 International Conference on Solid-State Sensors and Actuators. Digest of Technical Papers","","1991","","","490","493","It is noted that the recent applications of silicon processing technology to the manufacture of micro-mechanical devices may allow engineering of self-assembling systems. Preliminary experiments show that regular two-dimensional lattices of at least 1000 millimeter-sized elements can be mechanically self-assembled using a method analogous to crystal annealing. Physical considerations suggest that conditions become more favorable to the process as size decreases, and give rise to guidelines for optimal design. Applications to IC manufacturing are discussed. The method requires large numbers of identical, freely moving discrete elements fabricated with a controlled three-dimensional geometry, and thus may be an area in which micromachining technology can be used to advantage.<<ETX>>","","0-87942-585","10.1109/SENSOR.1991.148919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148919","","Micromachining;Annealing;Application software;Temperature;Computer aided manufacturing;Silicon;Lattices;Circuit testing;Assembly systems;Steady-state","integrated circuit technology;micromechanical devices;optimisation;semiconductor technology;simulated annealing","self-assembling electrical networks;micromachining technology;self-assembling systems;regular two-dimensional lattices;crystal annealing;optimal design;IC manufacturing;freely moving discrete elements;controlled three-dimensional geometry;1000 mm","","12","9","","","","","","IEEE","IEEE Conferences"
"Calpa: a tool for automating selective dynamic compilation","M. Mock; C. Chambers; S. J. Eggers","Dept. of Comput. Sci. & Eng., Washington Univ., Seattle, WA, USA; NA; NA","Proceedings 33rd Annual IEEE/ACM International Symposium on Microarchitecture. MICRO-33 2000","","2000","","","291","302","Selective dynamic compilation systems, typically driven by annotations that identify run-time constants, can achieve significant program speedups. However, manually inserting annotations is a tedious and time-consuming process that requires careful inspection of a program's static characteristics and run-time behavior and much trial and error in order to select the most beneficial annotations. Calpa is a system that generates annotations automatically for the DyC dynamic compiler. Calpa combines execution frequency and value profile information with a model of dynamic compilation cost and dynamically generated code benefit to choose run-time constants and other dynamic compilation strategies. For the programs tested so far, Calpa generates annotations of the same or better quality as those found by a human, but in a fraction of the time. The result was equal or-better program speedups from dynamic compilation, but without the need for programmer intervention.","1072-4451","0-7695-0924","10.1109/MICRO.2000.898079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898079","","Dynamic compiler;Runtime;Costs;Programming profession;Optimizing compilers;Dispatching;Computer science;Inspection;Frequency;Testing","optimising compilers;software tools","Calpa;selective dynamic compilation;run-time constants;run-time behavior;DyC dynamic compiler;execution frequency;value profile information","","6","34","","","","","","IEEE","IEEE Conferences"
"Impact of design-manufacturing interface on SoC design methodologies","J. -. Carballo; S. R. Nassif","IBM Austin Res. Lab., TX, USA; IBM Austin Res. Lab., TX, USA","IEEE Design & Test of Computers","","2004","21","3","183","191","Today's semiconductor manufacturing trends are increasingly influencing hardware design techniques, tools, and methodologies. We analyze these trends and describe their effects on design methodologies. These effects clearly include impacts on yield optimization resolution enhancement.","0740-7475;1558-1918","","10.1109/MDT.2004.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1302084","","Design methodology;Semiconductor device manufacture;Computer aided manufacturing;Logic testing;Computer integrated manufacturing;Data mining;Manufacturing industries;Packaging;Design for manufacture;Software design","system-on-chip;integrated circuit layout;integrated circuit manufacture;design for manufacture;integrated circuit reliability","semiconductor manufacture;integrated circuit yield optimization;system-on-chip design;design-manufacturing interface;design for manufacture;integrated circuit reliability","","7","12","","","","","","IEEE","IEEE Journals & Magazines"
"On the bitmap-image based presenting","L. Zhao; H. Yamamoto","Dept. of Inf. Sci., Utsunomiya Univ., Japan; Dept. of Inf. Sci., Utsunomiya Univ., Japan","IEEE Sixth International Symposium on Multimedia Software Engineering","","2004","","","98","105","This paper considers the problem of giving a presentation over heterogeneous computing environment including different kinds of networks and information devices. This style of presenting is useful in distance learning, distance meeting, distance conferencing and other situations. After discussing the importance of separating content creation and presenting, the bitmap-image based presenting (BIP) is considered. A large number of presentations are studied, which show that BIP does not require much data amount. Furthermore, the next methods are studied to perform BIP even more efficiently: (1) background and foreground separation; (2) color reduction; (3) coding optimization. Other techniques like motion compensation and layered coding are also discussed. Finally, a BIP based distance presenting system is presented. It works over a TCP/IP network, and has been tested with a number of platforms including Linux PC, Windows PC, PDA and cell phone emulator.","","0-7695-2217","10.1109/MMSE.2004.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376646","","Computer networks;Computer aided instruction;Optimization methods;Motion compensation;TCPIP;IP networks;Testing;Linux;Personal digital assistants;Cellular phones","distance learning;teleconferencing;motion compensation;image colour analysis;image coding;image representation;IP networks;Internet;technical presentation;multimedia systems","bitmap-image based presentation;heterogeneous computing environment;distance learning;distance meeting;distance conferencing;content creation;content presentation;color reduction;coding optimization;motion compensation;layered coding;distance presenting system;TCP/IP network;Linux PC;Windows PC;PDA;cell phone emulator","","","7","","","","","","IEEE","IEEE Conferences"
"A memory-based architecture for MPEG2 System protocol LSIs","M. Inamori; J. Naganuma; H. Wakabayashi; M. Endo","NTT LSI Labs., Atsugi, Japan; NTT LSI Labs., Atsugi, Japan; NTT LSI Labs., Atsugi, Japan; NTT LSI Labs., Atsugi, Japan","Proceedings ED&TC European Design and Test Conference","","1996","","","500","507","This paper proposes a memory-based architecture implementing the MPEG2 System protocol LSIs, and demonstrates its flexibility and performance. The memory-based architecture implements the full functionality of the MPEG2 System protocol for both multiplexing and de-multiplexing MPEG2-encoded streams. It consists of a core CPU, memories, and dedicated application-specific hardware. It is designed and optimized by hardware/software co-design techniques. The LSIs provide sufficient performance and flexibility for real-time applications of the MPEG2 System protocol.","1066-1409","0-8186-7424","10.1109/EDTC.1996.494347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494347","","Memory architecture;Protocols;Large scale integration;Hardware;Streaming media;Standards development;Computer architecture;Transform coding;Decoding;Code standards","memory architecture;large scale integration;protocols;data compression;video coding;audio coding;codecs;digital signal processing chips;multiplexing equipment;demultiplexing equipment;real-time systems","memory-based architecture;MPEG2 system protocol LSI;multiplexing;demultiplexing;MPEG2-encoded streams;core CPU;dedicated application-specific hardware;hardware/software codesign techniques;real-time applications","","4","16","","","","","","IEEE","IEEE Conferences"
"Model selection for support vector machines using an asynchronous parallel evolution strategy","T. P. Runarsson; S. Sigurdsson","Inst. of Sci., Iceland Univ., Reykjavik, Iceland; NA","International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003","","2003","1","","495","498 Vol.1","The application of a parallel evolutionary algorithm (ES) to model selection for support vector machines is examined. The problem of model selection is a computationally intense non-convex optimization problem. For this reason a parallel search strategy is desirable. A new non-blocking asynchronous ES is developed for this task. The algorithm is tested on five standard test sets optimizing a number of heuristic bounds on the expected generalization error.","","0-7803-7702","10.1109/ICNNSP.2003.1279319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1279319","","Support vector machines;Evolutionary computation;Testing;Genetic mutations;Computer science;Application software;Computational efficiency;Robustness;Quadratic programming;Parallel processing","support vector machines;parallel algorithms;evolutionary computation;optimisation","support vector machines;asynchronous parallel evolution strategy;parallel evolutionary algorithm;nonconvex optimization problem","","2","9","","","","","","IEEE","IEEE Conferences"
"Document image layout comparison and classification","Jianying Hu; R. Kashi; G. Wilfong","Lucent Technol., AT&T Bell Labs., Murray Hill, NJ, USA; NA; NA","Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)","","1999","","","285","288","The paper describes features and methods for document image comparison and classification at the spatial layout level. The methods are useful for visual similarity based document retrieval as well as fast algorithms for initial document type classification without OCR. A novel feature set called interval encoding is introduced to capture elements of spatial layout. This feature set encodes region layout information in fixed-length vectors which can be used for fast page layout comparison. The paper describes experiments and results to rank-order a set of document pages in terms of their layout similarity to a test document. We also demonstrate the usefulness of the features derived from interval encoding in a hidden Markov model based page layout classification system that is trainable and extendible.","","0-7695-0318","10.1109/ICDAR.1999.791780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791780","","Information retrieval;Image retrieval;Data mining;Shape measurement;Optical character recognition software;Testing;Electronic switching systems;Image databases;Spatial databases;Spatial resolution","document image processing;image classification;information retrieval;encoding;hidden Markov models","document image layout comparison;document image classification;spatial layout level;visual similarity based document retrieval;fast algorithms;initial document type classification;interval encoding;spatial layout;region layout information;fixed-length vectors;fast page layout comparison;document pages;layout similarity;test document;hidden Markov model based page layout classification system;HMM","","13","11","","","","","","IEEE","IEEE Conferences"
"Toward market-driven agents for electronic auction","Kwang Mong Sim; Eric Wong","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Shatin, China; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","","2001","31","6","474","484","While there are several existing agent-based systems addressing the crucial and difficult issues of automated negotiation and auction, this research has designed and engineered a society of trading agents with two distinguishing features: 1) a market-driven negotiation strategy and 2) a deal optimizing auction protocol. Unlike some of the existing systems where users manually select predefined trading strategies, in the market-driven approach, trading agents automatically select the appropriate strategies by examining the changing market situations. Results from a series of experiments suggest that the market-driven approach generally achieved more favorable outcomes as compared to the fixed strategy approach. Furthermore, it provides a more intuitive simulation of trading because trading agents are able to respond to different market situations with appropriate strategies. By augmenting the auction protocol with a deal optimization stage, trading agents can be programmed to optimize transaction deals by delaying the finalization of deals in search of better deals. Experimental results showed that by having a deal optimization stage, the auction protocol produced generally optimistic outcomes.","1083-4427;1558-2426","","10.1109/3468.983399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983399","","Consumer electronics;Protocols;Electronic equipment testing;Design engineering;Humans;Design optimization;Delay;Pricing;Research initiatives;Electronic commerce","electronic commerce;multi-agent systems;software agents","market-driven agents;electronic auction;agent-based systems;automated negotiation;trading agents;electronic commerce;multi agent systems;market-driven negotiation strategy;deal optimizing auction protocol","","22","12","","","","","","IEEE","IEEE Journals & Magazines"
"Product performance-evaluation using Monte Carlo simulation: a case study","V. Crk","Sensing & Control Div., Honeywell Int., Freeport,IL, USA","Annual Reliability and Maintainability Symposium. 2001 Proceedings. International Symposium on Product Quality and Integrity (Cat. No.01CH37179)","","2001","","","384","392","Product performance evaluation in the room and in the extreme environmental conditions is one of the most important design engineering activities. Is the product designed to operate within the specified tolerances over the specified environment and are the product performance parameters optimized so that the effects of the environmental stresses are minimized? These are the questions that a design engineer has to answer before any new product is released to the market. Therefore, environmental testing and monitoring of the product outputs, intended design performance and other product design parameters need to be performed. Such tests may include significant number of samples, may be time consuming and very costly. Consequently, environmental tests should be designed and planned very carefully and the data analysis methodologies should require the minimum resources for data collection. The proposed methodology is based on the idea of the product performance modeling and monitoring the performance degradation over the range of the environmental stresses. It is very flexible and is applicable in all situations when a product's performance can be modeled with a linear or nonlinear functions, whose parameters may be random, correlated and stress dependent. It provides the tools for performance stress evaluation and reliability estimation. The methodology is presented through the following case study.","0149-144X","0-7803-6615","10.1109/RAMS.2001.902497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902497","","Computer aided software engineering;Sensor phenomena and characterization;Voltage;Stress;Sensor systems;Product design;Testing;Consumer electronics;Transfer functions;Temperature sensors","product development;reliability;failure analysis;Monte Carlo methods;environmental testing","product performance evaluation;Monte Carlo simulation;case study;design engineering;product performance parameters;environmental stresses;environmental testing;reliability modelling","","1","3","","","","","","IEEE","IEEE Conferences"
"Algorithm for achieving minimum energy consumption in CMOS circuits using multiple supply and threshold voltages at the module level","Y. S. Dhillon; A. U. Diril; A. Chatterjee; Hsien-Hsin Sean Lee","Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA","ICCAD-2003. International Conference on Computer Aided Design (IEEE Cat. No.03CH37486)","","2003","","","693","700","This paper proposes an optimum methodology for assigning supply and threshold voltages to modules in a CMOS circuit such that the overall energy consumption is minimized for a given delay constraint. The modules of the circuit should have large enough gate depths such that the delay and energy penalties of the level shifters connecting them are negligible. Both static and dynamic energy are considered in the optimization. Energy savings of up to 48% have been achieved on various example circuits. The first step in the optimization finds optimum supply and threshold voltages for each module in the circuit. If the circuit has a large number of modules, this step might yield a correspondingly large number of different supply and threshold voltages for minimum energy consumption. Since having a large number of different supply and threshold voltages on an IC is not feasible in current technologies, an additional step clusters the multiple voltages obtained from the first step into a fixed number of supply and threshold voltages (for example, 2 different supply voltages and 2 different threshold voltages). In addition to the application of this method to circuit optimization, it can also be applied to a wide range of problems with delay constraints, such as software tasks running on a dynamically variable V/sub DD/ and V/sub th/ processor.","","1-58113-762","10.1109/ICCAD.2003.1257885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1257885","","Energy consumption;Threshold voltage;Delay;Dynamic voltage scaling;Circuit optimization;Circuit testing;Iterative algorithms;DH-HEMTs;Power engineering and energy;CMOS technology","CMOS integrated circuits;delays;circuit optimisation;gradient methods;modules;low-power electronics","minimum energy consumption;CMOS circuits;complementary metal oxide semiconductor integrated circuit;multiple supply;threshold voltages;delay constraint;circuit modules;level shifters;IC;integrated circuit;circuit optimization;V/sub DD/ processor;V/sub th/ processor;gradient search algorithm","","19","16","","","","","","IEEE","IEEE Conferences"
"The performance measurement of cryptographic primitives on palm devices","D. S. Wong; H. H. Fuentes; A. H. Chan","Coll. of Comput. Sci., Northeastern Univ., Boston, MA, USA; Coll. of Comput. Sci., Northeastern Univ., Boston, MA, USA; Coll. of Comput. Sci., Northeastern Univ., Boston, MA, USA","Seventeenth Annual Computer Security Applications Conference","","2001","","","92","101","We developed and evaluated several cryptographic system libraries for Palm OS(R) which include stream and block ciphers, hash functions and multiple-precision integer arithmetic operations. We noted that the encryption speed of SSC2 outperforms both ARC4 (Alleged RC4) and SEAL 3.0 if the plaintext is small. On the other hand, SEAL 3.0 almost doubles the speed of SSC2 when the plaintext is very large. We also observed that the optimized Rijndael with 8KB of lookup tables is 4 times faster than DES. In addition, our results show that implementing the cryptographic algorithms as system libraries does not degrade their performance significantly. Instead, they provide great flexibility and code management to the algorithms. Furthermore, the test results presented provide a basis for performance estimation of cryptosystems implemented on the PalmPilot/sup TM/.","","0-7695-1405","10.1109/ACSAC.2001.991525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991525","","Measurement;Cryptography;Handheld computers;Libraries;System testing;Seals;Microprocessors;Internet;Educational institutions;Computer science","notebook computers;cryptography;software libraries;digital arithmetic;software performance evaluation;operating systems (computers);table lookup","performance measurement;cryptographic primitives;cryptographic system libraries;Palm OS;stream ciphers;block ciphers;hash functions;multiple-precision integer arithmetic operations;SSC2;ARC4;Alleged RC4;SEAL 3.0;encryption speed;optimized Rijndael;lookup tables;code management;PalmPilot","","18","11","","","","","","IEEE","IEEE Conferences"
"A genetic algorithm for database query optimization","Jorng-Tzong Horng; Cheng-Yan Kao; Baw-Jhiune Liu","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA","Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence","","1994","","","350","355 vol.1","Numerous decision support applications have been modeled as set covering and partitioning problems. We propose an extension to the database query language SQL to enable applications of these problems to be stated and solved directly by the database system. This will lead to the benefits of improved data independence, increased productivity and better performance. Six operators, namely cover, mincover, sumcover, partition, minpartition, and sumpartition are extended. We present genetic algorithms for the implementation of access routines for the proposed operators. We found that our genetic algorithm approach for extended operations and query optimization performed well both on the computational effort and the quality of the solutions through a variety of test problems. This approach makes it possible for a DBMS to respond to queries involving the proposed operators in a predicate restricted amount of time.<<ETX>>","","0-7803-1899","10.1109/ICEC.1994.349926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=349926","","Genetic algorithms;Query processing;Relational databases;Database systems;Costs;Computer science;Application software;Database languages;Decision support systems;Partitioning algorithms","query processing;SQL;query languages;relational databases;genetic algorithms;search problems","genetic algorithm;database query optimization;decision support applications;set covering problems;partitioning problems;database query language SQL;data independence;mincover;sumcover;access routines;query optimization;predicate restricted time","","2","25","","","","","","IEEE","IEEE Conferences"
"A novel Evolutionary Algorithm and its application for the traveling salesman problem","Zhihua Cai; Jinguo Peng; Wei Gao; Wei Wei; Lishan Kang","Dept. of Comp. Sci. & Technol., China Univ. of Geosci., Wuhan, China; Dept. of Comp. Sci. & Technol., China Univ. of Geosci., Wuhan, China; Dept. of Comp. Sci. & Technol., China Univ. of Geosci., Wuhan, China; Dept. of Comp. Sci. & Technol., China Univ. of Geosci., Wuhan, China; Dept. of Comp. Sci. & Technol., China Univ. of Geosci., Wuhan, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","","2004","3","","2253","2257 Vol.3","Based on GaoTao Algorithm (GT), Which is a typical Evolutionary Algorithm for traveling salesman problem (TSP). We first designed mapping and optimal operators, and proposed some control strategies. Then an improved GuoTao algorithm (IGT) for TSP was proposed. This algorithm was tested on some standard test instances, The results show that the algorithm presented in this paper is more efficient and more effective than existing algorithms for TSP, and it can be easily and directly generalized to solve other NP-hard combinatorial optimization problems.","","0-7803-8273","10.1109/WCICA.2004.1341990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1341990","","Evolutionary computation;Traveling salesman problems;Application software;Testing;Computer science;Geology;Algorithm design and analysis;Optimal control","evolutionary computation;travelling salesman problems;mathematical operators;computational complexity","evolutionary algorithm;traveling salesman problem;GaoTao algorithm;mapping operators;optimal operators;NP-hard problems;combinatorial optimization problems","","2","","","","","","","IEEE","IEEE Conferences"
"Theoretical and experimental study of nematic liquid crystal display cells using the in-plane-switching mode","F. Di Pasquale; Hui Fang Deng; F. Anibal Fernandez; S. E. Day; J. B. Davies; M. T. Johnson; A. A. van der Put; J. M. A. van de Eerenbeemd; J. A. M. M. van Haaren; J. A. Chapman","Dept. of Electron. & Electr. Eng., Univ. Coll. London, UK; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Electron Devices","","1999","46","4","661","668","In this paper a two-dimensional (2-D) dynamic model, based on a tensor formulation and solved numerically by combining finite elements and finite differences, is proposed and used for analyzing nematic liquid crystal (LC) test cells with interdigital electrodes. We compare theoretical and experimental results concerning the switching behavior, response mechanism, and viewing angle characteristics of nematic LC pixel structures which use the in-plane-switching (IPS) mode. The good agreement observed between theory and experiment in terms of electro-optical properties validates our modeling and demonstrates its potential for design optimization. We show that the proposed LC test cells, using the in-plane-switching mode, ensure switching-ON and -OFF response times of 22 and 28 ms, respectively, and excellent viewing angle characteristics.","0018-9383;1557-9646","","10.1109/16.753698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=753698","","Liquid crystal displays;Testing;Electrodes;Two dimensional displays;Tensile stress;Liquid crystals;Delay;Application software;Laboratories;Voltage","nematic liquid crystals;liquid crystal displays;tensors;large screen displays","nematic liquid crystal display cells;in-plane-switching mode;2D dynamic model;tensor formulation;interdigital electrodes;switching behavior;response mechanism;viewing angle characteristics;electro-optical properties;design optimization;response times;22 ms;28 ms","","16","16","","","","","","IEEE","IEEE Journals & Magazines"
"Quantitative coronary angiography with deformable spline models","A. K. Klein; F. Lee; A. A. Amini","Dept. of Internal Med., New England Med. Center, Boston, MA, USA; NA; NA","IEEE Transactions on Medical Imaging","","1997","16","5","468","482","Although current edge-following schemes can be very efficient in determining coronary boundaries, they may fail when the feature to be followed is disconnected (and the scheme is unable to bridge the discontinuity) or branch points exist where the best path to follow is indeterminate. Here, the authors present new deformable spline algorithms for determining vessel boundaries, and enhancing their centerline features. A bank of even and odd S-Gabor filter pairs of different orientations are convolved with vascular images in order to create an external snake energy field. Each fitter pair will give maximum response to the segment of vessel having the same orientation as the filters. The resulting responses across filters of different orientations are combined to create an external energy field for snake optimization. Vessels are represented by B-Spline snakes, and are optimized on filter outputs with dynamic programming. The points of minimal constriction and the percent-diameter stenosis are determined from a computed vessel centerline. The system has been statistically validated using fixed stenosis and flexible-tube phantoms. It has also been validated on 20 coronary lesions with two independent operators, and has been tested for interoperator and intraoperator variability and reproducibility. The system has been found to be specially robust in complex images involving vessel branchings and incomplete contrast filling.","0278-0062;1558-254X","","10.1109/42.640737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=640737","","Angiography;Spline;Deformable models;Filters;Bridges;Image segmentation;Dynamic programming;Imaging phantoms;Lesions;Testing","angiocardiography;splines (mathematics);edge detection;modelling;dynamic programming;medical image processing;diagnostic radiography","quantitative coronary angiography;deformable spline models;edge-following schemes;branch points;external snake energy field;B-Spline snakes;percent-diameter stenosis;flexible-tube phantom;coronary lesions;intraoperator variability;medical diagnostic imaging;reproducibility;complex images;vessel branchings;incomplete contrast filling;computed vessel centerline;fixed stenosis","Algorithms;Computer Simulation;Contrast Media;Coronary Angiography;Coronary Disease;Coronary Disease;Coronary Vessels;Humans;Image Processing, Computer-Assisted;Models, Cardiovascular;Observer Variation;Phantoms, Imaging;Radiographic Image Enhancement;Reproducibility of Results;Signal Processing, Computer-Assisted;Software","96","52","","","","","","IEEE","IEEE Journals & Magazines"
"Methodology for support and analysis of indemnity requests due to electrical equipment damaged in Eletropaulo customers","N. Kagan; E. L. Ferrari; N. M. Matsuo; S. X. Duarte; C. A. S. Penin; A. J. Monteiro; I. T. Domingues","Polytech. Sch., Sao Paulo Univ., Brazil; Polytech. Sch., Sao Paulo Univ., Brazil; Polytech. Sch., Sao Paulo Univ., Brazil; Polytech. Sch., Sao Paulo Univ., Brazil; Polytech. Sch., Sao Paulo Univ., Brazil; NA; NA","10th International Conference on Harmonics and Quality of Power. Proceedings (Cat. No.02EX630)","","2002","1","","304","309 vol.1","This paper presents a management system basis for analysis and technical support in order to attend indemnity customers after electrical equipment damages. This system is already in use by Eletropaulo, but now is being improved. The job includes solicitation studies that costumers are subjected to, that are performed by computational simulations; automatic collect system data optimization and the improvement of the analysis program (PID). Diagnosis are automatically generated by this program and the engineer is able to take a brief decision. It is presented a general overview of all the project, including computational studies realized with aid of ATP (Alternative Transients Program).","","0-7803-7671","10.1109/ICHQP.2002.1221450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221450","","Data mining;Resumes;Laboratories;Performance evaluation;Software testing;Optical computing;Computational modeling;Surges;Informatics;Software performance","management;customer services;laboratory techniques;power supply quality;power system simulation;surges","methodology;indemnity requests support;indemnity requests analysis;electrical equipment damage;eletropaulo customers;management system;technical support;solicitation studies;computational simulations;automatic collect system data optimization;analysis program improvement;PID system;diagnosis automatic generation;alternative transients program;sags;swells;laboratory essays;surge application","","","1","","","","","","IEEE","IEEE Conferences"
"Access pattern-based code compression for memory-constrained embedded systems","O. Ozturk; H. Saputra; M. Kandemir; I. Kolcu","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; NA","Design, Automation and Test in Europe","","2005","","","882","887 Vol. 2","As compared to a large spectrum of performance optimizations, relatively little effort has been dedicated to optimize other aspects of embedded applications such as memory space requirements, power, real-time predictability, and reliability. In particular, many modern embedded systems operate under tight memory space constraints. One way of satisfying these constraints is to compress executable code and data as much as possible. While research on code compression have studied efficient hardware and software based code strategies, many of these techniques do not take application behavior into account, that is, the same compression/decompression strategy is used irrespective of the application being optimized. This paper presents a code compression strategy based on control flow graph (CFG) representation of the embedded program. The idea is to start with a memory image wherein all basic blocks are compressed, and decompress only the blocks that are predicted to be needed in the near future. When the current access to a basic block is over, our approach also decides the point at which the block could be compressed. We propose several compression and decompression strategies that try to reduce memory requirements without excessively increasing the original instruction cycle counts.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395694","","Embedded system;Image coding;Application software;Flow graphs;Power system reliability;Computer science;Power engineering and energy;Embedded computing;Power engineering computing;Optimization","embedded systems;data flow graphs;microprocessor chips;data compression;power consumption","access pattern-based code compression;memory-constrained embedded systems;memory space requirements;power consumption;real-time predictability;reliability;code compression;control flow graph representation;CFG representation;memory image;decompression","","4","22","","","","","","IEEE","IEEE Conferences"
"An undergraduate systems engineering design project for using constructive and virtual simulation for an armed UAV design","S. O. DeLong; P. West","Dept. of Syst. Eng., US Mil. Acad., West Point, NY, USA; Dept. of Syst. Eng., US Mil. Acad., West Point, NY, USA","Proceedings of the Winter Simulation Conference","","2002","2","","1799","1803 vol.2","This paper presents a design project for undergraduate systems engineering students in which Armed Unmanned Aerial Vehicles (AUAV's) are designed, using the systems engineering design process taught at the United States Military Academy, and tested using constructive and virtual simulation. These results are compared to theoretical results obtained through applying Lanchester analysis. Students first analyze the stakeholders' needs and develop alternatives. The students research commercial off the shelf (COTS) UAV airframes, sensors and weapon systems that meet the stakeholders' needs. Using design of experiments and response surface optimization, laboratory experimentation is conducted using Janus simulation and Janus Evaluator Tool Set (JETS) software to test the feasible alternatives under varying weather conditions and altitudes to examine performance against a predetermined threat. The students evaluate the alternatives using multi-attribute utility theory and encompassing all the objectives defined in the stakeholder analysis. Sensitivity analysis is applied and a recommendation is made to the decision maker.","","0-7803-7614","10.1109/WSC.2002.1166470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166470","","Design engineering;Systems engineering and theory;Modeling;Unmanned aerial vehicles;Automotive engineering;Engineering students;Process design;System testing;Sensor systems;Weapons","remotely operated vehicles;digital simulation;military aircraft;military computing;engineering education;computer aided instruction","undergraduate systems engineering design project;virtual simulation;armed UAV design;Armed Unmanned Aerial Vehicles;systems engineering design process;constructive simulation;commercial off the shelf UAV airframes;weapon systems;sensors systems;response surface optimization;Janus simulation;Janus Evaluator Tool Set software;multi-attribute utility theory","","1","6","","","","","","IEEE","IEEE Conferences"
"Real time blood flow velocity monitoring in the microcirculation","F. Sapuppo; D. Longo; M. Bucolo; M. Intaglietta; P. Arena; L. Fortuna","Dipartimento di Ingegneria Elettrica Elettronica e dei Sistemi, Universita degli Studi di Catania, Italy; Dipartimento di Ingegneria Elettrica Elettronica e dei Sistemi, Universita degli Studi di Catania, Italy; Dipartimento di Ingegneria Elettrica Elettronica e dei Sistemi, Universita degli Studi di Catania, Italy; NA; NA; NA","The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2004","1","","2219","2222","A real-time monitoring system based on the dual slit methodology for the characterization of the red blood cell velocity at the level of microcirculation has been developed. The analog photometric signals are acquired and processed using a hybrid hardware-software system that exploits a A/D conversion and an optimized correlation algorithm on an embedded system. It is implemented exploiting the resources of a general purpose board capable to extract the useful information from the noisy photometric signals, to process them, to show and save the results and, therefore, to make the experiments reproducible. Two different approaches to the crosscorrelation algorithm have been tested and their performances have been compared to each. The system has been tested in in vivo experiments on anaesthetized hamsters. Several microvessels have been observed and the results have been compared to the output of an analog crosscorrelator to verify their coherence.","","0-7803-8439","10.1109/IEMBS.2004.1403647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403647","Microcirculation;Blood Velocity;Monitoring System;Dual Slit Methodology","Blood flow;Monitoring;Photometry;Signal processing;Real time systems;Red blood cells;Embedded system;Data mining;Testing;Performance evaluation","haemorheology;blood;cellular biophysics;medical signal processing;correlation methods;blood vessels;bio-optics;photometry","real time blood flow velocity monitoring;microcirculation;dual slit methodology;photometric signal processing;hybrid hardware-software system;A/D conversion;optimized correlation algorithm;crosscorrelation algorithm;anaesthetized hamsters;microvessels","","2","8","","","","","","IEEE","IEEE Conferences"
"Ordering fuzzy sets generated by a neural network algorithm","L. M. Sztandera","Dept. of Comput. Sci., Philadelphia Coll. of Textiles & Sci., PA, USA","Proceedings of 3rd International Symposium on Uncertainty Modeling and Analysis and Annual Conference of the North American Fuzzy Information Processing Society","","1995","","","800","805","Ordering fuzzy subsets is an important event in dealing with fuzzy decision problems in many areas. This issue has been of concern for many researchers over the years. Also, in the last several years, there has been a large and energetic upswing in neuroengineering research aimed at synthesizing fuzzy logic with computational neural networks. The two technologies often complement each other: neural networks supply the brute force necessary to accommodate and interpret large amounts of sensor data and fuzzy logic provides a structural framework that utilizes and exploits these low-level results. As a neural network is well known for its ability to represent functions, and the basis of every fuzzy model is the membership function, so the natural application of neural networks in fuzzy models has emerged to provide good approximations to the membership functions that are essential to the success of the fuzzy approach. This paper evaluates and analyzes the performance of available methods of ranking fuzzy subsets on a set of selected examples that cover possible situations we might encounter as defining fuzzy subsets at each node of a neural network. Through this analysis, suggestions as to which methods have better performance for utilization in neural network architectures, as well as criteria for choosing an appropriate method for ranking are made.","","0-8186-7126","10.1109/ISUMA.1995.527799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=527799","","Fuzzy sets;Neural networks;Fuzzy neural networks;Performance analysis;Testing;Algorithm design and analysis;Logic;Application software;Computer science;Educational institutions","fuzzy set theory;fuzzy neural nets","fuzzy set ordering;neural network algorithm;fuzzy decision problems;neuroengineering research;fuzzy logic synthesis;sensor data;membership function approximation;performance;fuzzy subset ranking;ranking method selection criteria","","2","20","","","","","","IEEE","IEEE Conferences"
"Analysis and enhancement of planning and scheduling applications in a distributed simulation testbed","N. Julka; P. Lendermann; Chin Soon Chong; L. F. M. Liow","Production & Logistics Planning Group, Singapore Inst. of Manuf. Technol., Singapore; Production & Logistics Planning Group, Singapore Inst. of Manuf. Technol., Singapore; Production & Logistics Planning Group, Singapore Inst. of Manuf. Technol., Singapore; Production & Logistics Planning Group, Singapore Inst. of Manuf. Technol., Singapore","Proceedings of the 2004 Winter Simulation Conference, 2004.","","2004","2","","1799","1806 vol.2","Planning and scheduling applications and operations simulation models jointly represent the manufacturing activities of an enterprise. This paper relates to a framework that enables integration of both into a unified model and allows improvement of their performance with discrete event simulation (DES) technology. The high level architecture, which is the IEEE standard for interoperability of simulations, forms the backbone of this framework in which business applications can be reused with operations simulation models to generate an integrated simulation model. This enables a company to optimise not only operational processes such as shop floor or warehouse operations but also business processes such as planning, order management and scheduling through simulation using the same software infrastructure. A case study to demonstrate the feasibility of this framework is included and ongoing work on implementation of this framework in an industrial environment is presented.","","0-7803-8786","10.1109/WSC.2004.1371532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371532","","Analytical models;Testing;Application software;Job shop scheduling;Enterprise resource planning;Business process re-engineering;Production planning;Technology planning;Optimized production technology;Logistics","scheduling;planning;discrete event simulation;IEEE standards;software reusability;open systems","business planning;business scheduling;distributed simulation testbed;operations simulation models;discrete event simulation;IEEE standard;simulation interoperability","","2","10","","","","","","IEEE","IEEE Conferences"
"User modeling for adaptive visualization systems","G. O. Domik; B. Gutkauf","Paderborn Univ., Germany; NA","Proceedings Visualization '94","","1994","","","217","223","Meaningful scientific visualizations benefit the interpretation of scientific data, concepts and processes. To ensure meaningful visualizations, the visualization system needs to adapt to desires, disabilities and abilities of the user, interpretation aim, resources (hardware, software) available, and the form and content of the data to be visualized. We suggest describing these characteristics with four models: user model, problem domain/task model, resource model and data model. The paper makes suggestions for the generation of a user model as a basis for an adaptive visualization system. We propose to extract information about the user by involving the user in interactive computer tests and games. Relevant abilities tested are color perception, color memory, color ranking, mental rotation, and fine motor coordination.<<ETX>>","","0-8186-6627","10.1109/VISUAL.1994.346316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=346316","","Adaptive systems;Data visualization;Physics computing;Data models;Testing;Data mining;Animation;Application software","data visualisation;colour vision;user modelling;human factors;data structures;database management systems","user modeling;adaptive visualization systems;scientific visualizations;scientific data;interpretation aim;problem domain/task model;resource model;data model;interactive computer tests;games;color perception;color memory;color ranking;mental rotation;fine motor coordination","","6","26","","","","","","IEEE","IEEE Conferences"
"Cognitive radio testbed: further details and testing of a distributed genetic algorithm based cognitive engine for programmable radios","C. J. Rieser; T. W. Rondeau; C. W. Bostian; T. M. Gallagher","Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA","IEEE MILCOM 2004. Military Communications Conference, 2004.","","2004","3","","1437","1443 Vol. 3","This paper provides details of a distributed genetic algorithm (GA) based cognitive radio engine model for disaster communications and its implementation in a cognitive radio test bed using programmable radios. Future applications include tactical and covert communications. The cognitive system monitor (CSM) module presented here permits cross layer cognition and adaptation of a programmable radio by classifying the observed channel, matching channel behavior with operational goals, and passing these goals to a wireless system genetic algorithm (WSGA) adaptive controller module to evolve and optimize radio operation. The CSM module algorithm provides for parallel distributed operation and includes a learning classifier and meta-GA functions that work from a knowledge base (which may be distributed) in long term memory to synthesize matched channels and operational goals that are retained in short term memory. Experimental results show that the cognitive engine finds the best tradeoff between a host radio's operational parameters in changing wireless conditions, while the baseline adaptive controller only increases or decreases its data rate based on a threshold, often wasting usable bandwidth or excess power when it is not needed due its inability to learn.","","0-7803-8847","10.1109/MILCOM.2004.1495152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1495152","","Testing;Cognitive radio;Genetic algorithms;Engines;Programmable control;Adaptive control;Radio control;Monitoring;Cross layer design;Cognition","cognitive systems;distributed algorithms;genetic algorithms;adaptive control;software radio;telecommunication control;learning (artificial intelligence);telecommunication channels","cognitive radio testbed;distributed genetic algorithm;programmable radio;cognitive radio engine model;disaster communication;covert communication;tactical communication;cognitive system monitor;cross layer cognition;wireless system genetic algorithm;adaptive controller module;learning classifier;matched channel","","46","7","","","","","","IEEE","IEEE Conferences"
"Linear pose estimation from points or lines","A. Ansar; K. Daniilidis","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2003","25","5","578","589","Estimation of camera pose from an image of n points or lines with known correspondence is a thoroughly studied problem in computer vision. Most solutions are iterative and depend on nonlinear optimization of some geometric constraint, either on the world coordinates or on the projections to the image plane. For real-time applications, we are interested in linear or closed-form solutions free of initialization. We present a general framework which allows for a novel set of linear solutions to the pose estimation problem for both n points and n lines. We then analyze the sensitivity of our solutions to image noise and show that the sensitivity analysis can be used as a conservative predictor of error for our algorithms. We present a number of simulations which compare our results to two other recent linear algorithms, as well as to iterative approaches. We conclude with tests on real imagery in an augmented reality setup.","0162-8828;2160-9292;1939-3539","","10.1109/TPAMI.2003.1195992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1195992","","Iterative algorithms;Cameras;Computer vision;Constraint optimization;Application software;Closed-form solution;Image analysis;Algorithm design and analysis;Sensitivity analysis;Iterative methods","computer vision;real-time systems;sensitivity analysis;augmented reality;computational geometry","linear pose estimation;camera pose;computer vision;nonlinear optimization;geometric constraint;real-time applications;image noise;sensitivity analysis;error;simulations;linear algorithms;iterative approaches;augmented reality","","210","28","","","","","","IEEE","IEEE Journals & Magazines"
"A reduced circuit library design system","R. D. Kilmoyer; D. J. Hathaway; A. M. Chu","IBM Gen. Technol. Div., Essex Junction, VT, USA; IBM Gen. Technol. Div., Essex Junction, VT, USA; IBM Gen. Technol. Div., Essex Junction, VT, USA","Proceedings of the IEEE 1988 Custom Integrated Circuits Conference","","1988","","","24.4/1","24.4/4","A reduced circuit library using triple-level metal CMOS consisting of nine primitive logic circuits and five latch kernels is proposed for a gate array library. A grouping program has been written to combine these circuits automatically into complex functions which are then hierarchically placed and wired to achieve the density and performance of a more complex library. This approach provides a set of complex functions which is optimized for each specific application while reducing the resource needed for library development and maintenance.<<ETX>>","","","10.1109/CICC.1988.20927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=20927","","Software libraries;Wiring;Application specific integrated circuits;Design automation;Circuit synthesis;Latches;Integrated circuit technology;Capacitance;Design optimization;Circuit testing","cellular arrays;circuit layout CAD;CMOS integrated circuits;integrated logic circuits","ASIC;layout CAD;gate arrays;design system;reduced circuit library;triple-level metal;CMOS;primitive logic circuits;latch kernels;gate array library;grouping program;hierarchically placed;set of complex functions","","3","6","","","","","","IEEE","IEEE Conferences"
"A new continuous t-norm and its application in fuzzy control","Dan Chen; Huacan He; Yuqin Ji; Hui Wang","Dept. of Comput. Sci. & Eng., Northwestern Polytech. Univ., Xian, China; NA; NA; NA","Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393)","","2000","3","","1795","1798 vol.3","In conventional fuzzy control, MAX and MIN operators are usually adopted, but MIN and MAX operations only consider the extreme elements and ignore the contributions of other elements, which will inevitably affect the control performance. To solve this problem, we presented a new kind of t-norm and t-conorm, which can vary through Lukasiewicz operators, probability operators, and Zadeh operators with a correlation coefficient h ranging from [-1, 1]. On this basis we present simplified AND and OR operators to replace MAX and MIN operators in fuzzy control. In the simulation we use genetic algorithms to optimize the correlation coefficients and the simulation result shows that our method is better than the conventional one.","","0-7803-5995","10.1109/WCICA.2000.862783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=862783","","Fuzzy control;Genetic algorithms;Application software;Computer science;Optimization methods;Logic;Fuzzy systems;Testing;Fuzzy sets;Boundary conditions","fuzzy control;probability;genetic algorithms;fuzzy set theory","continuous t-norm;t-conorm;Lukasiewicz operators;probability operators;Zadeh operators;correlation coefficient;AND operator;OR operator","","","6","","","","","","IEEE","IEEE Conferences"
"CML III bipolar standard cell library","B. Tufte","Honeywell Inc., Plymouth, MN, USA","Proceedings of the 1988 Bipolar Circuits and Technology Meeting,","","1988","","","180","182","A 1.25 mu m current mode logic (CML) bipolar standard cell library with subnanosecond loaded gate delays is discussed. Unique computer-aided design CAD tools that produce accurate models of the library cells and optimize designs for area, speed, and power are also discussed. The interplay between the library and the tools can produce higher speed, lower power chips, at less cost.<<ETX>>","","","10.1109/BIPOL.1988.51073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=51073","","Software libraries;Macrocell networks;Circuit testing;Design automation;Costs;Power system modeling;Timing;Microcell networks;Design optimization;Very high speed integrated circuits","bipolar integrated circuits;cellular arrays;emitter-coupled logic;integrated logic circuits;logic CAD","bipolar standard cell library;current mode logic;subnanosecond loaded gate delays;CAD tools;models;1.25 micron","","1","","","","","","","IEEE","IEEE Conferences"
"End-winding region configuration of an HTS transformer","F. Zizek; Z. Jelinek; Z. Timoransky; H. Piel; F. Chovanec; P. Mozola; M. Polak","Skoda Vyzkum, Plzen, Czech Republic; Skoda Vyzkum, Plzen, Czech Republic; Skoda Vyzkum, Plzen, Czech Republic; NA; NA; NA; NA","IEEE Transactions on Applied Superconductivity","","2002","12","1","904","906","Single-phase 14 kVA transformer model with Bi-2223 superconducting windings in a nonmetallic cryostat has been designed and fabricated. Rated voltage of the primary and the secondary winding is 400 V and 200 V, respectively. Considerable attention was paid to the minimization of the radial component of the magnetic field at the end of the winding, where this field component, perpendicular to the tape, reduces critical current and increases AC losses. Additional laminated ferromagnetic rings shaping the magnetic field around the winding edges are used to reduce the radial field component. The additional rings also eliminate stray field differences along winding circumference. A finite element method (software ANSYS) was used for optimization of the ring cross-section and their position with respect to the winding. The calculations show that the rings reduce the radial component of the magnetic field by about 50% and the critical current of the winding can be increased from 53 A to 65 A.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2002.1018546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1018546","","High temperature superconductors;Magnetic fields;Critical current;Magnetic field measurement;Circuit testing;Superconducting films;Nitrogen;Finite element methods;Optimization methods;Coolants","high-temperature superconductors;superconducting transformers;transformer windings;bismuth compounds;strontium compounds;calcium compounds;losses;magnetic fields","critical current reduction;magnetic field distribution;superconducting transformer;end-winding region configuration;HTS transformer;single-phase transformer model;Bi-2223 superconducting windings;nonmetallic cryostat;radial component minimisation;AC losses;laminated ferromagnetic rings;magnetic field shaping;winding edges;radial field component reduction;ring cross-section optimisation;14 kVA;400 V;200 V;53 to 65 A;Bi/sub 2/Sr/sub 2/Ca/sub 2/Cu/sub 3/O","","6","6","","","","","","IEEE","IEEE Journals & Magazines"
"Graduate class for system-level low-power design","Naehyuck Chang; Hyeonmin Lim; Kyungsoo Lee; Youngjin Cho; Hyung Gyu Lee; Hojun Shim","Sch. of CSE, Seoul Nat. Univ., South Korea; Sch. of CSE, Seoul Nat. Univ., South Korea; Sch. of CSE, Seoul Nat. Univ., South Korea; Sch. of CSE, Seoul Nat. Univ., South Korea; Sch. of CSE, Seoul Nat. Univ., South Korea; Sch. of CSE, Seoul Nat. Univ., South Korea","2005 IEEE International Conference on Microelectronic Systems Education (MSE'05)","","2005","","","31","32","This paper describes a graduate-level class to introduce a concrete framework for system level low-power design. This class is not a university class, but one of the special programs of the IT-SoC Promotion Group with the support of the Korea IT Industry Promotion Agency established by the Korean Ministry of Information and Communication. It is open to EECS and CSE graduate students from various universities, who participate in the IT-SoC Academy. We organize a two-week program offered during the winter vacation, consisting of 20-hour lectures and 40-hour experiments. The class is experiment oriented, and the students. build their own power simulators for low-power embedded systems based on theories and concepts introduced in the class.","","0-7695-2374","10.1109/MSE.2005.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509350","","Hardware;Embedded system;Design optimization;Design methodology;Application software;Collaborative software;Permission;Computational modeling;Concrete;Communication industry","power consumption;electronic engineering education;low-power electronics;system-on-chip;educational courses;circuit simulation;embedded systems","system-level low-power design;IT-SoC Promotion Group;Korea IT Industry Promotion Agency;Korean Ministry of Information and Communication;EECS;CSE graduate students;experiment oriented class;power simulators;low-power embedded systems;graduate-level class","","","2","","","","","","IEEE","IEEE Conferences"
"Intelligent and free user configurable low cost data acquisition unit","K. Edelmoser; C. Anselmi","Power Electron. Sect., Tech. Univ. Wien, Austria; NA","Proceedings of the 1996 IEEE IECON. 22nd International Conference on Industrial Electronics, Control, and Instrumentation","","1996","2","","1301","1305 vol.2","Due to the low costs of modern electronic components, a versatile data acquisition unit (DAQ), as described in this paper, is an efficient replacement for expensive telemetric measurement circuits. This DAQ is the result of a study for a cheaper solution for tool parameter optimization, which so long has been performed by telemetric sensors. For general use it is necessary to generate a highly robust DAQ system, which is very small in size. Very low power consumption, for a long active operating time with standard types of battery cells, is also required. To be a convenient replacement for telemetric circuits, the data memory is configurable with a maximum size of up to 8 MB (up to 16 or 32 MB in the near future). A local processor is required, which should be able to perform real-time data compression and, depending on the application, analog signal path filtering and/or pre data calculations (e.g. validation). The signal path consists of an analog signal conditioning line with a simple anti-aliasing filter structure and a free configurable analog to digital converter (ADC) with a maximum sampling rate of 25 kHz for one channel. For downloading of user specific hardware and software, a monitor program and a serial interface allows communication with a host terminal or standard personal computer. First testing of this design has shown a wide range of replacement for applications where intelligent sensors with telemetric capability are needed At the end of this paper, a special application in the field of computer numerically controlled machines is presented.","","0-7803-2775","10.1109/IECON.1996.566067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566067","","Costs;Data acquisition;Telemetry;Circuits;Application software;Electronic components;Robustness;Energy consumption;Batteries;Data compression","data acquisition;computerised instrumentation;microcomputer applications;analogue-digital conversion;signal processing","data acquisition unit;free user configurable structure;costs;tool parameter optimization;CNC machines;hardware;software;analog signal conditioning line;anti-aliasing filter;analog to digital converter;monitor program;serial interface;personal computer;25 kHz;8 MB","","","8","","","","","","IEEE","IEEE Conferences"
"A cryptographically secure EW database with selective random access","N. Ikram; S. J. Shepherd","Dept. of Electr. Eng., Bradford Univ., UK; NA","MILCOM 97 MILCOM 97 Proceedings","","1997","3","","1407","1411 vol.3","This paper describes the design and implementation of a high security electronic warfare (EW) database based on building up a hierarchical information system employing multilevel security classifications. Users' activities in each classification are authorised via personal identification numbers (PINs). The information is encrypted using the software optimised encryption algorithm (SEAL) which enables random access to and decryption of records in the database as well as guaranteeing the integrity of the retrieved records. Only the required record is decrypted; the database as stored on the permanent media remains fully encrypted at all times providing a very high degree of database confidentiality and integrity. The application has been particularly targeted towards electronic warfare libraries and tests conducted to determine its utility in real-time systems.","","0-7803-4249","10.1109/MILCOM.1997.644999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=644999","","Cryptography;Databases;Electronic warfare;Data security;Information security;Information systems;Multilevel systems;Pins;Software algorithms;Seals","electronic warfare;cryptography;data integrity;database management systems;military computing;authorisation","cryptographically secure EW database;selective random access;design;implementation;high security electronic warfare database;hierarchical information system;multilevel security classifications;personal identification numbers;software optimised encryption algorithm;PIN;SEAL;decryption;integrity;required record;permanent media;database confidentiality;electronic warfare libraries;real-time systems","","","6","","","","","","IEEE","IEEE Conferences"
"Two dynamic performance tuning methods for portable parallel programs","K. Suzaki; T. Kurita; H. Tanuma; S. Hirano; Y. Ichisugi","Electrotech. Lab., Tsukuba, Japan; Electrotech. Lab., Tsukuba, Japan; Electrotech. Lab., Tsukuba, Japan; Electrotech. Lab., Tsukuba, Japan; Electrotech. Lab., Tsukuba, Japan","Proceedings 1st International Conference on Algorithms and Architectures for Parallel Processing","","1995","2","","585","594 vol.2","We present two dynamic performance tuning methods for portable parallel programs on various parallel computers. In parallel programs the affinity between parallel algorithms and the architecture of the target parallel computer is very important. In this paper we focus on the parallelism in view of the number of micro-tasks which are processing units in parallel programs. The presented methods estimate the optimal number of micro-tasks before the parallel processing is invoked. Furthermore, they shorten the execution time of the parallel program so that it is close to the optimal execution time. The estimation is based on the result of pre-executions of the program for different sizes of the data to be processed on a target parallel computer. One tuning method uses nearest-neighbor interpolation and the other uses spline interpolation for the estimation. We tested these tuning methods using a parallel square-matrix multiplication program written in Dataparallel C on three different parallel computers; a Paragon, an iPSC/2, and an nCUBE/2. In these experiments, the method using nearest-neighbor interpolation brought the execution time closer to the optimum than did the method using spline interpolation. The nearest-neighbor interpolation method yielded average execution times, which are given in terms of the optimal execution time, of 1.01 for the Paragon, 1.005 for the iPSC/2, and 1.052 for the nCUBE/2.<<ETX>>","","0-7803-2018","10.1109/ICAPP.1995.472244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472244","","Concurrent computing;Parallel processing;Portable computers;Parallel algorithms;Delay;Spline;Computer architecture;Optimizing compilers;Laboratories;Parallel programming","software portability;parallel programming;software performance evaluation;computational complexity","dynamic performance tuning methods;portable parallel programs;parallel computers;parallel algorithms;micro-tasks;optimal execution time;nearest-neighbor interpolation;spline interpolation;parallel square-matrix multiplication program","","","16","","","","","","IEEE","IEEE Conferences"
"Solid state","L. Geppert","NA","IEEE Spectrum","","1994","31","1","50","53","Competition among semiconductor manufacturers has intensified, quickening the pace of technological development. In the drive to capture market share, companies are turning out new generations of products with dizzying speed. Subhalf-micrometer lithographic feature sizes in commercial products have pegged new lows, while 275 MHz processing speeds have reached new highs. With feature sizes progressing toward 0.25 m, scientists are optimizing everything from process-monitoring software to circuit boards and modules. Some are already looking at the next step/spl minus/linewidths of 0.18 m and below/spl minus/that will be needed to make 1-gigabit memories. To keep up with product demand and rapidly advancing technology, new factories are going up all over the world, and marketing strategies have changed. The latest developments are discussed including microprocessors that can save power when machines are idle, and the new Alpha 275 MHz chip and Si-Ge technology.","0018-9235;1939-9340","","10.1109/6.249069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=249069","","Solid state circuits;Computer aided manufacturing;Application software;Reduced instruction set computing;Microprocessors;Silicon;Portable computers;Energy management;Testing;Conductors","electronics industry;microprocessor chips;electronic equipment manufacture;technological forecasting","solid state;semiconductor manufacturers;Alpha 275 MHz chip;process-monitoring software;microprocessors;Si-Ge technology;275 MHz","","2","","","","","","","IEEE","IEEE Journals & Magazines"
"Face recognition by using discriminative common vectors","H. Cevikalp; M. Wilkes","Dept. of Electr. Eng. & Comput. Sci., Vanderbilt Univ., Nashville, TN, USA; Dept. of Electr. Eng. & Comput. Sci., Vanderbilt Univ., Nashville, TN, USA","Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.","","2004","1","","326","329 Vol.1","In face recognition tasks, the dimension of the sample space is typically larger than the number of the samples in the training set. As a consequence, the within-class scatter matrix is singular and the linear discriminant analysis (LDA) method cannot be applied directly. This problem is also known as the ""small sample size"" problem. In this paper, we propose a new face recognition method based on the discriminative common vectors for the small sample size case. The discriminative common vectors representing the people in the face database were found by using the null space of the within-class scatter matrix. Then, these vectors were used for classification of new faces. Test results show that the proposed method is superior to other methods in terms of accuracy, efficiency, and numerical stability.","1051-4651","0-7695-2128","10.1109/ICPR.2004.1334118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334118","","Face recognition;Face detection;Null space;Light scattering;Vectors;Application software;Linear discriminant analysis;Optimized production technology;Databases;Testing","face recognition;numerical stability;vectors;matrix algebra;image classification","face recognition;discriminative common vectors;scatter matrix;linear discriminant analysis;face database;numerical stability","","4","11","","","","","","IEEE","IEEE Conferences"
"Hybrid phantom representation for simulation of CT systems using intrinsic shapes, tomographic volumes and higher-order surfaces","J. Peter; O. Nix; R. B. Schulz","Dept. of Biophys. & Med. Radiat. Phys., German Cancer Res. Center, Heidelberg, Germany; Dept. of Biophys. & Med. Radiat. Phys., German Cancer Res. Center, Heidelberg, Germany; Dept. of Biophys. & Med. Radiat. Phys., German Cancer Res. Center, Heidelberg, Germany","IEEE Symposium Conference Record Nuclear Science 2004.","","2004","4","","2453","2455 Vol. 4","We have extended our previously introduced hybrid phantom model to allow for phantom representation by higher-order surfaces (point meshes, polygonal meshes) in addition to any combination of intrinsic shapes (cubes, quadrics, super-quadrics) and voxel data originating from tomography (segmented CT/MRI volumes). Higher-order surfaces can be represented by uniform and non-uniform triangles obtained by means of Delaunay's algorithm. In order to optimize computational performance for intersection calculations, a variety of efficient rejection tests has been introduced and a novel numerical algorithm which relies on a strategy similar to cue optimization is proposed. The proposed phantom model allows for unlimited complexity and number of phantom compartments. Various hybrid phantom representation examples including associated Monte Carlo simulation studies are presented and demonstrate the enormous versatility of our simulation framework. Compared to previous implementations computational speed has improved for simulation of PET and SPECT systems by more than 40% on the same workstation with the same hardware.","1082-3654","0-7803-8700-70-7803-8701","10.1109/NSSMIC.2004.1462752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1462752","","Imaging phantoms;Computed tomography;Shape;Solid modeling;Computational modeling;Magnetic resonance imaging;Ray tracing;Application software;Testing;Positron emission tomography","phantoms;computerised tomography;positron emission tomography;single photon emission computed tomography;Monte Carlo methods;mesh generation","hybrid phantom representation;CT system simulation;intrinsic shapes;tomographic volumes;higher-order surfaces;point meshes;polygonal meshes;voxel data;tomography;uniform triangles;nonuniform triangles;Delaunay algorithm;computational performance;intersection calculations;numerical algorithm;cue optimization;Monte Carlo simulation;PET systems;SPECT systems","","1","7","","","","","","IEEE","IEEE Conferences"
"SAT solving using an epistasis reducer algorithm plus a GA","E. Rodriguez-Tello; J. Torres-Jimenez","Comput. Sci. Dept., ITESM Campus Cuernavaca, Morelos, Mexico; Comput. Sci. Dept., ITESM Campus Cuernavaca, Morelos, Mexico","Proceedings Fifth International Conference on Computational Intelligence and Multimedia Applications. ICCIMA 2003","","2003","","","188","193","A novel method for solving satisfiability (SAT) instances is presented. It is based on two components: a) an epistasis reducer algorithm (era) that produces a more suited representation (with lower epistasis) for a genetic algorithm (GA) by preprocessing the original SAT problem; and (b) a genetic algorithm that solves the preprocessed instances. Era is implemented by a simulated annealing algorithm (SA), which transforms the original SAT problem by rearranging the variables to satisfy the condition that the most related ones are in closer positions inside the chromosome. Results of experimentation demonstrated that the proposed combined approach outperforms GA in all the tests accomplished.","","0-7695-1957","10.1109/ICCIMA.2003.1238123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238123","","Genetic algorithms;Biological cells;Computer science;Simulated annealing;Testing;Convergence;Application software;Time factors;Constraint theory;Optimization methods","genetic algorithms;simulated annealing;computability","satisfiability;SAT;epistasis;genetic algorithm;GA;simulated annealing;SA","","","17","","","","","","IEEE","IEEE Conferences"
"Evolution strategies applied to perturbed objective functions","T. Back; U. Hammel","Dept. of Comput. Sci., Dortmund Univ., Germany; Dept. of Comput. Sci., Dortmund Univ., Germany","Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence","","1994","","","40","45 vol.1","We investigate the behavior of evolution strategies on noisy objective functions. We show for the simple sphere model that convergence velocity is not reduced as long as the noise level is small compared to the function value. If the noise level reaches a certain threshold, a size of the parent population greater than 1 improves the convergence precision significantly. Convergence reliability is tested for two nonconvex functions. Again the search process seems to be not influenced by low level noise. Interpreting the impact of noise purely as a modification of the selection process gives new insight into the role of selection in evolution strategies.<<ETX>>","","0-7803-1899","10.1109/ICEC.1994.350045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=350045","","Convergence;Noise level;Testing;Noise robustness;Genetic mutations;Electronic switching systems;Computer science;Design optimization;Application software;Computer simulation","perturbation techniques;minimisation;convergence of numerical methods;genetic algorithms;noise;search problems","perturbed objective functions;evolution strategies;noisy objective functions;simple sphere model;convergence velocity;noise level;function value;parent population;convergence precision;convergence reliability;nonconvex functions;search process;low level noise","","17","8","","","","","","IEEE","IEEE Conferences"
"Trellis-coded residual vector quantization: its geometrical advantages and application to image coding","M. A. U. Khan; M. J. T. Smith; S. W. McLaughlin","Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA","2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)","","2000","5","","2617","2620 vol.5","Residual vector quantization (RVQ), also known as multistage vector quantization, is investigated in the context of quantization cell shapes and is found to produce oblong cell shapes and suboptimal point densities. The oblong cell shapes are partially responsible for the performance degradation of RVQ compared with vector quantization (VQ). In an attempt to realize better cell shapes, a trellis-coded RVQ (TCRVQ) is suggested and is shown to provide optimal point densities and square cell shapes. In order to improve the performance of TCRVQ, an entropy-constrained TCRVQ (EC-TCRVQ) is designed and implemented for non-uniformly distributed sources. The simulation results indicate a performance improvement of 1.5 dB for EC-TCRVQ over entropy-constrained trellis-coded VQ. For an image coding application, we have developed conditional EC-TCRVQ, by employing adjacent vector conditioning in addition to residual vector conditioning. The simulation tests show that the 8/spl times/8 CEC-TCRVQ outperforms other predictive and trellis-based VQ schemes.","1520-6149","0-7803-6293","10.1109/ICASSP.2000.861005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=861005","","Vector quantization;Histograms;Application software;Image coding;Degradation;Design optimization;Shape measurement;Predictive models;Testing;Collaborative work","image coding;vector quantisation;trellis codes;entropy codes","trellis-coded residual vector quantization;image coding;multistage vector quantization;quantization cell shapes;geometrical advantages;oblong cell shapes;suboptimal point densities;square cell shapes;entropy-constrained TCRVQ;EC-TCRVQ;non-uniformly distributed sources;simulation results;adjacent vector conditioning;residual vector conditioning","","","13","","","","","","IEEE","IEEE Conferences"
"Performance comparable design of efficient synchronization protocols for distributed simulation","G. Chiola; A. Ferscha","Dipartimento di Inf., Torino Univ., Italy; NA","MASCOTS '95. Proceedings of the Third International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems","","1995","","","59","65","A large number of variations of distributed simulation protocols have been proposed in the literature. Their performances, however, could not be compared directly, due to different implementation strategies, different optimizations and different software environments as well as parallel hardware platforms. To rank the protocols accurate enough with respect to their execution performance the only practical evaluation, consists in implementing different alternatives and measure their relative performance on significant test cases. In this paper we demonstrate the possibility of simultaneous implementation of the three substantially different protocols starting from a single program skeleton. A clear separation of the simulation part and the synchronisation part of the protocols and reuse of code for more than one variant are the key concepts.<<ETX>>","","0-8186-6902","10.1109/MASCOT.1995.378709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=378709","","Protocols;Discrete event simulation;Computational modeling;Software performance;Hardware;Testing;Parallel processing;Constraint optimization;Distributed computing;Proposals","parallel programming;discrete event simulation;protocols;synchronisation","performance comparable design;efficient synchronization protocols;distributed simulation protocols;implementation strategies;software environments;parallel hardware platforms;execution performance","","2","10","","","","","","IEEE","IEEE Conferences"
"Determining optimal filters for binarization of degraded grayscale characters using genetic algorithms","Y. Ojima; S. Kirigaya; T. Wakahara","Fac. of Comput. & Inf. Sci., Hosei Univ., Tokyo, Japan; Fac. of Comput. & Inf. Sci., Hosei Univ., Tokyo, Japan; Fac. of Comput. & Inf. Sci., Hosei Univ., Tokyo, Japan","Eighth International Conference on Document Analysis and Recognition (ICDAR'05)","","2005","","","555","559 Vol. 2","Optimal binarization of degraded grayscale characters is a crucial step to subsequent character recognition. This paper proposes a new, promising binarization technique of grayscale characters using genetic algorithms (GA) to search for an optimal sequence of filters from among a set of rather simple, representative image processing filters. First, we classify degraded samples of grayscale characters into several categories. Then, in the learning stage, by selecting a training sample from each degradation category we apply GA to the combinatorial optimization problem of determining a sequence of filters that maximizes the fitness value between the filtered training sample and its target image ideally binarized by humans. Finally, in the testing stage, we apply the optimal sequence of filters thus obtained to remaining test samples for each degradation category. Experiments using the public ICDAR 2003 robust OCR dataset demonstrate promising results of binarization of grayscale characters against a wide variety of degradation causes.","1520-5363;2379-2140","0-7695-2420","10.1109/ICDAR.2005.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575606","","Filters;Degradation;Gray-scale;Genetic algorithms;Testing;Character recognition;Image processing;Humans;Robustness;Optical character recognition software","character recognition;image recognition;genetic algorithms;filtering theory","degraded grayscale characters;character recognition;genetic algorithm;optimal binarization;image processing filters;combinatorial optimization problem","","3","8","","","","","","IEEE","IEEE Conferences"
"Optimization of the real-time dispatch with constraints for secure operation of bulk power systems","W. R. Barcelo; W. W. Lemmon; H. R. Koen","Middle South Services, Inc., New Orleans, Louisiana; NA; NA","IEEE Transactions on Power Apparatus and Systems","","1977","96","3","741","757","A unique non-linear optimization program for exact real-time optimal dispatching with security constraints is presented. The basic optimization technique employs complex Lagrange multipliers and an innovative slack variable approach. The program was designed specifically for real-time application but can also be used off-line for optimal power flow solutions. Fundamental to the technique is a new Hessian matrix approximation, called a Diflex Hessian, which preserves sparsity and saves considerable computer core memory over the conventional Hessian. The Diflex Hesslan is derived in a unique mathematical style which was instrumental in its development. Other novel techniques of sparse matrix elimination and optimization are also described including unique logic for inequality constraint switching. Test results on a 1200-bus model are presented and indicate computer core memory and time requirements are comparable to a Newton power flow solution.","0018-9510","","10.1109/T-PAS.1977.32388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1601990","","Constraint optimization;Real time systems;Power systems;Power system security;Load flow;Sparse matrices;Dispatching;Lagrangian functions;Application software;Instruments","","","","19","10","","","","","","IEEE","IEEE Journals & Magazines"
"Event model interfaces for heterogeneous system analysis","K. Richter; R. Ernst","Inst. of Comput. & Commun. Network. Eng., Technische Univ. Braunschweig, Germany; Inst. of Comput. & Commun. Network. Eng., Technische Univ. Braunschweig, Germany","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","506","513","Complex embedded systems consist of hardware and software components from different domains, such as control and signal processing, many of them supplied by different IP vendors. The embedded system designer faces the challenge to integrate, optimize and verify the resulting heterogeneous systems. While format verification is available for some subproblems, the analysis of the whole system is currently limited to simulation or emulation. In this paper we tackle the analysis of global resource sharing, scheduling, and buffer sizing in heterogeneous embedded systems. For many practically used preemptive and non-preemptive hardware and software scheduling algorithms of processors and busses, semi-formal analysis techniques are known. However they cannot be used in system level analysis due to incompatibilities of their underlying event models. This paper presents a technique to couple the analysis of local scheduling strategies via an event interface model. We derive transformation rules between the most important event models and provide proofs where necessary. We use expressive examples to illustrate their application.","1530-1591","0-7695-1471","10.1109/DATE.2002.998348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998348","","Embedded system;Hardware;Processor scheduling;Embedded software;Process control;Control systems;Signal processing;Design optimization;Analytical models;Emulation","resource allocation;discrete event simulation;processor scheduling;buffer storage;embedded systems;synchronisation","heterogeneous systems;formal verification;simulation;emulation;global resource sharing;scheduling;buffer sizing;heterogeneous embedded systems;system level analysis;local scheduling;event interface model","","33","23","","","","","","IEEE","IEEE Conferences"
"Design and analysis of nonlinear hierarchical controllers for electric utility industry","A. Rubaai","Dept. of Electr. Eng., Howard Univ., Washington, DC, USA","Conference Record of the 2002 IEEE Industry Applications Conference. 37th IAS Annual Meeting (Cat. No.02CH37344)","","2002","1","","614","621 vol.1","This paper suggests a control strategy of coordinating multiple dynamic-braking units during the transients ensuing major disturbances. The control strategy considered in this study is a two-level hierarchy. The proposed two-level structure results from the decomposition of the overall problem into parallel sub-problems. This allows the retention of the closed-loop controls associated with each subsystem, which together constitute the lower level (Level I). The central coordinating controller forms the upper level (Level II). The coordination of the local controllers by the central controller, accounts for nonlinear terms and interconnections, and yields the global optimization of the overall system transient performance. The local controllers are not dependent on one another and are robust to any changes in the network configuration, due to the feedback or closed-loop control formulation inherent in the proposed strategy. To ensure physical realizability of the local controllers, the input was restricted to locally measurable signals. The methodology was implemented into a prototype software program, which was tested on a single machine connected to a very large network approximated by an infinite bus, and then on the IEEE four-generator test system. These studies considered fault clearing times greater that the critical, assuring an unstable condition. The well-damped optimal state and control trajectories illustrate the successful solution of the problem, indicating that the technique is a valuable tool dealing with transient control problems for large-scale systems.","0197-2618","0-7803-7420","10.1109/IAS.2002.1044154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044154","","Centralized control;Control systems;Nonlinear control systems;Software testing;System testing;Optimal control;Robust control;Feedback;Software prototyping;Large-scale systems","electricity supply industry;power system control;control system synthesis;nonlinear control systems;hierarchical systems;power system analysis computing;power system stability;control system analysis computing;closed loop systems","electric utility industry;nonlinear hierarchical controllers;control design;multiple dynamic-braking units;disturbances;transient performance;feedback;closed-loop control;control simulation;computer simulation;well-damped optimal state trajectories;large-scale systems","","","14","","","","","","IEEE","IEEE Conferences"
"A novel technology mapping method for AND/XOR expressions","Seok-Bum Ko; Jien-Chung Lo","Dept. of Electr. Eng., Saskatchewan Univ., Saskatoon, Sask., Canada; NA","33rd International Symposium on Multiple-Valued Logic, 2003. Proceedings.","","2003","","","133","138","In this paper we propose a novel technology mapping technique for Look-Up Table (LUT) - based Field Programmable Gate Arrays (FPGA). The proposed technology mapping technique is based on AND/exclusive-OR (XOR) expressions. The AND/XOR nature of the proposed techniques can map many important XOR-intensive applications, such as error detecting/correcting, data encryption/decryption, and computer arithmetic circuits efficiently in FPGA. The typical EDA tools deal mainly with AND/OR expressions and therefore are quite inefficient for XOR-intensive applications. We design a new approach and conduct experiments using MCNC benchmark circuits in FPGA environment to demonstrate the effectiveness of our proposed technology mapping technique. The proposed technique is superior to the typical methods with respect to area. When using the proposed technique, the number of CLB is reduced by 67.6% (speed-optimized one) and 57.7% (area-optimized one) and the total number of equivalent gate counts is also reduced by 65.5% compared to the typical methods.","0195-623X","0-7695-1918","10.1109/ISMVL.2003.1201397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201397","","Field programmable gate arrays;Table lookup;Circuit testing;Logic circuits;Boolean functions;Application software;Error correction;Cryptography;Automatic testing;Circuit synthesis","field programmable gate arrays;hardware description languages;table lookup;benchmark testing","look-up table;field programmable gate arrays;AND/exclusive-OR expressions;error detection;error correction;data encryption;data decryption;computer arithmetic circuits;EDA tools;XOR-intensive applications;MCNC benchmark circuits;hardware description languages","","","14","","","","","","IEEE","IEEE Conferences"
"Optimizing distributed production planning","S. Hahndel; P. Levi","Tech. Univ. Munchen, Germany; NA","Second International Conference on Intelligent Systems Engineering, 1994","","1994","","","419","424","Production planning is one of the big challenges of artificial intelligence because of its complexity and the dynamic behavior of the different components of a manufacturing environment. We present a new organization scheme for a distributed planning method to coordinate the planning and scheduling of several autonomous agents as intelligent robots and of manufacturing cells on the task level. An implementation of a multi-agent system is described that has been implemented on a net of workstations in order to test the practical relevance of the approach. Each agent has its own knowledge-base and decision component.<<ETX>>","","0-85296-621","10.1049/cp:19940660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=332001","","Manufacturing automation software;Knowledge based systems;Scheduling","computer aided production planning;knowledge based systems;scheduling","distributed production planning;manufacturing;organization scheme;scheduling;autonomous agents;intelligent robots;multi-agent system;knowledge-base system;decision component","","","","","","","","","IET","IET Conferences"
"Measuring Architecting Effort","E. Rommes; A. Postma; P. America","Philips Research, Eindhoven, The Netherlands; NA; NA","5th Working IEEE/IFIP Conference on Software Architecture (WICSA'05)","","2005","","","229","230","The amount of architecting effort is a factor in a software development project&#146;s efficiency. Before steps can be taken to optimize this factor, its current position must be know. We have measured the amount of architecting done in two industrial cases relating to the development of medical imaging systems.We discuss these cases and some of the problems that we encountered.","","0-7695-2548","10.1109/WICSA.2005.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620122","","Biomedical imaging;Medical diagnostic imaging;Image storage;System testing;Databases;Risk management;Programming;Industrial relations;Medical treatment;Diseases","","","","","5","","","","","","IEEE","IEEE Conferences"
"A hierarchical multiprocessor scheduling system for DSP applications","J. L. Pino; S. S. Bhattacharyya; E. A. Lee","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; NA; NA","Conference Record of The Twenty-Ninth Asilomar Conference on Signals, Systems and Computers","","1995","1","","122","126 vol.1","This paper discusses a hierarchical scheduling framework which reduces the complexity of scheduling synchronous data flow (SDF) graphs onto multiple processors. The core of this framework is a clustering algorithm that decreases the number of nodes before expanding the SDF graph into a precedence directed acyclic graph (DAG). The internals of the clusters are then scheduled with uniprocessor SDF schedulers which can optimize for memory usage. The clustering is done in such a manner as to leave ample parallelism exposed for the multiprocessor scheduler. We have developed the SDF composition theorem for testing if a clustering step is valid. The advantages of this framework are demonstrated with several practical, real-time examples.","1058-6393","0-8186-7370","10.1109/ACSSC.1995.540525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540525","","Processor scheduling;Digital signal processing;Signal processing algorithms;Clustering algorithms;Parallel processing;Runtime;Switches;Application software;Laboratories;Testing","processor scheduling;signal processing;data flow graphs;directed graphs;data flow computing;parallel algorithms","hierarchical multiprocessor scheduling system;DSP applications;synchronous data flow graphs;clustering algorithm;SDF graph;directed acyclic graph;uniprocessor SDF schedulers;memory usage;multiprocessor scheduler;SDF composition theorem;clustering step testing","","18","13","","","","","","IEEE","IEEE Conferences"
"P-functions: A new tool for the analysis and synthesis of binary programs","A. Thayse","Philips Research Laboratory, Brussels, Belgium","IEEE Transactions on Computers","","1981","C-30","2","126","134","Considers the realization of switching functions by programs composed of certain conditional transfers (binary programs). Methods exist for optimizing binary trees, i.e. binary programs without reconvergent instructions. This paper studies methods for optimizing binary simple programs (programs with possible reconvergent instructions, but where a variable may be tested only once during a computation) and binary programs. The hardware implementations of these programs involve either multiplexers or demultiplexers and OR-gates.","0018-9340;1557-9956;2326-3814","","10.1109/TC.1981.6312175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6312175","Binary decision programs or diagrams;hardware description languages;hardware realization of switching functions;software description","Multiplexing;Switches;Hardware;Software;Optimization;Bismuth;Logic gates","logic design;switching functions;trees (mathematics)","P-functions;binary programs;switching functions;conditional transfers;optimizing binary trees;reconvergent instructions;multiplexers;demultiplexers;OR-gates","","7","","","","","","","IEEE","IEEE Journals & Magazines"
"SPEED-a highly flexible slice structure and datapath generator","P. Duzy","Siemens AG, Munich, West Germany","Proceedings of the IEEE 1988 Custom Integrated Circuits Conference","","1988","","","14.6/1","14.6/4","A novel generator for bit-slice and datapath structures is presented. It is a multipurpose macrocell generator usable in high-speed applications. The main features are a high degree of architectural, functional, and topological flexibility as well as electrical and geometrical optimization capabilities. The datapath is based on a bus-oriented architecture. The number of internal buses is arbitrary. Since the generation of a control unit for basic decoding and internal clock distribution is optional, there is no overhead if a circuit consists of several datapaths. Testability and design-rule updatability concepts are covered.<<ETX>>","","","10.1109/CICC.1988.20868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=20868","","Pipeline processing;Timing;Decoding;Clocks;Circuit testing;System buses;Latches;Application software;Data engineering;Design engineering","bit-slice computers;circuit layout CAD;microprocessor chips","bit slice structure generator;electrical optimisation capability;testability concepts;SPEED;datapath generator;multipurpose macrocell generator;high-speed applications;features;flexibility;geometrical optimization capabilities;bus-oriented architecture;internal buses;clock distribution;design-rule updatability concepts","","","4","","","","","","IEEE","IEEE Conferences"
"Implementation of an MPEG 2 layer III multichannel audio encoder running in real time","A. J. Mason","R&D, BBC, London, UK","International Broadcasting Convention (Conf. Publ. No. 428)","","1996","","","460","465","The RACE COUGAR project has furthered the development of the MPEG 2 Layer III audio encoding process. Software simulations have been rigorously tested. The development of a multichannel encoder that can operate in real time was the next step. A brief overview of the perceptual coding techniques employed is included, followed by a comparison of the way these might be implemented in software simulation and in real time hardware. The influences on choice of hardware, of computational speed required, quantity of data to be handled, ease of interfacing and suitability of support systems are discussed. The process of porting, partitioning and optimising the simulation software so that it will run on a distributed processor system is described. The encoder is currently working with two channels. The extension of the system to encode 5 channels is reported.","0537-9989","0-85296-663","10.1049/cp:19960852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=642935","","Audio coding","audio coding","MPEG 2 layer III multichannel audio encoder;RACE COUGAR project;audio encoding;software simulations;real time operation;perceptual coding techniques;real time hardware;computational speed;interfacing;support systems;software porting;software partitioning;software optimisation;distributed processor system;DSP","","","","","","","","","IET","IET Conferences"
"Capacitive micromachined ultrasonic transducer arrays for medical imaging: experimental results","U. Demirci; O. Oralkan; J. A. Johnson; A. S. Ergun; M. Karaman; B. T. Khuri-Yakub","Edward L. Ginzton Lab., Stanford Univ., CA, USA; NA; NA; NA; NA; NA","2001 IEEE Ultrasonics Symposium. Proceedings. An International Symposium (Cat. No.01CH37263)","","2001","2","","957","960 vol.2","Capacitive micromachined ultrasonic transducer (cMUT) arrays provide broad bandwidth, high sensitivity, low mechanical impedance, and potential for electronic integration, and thus are promising for medical imaging applications. We have designed and fabricated 1D and 2D MUT arrays of various sizes using standard integrated circuit fabrication processes. We improved the device parameters for medical imaging applications to achieve fully functional 64- and 128-element linear 1D cMUT arrays. We have also built a computer controlled experimental setup for collecting pulse-echo data from the test phantoms using MUT arrays. In this paper the design and optimization of the immersion cMUTs for medical imaging system are discussed, and the phased array B-scan sector images taken by 1D MUT arrays are presented.","","0-7803-7177","10.1109/ULTSYM.2001.991878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991878","","Ultrasonic transducer arrays;Ultrasonic transducers;Biomedical imaging;Phased arrays;Application software;Bandwidth;Impedance;Fabrication;Medical control systems;Circuit testing","ultrasonic transducer arrays;capacitive sensors;biomedical transducers;biomedical ultrasonics;echo","capacitive micromachined ultrasonic transducer arrays;medical imaging;sensitivity;bandwidth;mechanical impedance;electronic integration;medical imaging applications;1D cMUT arrays;2D cMUT arrays;device parameters;pulse-echo data;test phantoms;phased array B-scan sector images","","7","9","","","","","","IEEE","IEEE Conferences"
"Pseudo-randomly generated estimator banks: a new tool for improving the threshold performance of direction finding","A. B. Gershman","Dept. of Electr. Eng., Ruhr-Univ., Bochum, Germany","IEEE Transactions on Signal Processing","","1998","46","5","1351","1364","A new powerful tool for improving the threshold performance of direction finding is considered. The main idea of our approach is to reduce the number of outliers in the DOA estimates using a previously proposed joint estimation strategy (JES). For this purpose, multiple different DOA estimators are calculated in a parallel manner for the same batch of data (i.e. for a single data record). Employing these estimators simultaneously, the JES improves the threshold performance because it removes outliers and exploits only ""successful"" estimators that are sorted out using a hypothesis testing procedure. We consider an efficient modification of the JES with application to the pseudo-randomly generated eigenstructure estimator banks based on secondand higher order statistics. Weighted MUSIC estimators based on the covariance and contracted quadricovariance matrices are chosen as appropriate underlying techniques for the second- and fourth-order estimator banks, respectively. Computer simulations with uncorrelated sources verify the dramatic improvements of threshold performance as compared with the conventional second- and fourth-order MUSIC algorithms. Simulations also show that in the second-order case, the threshold performance of our technique is close to that of the WSF method and stochastic/deterministic ML methods, which are known today as the most powerful (in the sense of estimation performance) and, at the same time, as the most computationally expensive DOA estimation techniques. The computational cost of our algorithm is much lower than that of the WSF and ML techniques because no multidimensional optimization is required.","1053-587X;1941-0476","","10.1109/78.668797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668797","","Direction of arrival estimation;Multiple signal classification;Testing;Application software;Higher order statistics;Covariance matrix;Computer simulation;Computational modeling;Stochastic processes;Maximum likelihood estimation","direction-of-arrival estimation;array signal processing;higher order statistics;parameter estimation;covariance matrices;eigenvalues and eigenfunctions;computational complexity;random processes","pseudo-randomly generated estimator banks;threshold performance;direction finding;DOA estimates;joint estimation strategy;data record;successful estimators;outliers reduction;hypothesis testing;eigenstructure estimator;second-order statistics;higher order statistics;weighted MUSIC estimators;covariance;quadricovariance matrices;fourth-order estimator banks;second-order estimator banks;computer simulations;uncorrelated sources;MUSIC algorithms;stochastic/deterministic ML methods;estimation performance;computational cost","","49","44","","","","","","IEEE","IEEE Journals & Magazines"
"Influence of electrode geometry on transport and separation efficiency of powders using traveling wave field technique","W. W. Machowski; W. Balachandran; D. Hu","Dept. of Manuf. & Eng. Syst., Brunel Univ., Uxbridge, UK; NA; NA","IEEE Transactions on Industry Applications","","1997","33","4","887","892","In this paper, the separation processes of powders using a three-phase traveling wave field are studied, both theoretically and experimentally. The separation properties of the nonuniform traveling wave field was investigated by studying the amplitude of its spatial harmonics. Finite element software was used to model the spatial distribution of an electric field created by electrodes of different shapes. It was established that cylindrical, square, and stripe electrodes produce the most optimized field distribution. Based on the modeling results, several flatbed traveling wave panels with stripe and cylindrical electrodes were constructed and tested. The separation characteristics of the panels were assessed by examining the direction of transport of a two-powder mixture with different size distributions for AC frequencies up to 300 Hz. It was discovered that, for the majority of panels, successful powder separation could be achieved for two frequency bands. The position of the bands depended mainly on electrode geometry (i.e., pitch and width). In order to further understand transport phenomena in both separation bands, particle trajectories were examined using a CCD camera with telemicroscopic lenses interfaced with a computer controlled image grabbing system.","0093-9994;1939-9367","","10.1109/28.605728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=605728","","Electrodes;Geometry;Powders;Frequency;Separation processes;Finite element methods;Shape;Testing;Charge coupled devices;Charge-coupled image sensors","electric fields;electrodes;powders;powder technology;finite element analysis;separation","electrode geometry;separation efficiency;transport efficiency;powders separation;nonuniform traveling wave field;three-phase traveling wave field;spatial harmonics;finite element software;electric field spatial distribution;optimized field distribution;stripe electrodes;square electrodes;cylindrical electrodes;two-powder mixture;size distributions;AC frequencies;frequency bands;transport phenomena;particle trajectories;CCD camera;telemicroscopic lenses;computer controlled image grabbing","","","6","","","","","","IEEE","IEEE Journals & Magazines"
"Using structure of data to improve classification","C. M. O'Keefe; G. A. Jarrad","Math. & Inf. Sci., CSIRO, Glen Osmond, SA, Australia; Math. & Inf. Sci., CSIRO, Glen Osmond, SA, Australia","Final Program and Abstracts on Information, Decision and Control","","2002","","","305","310","Statistical mixture-of-experts models are often used for data analysis tasks such as clustering, regression and classification. We consider two mixture-of-experts models, the shared mixture classifier and the hierarchical mixture-of-experts classifier. We discuss the initialisation and optimisation of the structure and parameters of each classifier. In particular, we initialise the hierarchical mixture of experts classifier with the public domain OC1 decision tree software. We compare the performance of the two classifiers on four datasets, two artificial and two real, finding that the hierarchical mixture-of-experts classifier achieves superior classification performance on the testing data.","","0-7803-7270","10.1109/IDC.2002.995419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995419","","Testing;Transmitting antennas;Antenna arrays;Signal to noise ratio;Feedback;Protocols;Mathematical model;Data analysis;Classification tree analysis;Decision trees","probability;statistical analysis;pattern classification;decision trees","statistical mixture-of-experts models;data analysis;clustering;regression;classification;shared mixture classifier;hierarchical mixture-of-experts classifier;initialisation;optimisation;public domain OC1 decision tree software;Gaussian mixture model","","","","","","","","","IEEE","IEEE Conferences"
"The x-Kernel: an architecture for implementing network protocols","N. C. Hutchinson; L. L. Peterson","Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA","IEEE Transactions on Software Engineering","","1991","17","1","64","76","A description is given of an operating system kernel, called the x-Kernel, that provides an explicit architecture for constructing and composing network protocols. The authors' experience implementing and evaluation several protocols in the x-Kernel shows that this architecture is general enough to accommodate a wide range of protocols, yet efficient enough to perform competitively with less-structured operating systems. Experimental results demonstrating the architecture's generality and efficiency are provided. The explicit structure provided by the x-Kernel has the following advantages. First, the architecture simplifies the process of implementing protocols in the kernel, making it easier to build and test novel protocols. Second, the uniformity of the interface between protocols avoids the significant cost of changing abstractions and makes protocol performance predictable. Third, it is possible to write efficient protocols by tuning the underlying architecture rather than heavily optimizing protocols themselves.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=67579","","Operating systems;Kernel;Access protocols;Hardware;Application software;Sockets;Computer architecture;Performance evaluation;Heart;Encoding","network operating systems;protocols","x-Kernel;architecture;network protocols;operating system kernel;interface","","315","37","","","","","","IEEE","IEEE Journals & Magazines"
"The electric-field Integral-equation method for the analysis and design of a class of rectangular cavity filters loaded by dielectric and metallic cylindrical pucks","F. Alessandri; M. Chiodetti; A. Giugliarelli; D. Maiarelli; G. Martirano; D. Schmitt; L. Vanni; F. Vitulli","Dept. of Electron. & Inf., Univ. of Perugia, Italy; Dept. of Electron. & Inf., Univ. of Perugia, Italy; NA; NA; NA; NA; NA; NA","IEEE Transactions on Microwave Theory and Techniques","","2004","52","8","1790","1797","Rectangular cavity filters loaded by metallic and dielectric cylindrical pucks represent a wide class of structures used for the realization of compact microwave filters for space and terrestrial communications. The electric-field integral-equation method has been applied here to develop a software tool for the design of such a class of filters. Resonators simultaneously containing a number of dielectric and metallic cylindrical pucks can be analyzed using the method presented here. Dielectric loaded rectangular waveguide filters and comb-line filters are two typical examples where this approach can be applied. The input and inter-cavity couplings realized by slots have been modeled using the proposed integral-equation method. The full-wave analysis and optimization of an entire filter can be performed using the software presented here. Dielectric and metallic losses have been taken into account and the tuning screws have been also included into the analysis. The prototype of a single resonator and complete passband dielectric loaded rectangular waveguide filter have been realized and measured to test the software, and a commercial software has also been used to validate the code. A number of examples are presented here. A very short computation time of the order of 1-s/frequency point for the analysis of an entire filter has been obtained, making the full-wave optimization of an entire filter an affordable task.","0018-9480;1557-9670","","10.1109/TMTT.2004.831583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318766","","Resonator filters;Microwave filters;Loaded waveguides;Rectangular waveguides;Dielectric losses;Microwave communication;Software tools;Performance analysis;Software performance;Fasteners","band-pass filters;comb filters;circuit optimisation;dielectric losses;microwave filters;waveguide filters;cavity resonator filters;rectangular waveguides;dielectric resonator filters;electric field integral equations;dielectric-loaded waveguides","electric-field integral-equation method;metallic cylindrical pucks;dielectric cylindrical pucks;microwave filters;space communications;terrestrial communications;resonators;passband dielectric loaded rectangular waveguide filters;comb-line filters;intercavity couplings;dielectric losses;metallic losses","","34","10","","","","","","IEEE","IEEE Journals & Magazines"
"High-level synthesis of recoverable microarchitectures","S. Y. Ohm; D. M. Blough; F. J. Kurdahi","Dept. of Electr. & Comput. Eng., California Univ., Irvine, CA, USA; Dept. of Electr. & Comput. Eng., California Univ., Irvine, CA, USA; Dept. of Electr. & Comput. Eng., California Univ., Irvine, CA, USA","Proceedings ED&TC European Design and Test Conference","","1996","","","55","62","Two algorithms that combine the operations of scheduling and recovery point insertion for high-level synthesis of recoverable microarchitectures are presented. The first uses a prioritized cost function in which functional unit cost is minimized first and register cost second. The second algorithm minimizes a weighted sum of functional unit and register costs. Both algorithms are optimal according to their respective cost functions and require less than 10 minutes of CPU time on widely-used high-level synthesis benchmarks. The best previous result reported several hours of CPU time for some of the same benchmarks.","1066-1409","0-8186-7424","10.1109/EDTC.1996.494128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494128","","High level synthesis;Microarchitecture;Cost function;Scheduling algorithm;Hardware;Registers;Processor scheduling;Digital systems;Fault tolerant systems;Application software","high level synthesis;reconfigurable architectures;scheduling;digital systems;data flow graphs","high-level synthesis;recoverable microarchitectures;scheduling;recovery point insertion;prioritized cost function;functional unit cost;register cost;weighted sum;benchmarks;fault-tolerant digital systems;CDFG;control and data flow graph","","4","15","","","","","","IEEE","IEEE Conferences"
"Use of simulation to test client-server models","Y. L. Deshpande; S. Taylor; R. Jenkins","University of Western Sydney; NA; NA","Proceedings Winter Simulation Conference","","1996","","","1210","1217","Simulation is used for many purposes: for example, to analyse a complex system, to visualise the functioning of a system, and to optimise or tune a system. While there is no limitation on the use of simulation, the general consensus is that an analytical solution, if one is possible, is always to be preferred to simulation as a methodology. In the field of information systems, client-server models exhibit a degree of complexity and richness not amenable to easy analytical solutions, except for some specific algorithms useful in limited contexts. Simulation could, therefore, be a good strategy to analyse the client-server systems and help in better implementation of feasible solutions. This paper examines the current state of client-server models and use of simulation in dealing with the problems encountered. The paper then compares the seven-layer OSI model for communications architecture and recommends that a similarly layered approach is likely to prove useful in simulating client-server systems. In the process, the paper also points out that the simulation models bring into a sharp focus the importance of software metrics, an area of vital importance in software development.","","0-7803-3383","10.1109/WSC.1996.873426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873426","","Analytical models;Computational modeling;Client-server systems;Visualization;Information systems;Information analysis;Software metrics;Computer simulation;System testing;Australia","","","","","23","","","","","","IEEE","IEEE Conferences"
"Reduced-order output feedback control design with specSDP, a code for linear/nonlinear SDP problems","J. -. Thevenet; P. Apkarian; D. Noll","Univ. Paul Sabatier, Toulouse, France; Univ. Paul Sabatier, Toulouse, France; Univ. Paul Sabatier, Toulouse, France","2005 International Conference on Control and Automation","","2005","1","","465","470 Vol. 1","Optimization problems with bilinear matrix inequalities (BMI) constraints arise frequently in automatic control and are difficult to solve due to the inherent nonconvexity. The purpose of this paper is to give an overview of the spectral SDP (semidefinite programming) method presented by Thevenetet al. (2004), along with practical information on how to basically use specSDP, a software designed to solve LMI/BMI-constrained optimization problems, with a linear or quadratic objective function. specSDP is a Fortran code, interfaced with Matlab. It is tested here against several examples of reduced order control problems from the benchmark collection COMPlib (Leibfritz and Lipinsky, 2003).","1948-3449;1948-3457","0-7803-9137","10.1109/ICCA.2005.1528164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1528164","Bilinear matrix inequality;spectral penalty function;reduced order control synthesis","Output feedback;Control design;Linear matrix inequalities;Symmetric matrices;Design optimization;Testing;Constraint optimization;Automatic control;Functional programming;Linear programming","reduced order systems;feedback;control system CAD;linear matrix inequalities;nonlinear programming","reduced-order control;output feedback control design;specSDP;linear-nonlinear spectral semidefinite programming;optimization problem;bilinear matrix inequality constraints;linear objective function;quadratic objective function;Fortran code;Matlab;COMPlib;spectral penalty function;control synthesis","","1","13","","","","","","IEEE","IEEE Conferences"
"GA with fuzzy inference system","R. Matousek; P. Osmera; J. Roupec","Inst. of Autom. & Comput. Sci., Brno Univ. of Technol., Czech Republic; NA; NA","Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)","","2000","1","","646","651 vol.1","Applications of genetic algorithms (GA) for optimisation problems are widely known as well as their advantages and disadvantages compared with classical numerical methods. In practical tests, GA appears a robust method with a broad range of applications. The determination of GA parameters could be complicated. Therefore for some real-life applications, several empirical observations of an experienced expert are needed to define these parameters. This fact degrades the applicability of a GA for most of the real-world problems and users. Therefore, this article discusses some possibilities with setting GA parameters. The setting method of GA parameters is based on the fuzzy control of values of GA parameters. The feedback for the fuzzy control of GA parameters is realized by virtue of the behavior of some GA characteristics. The goal of this article is to present the conception of the solution and some new ideas.","","0-7803-6375","10.1109/CEC.2000.870359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870359","","Fuzzy systems;Genetic algorithms;Fuzzy control;Automation;Computer science;Application software;Testing;Robustness;Degradation;Feedback","genetic algorithms;inference mechanisms;fuzzy control;fuzzy logic","fuzzy inference system;genetic algorithms;optimisation problems","","2","11","","","","","","IEEE","IEEE Conferences"
"The DAVID scientific satellite: Mission analysis and orbit selection","M. C. Francalanci; A. Salome; G. Varacalli; M. Ruggieri; C. Bonifazi","Space Eng. S.p.A, Roma, Italy; NA; NA; NA; NA","Proceedings, IEEE Aerospace Conference","","2002","1","","1","1","The LEO-based multi-experiment DAVID (DAta and Video Interactive Distribution) scientific mission of the Italian Space Agency is presently undertaking phase B on the program activities. It aims at testing the viability of the exchange of large amount of data, through a W-band user link and either a Ka-band inter orbit link or a direct-to-ground link, and the effectiveness of resource sharing techniques to countermeasure the propagation channel behavior. The summary results of the mission analysis activities are presented along with the orbit selection process. Emphasis is also placed on the optimization of the communication window with the Artemis satellite taking into account the Sun illumination conditions.","","0-7803-7231","10.1109/AERO.2002.1036843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1036843","","Sun;Earth;Testing;Resource management;Artificial satellites;Lighting;Antarctica;Acceleration;Space missions;Software tools","artificial satellites;satellite links;data communication;millimetre wave propagation;visual communication","DAVID scientific satellite;mission analysis;orbit selection;LEO-based multi-experiment scientific mission;data distribution;video interactive distribution;Italian Space Agency;W-band user link;Ka-band inter orbit link;Ka-band direct-to-ground link;resource sharing techniques;propagation channel behavior countermeasure;communication window optimization;Artemis satellite;Sun illumination conditions;solar panel illumination","","","6","","","","","","IEEE","IEEE Conferences"
"Using hidden Markov model for Chinese business card recognition","Yuan-Kai Wang; Kuo-Chin Fan; Y. T. Juang; T. H. Chen","NA; NA; NA; NA","Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)","","2001","1","","1106","1109 vol.1","Business card recognition is a difficult problem. Characters in business card are small with diverse font types. An approach using the left-right hidden Markov model is proposed for business card recognition. The hidden Markov model will output a top-10 candidate list as its recognition result. A postprocessing stage is followed to improve the recognition result. The postprocessing stage uses a bigram table as linguistic information to search for the optimized recognition result from the top-10 candidate list. Our experiments are built on the recognition of company item and address item in Chinese business cards. Bigram table and hidden Markov models are trained with a telephony database. 100 address items and 30 company items are used for testing. Experimental results reveal the validity of our proposed method.","","0-7803-6725","10.1109/ICIP.2001.959243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959243","","Hidden Markov models;Character recognition;Stochastic processes;Databases;Optical character recognition software;Companies;Testing;Feature extraction;Natural languages;Speech recognition","hidden Markov models;linguistics;optical character recognition;feature extraction;learning (artificial intelligence);document image processing","hidden Markov model;Chinese business card recognition;postprocessing stage;bigram table;linguistic information;optimized recognition result;company item;address item;telephony database;feature extraction;optical character recognition;OCR","","","10","","","","","","IEEE","IEEE Conferences"
"Zone content classification and its performance evaluation","Yalin Wang; R. Haralick; I. T. Phillips","Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; NA; NA","Proceedings of Sixth International Conference on Document Analysis and Recognition","","2001","","","540","544","We present an improved zone content classification method and its performance evaluation. We added two new features to the feature vector from one previously published method (Sivaramakrishnan et al., 1995). We assumed different independence relationships in two zone sets. We used an optimized binary decision tree to estimate the maximum zone content class probability in one set while using the Viterbi algorithm to find the optimal solution for a zone sequence in the other set. The training, pruning and testing data set for the algorithm include 1,600 images drawn from the UWCDROM III document image database. The classifier is able to classify each given scientific and technical document zone into one of the nine classes, 2 text classes (of font size 4 - 18pt and font size 19 - 32 pt), math, table, halftone, map/drawing, ruling, logo, and others. Compared with our previous work (Wang et al., 2000), it raised the accuracy rate to 98.52% from 97.53% and reduced the mean false alarm rate to 0.53% from 1.26%.","","0-7695-1263","10.1109/ICDAR.2001.953847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953847","","Decision trees;Image databases;Hidden Markov models;Computer science;Educational institutions;Viterbi algorithm;Testing;Equations;Classification tree analysis;Optical character recognition software","image classification;performance evaluation;document image processing;decision trees;probability;visual databases","zone content classification;performance evaluation;feature vector;independence relationship;optimized binary decision tree;maximum zone content class probability;Viterbi algorithm;UWCDROM III database;document image database;document classification;font size","","5","8","","","","","","IEEE","IEEE Conferences"
"Header compression in Handel-C-an Internet application and a new design language","K. Torkelsson; J. Ditmar","Ericsson Radio Syst. AB, Celoxica Ltd., UK; NA","Proceedings Euromicro Symposium on Digital Systems Design","","2001","","","2","7","In the ESPRIT project ""Software engineering for Hardware Design"", a critical and complex function in the Ericsson IPv6 router RXI820 was designed. The router is optimized for voice transmission in the mobile base station network. IP Header Compression (RFC 2507), compresses and restores long headers of packets in point-to-point message streams improving bandwidth utilization and real-time characteristics. The function was implemented in FPGA technology using a new high-level design language based on the software language ANSI-C. The design method used is similar to methods for software design. The resulting hardware can be tested in full speed on a PCI-board. In a parallel effort, a second group of designers using the same specification implemented the same functionality using traditional hardware design methods and tools. This enabled us to compare the efficiency of the two design methods. Using the new methods, the design was completed 3-4 times faster with similar results in terms of speed and area. This can be attributed to support for sequential logic and a compact representation in the language and to a software-like design methodology with fast turnaround in the design environment.","","0-7695-1239","10.1109/DSD.2001.952110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952110","","Internet;Design methodology;Hardware;Design engineering;Base stations;Bandwidth;Field programmable gate arrays;Software design;Testing;Logic design","project engineering;Internet;logic design;field programmable gate arrays","header compression;Handel-C;Internet;design language;ESPRIT project;Ericsson IPv6 router RXI820;voice transmission;mobile base station network;point-to-point message streams;bandwidth utilization;real-time characteristics;FPGA technology;software language ANSI-C;sequential logic;software-like design methodology","","4","2","","","","","","IEEE","IEEE Conferences"
"The approximation of density functions for use in queueing theory","H. Yang; L. Lipsky","Connecticut Univ., Storrs, CT, USA; Connecticut Univ., Storrs, CT, USA","Conference Proceedings., IEEE International Conference on Systems, Man and Cybernetics","","1989","","","612","617 vol.2","A procedure is developed for testing the accuracy of approximations to probability density functions (PDF) in queueing theory. The approximation can be constructed from the derivatives and moments of a given PDF, if they exist. Four subjects are considered which can be used as criteria to judge the goodness of approximations over the entire range of values of the traffic intensity parameter, rho . A detailed example is presented to discuss the effect which different derivatives/moments matching combinations have on the accuracy of the approximations. The exponential factor is used as an optimizing parameter.<<ETX>>","","","10.1109/ICSMC.1989.71370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=71370","","Queueing analysis;Polynomials;Testing;Probability density function;Traffic control;Steady-state;Computer science;Computer applications;Application software;Fault tolerance","probability;queueing theory","approximation;queueing theory;probability density functions;derivatives/moments matching;exponential factor;optimizing parameter","","","10","","","","","","IEEE","IEEE Conferences"
"Application of genetic based algorithms to optimal capacitor placement","V. Ajjarapu; Z. Albanna","Dept. of Electr. & Comput. Eng., Iowa State Univ., Ames, IA, USA; Dept. of Electr. & Comput. Eng., Iowa State Univ., Ames, IA, USA","Proceedings of the First International Forum on Applications of Neural Networks to Power Systems","","1991","","","251","255","Reactive power dispatch problem is still an active problem in the area of power system operation. Finding an optimum solution that provides a balance between system performance and the cost associated with the reactive power placement (in terms of improved voltage profile and voltage stability) is essential. The objective of the research is to explore the applicability of a new emerging global optimization technique, called a genetic algorithm to the reactive power dispatch problem. The algorithm is based on the mechanics of natural selection and has been tested in other fields with promising results.<<ETX>>","","0-7803-0065","10.1109/ANN.1991.213468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213468","","Capacitors;Genetic algorithms;Voltage;Reactive power;Application software;Power system stability;Cost function;Testing;Power system restoration;Power system economics","genetic algorithms;load dispatching;neural nets;power capacitors;power engineering computing;power systems;reactive power","load dispatching;reactive power;power systems;power capacitors;power engineering computing;capacitor placement;performance;cost;voltage profile;voltage stability;global optimization;genetic algorithm;natural selection","","19","3","","","","","","IEEE","IEEE Conferences"
"Using occurrence properties of defect report data to improve requirements","K. S. Wasson; K. N. Schmid; R. R. Lutz; J. C. Knight","Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; NA; NA; NA","13th IEEE International Conference on Requirements Engineering (RE'05)","","2005","","","253","262","Defect reports generated for faults found during testing provide a rich source of information regarding problematic phrases used in requirements documents. These reports indicate that faults often derive from instances of ambiguous, incorrect or otherwise deficient language. In this paper, we report on a method combining elements of linguistic theory and information retrieval to guide the discovery of problematic phrases throughout a requirements specification, using defect reports and correction requests generated during testing to seed our detection process. We found that phrases known from these materials to be problematic have occurrence properties in requirements documents that both allow the direction of resources to prioritize their correction, and generate insights characterizing more general locations of difficulty within the requirements. Our findings lead to some recommendations for more efficiently and effectively managing certain natural language issues in the creation and maintenance of requirements specifications.","1090-705X;2332-6441","0-7695-2425","10.1109/RE.2005.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531046","","Natural languages;Computer science;System testing;Software systems;Propulsion;Laboratories;Information resources;Information retrieval;Character generation;Software testing","natural languages;information retrieval;formal specification;system documentation;computational linguistics","defect report data occurrence properties;linguistic theory;information retrieval;requirements specification;natural languages","","6","15","","","","","","IEEE","IEEE Conferences"
"Large-scale simulation models of BGP","X. A. Dimitropoulos; G. F. Riley","Dept. of ECE, Georgia Inst. of Technol., Atlanta, GA, USA; Dept. of ECE, Georgia Inst. of Technol., Atlanta, GA, USA","The IEEE Computer Society's 12th Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, 2004. (MASCOTS 2004). Proceedings.","","2004","","","287","294","The complex nature of the border gateway protocol (BGP) is not amenable to analytical modeling, and thus simulation-based analysis methods are needed to understand its behavior. To this end, we investigate techniques that make large-scale BGP simulations feasible. The described techniques partially ameliorate the memory and execution time bottlenecks to yield large-scale BGP simulations. Our focus is cast on efficient sharing of BGP routing tables, execution time optimizations for simulation trials, and proper partitioning of parallel distributed BGP simulations. Moreover, we survey the requirements for realistic, Internet-like BGP simulations and develop a simulation toolset to expedite realistic configuration of a BGP simulator.","1526-7539","0-7695-2251","10.1109/MASCOT.2004.1348283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1348283","","Large-scale systems;Analytical models;Internet;Computational modeling;Protocols;Routing;Testing;Contracts;Scalability;Topology","digital simulation;telecommunication computing;routing protocols;Internet;parallel processing;software tools;optimisation","border gateway protocol;large-scale BGP simulations;memory bottlenecks;execution time bottlenecks;routing tables;Internet;simulation toolset;parallel distributed simulations;interdomain routing protocol;simulation optimization","","6","28","","","","","","IEEE","IEEE Conferences"
"Causality visualization using animated growing polygons","N. Elmqvist; P. Tsigas","Dept. of Comput. Sci., Chalmers Univ. of Technol., Goteborg, Sweden; Dept. of Comput. Sci., Chalmers Univ. of Technol., Goteborg, Sweden","IEEE Symposium on Information Visualization 2003 (IEEE Cat. No.03TH8714)","","2003","","","189","196","We present Growing Polygons, a novel visualization technique for the graphical representation of causal relations and information flow in a system of interacting processes. Using this method, individual processes are displayed as partitioned polygons with color-coded segments showing dependencies to other processes. The entire visualization is also animated to communicate the dynamic execution of the system to the user. The results from a comparative user study of the method show that the Growing Polygons technique is significantly more efficient than the traditional Hasse diagram visualization for analysis tasks related to deducing information flow in a system for both small and large executions. Furthermore, our findings indicate that the correctness when solving causality tasks is significantly improved using our method. In addition, the subjective ratings of the users rank the method as superior in all regards, including usability, efficiency, and enjoyability.","","0-7803-8154","10.1109/INFVIS.2003.1249025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249025","","Visualization;Animation;Humans;Color;Information systems;Information analysis;Usability;Chromium;Software engineering;Software testing","data visualisation;computer animation;concurrent engineering;graphical user interfaces;software engineering","causality visualization;animated growing polygons;graphical representation;causal relations;information flow;interactive systems;partitioned polygons;color coded segments;dynamic execution;Hasse diagram visualization;subjective user ratings;interactive animation;information visualization","","15","14","","","","","","IEEE","IEEE Conferences"
"Architectures for next generation military avionics systems","J. M. Borky; R. N. Lachenmaier; J. P. Messing; A. Frink","BDM Int. Inc., Albuquerque, NM, USA; NA; NA; NA","1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)","","1998","1","","265","281 vol.1","Modular, open, integrated architecture has emerged as a key enabler of both military and commercial avionics suites that combine affordable cost with the ability to deal with rapidly changing technology and evolving system requirements. This paper describes recent advances in avionics architecture for military aircraft, using the evolving architecture of the Joint Strike Fighter (JSF) as a specific illustration. Key architectural attributes include effective implementation of open systems principles, hardware and software scaleability to cover a range of applications, independence of specific implementing hardware to minimize parts obsolescence problems, support for information security and trusted computing, and design features that facilitate integration and promote reliable and maintainable systems. A number of tradeoffs must be dealt with in optimizing such designs. These include choice of a fundamental interconnection fabric and interfaces, decisions on centralized vs. distributed processing, similar decisions on integrated vs. special purpose sensors and apertures, choices about the real time operating system and applications software, and a number of issues associated with mechanical and electrical design. The architecture must include robust testability features that ensure that the resulting system design can be tested and qualified to the criteria associated with avionics. The paper devotes special attention to the vexing problem of technology obsolescence and to the architectural approaches that can help to mitigate it.","1095-323X","0-7803-4311","10.1109/AERO.1998.686826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=686826","","Aerospace electronics;Military aircraft;Computer architecture;Hardware;Application software;System testing;Costs;Open systems;Software maintenance;Information security","military avionics;military aircraft;open systems;distributed processing;aerospace computing;military computing","multirole tactical aircraft;architectural partitioning;Unified Avionics Network;integrated core processing;stores management;crew station interface;modular open integrated architecture;next generation avionics systems;military avionics systems;military aircraft;Joint Strike Fighter;parts obsolescence problems;information security;centralized processing;distributed processing;real time operating system;applications software","","3","8","","","","","","IEEE","IEEE Conferences"
"Unroll-based copy elimination for enhanced pipeline scheduling","Suhyun Kim; Soo-Mook Moon; Jinpyo Park; K. Ebcioglu","Sch. of Electr. Eng. & Comput. Sci., Seoul Nat. Univ., South Korea; Sch. of Electr. Eng. & Comput. Sci., Seoul Nat. Univ., South Korea; Sch. of Electr. Eng. & Comput. Sci., Seoul Nat. Univ., South Korea; NA","IEEE Transactions on Computers","","2002","51","9","977","994","Enhanced pipeline scheduling (EPS) is a software pipelining technique which can achieve a variable initiation interval (II) for loops with control flow via its code motion pipelining. EPS, however, leaves behind many renaming copy instructions that cannot be coalesced due to interferences. These copies take resources and, more seriously, they may cause a stall if they rename a multilatency instruction whose latency is longer than the II aimed for by EPS. This paper proposes a code transformation technique based on loop unrolling which makes those copies coalescible. Two unique features of the technique are its method of determining the precise unroll amount, based on an idea of extended live ranges, and its insertion of special bookkeeping copies at loop exits. The proposed technique enables EPS to avoid a serious slowdown from latency handling and resource pressure, while keeping its variable II and other advantages. In fact, renaming through copies, followed by unroll-based copy elimination, is EPS's solution to the cross-iteration register overwrite problem in software pipelining. It works for loops with arbitrary control flow that EPS must deal with, as well as for straightline loops. Our empirical study performed on a VLIW testbed with a two-cycle load latency shows that 86 percent of the otherwise uncoalescible copies in innermost loops become coalescible when unrolled 2.2 times on average. In addition, it is demonstrated that the unroll amount obtained is precise and the most efficient. The unrolled version of the VLIW code includes fewer no-op VLIW caused by stalls, improving the performance by a geometric mean of 18 percent on a 16-ALU machine.","0018-9340;1557-9956;2326-3814","","10.1109/TC.2002.1032620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1032620","","Pipeline processing;Interference;Delay;VLIW;Moon;Motion control;Performance evaluation;Testing;Optimizing compilers","processor scheduling;program control structures;pipeline processing;delays;performance evaluation;program processors;optimising compilers;parallelising compilers;programming theory","unroll-based copy elimination;enhanced pipeline scheduling;EPS;software pipelining;variable initiation interval;control flow;coalescible copies;extended live ranges;special bookkeeping copies;loop exits;code motion pipelining;latency handling;renaming;arbitrary control flow;straightline loops;VLIW testbed;two-cycle load latency;innermost loops;unroll amount;performance;cross-iteration register overwrite problem;modulo scheduling;modulo variable expansion;register allocation;iterated coalescing;code transformation technique;loop unrolling","","2","20","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating dependency and significance for high-dimensional data","M. R. Siracusa; K. Tieu; A. T. Ihler; J. W. Fisher; A. S. Willsky","Comput. Sci. & Artificial Intelligence Lab., MIT, Cambridge, MA, USA; Comput. Sci. & Artificial Intelligence Lab., MIT, Cambridge, MA, USA; NA; NA; NA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","","2005","5","","v/1085","v/1088 Vol. 5","Understanding the dependency structure of a set of variables is a key component in various signal processing applications which involve data association. The simple task of detecting whether any dependency exists is particularly difficult when models of the data are unknown or difficult to characterize because of high-dimensional measurements. We review the use of nonparametric tests for characterizing dependency and how to carry out these tests with high-dimensional observations. In addition we present a method to assess the significance of the tests.","1520-6149;2379-190X","0-7803-8874","10.1109/ICASSP.2005.1416496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416496","","Testing;Statistical distributions;Laboratories;Application software;Machine learning;Parametric statistics;Statistical analysis;Computer science;Artificial intelligence;Signal processing","optimisation;signal processing;nonparametric statistics","dependency structure;high-dimensional data;signal processing;data association;nonparametric tests","","1","8","","","","","","IEEE","IEEE Conferences"
"Optimal placements of flexible objects. I. Analytical results for the unbounded case","A. Albrecht; S. K. Cheung; K. C. Hui; K. S. Leung; C. K. Wong","Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, Hong Kong; NA; NA; NA; NA","IEEE Transactions on Computers","","1997","46","8","890","904","The authors consider optimal placements of two-dimensional flexible (elastic, deformable) objects. The objects are discs of equal size placed within a rigid boundary. The paper is divided into two parts. In the first part, analytical results for three types of regular, periodic arrangements-the hexagonal, square, and triangular placements-are presented. The regular arrangements are analyzed for rectangular boundaries and radii of discs that are small compared to the area of the placement region, because, in this case, the influence of boundary conditions can be neglected. This situation is called the unbounded case. They show that, for the unbounded case among the three regular placements, the type of hexagonal arrangements provides the largest number of placed units for the same deformation depth. Furthermore, it can be proved that these regular placements are not too far from the truly optimal arrangements. For example, hexagonal placements differ at most by the factor of 1.1 from the largest possible number of generally shaped units in arbitrary arrangements. These analytical results are used as guidances for testing stochastic algorithms optimizing placements of flexible objects. In the second part, mainly two problems are considered: the underlying physical model and a simulated annealing algorithm maximizing the number of flexible discs in equilibrium placements. Along with the physical model, an approximate formula is derived, reflecting the deformation/force relationship for a large range of deformations.","0018-9340;1557-9956;2326-3814","","10.1109/12.609278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=609278","","Computer aided software engineering;Algorithm design and analysis;Stochastic processes;Simulated annealing;Shape;Elasticity;Boundary conditions;Testing;Deformable models;Amorphous materials","simulated annealing;deformation;testing;algorithm theory;computational geometry","optimal 2D flexible object placement;analytical results;unbounded case;rigid boundary;discs;regular periodic arrangements;hexagonal placements;square placements;triangular placements;stochastic algorithm testing;physical model;simulated annealing algorithm;maximized flexible discs;equilibrium placements;approximate formula;deformation/force relationship","","3","17","","","","","","IEEE","IEEE Journals & Magazines"
"Using distributed objects for digital library interoperability","A. Paepcke; S. B. Cousins; H. Garcia-Molina; S. W. Hassan; S. P. Ketchpel; M. Roscheisen; T. Winograd","Stanford Univ., CA, USA; Stanford Univ., CA, USA; Stanford Univ., CA, USA; Stanford Univ., CA, USA; Stanford Univ., CA, USA; Stanford Univ., CA, USA; Stanford Univ., CA, USA","Computer","","1996","29","5","61","68","Information repositories are just one of many services tomorrow's digital libraries might offer. Other services include automated news summarization, trend analysis across news repositories, and copyright-related facilities. This distributed collection of services has the potential to be enormously helpful in performing information-intensive tasks. It could also turn such tasks into confusing, frustrating annoyances by forcing programmers and users to learn many interfaces and by confronting users with the bewildering details of fee-based services that were previously only accessible to professional librarians. The Stanford Digital Library project is addressing the problem of interoperability, which is particularly important because standardization efforts are lagging behind the development of digital library services. The authors used CORBA to implement information-access and payment protocols. These protocols provide the interface uniformity necessary for interoperability, while leaving implementers a large amount of leeway to optimize performance and to provide choices in service performance profiles. The authors' initial experience indicates that a distributed object framework does give clients and servers the flexibility to manage their communication and processing resources effectively.","0018-9162;1558-0814","","10.1109/2.493458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493458","","Software libraries;Web sites;Access protocols;Testing;Permission;Standards development;Design optimization;Web server;Context-aware services;Linux","library automation;academic libraries;open systems;object-oriented programming;client-server systems;user interfaces;software standards;full-text databases","distributed objects;digital library interoperability;Information repositories;automated news summarization;trend analysis;news repositories;copyright;interfaces;fee-based services;professional librarians;Stanford Digital Library project;standardization;CORBA;information-access;payment protocols;performance;distributed object framework;client server","","9","11","","","","","","IEEE","IEEE Journals & Magazines"
"A New Approach to Multi-Segmented Rotor Slot Design for Induction Motors Part III","P. Diamant","The Louis Allis Co., Div. of Litton Ind.","IEEE Transactions on Power Apparatus and Systems","","1972","PAS-91","6","2397","2404","The ""Linear Equi-Permeance Rotor Slot"" Model Theory, established in Parts I and II of this paper, have furnished the ""conceptual tooling"" and the methods for the efficient handling of multi-segmented rotor slots, in optimization analyses with respect to shape, number of segments and proportions. In essence, what has been created, is the availability of such design analysis techniques, which do overcome the ""non-linearity barrier"" of design optimizations with multi-segmented rotor slots and consequently, make the true overall motor performance optimization a practical reality, with full consideration given to the entire acceleration speed range too.","0018-9510","","10.1109/TPAS.1972.293397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4075007","","Rotors;Induction motors;Design optimization;Testing;Performance analysis;Shape;Application software;Optimization methods;Availability;Acceleration","","","","3","4","","","","","","IEEE","IEEE Journals & Magazines"
"A least squares solution for optimal power flow sensitivity calculation","S. V. Venkatesh; W. -. E. Liu; A. D. Papalexopoulos","Empros Syst. Int., Minneapolis, MN, USA; Empros Syst. Int., Minneapolis, MN, USA; NA","[Proceedings] Conference Papers 1991 Power Industry Computer Application Conference","","1991","","","379","385","Sensitivities of an optimal power flow (OPF) solution to small changes in bus loads, flow limits, bus voltage limits, and other OPF constraints are becoming increasingly important to the electric utility industry. Many of these sensitivities are not produced by most existing OPF algorithms. A least-squares-based algorithm that is suitable for post-OPF sensitivity calculations is proposed. The algorithm provides acceptable sensitivities of optimal solutions under the consideration of all important operational and security constraints. The optimal solutions need not be exact. This is especially important for real-time applications where only approximate, but close to the true optimum, solutions are available. The proposed algorithm has been implemented in a production grade computer software and is sufficiently general to be used by other electric power utilities. It is intended to be used as part of the OPF calculations in the new energy management system (EMS) of the Pacific Gas and Electric Company. Test results on a 1700-bus system are presented.<<ETX>>","","0-87942-620","10.1109/PICA.1991.160605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=160605","","Least squares methods;Load flow;Voltage;Power industry;Application software;Least squares approximation;Software algorithms;Production;Energy management;Medical services","least squares approximations;load flow;load management;optimisation;power system analysis computing;real-time systems;sensitivity analysis","load flow;USA;power system analysis computing;least squares approximations;optimal power flow sensitivity;electric utility;algorithms;security constraints;real-time;computer software;energy management system;EMS","","","12","","","","","","IEEE","IEEE Conferences"
"An insight-based methodology for evaluating bioinformatics visualizations","P. Saraiya; C. North; K. Duca","Dept. of Comput. Sci., Virginia Tech., Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech., Blacksburg, VA, USA; NA","IEEE Transactions on Visualization and Computer Graphics","","2005","11","4","443","456","High-throughput experiments, such as gene expression microarrays in the life sciences, result in very large data sets. In response, a wide variety of visualization tools have been created to facilitate data analysis. A primary purpose of these tools is to provide biologically relevant insight into the data. Typically, visualizations are evaluated in controlled studies that measure user performance on predetermined tasks or using heuristics and expert reviews. To evaluate and rank bioinformatics visualizations based on real-world data analysis scenarios, we developed a more relevant evaluation method that focuses on data insight. This paper presents several characteristics of insight that enabled us to recognize and quantify it in open-ended user tests. Using these characteristics, we evaluated five microarray visualization tools on the amount and types of insight they provide and the time it takes to acquire it. The results of the study guide biologists in selecting a visualization tool based on the type of their microarray data, visualization designers on the key role of user interaction techniques, and evaluators on a new approach for evaluating the effectiveness of visualizations for providing insight. Though we used the method to analyze bioinformatics visualizations, it can be applied to other domains.","1077-2626;1941-0506;2160-9306","","10.1109/TVCG.2005.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432690","Index Terms- Evaluation/methodology;graphical user interfaces (GUI);information visualization;visualization systems and software;visualization techniques and methodologies.","Bioinformatics;Data visualization;Data analysis;Graphical user interfaces;Gene expression;Character recognition;Software systems;System testing;Proteins;Biology computing","data visualisation;graphical user interfaces;genetics;biology computing;very large databases;data analysis","insight-based methodology;bioinformatics visualization;gene expression microarrays;data analysis;open-ended user tests;user interaction techniques;graphical user interfaces","Algorithms;Artificial Intelligence;Computational Biology;Computer Graphics;Database Management Systems;Databases, Factual;Gene Expression Profiling;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Numerical Analysis, Computer-Assisted;Oligonucleotide Array Sequence Analysis;Online Systems;Pilot Projects;Research;User-Computer Interface","115","42","","","","","","IEEE","IEEE Journals & Magazines"
"Neural network learning through optimally conditioned quadratically convergent methods requiring NO LINE SEARCH","H. S. M. Beigi","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","Proceedings of 36th Midwest Symposium on Circuits and Systems","","1993","","","109","112 vol.1","Neural network learning algorithms based on conjugate gradient techniques and quasi Newton techniques such as Broyden, DFP, BFGS, and SSVM algorithms require exact or inexact line searches in order to satisfy their convergence criteria. Line searches are very costly and slow down the learning process. This paper presents new neural network learning algorithms based on Hoshino's weak line search technique and Davidon's optimally conditioned line search free technique. Also, a practical method of using these optimization algorithms is presented such that they will avoid getting trapped in local minima for the most part. The global minimization problem is a serious one when quadratically convergent techniques such as quasi Newton methods are used. Furthermore, to display the performance of the proposed learning algorithms, the more practical algorithm based on Davidon's minimization technique is used in conjunction with a cursive handwriting recognition problem. For comparison with other algorithms, also a few small benchmark tests are conducted and reported.<<ETX>>","","0-7803-1760","10.1109/MWSCAS.1993.343053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=343053","","Neural networks;Handwriting recognition;Convergence;Newton method;Optimization methods;Electronic mail;Optical character recognition software;Displays;Minimization methods;Backpropagation algorithms","minimisation;Newton method;handwriting recognition;optimisation;feedforward neural nets;learning (artificial intelligence);convergence of numerical methods","neural network learning algorithms;optimally conditioned quadratically convergent methods;conjugate gradient techniques;quasi Newton techniques;convergence criteria;line search elimination;weak line search technique;optimally conditioned line search free technique;optimization algorithms;global minimization problem;Davidon minimization technique;cursive handwriting recognition problem","","","12","","","","","","IEEE","IEEE Conferences"
"Using a collection of humans as an execution testbed for swarm algorithms","D. W. Palmer; M. Kirschenbaum; J. P. Murton; M. A. Kovacina; D. H. Steinberg; S. N. Calabrese; K. M. Zajac; C. M. Hantak; J. E. Schatz","John Carroll Univ., University Heights, OH, USA; John Carroll Univ., University Heights, OH, USA; John Carroll Univ., University Heights, OH, USA; NA; NA; NA; NA; NA; NA","Proceedings of the 2003 IEEE Swarm Intelligence Symposium. SIS'03 (Cat. No.03EX706)","","2003","","","58","64","To gain insight into swarm algorithms, researchers can study insect societies and other natural collectives, program multi-agent software simulations or build groups of cooperating robots. In our research, we consider another resource: swarms of humans. Human swarms provide three primary benefits: quick feedback and evaluation of swarm algorithms, experience with high-level swarm directives instead of low-level agent programs, and a source of swarm algorithms that can potentially be reverse-engineered for use in other applications. Planning is a human's preferred problem solving methodology because we are intelligent creatures with high-level communication skills. Due to the intelligence of the agents, human swarms can be quickly programmed, for subsequent observation and analysis. This paper describes human swarm experiments designed for gathering information on swarm algorithms. At these events 100 volunteers, wearing data-encoded T-shirts, work together to perform tasks of differing degrees of complexity. Researchers provide simple instructions for each task (programming the swarm), record the swarm's behavior (videotaped observation) and analyze the results (problem identification and algorithm-mining). We demonstrate the viability of this research by presenting the quick identification of a swarm algorithm ""bug"" and by producing a software implementation of a swarm algorithm gleaned from our observations.","","0-7803-7914","10.1109/SIS.2003.1202248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202248","","Humans;Testing;Software algorithms;Algorithm design and analysis;Robots;Insects;Feedback;Problem-solving;Intelligent agent;Probability","evolutionary computation;search problems;optimisation","human collection;execution testbed;swarm algorithms;data-encoded T-shirts;videotaped observation;swarm programming;problem identification;algorithm-mining;swarm algorithm bug identification;software implementation","","8","13","","","","","","IEEE","IEEE Conferences"
"Increasing on-chip memory space utilization for embedded chip multiprocessors through data compression","M. Kandemir; M. J. Irwin; O. Ozturk","Pennsylvania State University; Pennsylvania State University; Pennsylvania State University","2005 Third IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS'05)","","2005","","","87","92","Minimizing the number of off-chip memory references is very important in chip multiprocessors from both the performance and power perspectives. To achieve this the distance between successive reuses of the same data block must be reduced. However, this may not be possible in many cases due to data dependences between computations assigned to different processors. This paper focuses on software-managed on-chip memory space utilization for embedded chip multiprocessors and proposes a compression-based approach to reduce the memory space occupied by data blocks with large inter-processor reuse distances. The proposed approach has two major components: a compiler and an ILP (integer linear programming) solver. The compiler's job is to analyze the application code and extract information on data access patterns. This access pattern information is then passed to our ILP solver, which determines the data blocks to compress/decompress and the times (the program points) at which to compress/decompress them. We tested the effectiveness of this ILP based approach using access patterns extracted by our compiler from application codes. Our experimental results reveal that the proposed approach is very effective in reducing power consumption. Moreover, it leads to a lower energy consumption than an alternate scheme evaluated in our experiments for all the test cases studied.","","1-59593-161","10.1145/1084834.1084860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076318","chip multiprocessors;data compression;optimizing compiler","Data compression;Data mining;Testing;Energy consumption;Space technology;Costs;Program processors;Integer linear programming;Information analysis;Pattern analysis","","","","2","22","","","","","","IEEE","IEEE Conferences"
"Development of computer-controlled one-chip car tuner IC","H. Ishii; T. Asami; S. Sugayama; T. Imai","Sanyo Electric Co. Ltd., Gunma, Japan; NA; NA; NA","IEEE Transactions on Consumer Electronics","","1992","38","3","482","490","The authors have developed a computer-controlled, one-chip car tuner IC that includes all FM stereo tuner functions for single-IF diversity, as well as an AM upconversion tuner. They describe the features and technological advantages of the AM upconversion tuner system block, which operates under computer control. The results of field testing have confirmed that the new one-chip IC with a built-in AM upconversion system satisfies the tuner specifications of automakers.<<ETX>>","0098-3063;1558-4127","","10.1109/30.156726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=156726","","Tuners;Frequency;Circuit optimization;Tuning;Receiving antennas;Diodes;Varactors;Software performance;Loaded antennas;Radiofrequency amplifiers","amplitude modulation;automobiles;digital integrated circuits;frequency modulation;Hi-Fi equipment;radio receivers;telecommunications computer control;tuning","computer controlled IC;one-chip car tuner IC;FM stereo tuner;single-IF diversity;AM upconversion tuner;field testing;tuner specifications","","1","2","","","","","","IEEE","IEEE Journals & Magazines"
"Multiparameter self-optimizing systems using correlation techniques","K. Narendra; L. McBride","Harvard University, Cambridge, MA, USA; NA","IEEE Transactions on Automatic Control","","1964","9","1","31","38","A class of self-optimizing systems which continually alter their parameters to reduce a mean-square performance criterion is described. The change in each parameter is determined from an error gradient in parameter space computed by cross-correlation methods which are independent of signal spectra and require no test signal or parameter perturbation. Applications of this technique to both open-loop and closed-loop systems are included and it is shown that a combination of such self-optimizing systems is a possible solution to the adaptive control problem. Computer simulation results are included to demonstrate the practicality of the proposed systems.","0018-9286;1558-2523;2334-3303","","10.1109/TAC.1964.1105638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1105638","","Testing;Application software;Adaptive control;Computer simulation;Linear systems;Differential equations;Signal design;Automatic control;Error correction;Weight control","","Adaptive systems;Optimal control","","37","8","","","","","","IEEE","IEEE Journals & Magazines"
"On improving the performance of data partitioning oriented parallel irregular reductions","E. Gutierrez; O. Plata; E. L. Zapata","Dept. of Comput. Archit., Malaga Univ., Spain; NA; NA","Proceedings 10th Euromicro Workshop on Parallel, Distributed and Network-based Processing","","2002","","","445","452","Different parallelization techniques for reductions have been classified in this paper into two classes: LPO (loop partitioning-oriented techniques) and DPO (data partitioning-oriented techniques). We have analyzed both classes in terms of a set of performance properties: data locality, memory overhead, parallelism and workload balancing. We propose several techniques to increase the exploited parallelism and to introduce load balancing into a DPO method. Regarding parallelism, the solution is based on the partial expansion of the reduction array. For load balancing, the first technique is generic, as it can deal with any kind of load unbalance present in the problem domain. The second technique handles a special case of load unbalancing appearing when there are a large number of write operations on small regions of the reduction arrays. Efficient implementations of the proposed optimizing solutions for the DWA-LIP (data write affinity-loop index prefetching) DPO method are presented, experimentally tested on static and dynamic kernel codes, and compared with other parallel reduction methods.","","0-7695-1444","10.1109/EMPDP.2002.994330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994330","","Yarn;Parallel processing;Privatization;Computer architecture;Load management;Optimization methods;Testing;Kernel;Concurrent computing;Optical computing","parallel algorithms;shared memory systems;resource allocation;arrays;numerical analysis;software performance evaluation","data partitioning-oriented parallel irregular reductions;performance improvement;parallelization techniques;loop partitioning-oriented techniques;performance properties;data locality;memory overhead;parallelism;workload balancing;reduction array partial expansion;load unbalancing;write operations;optimizing solutions;DWA-LIP method;data-write affinity;loop-index prefetching;static kernel codes;dynamic kernel codes","","1","11","","","","","","IEEE","IEEE Conferences"
"A model for adaptable systems for transaction processing","B. Bhargava; J. Riedl","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA","Proceedings. Fourth International Conference on Data Engineering","","1988","","","40","50","A model is presented for an adaptable system that allows online switching of classes of algorithms for database transaction processing. The basic idea is to identify conditions on the state of processing that will maintain consistency during the switch from one class to another. The classes of concurrency control algorithms and the formalism of history for transaction processing and serializability have been used to develop this research. In addition to the formalism, the precise conditions for switching digraph-serializable (DSR) algorithms have been given. This research is being applied to switching network partition protocols (conservative to optimistic), commit protocols, recovery block software, and has led towards the design of an adaptable and reconfigurable distributed database system. An experimental system called RAID has been implemented to test these ideas; it has been noted that adaptability provides for varying performance requirements and deals with failures of sites, transactions, and other components of the system.<<ETX>>","","0-8186-0827","10.1109/ICDE.1988.105444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=105444","","Concurrency control;Partitioning algorithms;Application software;Transaction databases;Switches;Protocols;Optimization methods;Distributed databases;History;Design optimization","database theory;distributed databases;protocols","diagram serializable algorithms;model;adaptable systems;transaction processing;online switching;concurrency control algorithms;serializability;switching network partition protocols;commit protocols;recovery block software;distributed database system;RAID","","3","15","","","","","","IEEE","IEEE Conferences"
"A multi-level lexical-semantics based language model design for guided integrated continuous speech recognition","F. J. Valverde-Albacete; J. M. Pardo","Area de Tecnologia Electron., Univ. Carlos III, Madrid, Spain; NA","Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96","","1996","1","","224","227 vol.1","We present a continuous speech recognition architecture with a tightly coupled language model that tries to improve the dwindling performance of the normal stack decoder with increasing lexicon size. We solve the problem of recognition by means of two mutually recursive functions. The first one uses an auxiliary retrieval function to obtain lexicalized (already built) solutions to the problem, and merges these solutions with the ones built by the second function. This second one describes the acoustical and semantic recognition process as a search problem defined with the help of the first function, and solved with the help of the A* strategy. As a linguistic model, we use a hierarchy of linguistic levels each of which has a particular meaning structure, a lexicon of lexicalized forms, their lexicalization probabilities, and a local lexical grammar describing how the semantic categories of the level can be built. The process can further be optimized if targets, constraints on the possible solutions, are given to the recognition process to guide and restrict it. Target guidance implies a mechanism for target focusing, locally matching targets to the recognition state, and target prediction with the help of a lexical local grammar. We are testing the architecture in a DARPA RM-like application.","","0-7803-3555","10.1109/ICSLP.1996.607082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607082","","Natural languages;Decoding;Speech recognition;Search problems;Target recognition;Cost function;Constraint optimization;Testing;Buildings;Horses","speech recognition;natural language interfaces;computational linguistics;software performance evaluation;recursive functions;search problems;grammars","multi-level lexical-semantics;language model design;guided integrated continuous speech recognition;performance;normal stack decoder;lexicon size;mutually recursive functions;retrieval function;acoustical recognition;semantic recognition;search problem;A* strategy;linguistic model;lexicon;local lexical grammar;optimization;DARPA RM","","","9","","","","","","IEEE","IEEE Conferences"
"Optimal grouping of basis functions","Z. Baharav","Agilent Technol. Labs., Palo Alto, CA, USA","IEEE Transactions on Antennas and Propagation","","2001","49","4","567","573","When using the method of moments to solve scattering problems, a crucial factor is the appropriate choice of basis functions. A judicious choice of the basis functions can lead to a sparse impedance matrix or to a sparse solution vector, both of which will reduce the computational burden. However, pulse basis functions are usually used, both due to their simple description and because there are already many software tools written for this aim. Therefore, some efforts have been invested in transforming a formulation that is the result of using pulse basis functions to one using preferred basis functions. This transform produces grouping of the pulse functions into a new, more appropriate set of basis functions. This paper discusses the search for an optimal grouping, where optimality is measured in terms of the sparseness of the resulting solution vector. Various options are demonstrated and discussed, and numerical examples are given. It is important to note that the optimal grouping suggested herein is expected to be optimal on the average, when tried on many sample problems. Adjusting the optimization to a more specific set of problems and scenarios will lead to even better performances and is a viable option in practical cases.","0018-926X;1558-2221","","10.1109/8.923316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923316","","Sparse matrices;Impedance;Scattering;Moment methods;Testing;Software tools;Helium;Degradation;Canning","electromagnetic wave scattering;impedance matrix;optimisation;method of moments;sparse matrices","optimal grouping;basis functions;method of moments;scattering problems;sparse impedance matrix;sparse solution vector;pulse basis functions;software tools;transform;EM wave scattering;matrices","","2","17","","","","","","IEEE","IEEE Journals & Magazines"
"Crystal Ball for Six Sigma Tutorial","Goldman; Evans-Hilton; Emmett","Decisioneering Inc., Denver, CO, USA; Decisioneering Inc., Denver, CO, USA; NA","Proceedings of the 2003 Winter Simulation Conference, 2003.","","2003","1","","293","300 Vol.1","In an increasingly competitive market, businesses are turning to new practices like Six Sigma, a structured methodology for accelerated process improvement, to help reduce costs and increase efficiency. Monte Carlo simulation can help Six Sigma practitioners understand the variation inherent in a process or product, and in turn, can be used to identify and test potential improvements. The benefits of understanding and controlling the sources of variability include increased productivity, reduced waste, and sales driven through improved customer satisfaction. This tutorial uses Crystal Ball/spl reg/ Professional Edition, a suite of easy-to-use Microsoft Excel add-in software, to demonstrate how stochastic simulation and optimization can be used in a Six Sigma analysis of a technical support call center.","","0-7803-8131","10.1109/WSC.2003.1261436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261436","","Tutorial;Six sigma;Turning;Acceleration;Costs;Testing;Productivity;Marketing and sales;Customer satisfaction;Spreadsheet programs","software packages;six sigma (quality);spreadsheet programs;business process re-engineering;digital simulation;Monte Carlo methods;productivity;customer satisfaction;call centres","Six Sigma Tutorial;competitive market;businesses;structured methodology;accelerated process improvement;cost reduction;Monte Carlo simulation;potential improvements;variability source control;productivity;waste reduction;customer satisfaction;Crystal Ball Professional Edition;easy-to-use Microsoft Excel add-in software;stochastic simulation;optimization;Six Sigma analysis;technical support call center","","2","3","","","","","","IEEE","IEEE Conferences"
"On the creation of a new course entitled ""Modeling of Engineering Systems""","E. E. Yaz","Dept. of Electr. Eng., Arkansas Univ., Fayetteville, AR, USA","Proceedings Frontiers in Education 1995 25th Annual Conference. Engineering Education for the 21st Century","","1995","2","","4a5.12","4a5.13 vol.2","This paper explains the preparations undertaken to offer a new course on modeling of engineering systems. It details the factors motivating the development of such a course, its objectives, and the work plan being followed for its preparation. Although this course will be offered to the electrical engineering students, it should be of significant interest to other engineering majors as well. The course involves the use of SIMULINK simulation software.","0190-5848","0-7803-3022","10.1109/FIE.1995.483180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=483180","","Systems engineering and theory;Mathematical model;Design engineering;Biological system modeling;Analytical models;Engineering students;Job design;Performance analysis;Design optimization;System testing","circuit analysis computing;electrical engineering education;computer aided instruction;digital simulation;educational courses","educational course;engineering systems modeling;motivating factors;work plan;engineering education;electrical engineering students;engineering majors;SIMULINK;software package;simulation;computer aided instruction;Modeling of Engineering Systems","","","6","","","","","","IEEE","IEEE Conferences"
"Architecture of the simplified Chinese embedded system STARTH","P. X. Mao; Zhizhong Tang; M. Chen; Youming Zhang; Vern Zhang","Dept. of Comput. Sci., Tsinghua Univ., Beijing, China; NA; NA; NA; NA","Proceedings Fourth International Conference/Exhibition on High Performance Computing in the Asia-Pacific Region","","2000","2","","1167","1170 vol.2","There is a trend for the information products that are integrated by computer, communication, and consumer electronics. The OS is required more compact and practical. An embedded system STARTH is developed based on the core of the Motorola PPSM that is a real time 32-bit kernel with prioritized interrupt scheduling. All tasks are interrupt-driven. The PPSM kernel does not access hardware device directly. The kernel controls all peripherals indirectly, through software device drivers. The PPSM tools consist of pen input, graphics, database, text, character input, system and communication. The PPSM toolsets, together with its device drivers, provides the basic contro of the LCD, the drawing functions, the real time clock and the UART. The architecture of the embedded system STARTH on the Dragon Ball EZ platform is discussed in parts in details. The development environments of both software and hardware are described. The system is analyzed from its initialization, registration to system management, even the applications programming. The STARTH is tested and run on the hardware system of Dragon Ball platform. It is found that STARTH is practical and reliable for personal information devices.","","0-7695-0590-20-7695-0589","10.1109/HPC.2000.843622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=843622","","Embedded system;Kernel;Hardware;Computer architecture;Consumer electronics;Operating systems;Real time systems;Processor scheduling;Communication system control;Graphics","personal information systems;consumer electronics;embedded systems;application generators;application program interfaces;parallel processing","Chinese embedded system STARTH;information products;Motorola PPSM;real time 32-bit kernel;prioritized interrupt scheduling;hardware device;software device drivers;pen input;graphics;UART;Dragon Ball EZ platform;development environments;system management;applications programming;personal information devices","","","9","","","","","","IEEE","IEEE Conferences"
"High voltage RESURF DMOS process development using DFM techniques: a case study","M. Redford; M. Fallon; Z. A. Shafi; J. McGinty; N. S. Rankin; A. J. Walton","Nat. Semicond. (UK) Ltd., Greenock, UK; NA; NA; NA; NA; NA","IWSM. 1998 3rd International Workshop on Statistical Metrology (Cat. No.98EX113)","","1998","","","78","81","Using design for manufacturability (DFM) techniques, it is possible to reduce the number of split runs of silicon, and the number of cycles of learning, needed in the development of a technology. This ultimately results in a process which consistently meets its specifications, and can be manufactured without great engineering effort. The purpose of this work was to develop a process which consistently yielded >400 V breakdown for applications in the power supply market. Based on the results of the analysis, a process was chosen which was proven after one iteration of silicon, thereby saving on development time and costs. This paper reviews the work undertaken to determine the optimum process flow in order to achieve the desired electrical performance, and analysis of the predicted breakdown distribution. A comparison is made to actual distributions obtained from silicon. Then, from experimental split lots, a sensitivity analysis is performed. A comparison of simulated and actual results are compared. In conclusion, we describe learning as a result of the analyses, and the advantages and disadvantages of the method/software and methodology used when running and gathering Si data.","","0-7803-4338","10.1109/IWSTM.1998.729775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=729775","","Voltage;Design for manufacture;Silicon;Electric breakdown;Manufacturing processes;Power engineering and energy;Power supplies;Costs;Performance analysis;Sensitivity analysis","MOS integrated circuits;high-voltage techniques;power integrated circuits;power supply circuits;design for manufacture;electric breakdown;integrated circuit reliability;integrated circuit testing;optimisation;semiconductor process modelling;integrated circuit design;technology CAD (electronics)","HV RESURF DMOS process development;high voltage RESURF DMOS process development;DFM techniques;design for manufacturability;Si split runs;learning cycles;technology development;process specifications;breakdown;power supply market;silicon iterations;development time;development costs;optimum process flow;electrical performance;predicted breakdown distribution;sensitivity analysis;Si data;400 V;SiO/sub 2/-Si;Si","","","7","","","","","","IEEE","IEEE Conferences"
"Automatic analysis of functional program style","G. Michaelson","Dept. of Comput. & Electr. Eng., Heriot-Watt Univ., Edinburgh, UK","Proceedings of 1996 Australian Software Engineering Conference","","1996","","","38","46","Functional programming style is discussed and seven simple semantic style rules, based on program transformation, are enunciated. An automatic style analyser developed from these rules is then presented.","","0-8186-7635","10.1109/ASWEC.1996.534121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534121","","Computer languages;Automatic programming;Natural languages;Functional programming;Testing;Writing;Humans;Computer errors;Program processors;Optimizing compilers","functional programming;functional languages;automatic programming","automatic analysis;functional program style;functional programming style;simple semantic style rules;program transformation;automatic style analyser","","2","25","","","","","","IEEE","IEEE Conferences"
"Design of a Wide-Band Phased-Array Antenna for the Next Generation Radio Telescopes","A. B. Smolders; M. J. Arts; M. E. J. Jeuken; M. C. van Beurden","Netherlands Foundation for Research in Astronomy (NFRA), P.O. Box 2, 7990 AA Dwingeloo, The Netherlands, tel: +31 521 595100, fax: +31 521 597332, E-mail: smolders@nfra.nl; Netherlands Foundation for Research in Astronomy (NFRA), P.O. Box 2, 7990 AA Dwingeloo, The Netherlands, tel: +31 521 595100, fax: +31 521 597332; Eindhoven University of Technology, P.O. Box 513, 5600 MB Eindhoven, The Netherlands; Eindhoven University of Technology, P.O. Box 513, 5600 MB Eindhoven, The Netherlands","1998 28th European Microwave Conference","","1998","1","","161","166","The next generation of radio telescopes will have two orders of magnitude more sensitivity than presentday radio telescopes. At this moment, a lot of effort is put in defining this new radio telescope: SKAI, the Square Kilometre Array Interferometer. For that purpose, four demonstrator phased-array systems are being developed at NFRA. The One Square Metre Array (OSMA) is the second demonstrator system and is currently being tested. For OSMA, a wide-band printed antenna element with an integrated balun was developed and tested experimentally. Electromagnetic models and corresponding software were used to predict and optimise the scan performance of the wide-band element in an array environment. Theoretical and experimental results of several prototype arrays will be presented in this paper.","","","10.1109/EUMA.1998.338110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4139066","","Optical design;Broadband antennas;Radio astronomy;Radio interferometry;System testing;Impedance matching;Electromagnetic modeling;Software performance;Wideband;Software prototyping","","","","1","","","","","","","IEEE","IEEE Conferences"
"Improved methods for memory module characterization","M. Carmona; G. Eggers; S. Leseduarte; A. Legen; A. Wolter; S. Neugebauer; J. Thomas","Infineon Technol. AG, Munich, Germany; Infineon Technol. AG, Munich, Germany; Infineon Technol. AG, Munich, Germany; Infineon Technol. AG, Munich, Germany; Infineon Technol. AG, Munich, Germany; Infineon Technol. AG, Munich, Germany; Infineon Technol. AG, Munich, Germany","EuroSimE 2005. Proceedings of the 6th International Conference on Thermal, Mechanial and Multi-Physics Simulation and Experiments in Micro-Electronics and Micro-Systems, 2005.","","2005","","","139","146","An accurate prediction of junction temperatures in memory modules is necessary for an appropriate design of a thermal system solution. For this reason, a thermal analysis of the accuracy of usual techniques for modelling the different components of a memory module is afforded in this paper. A first theoretical analysis of the characteristics and problems of the different components (DRAMs and PCB) is first performed. Regarding DRAMs, a static compact model is extracted from its detailed description by the use of a FEM software. This extraction involves an optimisation procedure of an error function based on a set of boundary conditions. A proposal of a physically-motivated error function is provided. For the PCB, a simplified model is extracted also from a detailed FEM model, but with reduced number of boundary conditions. A measurement setup has been developed with the intention to study the goodness of the developed models. For this reason, two simplified boundary conditions sets were used: simple natural convection and optimised board cooling condition, which could allow us to avoid inaccuracies in the environment conditions when performing the correlation of simulation results. Simulation results have been compared with measurements, providing a good correlation. For a simple natural convection boundary, results are providing a good accuracy for both compact models: the five resistance model and the simple two resistance model. For the optimised board cooling condition, the two resistance model is overestimating the cooling conditions through the PCB thermal connection. Only a disagreement is obtained in the temperature distribution of DRAM case temperature over the module. The reason is thought to be the unequal power distribution over the DRAMs.","","0-7803-9062","10.1109/ESIME.2005.1502789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1502789","","Boundary conditions;Random access memory;Cooling;Predictive models;Temperature distribution;Energy consumption;Uncertainty;Packaging;Appropriate technology;Performance analysis","thermal management (packaging);modules;DRAM chips;printed circuits;thermal analysis;integrated circuit testing;finite element analysis;natural convection;electronic engineering computing","memory module characterization;junction temperatures;thermal analysis;printed circuit boards;static compact model;FEM software;optimisation procedure;physically-motivated error function;simple natural convection;optimised board cooling condition;simulation;resistance model;PCB thermal connection;temperature distribution;DRAM case temperature;power distribution","","1","11","","","","","","IEEE","IEEE Conferences"
"An overview of the ATLAS high-level trigger dataflow and supervision","J. T. Baines; C. P. Bee; A. Bogaerts; M. Bosman; D. Botterill; B. Caron; A. dos Anjos; F. Etienne; S. Gonzalez; K. Karr; W. Li; C. Meessen; G. Merino; A. Negri; J. L. Pinfold; P. Pinto; Z. Qian; F. Touchard; P. Werner; S. Wheeler; F. J. Wickens; W. Wiedenmann; G. Zobernig","Rutherford Appleton Lab., Oxon, UK; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Nuclear Science","","2004","51","3","361","366","The ATLAS high-level trigger (HLT) system provides software-based event selection after the initial LVL1 hardware trigger. It is composed of two stages, the LVL2 trigger and the event filter (EF). The LVL2 trigger performs event selection with optimized algorithms using selected data guided by Region of Interest pointers provided by the LVL1 trigger. Those events selected by LVL2 are built into complete events, which are passed to the EF for a further stage of event selection and classification using off-line algorithms. Events surviving the EF selection are passed for off-line storage. The two stages of HLT are implemented on processor farms. The concept of distributing the selection process between LVL2 and EF is a key element in the architecture, which allows it to be flexible to changes (luminosity, detector knowledge, background conditions, etc.) Although there are some differences in the requirements between these subsystems there are many commonalities. An overview of the dataflow (event selection) and supervision (control, configuration, monitoring) activities in the HLT is given, highlighting where commonalities between the two subsystems can be exploited and indicating where requirements dictate that implementations differ. An HLT prototype system has been built at CERN. Functional testing is being carried out in order to validate the HLT architecture.","0018-9499;1558-1578","","10.1109/TNS.2004.828875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310526","","Large Hadron Collider;Detectors;Filters;Physics;Event detection;Hardware;Classification algorithms;Prototypes;Testing;Laboratories","position sensitive particle detectors;trigger circuits;data acquisition;reviews;optimisation","overview;ATLAS high-level trigger dataflow;ATLAS high-level trigger supervision;software-based event selection;LVL2 trigger;event filter;optimized algorithms;off-line storage;luminosity;background conditions;control activity;configuration activity;monitoring activity;prototype system;CERN;data acquisition","","2","11","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability and thermal structure design for CMOS image sensor","Hsiang-Chen Hsu; Hui-Yu Lee; Yu-Chia Hsu; Shen-Li Fu; Chung-Chi Yang; Kuan-Chieh Huang","Dept. of Mech. Eng., I-Shou Univ., Tainan, Taiwan; Dept. of Mech. Eng., I-Shou Univ., Tainan, Taiwan; Dept. of Mech. Eng., I-Shou Univ., Tainan, Taiwan; NA; NA; NA","2005 7th Electronic Packaging Technology Conference","","2005","2","","5 pp.","","The characteristic of overall structure for CMOS image sensor has been studied in this paper. This paper demonstrates the thermal design for the material of compound, UV glue as well as glass. A three-dimensional solid model of CMOS image sensor based on finite element ANSYS software is developed to predict the thermal strain distributions. It is found that the peak stress in UV glue plays a significant role through the manufacturing process. The predicted thermal strains were based on previous researches with the Moire interferometry experimental scheme. The developed finite element 3D model is compared with the JEDEC standard JESD22-A104 reliability thermal cycle test (TCT). The failure mode trend for both predicted model and TCT experimental result were found to be good agreement. A series of comprehensive parametric studies were conducted in this paper. The design rules for thermal optimization of CMOS image sensor are summarized","","0-7803-9578","10.1109/EPTC.2005.1614474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1614474","","CMOS image sensors;Semiconductor device modeling;Predictive models;Finite element methods;Capacitive sensors;Glass;Solid modeling;Thermal stresses;Manufacturing processes;Interferometry","adhesives;CMOS image sensors;finite element analysis;integrated circuit reliability;thermal management (packaging)","thermal structure design;CMOS image sensor;3D solid model;finite element ANSYS software;thermal strain distributions;UV glue;finite element 3D model;thermal cycle test;failure mode trend;thermal optimization","","2","10","","","","","","IEEE","IEEE Conferences"
"Automatic ellipsometry measurement for anisotropic materials","Y. Li; B. You; L. Zhang; X. Yao","Electron Mater. Res. Lab., Xi'an Jiaotong Univ., China; Electron Mater. Res. Lab., Xi'an Jiaotong Univ., China; Electron Mater. Res. Lab., Xi'an Jiaotong Univ., China; Electron Mater. Res. Lab., Xi'an Jiaotong Univ., China","ISAF '92: Proceedings of the Eighth IEEE International Symposium on Applications of Ferroelectrics","","1992","","","613","616","A novel variable-medium and variable-angle ellipsometry (VMAE) system which is able to make measurements on anisotropic materials is described. A rotating analyzer ellipsometer operates with a variable angle of the laser beam, a variable azimuth of the sample, etc. Using this approach, equations for anisotropic bulk materials and thin films are obtained. The variable equations for the calculation of the optical constants of anisotropic materials can be solved by a PC-type computer combining the Powell, Rosenbrock, and Palmer direct optimization methods. The software structure is described. Experimental results are compared with theoretical values to test the reliability of the system.<<ETX>>","","0-7803-0465","10.1109/ISAF.1992.300740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=300740","","Ellipsometry;Anisotropic magnetoresistance;Equations;Laser beams;Azimuth;Optical materials;Transistors;Geometrical optics;Optical films;Optical computing","computerised instrumentation;ellipsometry;lead compounds;lithium compounds;microcomputer applications;optical constants;refractive index measurement;titanium compounds","system reliability testing;PC;refractive index;variable angle ellipsometry system;automatic ellipsometry measurement;anisotropic materials;VMAE;rotating analyzer ellipsometer;thin films;variable equations;optical constants;direct optimization methods;software structure;He-Ne laser;LiNbO/sub 3/;PLZT;TiO/sub 2/ film;PbLaZrO3TiO3","","","10","","","","","","IEEE","IEEE Conferences"
"MOVA-equipment development and the implementation of a national trial","J. S. Coulter; T. Delaney; G. Fisher","NA; NA; NA","IEE Colloquium on UK Developments in Road Traffic Signalling","","1988","","","2/1","2/8","MOVA-Microprocessor Optimised Vehicle Actuation-is a control strategy for isolated signals, developed over recent years at the Transport and Road Research Laboratory. The paper describes the hardware of the system, the software, and the transfer of the FORTRAN test programs into the actual system.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=209194","","Microcomputer applications;Traffic control (transportation)","microcomputer applications;road traffic;signalling;traffic computer control","TRRL;traffic lights;MOVA;Microprocessor Optimised Vehicle Actuation;isolated signals;Transport and Road Research Laboratory;hardware;software;FORTRAN test programs","","","","","","","","","IET","IET Conferences"
"Nanometer mixed-signal system-on-a-chip design","E. Chou; B. Sheu","NA; NA","IEEE Circuits and Devices Magazine","","2002","18","4","7","17","A mixed-signal system-on-a-chip (SoC) design methodology and the supporting CAD tools are presented. A known tools set is identified for illustration purposes and some alternative tools can equally accomplish the task.","8755-3996;1558-1888","","10.1109/MCD.2002.1021118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021118","","System-on-a-chip;Integrated circuit modeling;Design methodology;Integrated circuit testing;Circuit testing;Design automation;System testing;Research and development;Design engineering;Prototypes","mixed analogue-digital integrated circuits;integrated circuit layout;circuit layout CAD;hardware-software codesign;product development;design for testability","mixed-signal system-on-a-chip design;CAD tools;integrated-circuit design;design methodology;nanometer system-on-a-chip;design tools;planning stage;R&D prototype stage;product development stage;testability;functional model;floating point system model;front-end design flow;bit-true model;behavioral model;layout optimization;signal integrity effects;top-down design","","7","","","","","","","IEEE","IEEE Journals & Magazines"
"A heuristic method for ceneralized hypercube encoding","K. K. Singh; S. K. Chang; C. C. Yang","University of Illinois at Chicago Circle; NA; NA","COMPSAC 79. Proceedings. Computer Software and The IEEE Computer Society's Third International Applications Conference, 1979.","","1979","","","531","534","Similarity retrieval from a pic torial data base can be made more efficient by encoding the original data base into certain convenient format. Generalized hyper cube (GH) encoding is one such technique. To optimize GH encoding, a heuristic approach has been formulated. Two optimization problems have been considered here: (1) given the handle length m, find optimal GHm encoding; (2) given the threshold density, find optimal GHm encoding such that each hypercube has den sity no less than threshold density.","","","10.1109/CMPSAC.1979.762553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=762553","","Hypercubes;Encoding;Information retrieval;Spatial databases;Testing;Laboratories;Distributed computing;Knowledge based systems;Pattern recognition;Pattern analysis","","","","","","","","","","","IEEE","IEEE Conferences"
"Automatic tuning of two-level caches to embedded applications","A. Gordon-Ross; F. Vahid; N. Dutt","Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; Dept. of Comput. Sci. & Eng., California Univ., Riverside, CA, USA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","208","213 Vol.1","The power consumed by the memory hierarchy of a microprocessor can contribute to as much as 50% of the total microprocessor system power, and is thus a good candidate for optimizations. We present an automated method for tuning two-level caches to embedded applications for reduced energy consumption. The method is applicable to both a simulation-based exploration environment and a hardware-based system prototyping environment. We introduce the two-level cache tuner, or TCaT - a heuristic for searching the huge solution space of possible configurations. The heuristic interlaces the exploration of the two cache levels and searches the various cache parameters in a specific order based on their impact on energy. We show the integrity of our heuristic across multiple memory configurations and even in the presence of hardware/software partitioning - a common optimization capable of achieving significant speedups and/or reduced energy consumption. We apply our exploration heuristic to a large set of embedded applications. Our experiments demonstrate the efficacy of our heuristic: on average the heuristic examines only 7% of the possible cache configurations, but results in cache sub-system energy savings of 53%, only 1% more than the optimal cache configuration. In addition, the configured cache achieves an average speedup of 30% over the base cache configuration due to tuning of cache line size to the application's needs.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268850","","Microprocessors;Runtime;Energy consumption;Hardware;Process design;Virtual prototyping;Tuners;Application software;Embedded system;Space exploration","microprocessor chips;cache storage;circuit tuning;embedded systems;memory architecture","automatic tuning;two-level caches;embedded applications;memory hierarchy;microprocessor;energy consumption;simulation-based exploration environment;hardware-based system prototyping environment;TCaT;cache parameters;multiple memory configurations;hardware-software partitioning;base cache configuration;cache line size;configurable cache;cache hierarchy;architecture tuning","","47","21","","","","","","IEEE","IEEE Conferences"
"The Dynamics Of Soviet Greenhouse Gas Emissions","A. A. Makarov; I. A. Bashmakov","NA; NA","Proceedings of the 25th Intersociety Energy Conversion Engineering Conference","","1990","4","","444","449","","","0-8169-0490","10.1109/IECEC.1990.716531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=716531","","Global warming;Power generation economics;Carbon dioxide;Energy consumption;Computer aided software engineering;Testing;Cost function;Optimization methods;Production systems;Petroleum","","","","","6","","","","","","IEEE","IEEE Conferences"
"Component tolerance and circuit performance: a case study","R. V. White","AT&T Bell Lab., Mesquite, TX, USA","Proceedings Eighth Annual Applied Power Electronics Conference and Exposition,","","1993","","","922","927","The author presents a continuation of work presented at APEC 1992 (see p.28-35), in which a simple circuit was simulated to predict yield in the manufacturing process. The simulation results had some anomalies. This was because the component values were assumed to be uniformly distributed between the tolerance limits. The author updates that assumption, and uses normally distributed component values. Various simulations are run with varying component tolerance and quality levels. From the mean and variance of the simulated results, plots are made that show the specification limits that would have to be set to meet a given yield requirement.<<ETX>>","","0-7803-0983","10.1109/APEC.1993.290679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=290679","","Circuit optimization;Computer aided software engineering;Monte Carlo methods;Circuit testing;Aerospace electronics;Aerospace engineering;Detectors;Costs;Research and development;Documentation","digital simulation;electronic engineering computing;integrated circuit manufacture;quality control;statistical analysis","component tolerance;overcurrent detector circuit;component quality levels;Monte Carlo simulation;manufacturing process yield;circuit performance;normally distributed component values","","14","2","","","","","","IEEE","IEEE Conferences"
"Analyzing Cell Designs by Computer for Optimum Performnce","E. A. Wagner","EXIDE INDUSTRAIAL BATTERY DIVISION","INTELEC - 1978 International Telephone Energy Conference","","1978","","","234","236","A practical approach is used to demonstrate that the electrical resistance of the component parts of a lead-acid cell can be correlated to the total cell resistance and the electrical performance of the cell. A computer program is used to analyze the cell design for the optimum use of materials.","","","10.1109/INTLEC.1978.4793553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4793553","","Performance analysis;Voltage;Batteries;Electric resistance;Testing;Gravity;Computer industry;Application software;Design optimization;Laboratories","","","","2","","","","","","","IEEE","IEEE Conferences"
"Behavior of keys in random databases","O. Seleznjev; B. Thalheim","Fac. of Math. & Mech., Moscow State Univ., Russia; NA","Proceedings SCCC'98. 18th International Conference of the Chilean Society of Computer Science (Cat. No.98EX212)","","1998","","","171","183","The wide class of stochastic models for databases was considered. The properties of key systems and functional dependencies in stochastic databases were investigated in the average case setting. Comparing with the worst case setting the exponential size of minimal key system is rather unusual in average. For several stochastic models the Poisson approximations of characteristics of the most probable minimal key candidates have been derived in terms of the Renyi entropies. The proposed general method of analysis is based on probabilistic and information theory results. As the first and necessary step for a statistical analysis, several probabilistic models for random tables and relations have been investigated. The outline of possible further practical applications may be as the follows (i) fitting the corresponding stochastic model; (ii) estimation of the model parameters for a given data; (iii) approximation and analysis of key system characteristics. Besides the considered discrete distributions, the Markovian model for stochastic dependencies and the corresponding multivariate normal approximations for discrete distributions can be considered. For the normal distribution, the well-known technique can be applied for the estimation of the model parameters.","","0-8186-8616","10.1109/SCCC.1998.730797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730797","","Databases;Computer science;Entropy;Testing;Mathematics;Application software;Pattern recognition;Digital circuits;Geology;Design optimization","database theory;stochastic systems;random processes;entropy;statistical analysis;probability","random databases;key behavior;stochastic models;functional dependencies;stochastic databases;average case setting;minimal key system;Poisson approximations;Renyi entropies;information theory;statistical analysis;probabilistic models;random tables;random relations;model parameter estimation;discrete distributions;Markovian model;multivariate normal approximations;stochastic dependencies;normal distribution","","1","14","","","","","","IEEE","IEEE Conferences"
"A methodology for design verification","E. Hu; B. Yeh; T. Chan","Apple Comput. Inc., Santa Clara, CA, USA; Apple Comput. Inc., Santa Clara, CA, USA; Apple Comput. Inc., Santa Clara, CA, USA","Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit","","1994","","","236","239","Recent advancements in design automation tools have helped to shorten the design time for many ASICs. The functional verification of these ASICs, however, remains a wholly labor intensive and sequential task. This paper documents a parallel flow methodology that addresses the problem with a different approach to resource distribution for verification. Such a distribution allows for more time and resources to be dedicated to the verification task while still supporting a shortened design cycle.<<ETX>>","","0-7803-2020","10.1109/ASIC.1994.404568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=404568","","Design methodology;Testing;Design engineering;Software tools;Computer aided engineering;Hardware design languages;Application specific integrated circuits;Design automation;Resource management;Design optimization","hardware description languages;application specific integrated circuits;circuit analysis computing;logic CAD","design verification;design automation tools;design time;ASICs;functional verification;parallel flow methodology;resource distribution;verification task;shortened design cycle;HDL;RTL;CAE tools","","","","","","","","","IEEE","IEEE Conferences"
"Statistical foundations of audit trail analysis for the detection of computer misuse","P. Helman; G. Liepins","Dept. of Comput. Sci., New Mexico Univ., Albuquerque, NM, USA; NA","IEEE Transactions on Software Engineering","","1993","19","9","886","901","We model computer transactions as generated by two stationary stochastic processes, the legitimate (normal) process N and the misuse process M. We define misuse (anomaly) detection to be the identification of transactions most likely to have been generated by M. We formally demonstrate that the accuracy of misuse detectors is bounded by a function of the difference of the densities of the processes N and M over the space of transactions. In practice, detection accuracy can be far below this bound, and generally improves with increasing sample size of historical (training) data. Careful selection of transaction attributes also can improve detection accuracy; we suggest several criteria for attribute selection, including adequate sampling rate and separation between models. We demonstrate that exactly optimizing even the simplest of these criteria is NP-hard, thus motivating a heuristic approach. We further differentiate between modeling (density estimation) and nonmodeling approaches.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.241771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=241771","","Monitoring;Laboratories;System testing;Intrusion detection;Physics computing;Computer science;Stochastic processes;Detectors;Space stations;Sampling methods","auditing;computer crime;security of data;stochastic processes;transaction processing","audit trail analysis;computer misuse;computer transactions;stationary stochastic processes;misuse detectors;detection accuracy;transaction attributes;NP-hard;heuristic approach;density estimation;modeling;statistical foundations;system security","","53","23","","","","","","IEEE","IEEE Journals & Magazines"
"A rapidly convergent method for equality constrained function minimization","R. G. Brusch","General Dynamics","1973 IEEE Conference on Decision and Control including the 12th Symposium on Adaptive Processes","","1973","","","80","81","This paper presents a new function minimization algorithm for minimizing nonlinear functions of a finite number of variables subject to nonlinear equality constraints. The algorithm also provides for the explicit handling of upper and lower bounds on each of the independent variables. Although other more general inequality constraint can be transformed into an equality constraint at the expense of introducing an additional slack variable. The algorithm proposed combines the idea of a ""balance function,"" developed independently in References 1 and 2 with a second order method for updating the balance function La grange multipliers originally developed in Reference 3. This updating technique makes use of the current estimate of the inverse Hessian of the balance function which is a byproduct of the unconstrained minimization of the balance function using the Fletcher-Powell algorithm.","","","10.1109/CDC.1973.269134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4045047","","Minimization methods;Testing;Convergence;Constraint optimization;Constraint theory;Optimal control;Application software;Publishing","","","","","","","","","","","IEEE","IEEE Conferences"
"Performance of meshed comb constellations for minimizing target revisit time","B. Jackson","Ball Aerosp. & Technol. Corp., Boulder, CO, USA","1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)","","1998","5","","59","69 vol.5","Walker satellite constellations are commonly proposed for remote sensing applications requiring global access. However, for many applications, target revisit time may be as important as coverage area. A subclass of non-Walker constellations called ""meshed comb"" constellations can outperform most Walker constellations in coverage and/or revisit time in many situations. Exhaustive comparisons were made of an eight-satellite meshed comb constellation with a variety of eight-satellite Walker constellations against several types of target decks. The results showed that the meshed comb constellation typically had the lowest maximum revisit time; and the average revisit time was comparable to or better than any of the Walker constellations. In addition, the meshed comb constellation usually had the lowest standard deviation for revisit time, as well as the lowest 2/spl sigma/ and 3/spl sigma/ revisit times.","1095-323X","0-7803-4311","10.1109/AERO.1998.685792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=685792","","Orbits;Remote monitoring;Remote sensing;Business;Satellite constellations;Testing;Computer aided software engineering;Hazards;Surveillance;Design optimization","target tracking;remote sensing;satellite communication;minimisation","meshed comb constellations;Walker satellite constellations;remote sensing;global access;target revisit time;coverage area;nonWalker constellations;target decks","","","3","","","","","","IEEE","IEEE Conferences"
"The bi-weighted TSP problem for VLSI crosstalk minimization","D. F. Wong; Muzhou Shao","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","2002 IEEE International Symposium on Circuits and Systems. Proceedings (Cat. No.02CH37353)","","2002","3","","III","III","We consider a special traveling salesman problem (TSP) called bi-weighted TSP. The problem of determining an optimal ordering for a set of parallel wires to minimize crosstalk noise can be formulated as a bi-weighted TSP problem. Let G be an undirected complete weighted graph where the weight (cost) on each edge is either 1 or 1+/spl alpha/. The objective of the bi-weighted TSP problem is to find a minimum cost Hamiltonian path in G. Existing algorithms for general TSP (e.g., nearest-neighbor algorithm and Christofide algorithm) can be applied to solve this problem. In this paper, we prove that the nearest-neighbor algorithm has worst case performance ratio of 1+/spl alpha//2 and the bound is tight. We also show that the algorithm is asymptotically optimal when m is o(n/sup 2/), where n is the number of nodes in G and m is the number of edges with cost 1+/spl alpha/. Analysis of the Christofide algorithm is also presented.","","0-7803-7448","10.1109/ISCAS.2002.1010337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010337","","Very large scale integration;Crosstalk;Cost function;Traveling salesman problems;Wires;Algorithm design and analysis;Optimized production technology;Polynomials;Software testing;Performance analysis","VLSI;crosstalk;integrated circuit noise;integrated circuit layout;travelling salesman problems;graph theory;minimisation;network routing","bi-weighted TSP problem;VLSI crosstalk minimization;traveling salesman problem;optimal ordering;wire ordering problem;parallel wires;crosstalk noise;undirected complete weighted graph;minimum cost Hamiltonian path;nearest-neighbor algorithm;Christofide algorithm","","","9","","","","","","IEEE","IEEE Conferences"
"Computational Steering","E. Kraemer; J. Vetter","Washington University in St. Louis; NA","Proceedings of the Thirty-First Hawaii International Conference on System Sciences","","1998","7","","126","126","","","0-8186-8255","10.1109/HICSS.1998.649205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649205","","Application software;Visualization;Checkpointing;Load management;Testing;Computer science;Resource management;State feedback;Environmental management;Optimization","","","","","","","","","","","IEEE","IEEE Conferences"
"Probability density estimation from optimally condensed data samples","M. Girolami; Chao He","Sch. of Inf. & Commun. Technol., Paisley Univ., UK; Sch. of Inf. & Commun. Technol., Paisley Univ., UK","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2003","25","10","1253","1264","The requirement to reduce the computational cost of evaluating a point probability density estimate when employing a Parzen window estimator is a well-known problem. This paper presents the Reduced Set Density Estimator that provides a kernel-based density estimator which employs a small percentage of the available data sample and is optimal in the L/sub 2/ sense. While only requiring /spl Oscr/(N/sup 2/) optimization routines to estimate the required kernel weighting coefficients, the proposed method provides similar levels of performance accuracy and sparseness of representation as Support Vector Machine density estimation, which requires /spl Oscr/(N/sup 3/) optimization routines, and which has previously been shown to consistently outperform Gaussian Mixture Models. It is also demonstrated that the proposed density estimator consistently provides superior density estimates for similar levels of data reduction to that provided by the recently proposed Density-Based Multiscale Data Condensation algorithm and, in addition, has comparable computational scaling. The additional advantage of the proposed method is that no extra free parameters are introduced such as regularization, bin width, or condensation ratios, making this method a very simple and straightforward approach to providing a reduced set density estimator with comparable accuracy to that of the full sample Parzen density estimator.","0162-8828;2160-9292;1939-3539","","10.1109/TPAMI.2003.1233899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1233899","","Constraint optimization;Computational efficiency;Optimization methods;Kernel;Density functional theory;Testing;Application software;Chaos;Support vector machines","parameter estimation;learning automata;pattern recognition","point probability density;computational cost;probability density estimation;Parzen window;sparse representation;support vector machine","","118","35","","","","","","IEEE","IEEE Journals & Magazines"
"Extended whole program paths","S. Tallam; R. Gupta; Xiangyu Zhang","Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA","14th International Conference on Parallel Architectures and Compilation Techniques (PACT'05)","","2005","","","17","26","We describe the design, generation and compression of the extended whole program path (eWPP) representation that not only captures the control flow history of a program execution but also its data dependence history. This representation is motivated by the observation that typically a significant fraction of data dependence history can be recovered from the control flow trace. To capture the remainder of the data dependence history we introduce disambiguation checks in the program whose control flow signatures capture the results of the checks. The resulting extended control flow trace enables the recovery of otherwise unrecoverable data dependences. The code for the checks is designed to minimize the increase in the program execution time and the extended control flow trace size when compared to directly collecting control flow and dependence traces. Our experiments show that compressed eWPPs are only 4% of the size of combined compressed control flow and dependence traces and their collection requires 20% more runtime overhead than overhead required for directly collecting the control flow and dependence traces.","1089-795X","0-7695-2429","10.1109/PACT.2005.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515577","","History;Size control;Runtime;Application software;Software debugging;Software testing;Intrusion detection;Optimizing compilers;Registers;Computer science","program compilers;program control structures","extended whole program path;control flow history;data dependence history;control flow signatures;program execution time","","8","21","","","","","","IEEE","IEEE Conferences"
"An Algorithm for Linear Inequalities and its Applications","Y. Ho; R. L. Kashyap","Division of Engineering and Applied Physics, Harvard University, Cambridge, Mass.; Division of Engineering and Applied Physics, Harvard University, Cambridge, Mass.","IEEE Transactions on Electronic Computers","","1965","EC-14","5","683","688","An exponentially convergent and finite algorithm is presented for the determination of the solution  of the linear inequalities A>0 for a given matrix A, or for determining the non-existence of solution for A>0. This result is useful in threshold-switching theory and in pattern classification problems. Experiments indicate extremely rapid convergence of the method.","0367-7508","","10.1109/PGEC.1965.264207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4038553","","Convergence;Pattern classification;Vectors;Application software;Mathematical programming;Linear programming;Costs;Relaxation methods;Testing;Optimization methods","","","","122","13","","","","","","IEEE","IEEE Journals & Magazines"
"Worst case analysis or ""preparing for Murphy""","J. T. Fridenberg; R. A. Fettig","Fridenberg Res. Inc., Sierra Madre, CA, USA; NA","Wescon/96","","1996","","","439","442","Murphy's Law-""If anything can go wrong, it will""-is the basis for performing Worst Case Analysis(WCA). A WCA provides a method for verifying circuit performance and derating requirements over a program specified environment and life. WCA should be an integral part of the design of every electronic assembly. The results will serve to assure proper system operation under the most unfavorable combination of realizable conditions. The WCA should be performed as the design evolves, but prior to the design finalization.","1095-791X","0-7803-3274","10.1109/WESCON.1996.554538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=554538","","Computer aided software engineering;Performance analysis;Circuit testing;Robustness;Hardware;Circuit optimization;Voltage;Costs;Temperature;Transient analysis","assembling;circuit reliability;iterative methods","worst case analysis;Murphy's Law;circuit performance;derating requirements;program specified environment;electronic assembly;system operation;realizable conditions;design evolution","","","","","","","","","IEEE","IEEE Conferences"
"Hybrid spread spectrum signal acquisition in the presence of worst case multitone jamming","Siew-Kee Loh","NA","MILCOM 97 MILCOM 97 Proceedings","","1997","1","","98","102 vol.1","The acquisition performance of a hybrid slow frequency hopped direct sequence spread spectrum (SFH/DS SS) communication link in the presence of multitone jamming is analysed. Equations are derived to enable analysis of the optimisation of the jamming strategy. It is shown that the jamming strategy may be optimised by varying the number of tones. It is also shown that a higher DS spreading gain is preferable in improving the anti-jam capability of the sychronisation system against the multitone jammer.","","0-7803-4249","10.1109/MILCOM.1997.648677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648677","","Spread spectrum communication;Computer aided software engineering;Jamming;Frequency synchronization;Performance analysis;Equations;Matched filters;Testing;Laboratories;Bit error rate","spread spectrum communication;frequency hop communication;jamming;signal detection;synchronisation;radio links","hybrid spread spectrum signal acquisition;worst case multitone jamming;acquisition performance;SFH/DS SS;communication link;slow frequency hopped spread spectrum;multitone jamming;jamming optimisation;DS spreading gain;anti-jam capability;false acquisition probability;sychronisation system;equations","","1","6","","","","","","IEEE","IEEE Conferences"
"A multi-pronged approach to defect management","D. E. Paradis; R. L. Guldi; J. N. Lalena; J. W. Ritchison","Texas Instrum. Inc., Dallas, TX, USA; Texas Instrum. Inc., Dallas, TX, USA; Texas Instrum. Inc., Dallas, TX, USA; Texas Instrum. Inc., Dallas, TX, USA","Proceedings of International Symposium on Semiconductor Manufacturing","","1995","","","74","79","This paper describes a comprehensive, three pronged approach to defect management combining the elements of continuous in-line monitoring, yield loss quantification, and short loop analysis. In-line monitoring provides both a historical defect baseline and a trigger for initiating corrective action in the event of out-of-control situations. Yield loss quantification involving conventional bit mapping or bin mapping partitions the yield loss by defect type and process level. Loop monitors relate trouble spots to root causes, providing a continuous measure of the progress of process and equipment improvement programs.","","0-7803-2928","10.1109/ISSM.1995.524363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=524363","","Inspection;Monitoring;Testing;Manufacturing;Management training;Production;Instruments;Probes;Software maintenance;Design optimization","inspection;integrated circuit manufacture;integrated circuit yield","defect management;continuous in-line monitoring;yield loss quantification;short loop analysis;bit mapping;bin mapping;integrated circuit manufacturing","","","","","","","","","IEEE","IEEE Conferences"
"Concurrent Certifications by Intervals of Timestamps in Distributed Database Systems","C. Boksenbaum; M. Cart; J. Ferrie; J. -. Pons","Centre de Recherche en Informatique, University of Montpellier; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","409","419","This paper introduces, as an optimistic concurrency control method, a new certification method by means of intervals of timestamps, usable in a distributed database system. The main advantage of this method is that it allows a chronological commit order which differs from the serialization one (thus avoiding rejections or delays of transactions which occur in usual certification methods or in classical locking or timestamping ones). The use of the dependency graph permits both classifying this method among existing ones and proving it. The certification protocol is first presented under the hypothesis that transactions' certifications are processed in the same order on all the concerned sites; it is then extended to allow concurrent certifications of transactions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702233","Certification;concurrency control;dependency graph;distributed databases;intervals of timestamps","Certification;Database systems;Concurrency control;Spatial databases;Transaction databases;Delay;Protocols;Distributed databases;Testing;System recovery","","Certification;concurrency control;dependency graph;distributed databases;intervals of timestamps","","22","26","","","","","","IEEE","IEEE Journals & Magazines"
"An executable model for a family of election algorithms","W. Shi; J. -. Corriveau","Sch. of Comput. Sci., Carleton Univ., Ottawa, Ont., Canada; NA","18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.","","2004","","","178","","Summary form only given. We present an executable model for a family of algorithms dealing with leader election in a ring topology. We follow the traditional approach of system family engineering. That is, we develop a feature model that captures variability across these algorithms. We then proceed to produce a generator. This generator receives as inputs specific values for each of the variation points (i.e., features) we identify. And it produces the behavior corresponding to the specific configuration of features at hand. Contrary to existing generative programming literature, we do not resort to C++ meta-programming but instead develop an executable model using Rational Rose RT. More precisely, we have designed a single state chart that can model all the algorithms of the family we studied. We focus here on how to obtain such a state chart, rather than on the identification of the features we used, or on ROSE-RT semantics. We do believe however that our approach can be reused to provide a semantically unified and executable modelling approach for other families of algorithms.","","0-7695-2132","10.1109/IPDPS.2004.1303186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303186","","Nominations and elections;Topology;Systems engineering and theory;Algorithm design and analysis;Testing;Computer science;Drives;Design optimization;Costs;Software engineering","topology;systems engineering;distributed algorithms","ring topology;system family engineering;generative programming;C++ meta-programming;state chart;ROSE-RT semantics;election algorithms","","","12","","","","","","IEEE","IEEE Conferences"
"Job shop scheduling with group-dependent setups, finite buffers, and long time horizon","P. B. Luh; Ling Gou; T. Odahara; M. Tsuji; K. Yoneda; T. Hasegawa; Y. Kyoya","Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Electr. & Syst. Eng., Connecticut Univ., Storrs, CT, USA; NA; NA; NA; NA; NA","Proceedings of 1995 34th IEEE Conference on Decision and Control","","1995","4","","4184","4189 vol.4","The design and implementation of a scheduling system for the manufacturing of Toshiba's gas insulated switchgears is presented. The manufacturing is characterized by significant machine setup times, strict local buffer capacities, the option of choosing a few exclusive-or routes for processing, and long horizon as compared to the time resolution required. Our goal is to obtain near optimal schedules with quantifiable quality in a computationally efficient manner. To achieve this goal, a novel integer optimization formulation with a separable structure is developed and a solution methodology based on a combined Lagrangian relaxation technique and heuristics is developed. The method has been implemented using the object-oriented programming language C++, and preliminary testing results show that effective schedules can be efficiently generated while considering the special features.","0191-2216","0-7803-2685","10.1109/CDC.1995.478801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=478801","","Job shop scheduling;Optimal scheduling;Gas insulation;Production;Manufacturing;Switchgear;Object oriented programming;Testing;Geographic Information Systems;Software systems","production control;computer aided production planning;object-oriented programming;optimisation;relaxation theory;gas insulated switchgear","Job shop scheduling;group-dependent setups;finite buffers;scheduling;Toshiba;gas insulated switchgear manufacture;machine setup times;integer optimization;Lagrangian relaxation;heuristics;object-oriented programming;production control","","2","15","","","","","","IEEE","IEEE Conferences"
"Efficient local search with search space smoothing: a case study of the traveling salesman problem (TSP)","Jun Gu; Xiaofei Huang","Dept. of Electr. & Comput. Eng., Calgary Univ., Alta., Canada; Dept. of Electr. & Comput. Eng., Calgary Univ., Alta., Canada","IEEE Transactions on Systems, Man, and Cybernetics","","1994","24","5","728","735","Local search is very efficient to solve combinatorial optimization problems. Due to the rugged terrain surface of the search space, it often gets stuck at a locally optimum configuration. In this paper, we give a local search method with a search space smoothing technique. It is capable of smoothing the rugged terrain surface of the search space. Any conventional heuristic search algorithm can be used in conjunction with this smoothing method. In a parameter space, by altering the shape of the objective function, the original problem instance is transformed into a series of gradually simplified problem instances with smoother terrain surfaces. Using an existing local search algorithm, an instance with the simplest terrain structure is solved first, the original problem instance with more complicated terrain structure is solved last, and the solutions of the simplified problem instances are used to guide the search of more complicated ones. A case study of using such technique to solve the traveling salesman problem (TSP) is described. We tested this method with numerous randomly generated TSP instances. We found that it has significantly improved the performance of existing heuristic search algorithms.<<ETX>>","0018-9472;2168-2909","","10.1109/21.293486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=293486","","Space exploration;Smoothing methods;Computer aided software engineering;Traveling salesman problems;Heuristic algorithms;Cities and towns;Search methods;Testing;Rough surfaces;Surface roughness","operations research;search problems;combinatorial mathematics;optimisation;heuristic programming","efficient local search;search space smoothing;traveling salesman problem;TSP;combinatorial optimization;rugged terrain;locally optimum configuration;heuristic search algorithm","","50","30","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal sensor configuration for complex systems with application to signal detection in structures","P. Sadegh; J. C. Spall","Dept. of Math. Modelling, Tech. Univ., Lyngby, Denmark; NA","Proceedings of the 17th IEEE Instrumentation and Measurement Technology Conference [Cat. No. 00CH37066]","","2000","1","","330","334 vol.1","The paper considers the problem of sensor configuration for complex systems. The contribution of the paper is twofold. Firstly, we define an appropriate criterion that is based on maximizing overall sensor responses while minimizing redundant information as measured by correlations between multiple sensor outputs. Secondly, we describe an efficient and practical algorithm to achieve the optimization goals, based on simultaneous perturbation stochastic approximation (SPSA). SPSA avoids the need for detailed modeling of the sensor response by simply relying on observed responses as obtained by limited experimentation with test sensor configurations. We illustrate the application of the approach to optimal placement of acoustic sensors for signal detection in structures. This includes both a computer simulation study for an aluminum plate, and real experimentations on a steel I-beam.","1091-5281","0-7803-5890","10.1109/IMTC.2000.846880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=846880","","Sensor systems and applications;Acoustic sensors;Sensor systems;Approximation algorithms;Stochastic processes;Acoustic testing;Application software;Signal detection;Computer simulation;Aluminum","intelligent sensors;intelligent structures;aluminium;steel;signal detection;digital simulation;optimisation","optimal sensor configuration;complex systems;signal detection;sensor responses;redundant information;correlations;practical algorithm;optimization;simultaneous perturbation stochastic approximation;detailed modeling;observed responses;optimal placement;acoustic sensors;computer simulation;Al plate;steel I-beam;Al","","","23","","","","","","IEEE","IEEE Conferences"
"Pattern discovery: a data driven approach to decision support","A. K. C. Wong; Yang Wang","Dept. of Syst. Design Eng., Univ. of Waterloo, Ont., Canada; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","","2003","33","1","114","124","Decision support nowadays is more and more targeted to large scale complicated systems and domains. The success of a decision support system relies mainly on its capability of processing large amounts of data and efficiently extracting useful knowledge from the data, especially knowledge which is previously unknown to the decision makers. With a large scale system, traditional knowledge acquisition models become inefficient and/or more biased, due to the subjectivity of the experts or the pre-assumptions of certain ideas or algorithmic procedures. Today, with the rapid development of computer technologies, the capability of collecting data has been greatly advanced. Data becomes the most valuable resource for an organization. We present a fundamental framework toward intelligent decision support by analyzing a large amount of mixed-mode data (data with a mixture of continuous and categorical values) in order to bridge the subjectivity and objectivity of a decision support process. By considering significant associations of artifacts (events) inherent in the data as patterns, we define patterns as statistically significant associations among feature values represented by joint events or hypercells in the feature space. We then present an algorithm which automatically discovers statistically significant hypercells (patterns) based on: 1) a residual analysis, which tests the significance of the deviation when the occurrence of a hypercell differs from its expectation, and 2) an optimization formulation to enable recursive discovery. By discovering patterns from data sets based on such an objective measure, the nature of the problem domain will be revealed. The patterns can then be applied to solve specific problems as being interpreted or inferred with.","1094-6977;1558-2442","","10.1109/TSMCC.2003.809869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1193066","","Data mining;Large-scale systems;Decision support systems;Pattern analysis;Artificial intelligence;Software systems;Knowledge acquisition;Bridges;Algorithm design and analysis;Automatic testing","pattern recognition;decision support systems;data mining;very large databases;data analysis;optimisation;probability","pattern discovery;data driven approach;decision support system;knowledge acquisition;organization;data analysis;data mining;residual analysis;statistically significant hypercells;optimization;data sets;probabilistic density functions;association rules;pattern based data query","","36","44","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic generation of fast timed simulation models for operating systems in SoC design","S. Yoo; G. Nicolescu; L. Gauthier; A. A. Jerraya","SLS Group, TIMA Lab., Grenoble, France; NA; NA; NA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","620","627","To enable fast and accurate evaluation of HW/SW implementation choices of on-chip communication, we present a method to automatically generate timed OS simulation models. The method generates the OS simulation models with the simulation environment as a virtual processor Since the generated OS simulation models use final OS code, the presented method can mitigate the OS code equivalence problem. The generated model also simulates different types of processor exceptions. This approach provides two orders of magnitude higher simulation speedup compared to the simulation using instruction set simulators for SW simulation.","1530-1591","0-7695-1471","10.1109/DATE.2002.998365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998365","","Operating systems;Communication networks;Switches;Network-on-a-chip;Design optimization;Switching circuits;Packet switching;Communication switching;Buildings;Network synthesis","instruction sets;circuit simulation;operating systems (computers);virtual machines;logic simulation;hardware-software codesign;application specific integrated circuits;integrated circuit design","automatic generation;fast timed simulation models;operating systems;SoC design;HW/SW implementation;simulation environment;virtual processor;final OS code;code equivalence problem;processor exceptions;simulation speedup;instruction set simulators","","29","23","","","","","","IEEE","IEEE Conferences"
"Benchmarking for graph transformation","G. Varro; A. Schurr; D. Varro","Dept. of Comput. Sci. & Inf. Theor., Budapest Univ. of Technol. & Econ., Hungary; NA; NA","2005 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC'05)","","2005","","","79","88","Model transformation (MT) is a key technology in the model-driven development approach of software engineering that provides automated means to capture the evolution of models and mappings between modeling languages. The pattern and rule-based paradigm of graph transformation is considered a very popular approach for specifying such model transformations. While the expressiveness of different MT specification techniques is frequently compared on well-known transformation problems (e.g. UML-to-XMI, or UML-to-EJB mappings), no such benchmarks exist currently for comparing the performance of different model transformation tools. In the paper, we propose a systematic method for quantitative benchmarking in order to assess the performance of graph transformation tools. Typical features of the graph transformation paradigm and various optimization strategies exploited in different toots are identified and categorized. Moreover, the performance of several popular graph transformation tools is measured and compared on a well-known distributed mutual exclusion problem.","1943-6092;1943-6106","0-7695-2443","10.1109/VLHCC.2005.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509491","","Software engineering;Real time systems;Data systems;Information systems;Computer architecture;Microwave integrated circuits;Modeling;Productivity;Automation;Civil engineering","graph grammars;formal specification;benchmark testing;specification languages","benchmarking;graph transformation;model transformation;software engineering;model evolution;modeling languages;pattern-based specification;rule-based specification;distributed mutual exclusion problem","","24","31","","","","","","IEEE","IEEE Conferences"
"CAD challenges for microsensors, microactuators, and microsystems","S. D. Senturia","Dept. of Electr. Eng. & Comput. Sci., MIT, Cambridge, MA, USA","Proceedings of the IEEE","","1998","86","8","1611","1626","In parallel with the development of new technologies, new device configurations, and new applications for microsensors, microactuators, and microsystems, also referred to as microelectromechanical devices and systems (MEMS), there has arisen a growing need for computer-aided engineering and design systems. There is a wide range of design problems: process simulation, solid-body geometric renderings from photomasks and process descriptions, energetically correct simulations of behavior across multiple coupled energy domains, extraction of lumped low-order models of device behavior, optimization of geometry and process sequence, and design of full systems that include MEMS devices. Because of the computational demands of the modeling required to support full computer-aided design (CAD), there is a premium on fast and memory-efficient algorithms that help the designer, both by automating, where possible, complex sets of related tasks and by providing rapid computational prototyping at critical points in the design cycle. This paper presents an overview of the present state of the art in CAD for MEMS, with particular emphasis on the role of macromodels and test structures as part of the design environment.","0018-9219;1558-2256","","10.1109/5.704266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=704266","","Microsensors;Microactuators;Design automation;Solid modeling;Microelectromechanical devices;Micromechanical devices;Process design;Computational modeling;Algorithm design and analysis;Application software","microactuators;microsensors;micromechanical devices;CAD","CAD;microsensor;microactuator;microsystem;microelectromechanical device;MEMS;process simulation;computer-aided design;algorithm;computational prototyping;macromodel;test structure;solid-body geometric rendering;photomask;lumped low-order model;multiple coupled energy domains;optimization","","116","","","","","","","IEEE","IEEE Journals & Magazines"
"On-line energy analysis in PV buildings connected to the utility grid","F. J. Argul; M. Castro; A. Delgado; F. Mur; R. Sebastian; J. Peire","Electr. & Comput. Eng. Dept., Ciudad Universitaria, Madrid, Spain; Electr. & Comput. Eng. Dept., Ciudad Universitaria, Madrid, Spain; Electr. & Comput. Eng. Dept., Ciudad Universitaria, Madrid, Spain; Electr. & Comput. Eng. Dept., Ciudad Universitaria, Madrid, Spain; Electr. & Comput. Eng. Dept., Ciudad Universitaria, Madrid, Spain; Electr. & Comput. Eng. Dept., Ciudad Universitaria, Madrid, Spain","IEEE 2002 28th Annual Conference of the Industrial Electronics Society. IECON 02","","2002","3","","2554","2559 vol.3","This paper presents a new software tool to estimate the energy flows in grid-connected photovoltaic (PV) buildings. The main aim of this simulator is to predict the energy balance and economic behaviour of PV buildings connected to the utility grid so that different alternatives can be tested to optimise designs. Consumption analysis is performed by considering six parameters that determine the building's response, following a methodology based on data from monitored buildings, an internal reference building and regression analysis. Photovoltaic energy estimates are obtained by calculating solar radiation hourly values and simulating individual PV cells. The resulting energy balance and the economic behaviour of the building are then estimated and the results given in tables, diagrams and several graphic formats. As an example, a university office building located in Madrid (Spain) is presented and the conclusions reached during several simulation runs are discussed. To make the tool widely available to any potential users it has been programmed in a Java applet format and deployed on a World Wide Web server. By simply using an up-to-date WWW browser, the simulator can be concurrently run by multiple users on the Internet.","","0-7803-7474","10.1109/IECON.2002.1185376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185376","","Buildings;Photovoltaic systems;Solar power generation;Power generation economics;Economic forecasting;Software tools;Predictive models;Testing;Design optimization;Performance analysis","building integrated photovoltaics;load flow;statistical analysis;Java;Internet;power engineering computing","on-line energy analysis;PV buildings;utility grid;software tool;energy flows estimation;grid-connected photovoltaic buildings;simulator;energy balance prediction;economic behaviour prediction;monitored buildings;internal reference building;regression analysis;solar radiation hourly values;individual PV cells;university office building;Madrid;Spain;Java applet format;World Wide Web server;Internet;WWW browser;energy balance;economic behaviour;graphic formats","","","10","","","","","","IEEE","IEEE Conferences"
"Composite device model enhances worst case simulation of bipolar analog ASICs","D. Bray","Valid Logic Syst. Inc., San Jose, CA, USA","Proceedings., Second Annual IEEE ASIC Seminar and Exhibit,","","1989","","","P13","3/1","It is shown that practical simulations of worst-case performance of analog ASIC (application-specific integrated circuit) designs are enhanced by the use of a composite device model containing low, typical, and high sets of parameters. The composite model allows process related parameters, mismatch effects, and correlation between parameters to be included in single sets of simulations using sensitivity and Monte Carlo techniques. Results of these simulations provide the designer with design optimization data, sensitivity, test limits, and yield data.<<ETX>></ETX>","","","10.1109/ASIC.1989.123244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=123244","","Computer aided software engineering;Application specific integrated circuits;Integrated circuit modeling;Circuit simulation;Operational amplifiers;Buffer storage;Temperature;Monte Carlo methods;Circuit analysis;Voltage","application specific integrated circuits;bipolar integrated circuits;circuit analysis computing;linear integrated circuits;Monte Carlo methods;semiconductor device models;sensitivity analysis","computer aided analysis;worst case simulation;bipolar analog ASICs;application-specific integrated circuit;composite device model;process related parameters;mismatch effects;sensitivity;Monte Carlo techniques;design optimization data;test limits;yield data","","","4","","","","","","IEEE","IEEE Conferences"
"A digital-signal-processor-based measurement system for on-line fault detection","A. Baccigalupi; A. Bernieri; A. Pietrosanto","Dipt. di Inf. e Sistemistica, Naples Univ., Italy; NA; NA","IEEE Transactions on Instrumentation and Measurement","","1997","46","3","731","736","This paper deals with the design, construction, and setting up of a measurement apparatus, based on an architecture using two parallel digital signal processors (DSP's), for on-line fault detection in electric and electronic devices. In the proposed architecture, the first DSP monitors a device output on-line in order to detect faults, whereas the second DSP estimates and updates the system-model parameters in real-time in order to track their eventual drifts. The problems which arose when the proposed apparatus was applied to a single-phase inverter are discussed, and some of the experimental results obtained in fault and nonfault conditions are reported.","0018-9456;1557-9662","","10.1109/19.585442","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=585442","","Fault detection;Electrical fault detection;Digital signal processing;Master-slave;Digital signal processors;Electric variables measurement;Computer architecture;Bandwidth;Signal sampling;Signal design","fault diagnosis;signal processing;VLSI;parallel processing;real-time systems;automatic test equipment;parameter estimation","digital-signal-processor;on-line fault detection;design;construction;measurement apparatus;architecture;parallel digital signal processors;debugging system;DSP;fault location;system-model parameters;real-time;drifts;single-phase inverter;nonfault conditions;parameter estimation;fault detection;software optimisation","","20","15","","","","","","IEEE","IEEE Journals & Magazines"
"Streaming video with optimized reconstruction-based DCT","Xiao Su; B. W. Wah","Dept. of Electr. & Comput. Eng., Illinois Univ., Urbana, IL, USA; NA","2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)","","2000","1","","271","274 vol.1","One fundamental problem with streaming video data over unreliable IP networks is that packets may be dropped or arrive too late for real-time playback. Traditional error-control schemes are not attractive because they either add redundant information that may worsen network traffic, or rely solely on decoders with inadequate error concealment. This paper presents a joint sender-receiver approach in designing transforms for multiple-description coding in order to conceal network losses in streaming real-time video over the Internet. On the receiver side, we adopt a simple interpolation-based reconstruction, as sophisticated concealment techniques cannot be employed in software-based real-time playback. On the sender side, we design an optimized reconstruction-based discrete cosine transform (ORB-DCT) with the objective of minimizing the mean squared error, assuming that some of the descriptions are lost and that the missing information is reconstructed by simple averaging at the destination. Experimental results show that our proposed ORB-DCT performs better than the original DCT in real Internet tests. Future research includes finding a perceptual-based quantization matrix based on extended basis images derived for reconstruction, and incorporating the effects of quantization and inverse quantization in the design.","","0-7803-6536","10.1109/ICME.2000.869594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869594","","Streaming media;Discrete cosine transforms;Image reconstruction;Quantization;IP networks;Computer errors;Telecommunication traffic;Decoding;Discrete transforms;Design optimization","Internet;Discrete cosine transforms;Video coding;Quantization (signal);Interpolation;Image reconstruction","video data streaming;unreliable IP networks;network traffic;sender-receiver approach;multiple-description coding;network losses;Internet;interpolation-based reconstruction;optimized reconstruction-based discrete cosine transform;minimized mean squared error;perceptual-based quantization matrix;extended basis images;inverse quantization","","1","7","","","","","","IEEE","IEEE Conferences"
"Correct performance of transaction capabilities","T. Arts; I. van Langevelde","Comput. Sci. Lab., Ericsson, Alvsjo, Sweden; NA","Proceedings Second International Conference on Application of Concurrency to System Design","","2001","","","35","42","The correctness of an optimisation of the Transport Capabilities Application Part of the Signalling System No. 7 is formalised as a branching bisimulation which is relaxed to allow certain actions to be executed in any order. It is demonstrated how this correctness can be checked by a combination of an automated test of branching bisimulation and a manual test of commutation. Using this approach, two bugs in the design were found and eliminated.","","0-7695-1071","10.1109/CSD.2001.981762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981762","","Design optimization;Protocols;Automatic testing;Computer bugs;Signal design;Automata;Computer science;Laboratories;Application software;Merging","concurrency theory;parallel programming;bisimulation equivalence;finite state machines","branching bisimulation;automated test;multi-processor architectures;equivalence;TCAP;finite state machines;equivalence relation;process algebras;concurrent programming","","2","15","","","","","","IEEE","IEEE Conferences"
"Value-conscious cache: simple technique for reducing cache access power","Yen-Jen Chang; Chia-Lin Yang; Feipei Lai","Dept. of Comput. Sci., Nat. ChungHsing Univ., Taichung, Taiwan; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","16","21 Vol.1","Most microprocessors employ the on-chip caches to bridge the performance gap between the processor and main memory. However, the cache accesses usually contribute significantly to the total power consumption of the chip. Based on the observation that an overwhelming majority of the cache access bits are '0', in this paper we propose a value-conscious (VC) cache to reduce the average cache power consumption during an access. Unlike the conventional cache with differential-bitline implementation, the VC cache is a single-bitline design. Depending on the access bit value, the VC cache can dynamically prevent the bitline from being discharged such that the power dissipated in accessing '0' is much less than the power dissipated in accessing '1'. The implementation of the VC cache is a circuit-level technique, which is software independent and orthogonal to other low power techniques at architecture-level. The experimental results based on the SPEC2000 and MediaBench traces show that without compromise of both performance and stability, by exploiting the prevalence of '0' bits in access data the VC cache can reduce the average cache read and write power by about 18%/spl sim/22% and 36%/spl sim/40%, respectively.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268821","","Virtual colonoscopy;Energy consumption;Microprocessors;Writing;Bridge circuits;Circuit stability;Pressing;Batteries;Random access memory;Capacitance","microprocessor chips;cache storage;memory architecture;circuit optimisation;low-power electronics","value-conscious cache;cache access power reduction;VC;average cache power consumption;single-bitline design;access bit value;circuit-level technique;software independent;low power techniques;architecture-level;SPEC2000;MediaBench;access data","","1","11","","","","","","IEEE","IEEE Conferences"
"The efficient implementation of wait statements","I. Stahl","Stockholm Sch. of Econ., Sweden","1998 Winter Simulation Conference. Proceedings (Cat. No.98CH36274)","","1998","1","","523","530 vol.1","This paper gives an overview of some of the factors that determine the efficiency of differently implemented wait statements. The purpose is to give some guidance to simulation modelers as to what system to choose and, for the chosen system, what wait constructs to use in order to make program execution efficient. It is also aimed at informing constructors of simulation software about what issues are important when implementing these statements. In particular, it presents some brand new features that can speed up the execution of wait statements, in particular in some versions of GPSS.","","0-7803-5133","10.1109/WSC.1998.745030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=745030","","Programming profession;Marine vehicles;Testing;Discrete event simulation;Delay;Storms;Clocks;Debugging;Program processors;Optimizing compilers","discrete event simulation;software performance evaluation","wait statements;discrete event simulation;system selection;program execution;simulation software;GPSS","","1","14","","","","","","IEEE","IEEE Conferences"
"Application-compliant networking on embedded systems","S. Beyer; K. Mayes; B. Warboys","Dept. of Comput. Sci., Univ. of Manchester, UK; Dept. of Comput. Sci., Univ. of Manchester, UK; Dept. of Comput. Sci., Univ. of Manchester, UK","Proceedings 3rd IEEE International Workshop on System-on-Chip for Real-Time Applications","","2002","","","53","58","Network protocol stacks are traditionally encapsulated within system software, forcing the application programmer to use general-purpose communication end-point abstractions. The application programmer is denied the flexibility of implementing application-specific performance improvements. Application-level networking provides the application programmer with the ability to tailor the protocol stack to the needs of the application. This is particularly useful in special-purpose systems, such as embedded networked appliances. We describe the design of an application-compliant TCP/IP implementation for the Arena runtime library operating system, which aims at separating mechanism from policy. The role of policy and mechanism in network protocols and their effects on networked embedded systems is investigated. The resulting system is optimised for embedded systems based on a multithreaded single-application model. Experiments were carried out on an embedded system test platform and performance results are given.","","0-7803-7686","10.1109/IWNA.2002.1241336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241336","","Embedded system;Application software;Protocols;Programming profession;System software;Home appliances;TCPIP;Runtime library;Operating systems;System testing","embedded systems;transport protocols;multi-threading;operating systems (computers)","network protocol stack;system software;TCP/IP;library operating system;embedded system;multithreaded single-application model;application-compliant networking","","1","27","","","","","","IEEE","IEEE Conferences"
"Feedback control scheduling in distributed real-time systems","J. A. Stankovic; Tian He; T. Abdelzaher; M. Marley; Gang Tao; Sang Son; Chenyang Lu","Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA","Proceedings 22nd IEEE Real-Time Systems Symposium (RTSS 2001) (Cat. No.01PR1420)","","2001","","","59","70","Distributed soft real-time systems are becoming increasingly unpredictable due to several important factors such as the increasing use of commercial-off-the-shelf components, the trend towards open systems, and the proliferation of data-driven applications whose execution parameters vary significantly with input data. Such systems are less amenable to traditional worst-case real-time analysis. Instead, system-wide feedback control is needed to meet performance requirements. In this paper, we extend our previous work on developing software control algorithms based on a theory of feedback control to distributed systems. Our approach makes three important contributions. First, it allows the designer for a distributed real-time application to specify the desired temporal behavior of system adaptation, such as the speed of convergence to desired performance upon load or resource changes. This is in contrast to specifying only steady-state metrics, e.g., deadline miss ratio. Second, unlike QoS optimization approaches, our solution meets performance guarantees without accurate knowledge of task execution parameters-a key advantage in an unpredictable environment. Third, in contrast to ad hoc algorithms based on intuition and testing, our solution has a basis in the theory and practice of feedback control scheduling. Performance evaluation reveals that the solution not only has excellent steady state behavior, but also meets stability, overshoot, and settling time requirements. We also show that the solution outperforms several other algorithms available in the literature.","","0-7695-1420","10.1109/REAL.2001.990596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990596","","Feedback control;Real time systems;Application software;Steady-state;Open systems;Control systems;Software algorithms;Scheduling algorithm;Testing;Stability","real-time systems;scheduling;distributed processing;feedback","real-time systems;software control algorithms;feedback control;distributed systems;distributed real-time application;soft real-time systems","","44","14","","","","","","IEEE","IEEE Conferences"
"Displacement structure approach to singular root distribution problems: the imaginary axis case","D. Pal; T. Kailath","Inf. Syst. Res. Lab., AT&T Bell Labs., Holmdel, NJ, USA; NA","IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications","","1994","41","2","138","148","A general theory of tabular form root distribution procedures based on LDL* factorization of Bezoutians has been presented in this paper. In particular, we have concentrated on the singular cases arising in the Routh test. A one-to-one correspondence has been established between the rank profile of the underlying Bezoutians and the occurrence of the singular cases. Combining this interpretation with the newly developed factorization procedures of Pal and Kailath (1989), it has been possible to extend the new unified approach of Lev-Ari, Bistritz, and Kailath (1991) to the singular cases. By doing so not only did we obtain all the known procedures but we also obtained new results.<<ETX>>","1057-7122;1558-1268","","10.1109/81.269050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=269050","","Computer aided software engineering;Testing;Polynomials;Management information systems;Laboratories;Stability;Linear systems;Costs;Reflection","polynomials;linear systems;matrix algebra;stability","singular root distribution problems;imaginary axis case;displacement structure approach;tabular form root distribution procedures;LDL* factorization of Bezoutians;Routh test;rank profile;factorization procedures;polynomials;singularities;linear system stability;quasi Hankel form","","6","31","","","","","","IEEE","IEEE Journals & Magazines"
"Displacement structure approach to singular root distribution problems: the unit circle case","D. Pal; T. Kailath","Commun. Sci. Res. Div., AT&T Bell Labs., Holmdel, NJ, USA; NA","IEEE Transactions on Automatic Control","","1994","39","1","238","245","A general theory of tabular form root distribution procedures based on LDL* factorization of Bezoutians is presented. In particular, the authors concentrate on the singular cases arising in the Schur-Cohn test. A one-to-one correspondence is established between the rank profile of the underlying Bezoutians and the occurrence of the singular cases. Combining this interpretation with the newly developed factorization procedures of Pal and Kailath, it is possible to extend the new unified approach of Lev-Ari, Bistritz, and Kailath(1991) to the singular cases. By doing so, not only do the authors derive the well known Schur-Cohn procedure, but they also obtain new results. In fact, the authors are able to derive a new completely recursive procedure to deal with the ""first kind of singularity"".<<ETX>>","0018-9286;1558-2523;2334-3303","","10.1109/9.273377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=273377","","Computer aided software engineering;Testing;Polynomials;Stability;Linear systems;Reflection;History;Circuits and systems;Management information systems;Laboratories","polynomials;matrix algebra","displacement structure approach;singular root distribution problems;unit circle;LDL* factorization;Bezoutians;Schur-Cohn test;rank profile;recursive procedure;singularity","","6","35","","","","","","IEEE","IEEE Journals & Magazines"
"Implementation of network flow programming to the hydrothermal coordination in an energy management system","C. -. Li; P. J. Jap; D. L. Streiffert","ESCA Corp., Bellevue, WA, USA; ESCA Corp., Bellevue, WA, USA; ESCA Corp., Bellevue, WA, USA","IEEE Transactions on Power Systems","","1993","8","3","1045","1053","Hydrothermal coordination (HTC), consisting of hydro optimization and thermal unit commitment, is a major function in a power system for allocating its generating resources to achieve the system's maximum economy. The optimality conditions of an incremental network flow programming (INFP) approach are described. The implementation of INFP in an energy management system (EMS) and its interface with the existing unit commitment (UC) software are presented. Some new features are described in detail. The combined HTC and UC package has been delivered to a power utility, Tenaga National Berhad (TNB) in West Malaysia. Internal tests and factory acceptance tests have shown that NFP with a modified Superkilter algorithm is a powerful tool for hydro network flow optimization.<<ETX>>","0885-8950;1558-0679","","10.1109/59.260895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=260895","","Intelligent networks;Energy management;Rivers;Reservoirs;Thermal loading;Economic forecasting;Power generation economics;Transportation;Packaging;Testing","hydroelectric power stations;hydrothermal power systems;load flow;load management;power system analysis computing;thermal power stations","generating resources allocation;network flow programming;hydrothermal coordination;energy management system;hydro optimization;thermal unit commitment;EMS;unit commitment;software;power utility;Tenaga National Berhad;West Malaysia;factory acceptance tests;modified Superkilter algorithm","","42","9","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient computation of dominators in multiple-output circuit graphs","R. Krenz","R. Inst. of Technol., IMIT-KTH, Stockholm, Sweden","2005 IEEE International Symposium on Circuits and Systems","","2005","","","2223","2226 Vol. 3","We present an efficient technique for computing dominators in multiple-output circuit graphs. Dominators provide information about the origin and the end of reconverging paths in a graph. This information is widely used in CAD applications such as satisfiability checking, equivalence checking, ATPG, technology mapping, decomposition of Boolean functions and power optimization. Experiments on a large set of benchmarks show a significant performance improvement of our new technique in comparison to the well-known algorithm, presented by T. Lengauer and R.E. Tarjan (1979), for computing dominators in flowgraphs. We demonstrate that, in contrast to previous techniques, our algorithm obtains performance improvements especially for large benchmarks.","0271-4302;2158-1525","0-7803-8834","10.1109/ISCAS.2005.1465064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465064","","Circuits;Automatic test pattern generation;Space technology;Boolean functions;Design automation;Electronic design automation and methodology;Application software;Information analysis;Motion analysis;Joining processes","flow graphs;circuit layout CAD","efficient dominator computation;multiple-output circuit graphs;reconverging paths;CAD applications;satisfiability checking;equivalence checking;technology mapping;Boolean function decomposition;power optimization;benchmarks;flowgraphs;electronic design automation algorithms","","","15","","","","","","IEEE","IEEE Conferences"
"Impact of data transformations on memory bank locality","M. Kandemir","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., USA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","506","511 Vol.1","High-energy consumption presents a problem for sustainable clock frequency and deliverable performance. In particular, memory energy consumption of array-intensive applications can be overwhelming due to poor cache locality. One option for reducing memory energy is to adopt a banked memory architecture, where memory space is divided into banks and each bank can be powered down if it is not in active use. An important issue here is the bank access pattern, which determines opportunities for saving energy. In this paper, we present a compiler-based data layout transformation strategy for increasing the effectiveness of a banked memory architecture. The idea is to transform the array layouts in memory in such a way that two loop iterations executed one after another access the data in the same bank as much as possible; the remaining banks can be placed into a low-power mode. Our simulation-based experiments with nine array-intensive applications show significant savings in memory energy consumption.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268896","","Energy consumption;Costs;Memory architecture;SDRAM;Computer science;Power engineering and energy;Clocks;Frequency;Application software;Banking","memory architecture;integrated circuit design;circuit optimisation;low-power electronics","data transformations;memory bank locality;clock frequency;clock deliverable performance;memory energy consumption;array-intensive applications;cache locality;banked memory architecture;memory space;bank access pattern;data layout transformation strategy;array layouts;loop iterations;low-power mode;memory banking;operating modes","","7","17","","","","","","IEEE","IEEE Conferences"
"Time-energy design space exploration for multi-layer memory architectures","R. Szymanek; F. Catthoor; K. Kuchcinski","Dept. of Comput. Sci., Lund Univ., Sweden; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","1","","318","323 Vol.1","This paper presents an exploration algorithm which examines execution time and energy consumption of a given application, while considering a parameterized memory architecture. The input to our algorithm is an application given as an annotated task graph and a specification of a multi-layer memory architecture. The algorithm produces Pareto trade-off points representing different multi-objective execution options for the whole application. Different metrics are used to estimate parameters for application-level Pareto points obtained by merging all Pareto diagrams of the tasks composing the application. We estimate application execution time although the final scheduling is not yet known. The algorithm makes it possible to trade off the quality of the results and its runtime depending on the used metrics and the number of levels in the hierarchical composition of the tasks' Pareto points. We have evaluated our algorithm on a medical image processing application and randomly generated task graphs. We have shown that our algorithm can explore huge design space and obtain (near) optimal results in terms of Pareto diagram quality.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268867","","Space exploration;Memory architecture;Bandwidth;Scheduling;Memory management;Computer science;Energy consumption;Application software;Algorithm design and analysis;Computer architecture","memory architecture;integrated circuit design;Pareto optimisation","time-energy design;space exploration;multilayer memory architectures;exploration algorithm;execution time;energy consumption;parameterized memory architecture;algorithm;task graph;Pareto trade-off points;multi-objective execution options;Pareto diagrams;scheduling;hierarchical composition;medical image processing application","","12","16","","","","","","IEEE","IEEE Conferences"
"MAP-microcomputer applications project","L. P. Huelsman","Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA","IEEE Circuits and Devices Magazine","","1989","5","1","40","42","The formation and operation of MAP, a microcomputer applications project designed to involve undergraduate engineering students in microcomputers, are described. The project was set up to accommodate students with a wide range of backgrounds, starting at one end of the scale with those with no microcomputer experience at all, and at the other end of the scale, including students with their own equipment and with several years of experience using it. The project has resulted in the generation of a wide range of powerful and useful computational tools suitable for use in a microcomputer-based engineering workstation environment. The programs have included analysis optimization, and sensitivity determination, as well as several programs in the area of filter design and approximation.<<ETX>>","8755-3996;1558-1888","","10.1109/101.17238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=17238","","Microcomputers;Educational institutions;Engineering students;Application software;Testing;Employment;Layout;Costs;Knowledge engineering;Job shop scheduling","circuit CAD;educational aids;engineering workstations;microcomputer applications","MAP;MAP;microcomputer applications project;undergraduate engineering students;students;computational tools;engineering workstation environment;analysis optimization;sensitivity;filter design","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Improved implementation of four-standard procedure for calibrating six-port reflectometers","L. Qiao; S. P. Yeo","Dept. of Electr. Eng., Nat. Univ. of Singapore, Singapore; Dept. of Electr. Eng., Nat. Univ. of Singapore, Singapore","IEEE Transactions on Instrumentation and Measurement","","1995","44","3","632","636","The present paper describes an improved version of a four-standard calibration procedure for six-port reflectometers that utilizes simple iteration computations instead of the other more cumbersome optimization techniques attempted previously. The four standards required are a matched load (with a return loss not more than -40 dB) and three highly reflecting terminations (each with a return loss not less than -2 dB).<<ETX>>","0018-9456;1557-9662","","10.1109/19.387297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=387297","","Calibration;Instruments;Nonlinear equations;Measurement standards;Reflection;Hardware;Detectors;Software standards;Particle measurements;Testing","calibration;reflectometers;iterative methods;measurement standards;microwave reflectometry","four-standard procedure;six-port reflectometers;calibration;iteration;optimization;matched load;return loss;reflecting terminations;-40 dB;-2 dB","","14","9","","","","","","IEEE","IEEE Journals & Magazines"
"A power reduction technique with object code merging for application specific embedded processors","T. Ishihara; H. Yasuura","Dept. of Comput. Sci. & Commun. Eng., Kyushu Inst. of Technol., Fukuoka, Japan; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","617","623","In this paper, a power reduction technique which merges frequently executed sequences of object codes into a set of single instructions is proposed. The merged sequence of object codes is restored by an instruction decompressor before decoding the object codes. The decompressor is implemented by a ROM. In many programs, only a few sequences of object codes are frequently executed. Therefore, merging these frequently executed sequences into a single instruction leads to a significant energy reduction. Our experiments with actual read only memory (ROM) modules and some benchmark program demonstrate significant energy reductions up to more than 65% at best case over an instruction memory without the object code merging.","","0-7695-0537","10.1109/DATE.2000.840849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840849","","Merging;Read only memory;Energy consumption;Application software;System-on-a-chip;Computer science;Power engineering and energy;Information science;Image restoration;Decoding","low-power electronics;embedded systems;application specific integrated circuits;cache storage;circuit optimisation;integer programming;microprocessor chips","power reduction technique;object code merging;application specific embedded processors;frequently executed sequences;instruction decompressor;energy reductions;instruction memory","","6","10","","","","","","IEEE","IEEE Conferences"
"Enhancing optimal transmission or subtransmission planning by using decision trees","J. Peco; E. F. Sanchez-Ubeda; T. Gomez","Inst. de Investigacion Tecnologica, Univ. Pontificia Comillas, Madrid, Spain; NA; NA","PowerTech Budapest 99. Abstract Records. (Cat. No.99EX376)","","1999","","","175","","Due to the large size of electric power systems, there is a very high computational burden when obtaining the optimum network by using classical optimization techniques. Several authors have used heuristics and/or sensitivities in order to guide the search of optimal network investments. This paper proposes an automatic learning approach in order to decide whether a network change will improve the overall costs or not. More specifically, decision trees methods are used to identify a set of simple and reliable rules which combine criteria based on both heuristics and sensitivities. These decision trees are integrated in a subtransmission planning tool, improving dramatically both the ""optimality"" of the resultant network and the computational time.","","0-7803-5836","10.1109/PTC.1999.826607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=826607","","Decision trees;Computer networks;Space exploration;Power system planning;Investments;Genetic algorithms;Testing;Computer aided software engineering;Cost function;Propagation losses","power transmission planning;decision trees","optimal transmission planning enhancement;optimal subtransmission planning enhancement;classical optimization techniques;heuristics;optimal network investments;automatic learning approach;subtransmission planning tool;genetic algorithms;planning rules","","2","3","","","","","","IEEE","IEEE Conferences"
"Genetic algorithm-based combinatorial parametric optimization for the calibration of microscopic traffic simulation models","T. Ma; B. Abdulhai","Dept. of Civil Eng., Toronto Univ., Downsview, Ont., Canada; NA","ITSC 2001. 2001 IEEE Intelligent Transportation Systems. Proceedings (Cat. No.01TH8585)","","2001","","","848","853","We introduce GENOSIM, genetic optimizer for traffic micro-simulation models. GENOSIM is developed as a pilot software, employing state of the art combinatorial parametric optimization to automate the tedious task of calibrating traffic microscopic simulation models. The employed global search technique, genetic algorithms, is integrated with a dynamic traffic microscopic simulation model for the City of Toronto, Canada using Paramics microsimulation suite. The output of GENOSIM is the near-optimal values of its car-following, lane changing and dynamic routing parameters. The results obtained are very encouraging.","","0-7803-7194","10.1109/ITSC.2001.948771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948771","","Calibration;Microscopy;Traffic control;Intelligent transportation systems;Genetic algorithms;Telecommunication traffic;Predictive models;Search methods;System testing;Civil engineering","road traffic;traffic engineering computing;digital simulation;genetic algorithms;calibration","Combinatorial Parametric Optimization;Calibration;microscopic traffic simulation;road traffic;simulation models;GENOSIM;Paramics;car-following;lane changing;dynamic routing;genetic algorithms","","6","13","","","","","","IEEE","IEEE Conferences"
"Face recognition for images from the same unknown person","Yea-Shuan Huang; Yao-Hong Tsai; Hong-Hsin Chao; Yi-Tsung Chien","Adv. Technol. Center, Ind. Technol. Res. Inst., Hsinchu, Taiwan; Adv. Technol. Center, Ind. Technol. Res. Inst., Hsinchu, Taiwan; Adv. Technol. Center, Ind. Technol. Res. Inst., Hsinchu, Taiwan; Adv. Technol. Center, Ind. Technol. Res. Inst., Hsinchu, Taiwan","IEEE 37th Annual 2003 International Carnahan Conference onSecurity Technology, 2003. Proceedings.","","2003","","","527","530","This paper mainly introduces (1) a face recognition method by using a newly designed radial basis function (RBF) neural net which can iteratively reduce a purposely defined classification-oriented error function, and (2) a decision-making mechanism by accumulating multiple individual face recognition results of the same unknown targeted person. To experiment on 50 persons (each person has 32 training samples and 100 testing samples), although the recognition rate of individual sample is only 86.5%, a perfect recognition accuracy (i.e. 100% accuracy) has been achieved by accumulating 20 temporal face images. This shows that the proposed approaches can produce various degrees of security to support different face recognition applications.","","0-7803-7882","10.1109/CCST.2003.1297614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1297614","","Face recognition;Face detection;Image recognition;Radial basis function networks;Maximum likelihood detection;Neurons;Testing;Application software;Target tracking;Target recognition","face recognition;radial basis function networks;target tracking;learning (artificial intelligence);image classification;biometrics (access control)","face recognition;radial-basis function;optimization;target tracking;classification-oriented error function;decision-making mechanism;temporal face images;accumulation mechanism","","1","7","","","","","","IEEE","IEEE Conferences"
"Development of an ultra-flat SAW filter module and its application to FASS: a frequency agile signal source","T. L. Bagwell; C. A. Johnsen; R. C. Bray; S. Carp","Hewlett Packard, Santa Rosa, CA, USA; Hewlett Packard, Santa Rosa, CA, USA; Hewlett Packard, Santa Rosa, CA, USA; NA","Proceedings., IEEE Ultrasonics Symposium,","","1989","","","97","102 vol.1","The development of a fast, agile signal in the frequency range of 10 MHz to 3 GHz based on direct digital synthesis of signals from 14 to 58 MHz is described. A high-performance SAW (surface acoustic wave) filter module was designed to provide IF (intermediate frequency) filtering after the baseband signal was converted to a center frequency of 304 MHz. The 44-MHz-wide filter module showed typical passband ripple less than 0.4 dB peak-to-peak. Triple transit, electromagnetic feedthrough, and other spurious responses were rejected by at least 50 dB. The circuit, the SAW filter, and its package were designed with careful attention to parasitics in order to achieve this performance. The filtering was accomplished by two identical SAW filters on 128 degrees Y-rotated, X LiNbO/sub 3/ operated in cascade, with isolation amplifiers buffering each filter. The isolation amplifiers were designed to provide a low-impedance, balanced termination to each IDT (interdigital transducer), minimizing triple transit and EM feedthrough. The SAW filter was designed using the Remez exchange algorithm and optimized to compensate for and reduce second-order effects. An absorbing layer of polyimide was an important factor in assuring repeatably low spurious time-domain responses from the chip edges, while being compatible with 300 degrees C package sealing. An on-wafer test hardware and software system was developed to predict packaged-device ripple performance and a precision test fixture with two-port calibration standards was developed to test feedthrough and spurious time-domain responses in the same environment as the final circuit.<<ETX>>","","","10.1109/ULTSYM.1989.66965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=66965","","SAW filters;Circuit testing;Packaging;Software testing;System testing;Surface acoustic waves;Frequency conversion;Filtering;Time domain analysis;Frequency synthesizers","acoustic generators;acoustic signal processing;signal synthesis;surface acoustic wave filters;ultrasonic devices","intermediate frequency filtering;ultra-flat SAW filter module;frequency agile signal source;direct digital synthesis;isolation amplifiers;interdigital transducer;triple transit;EM feedthrough;Remez exchange algorithm;second-order effects;polyimide;ripple;time-domain responses;14 to 58 MHz;10 MHz to 3 GHz;0.4 dB;304 MHz;LiNbO/sub 3/","","","4","","","","","","IEEE","IEEE Conferences"
"A Stochastic Neural Model for Graph Problems: Software and Hardware Implementation","G. Grossi; F. Pedersini","Dipartimento di Scienze dell'Informazione, Universita degli Studi di Milano, Via Comelico 39, 20135 Milano, Italy. E-mail: grossi@dsi.unimi.it; NA","2005 International Conference on Neural Networks and Brain","","2005","1","","115","120","This article describes a novel neural stochastic model for solving graph problems. The neural system has been tested on random graphs, showing better performance than other well-known heuristics for the same problems. Furthermore, a simplified version of the proposed model has been developed in such a way that it can be easily implemented in hardware using programmable logic chips, such as FPGAs","","0-7803-9422","10.1109/ICNNB.2005.1614579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1614579","","Stochastic processes;Hardware;Field programmable gate arrays;Integrated circuit synthesis;Network synthesis;Constraint optimization;Cost function;Electronic mail;System testing;Programmable logic arrays","field programmable gate arrays;graph theory;Hopfield neural nets;stochastic processes","stochastic neural model;graph problems;random graphs;programmable logic chips;FPGA","","","12","","","","","","IEEE","IEEE Conferences"
"Network simulations with OPNET","Xinjie Chang","Network Technol. Res. Center, Nanyang Technol. Univ., Singapore","WSC'99. 1999 Winter Simulation Conference Proceedings. 'Simulation - A Bridge to the Future' (Cat. No.99CH37038)","","1999","1","","307","314 vol.1","Several computer network simulators are compared. One of the most powerful simulation software packages - OPNET (OPtimized Network Engineering Tool) s introduced in detail. The implementation details of the network models in OPNET are given. Some simulation examples are also illustrated.","","0-7803-5780","10.1109/WSC.1999.823089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=823089","","Computational modeling;Object oriented modeling;Protocols;Analytical models;Computer simulation;Packaging;Discrete event simulation;Testing;NIST;Circuit simulation","digital simulation;computer networks;software packages;telecommunication computing","computer network simulators;simulation software package;OPNET;Optimized Network Engineering Tool;network model implementation","","4","13","","","","","","IEEE","IEEE Conferences"
"Advanced analysis of dynamic neural control advisories for process optimization and parts maintenance","J. P. Card; W. T. Chan; A. Cao; W. Martin; J. Morgan","IBEX Process Technol. Inc., Lowell, MA, USA; IBEX Process Technol. Inc., Lowell, MA, USA; IBEX Process Technol. Inc., Lowell, MA, USA; IBEX Process Technol. Inc., Lowell, MA, USA; NA","Advanced Semiconductor Manufacturing Conference and Workshop, 2003 IEEEI/SEMI","","2003","","","315","321","This paper details an advanced set of analyses designed to drive specific process variable setpoint adjustments or maintenance actions required for cost effective process control using the Dynamic Neural Controller/spl trade/ (DNC) wafer-to-wafer advisories for semiconductor manufacturing advanced process control. The new analytic displays and metrics are illustrated using data obtained on a LAM 4520XL at STMicroelectronics as part of a SEMATECH SPIT beta test evaluation. The DNC represents a comprehensive modeling environment that uses as its input extensive process chamber information and history of the time since maintenance actions occurred. The DNC uses a neural network to predict multiple quality output metrics and a closed-loop risk-based optimization to maximize process quality performance while minimizing overall cost of tool operation and machine downtime. The software responds in an advisory mode on a wafer-to-wafer basis as to the optimal actions to be taken. In this paper, we present three specific instances of patterns arising during wafer processing over time that signal the process or equipment engineer to the need for corrective action: either a process setpoint adjustment or specific maintenance actions. Based on the controller's recommended corrective action set with the overall risk reduction predicted by such actions, a metric of corrective action ""urgency"" can be created. The tracking of this metric over time yields different pattern types that signify a quantified need for a specific type of corrective action. Three basic urgency patterns are found: 1. a pattern in a given maintenance action over time showing increasing urgency or ""risk reduction"" capability for the action; 2. a pattern in a process variable specific to a given recipe indicating a chronic request over time to only adjust the variable setpoint either above or below the current target; 3. a pattern in a process variable existing over all recipes processed through the chamber indicating chronic request to adjust the variable setpoint in either or both directions over time. This pattern is a pointer to the need for a maintenance action that is either corroborated by the urgency graph for that maintenance action, or if no such action has been previously taken, a guide to the source of the equipment malfunction.","1078-8743","0-7803-7673-00-7803-7681","10.1109/ASMC.2003.1194514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194514","","Process control;Risk management;Costs;Manufacturing processes;Pulp manufacturing;Semiconductor device manufacture;Displays;Testing;Semiconductor device modeling;History","semiconductor device manufacture;neurocontrollers;maintenance engineering;process control;closed loop systems","dynamic neural controller;process optimization;parts maintenance;semiconductor manufacturing;closed-loop control;urgency metric;risk analysis;wafer-to-wafer advisory;advanced process control","","","12","","","","","","IEEE","IEEE Conferences"
"The V compiler: automatic hardware design","V. Berstis","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Design & Test of Computers","","1989","6","2","8","17","The V language describes VLSI systems concisely through the use of sequential algorithmic descriptions. Because V includes high-level constructs such as queues, asynchronous calls, and cycle-blocks, designs are more readily described and optimized into efficient hardware implementations. The implementations can then be tuned for space, time, or other objectives using annotations. From the input description, the V compiler generates both a register-transfer-level specification and a software simulator. Thus, a single description is suitable for both functional simulation and input to logic synthesis. The author describes parsing. scheduling, and resource sharing using the V compiler. He discusses synthesis and simulation, annotations, and high-level constructs.<<ETX>>","0740-7475;1558-1918","","10.1109/54.19131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=19131","","Hardware;Algorithm design and analysis;Logic;Resource management;Computer languages;Tree graphs;Design optimization;Scheduling algorithm;Design automation;Writing","circuit CAD;logic CAD;specification languages","V compiler;automatic hardware design;V language;VLSI systems;sequential algorithmic descriptions;high-level constructs;queues;asynchronous calls;cycle-blocks;register-transfer-level specification;software simulator;functional simulation;logic synthesis;parsing;scheduling;resource sharing;synthesis;annotations","","12","12","","","","","","IEEE","IEEE Journals & Magazines"
"Optimization of dedicated scintimammography procedure using detector prototypes and compressible phantoms","S. Majewski; E. Curran; C. Keppel; D. Kieper; B. Kross; A. Pulumbo; V. Popov; A. G. Weisenberger; B. Welch; R. Wojcik; M. B. Williams; A. R. Goode; M. More; G. Zang","Thomas Jefferson Nat. Accelerator Fac., Newport News, VI, USA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","2000 IEEE Nuclear Science Symposium. Conference Record (Cat. No.00CH37149)","","2000","3","","22/62","22/66 vol.3","Jefferson Lab., Hampton University, and University of Virginia are collaborating on optimization of dedicated scintimammography mini gamma cameras. Several prototype imagers with a FOV of 4""/spl times/4"" and 8""/spl times/6"" were tested for an application in a dual modality mode as an adjunct technique to digital mammography imagers, or as stand-alone instruments in a dedicated breast SPECT mode. The goal of this study was to obtain experimental data allowing for selection of the best imaging geometry to detect small lesions labeled with Tc-99m. The design of the small scintimammography gamma camera prototypes used in these studies, with an active FOV from 10 cm/spl times/10 cm and 15 cm/spl times/20 cm, is based on an array of Hamamatsu R7600-00-C8 position sensitive photomultiplier tubes (PSPMTs). Optically coupled to the PSPMT array via specially designed efficient multi-element light guide is a matrix of NaI(Tl) scintillator pixels (made by Bicron Corporation) with each element 3 mm/spl times/3 mm/spl times/6 mm in size and separated by 0.4 mm thick septa. Several designs of this basic structure were used in prototypes used to detect small lesions inserted in gelatin breast phantoms under compression and in a non-compressed SPECT mode. Anthropomorphic torso phantoms were also employed to simulate realistic scatter radiation fields. Two data acquisition systems were used to collect and analyze the data: one based on a Macintosh G3 workstation with FERA ADCs and one based on a PC computer running Windows NT/KmaxNT software which makes use of two sixteen channel ADC PCl cards. The present results show that the preferred imaging geometry is planar imaging with two opposing detector heads and breast under compression, however, further study of the dedicated breast SPECT is warranted.","1082-3654","0-7803-6503","10.1109/NSSMIC.2000.949382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949382","","Breast;Prototypes;Optical imaging;Cameras;Lesions;Optical scattering;Optical arrays;Imaging phantoms;Collaboration;Testing","mammography;cameras;photomultipliers;biomedical equipment;single photon emission computed tomography","dedicated scintimammography procedure optimization;detector prototypes;compressible phantoms;Hamamatsu R7600-00-C8 position sensitive photomultiplier tubes array;nuclear medicine;medical diagnostic imaging;multielement light guide;NaI(Tl) scintillator pixels matrix;medical instrumentation;0.3 to 6 mm;4 to 8 in;NaI:Tl","","","24","","","","","","IEEE","IEEE Conferences"
"Combining reinforcement learning with GA to find co-ordinated control rules for multi-agent system","S. Mikami; M. Wada; Y. Kakazu","Fac. of Eng., Hokkaido Inst. of Technol., Sapporo, Japan; Fac. of Eng., Hokkaido Inst. of Technol., Sapporo, Japan; Fac. of Eng., Hokkaido Inst. of Technol., Sapporo, Japan","Proceedings of IEEE International Conference on Evolutionary Computation","","1996","","","356","361","In a multi-agent application, it is necessary to find co-ordinated control rules that maximise a global objective function. To establish coordination, a real-time synchronous communication is normally assumed. However, communication is often limited to asynchronous and very time delayed methods in many practical applications. The paper intends to propose a method to search for co-ordinated plans under limited information exchange. Our approach is to combine on-line local optimisation by reinforcement learning (RL) and asynchronous global combinatorial optimisation by genetic algorithms. The GA search modifies RL's search direction to find a co-ordinated plan, whereas the RL tries to obtain that plan in real-time. Information on which direction is better to find by RL is given through long-term (not instant) communication. The direction is given by a state compression mapping. This is therefore a Lamarckian type GA that inherits acquired knowledge from RL. By using a seesaw balancing problem as a test bed, the performance of the proposed method is shown.","","0-7803-2902","10.1109/ICEC.1996.542389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=542389","","Learning;Control systems;Multiagent systems;Communication system control;Delay effects;Mobile robots;Power generation;Delay lines;Mobile communication;Traffic control","learning (artificial intelligence);genetic algorithms;cooperative systems;software agents;knowledge based systems;planning (artificial intelligence);intelligent control;knowledge acquisition","reinforcement learning;coordinated control rules;multi-agent system;maximised global objective function;real-time synchronous communication;limited information exchange;on-line local optimisation;asynchronous global combinatorial optimisation;genetic algorithms;searching;coordinated plan;long-term communication;state compression mapping;Lamarckian type genetic algorithm;acquired knowledge inheritance;seesaw balancing problem","","2","10","","","","","","IEEE","IEEE Conferences"
"Virtual-prototyping satellite electrical power systems using the virtual test bed","Zhenhua Jiang; Shengyi Liu; R. A. Dougal","South Carolina Univ., Columbia, SC, USA; NA; NA","Proceedings IEEE SoutheastCon 2002 (Cat. No.02CH37283)","","2002","","","113","120","Satellite electrical power subsystems (SEPS) consist of components of a wide range of power levels, complex nonlinear behaviors, and multiple disciplines. High manufacturing cost and complexity of SEPS make it essential to perform simulation studies and build virtual-prototypes prior to construction of real hardware. The Virtual Test Bed (VTB) provides a computational environment that allows rapid modeling, simulation and virtual-prototyping of such complex systems. The VTB is also endowed with mechanisms to import models, and to co-simulate with other standard software packages. In this paper, a representative satellite electrical power system is virtually prototyped in VTB to demonstrate: (1) modeling of interdisciplinary devices such as solar array and battery system, and other power devices such as power converters; (2) cosimulation using MatLab/Simulink models for battery charge/discharge controller, and protection circuits; and (3) virtual-prototyping and simulation of the system to optimize performance. The simulation study focuses on the dynamic behaviors of solar array and battery in orbit cycles, and the system responses under different fault conditions.","","0-7803-7252","10.1109/SECON.2002.995569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995569","","Satellites;Power systems;Power system simulation;Power system modeling;Mathematical model;Batteries;Computational modeling;Power system protection;Circuit simulation;Virtual manufacturing","artificial satellites;space vehicle power plants;photovoltaic power systems;solar cell arrays;aerospace simulation;aerospace computing;secondary cells;battery chargers;power convertors;power engineering computing","satellite electrical power systems;virtual test bed;virtual-prototyping;computational environment;rapid modeling;simulation;software packages;solar array;battery system;power converters;MatLab/Simulink models;battery charge/discharge controller;dynamic behaviors;fault conditions","","","","","","","","","IEEE","IEEE Conferences"
"Software Implementation of a PN Spread Spectrum Receiver to Accommodate Dynamics","C. Cahn; D. Leimer; C. Marsh; F. Huntowski; G. LaRue","Magnavox, Torrance, CA, USA; NA; NA; NA; NA","IEEE Transactions on Communications","","1977","25","8","832","840","To optimize the threshold of a pseudonoise (PN) spread spectrum modem for use over an aircraft/satellite communications link at SHF, the effects of Doppler must be taken into account. Reconstitution of carrier phase by a Costas loop to coherently demodulate the PSK data and also the delay-lock error voltage has typically been the practice in PN modems intended for ground applications. To accommodate the platform dynamics, the Costas loop must have a relatively wide bandwidth, and this implies a significant threshold degradation. An alternate implementation employs a noncoherent carrier tracking loop which maintains frequency lock rather than phase lock. Now, the delay-lock error voltage is noncoherently demodulated. For the airborne application, analysis and simulations show this implementation will extend the receiver's tracking threshold significantly (up to 6 dB) for the worst case dynamics profile. An experimental project was undertaken to modify an existing ground PN modem (AN/USC-28, ADM version) for flight test. A software implementation of the digital tracking algorithms was selected where a HP-2100A minicomputer controls carrier frequency and PN code phase via digital phase shifters. The Costas demodulator for extracting PSK data resides entirely in software, and is completely segregated from PN tracking. In laboratory testing of the receiver with simulated dynamics and in actual flight tests, the demonstrated performance was found to approach closely the goals established by the analyses and simulations.","0090-6778;1558-0857","","10.1109/TCOM.1977.1093904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1093904","","Spread spectrum communication;Modems;Analytical models;Testing;Phase shift keying;Delay;Voltage;Aerospace simulation;Aircraft;Satellite communication","","Aircraft communications;Costas loops;Doppler effect;PSK modulation/demodulation;Pseudonoise-coded communication;Satellite communication, multiaccess","","25","8","","","","","","IEEE","IEEE Journals & Magazines"
"C Compiler Retargeting Based on Instruction Semantics Models","J. Ceng; M. Hohenauer; R. Leupers; G. Ascheid; H. Meyr; G. Braun","Aachen University of Technology; NA; NA; NA; NA; NA","Design, Automation and Test in Europe","","2005","","","1150","1155","Efficient architecture exploration and design of application specific instruction-set processors (ASIPs) requires retargetable software development tools, in particular C compilers that can be quickly adapted to new architectures. A widespread approach is to model the target architecture in a dedicated architecture description language (ADL) and to generate the tools automatically from the ADL specification. For C compiler generation, however, most existing systems are limited either by the manual retargeting effort or by redundancies in the ADL models that lead to potential inconsistencies. We present a new approach to retargetable compilation, based on the LISA 2.0 ADL with instruction semantics, that minimizes redundancies while simultaneously achieving a high degree of automation. The key of our approach is to generate the mapping rules needed in the compiler's code selector from the instruction semantics information. We describe the required analysis and generation techniques, and present experimental results for several embedded processors.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395749","","Application specific processors;Computer architecture;Application software;Programming;Architecture description languages;Automation;Process design;Signal processing;System-level design;Optimizing compilers","","","","6","20","","","","","","IEEE","IEEE Conferences"
"First International Newspaper Segmentation contest","B. Gatos; S. L. Mantzaris; A. Antonacopoulos","Dept. of Digital Technol., Lambrakis Press Archives, Athens, Greece; NA; NA","Proceedings of Sixth International Conference on Document Analysis and Recognition","","2001","","","1190","1194","This paper presents the results of the First International Newspaper Segmentation contest that was organized on the frame of ICDAR 2001 conference. The aim of this contest was to evaluate all existing algorithms for document image segmentation that can be applied to Newspaper page segmentation. We evaluated the performance of three different newspaper segmentation algorithms on tracing all basic entities that appear in newspaper pages from the beginning of the previous century up to the present. The selected entities are text regions, lines and images/drawings. Both training and test sets come from Greek and English newspapers. The performance evaluation method is based on counting the number of matches between the entities detected by the algorithms and the entities of the ground truth. In order to rank the global performance of each participant, we employed a metric that combines the average values of detection rate and recognition accuracy.","","0-7695-1263","10.1109/ICDAR.2001.953973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953973","","Image segmentation;Testing;Graphics;Computer science;Training data;Layout","publishing;document image processing;image segmentation;software performance evaluation","First International Newspaper Segmentation contest;document image segmentation;newspaper page segmentation;performance evaluation","","16","11","","","","","","IEEE","IEEE Conferences"
"Improvements to the *CGA enabling online intrinsic evolution in compact EH devices","G. R. Kramer; J. C. Gallagher","Dept. of Comput. Sci. & Eng., Wright State Univ., Dayton, OH, USA; Dept. of Comput. Sci. & Eng., Wright State Univ., Dayton, OH, USA","NASA/DoD Conference on Evolvable Hardware, 2003. Proceedings.","","2003","","","225","231","Recently, we proposed a neuromorphic intrinsic online evolvable hardware (EH) system designed to learn control laws of physical devices. Since we intend to eventually build this device using mixed signal VLSI techniques, and because we intend to address control applications in which small size and low power consumption are critical, we are extremely concerned with the design of physically compact devices. This paper focuses on the evolutionary algorithm (EA) portion of our proposed system. We discuss modifications to our previously reported *CGA that significantly increases its performance against dynamic optimization problems without significantly increasing the amount of hardware required for implementation. We demonstrate the efficacy of our improvement by testing against two series of moving peak benchmarks. We conclude with discussions of both the implications of our findings and our plans for future work.","","0-7695-1977","10.1109/EH.2003.1217670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1217670","","Hardware;Very large scale integration;Evolutionary computation;Engines;Legged locomotion;Control systems;Vehicle dynamics;Benchmark testing;Computer science;Design engineering","genetic algorithms;reconfigurable architectures;VLSI;hardware-software codesign;neural nets;performance evaluation","CGA enabling online intrinsic evolution;compact EH device;neuromorphic intrinsic online evolvable hardware;control law learning;physical device;mixed signal VLSI technique;control application;power consumption;physically compact device design;evolutionary algorithm;EA engine;dynamic optimization problem;improvement efficacy;benchmark testing;hardware control;very large scale integration","","5","15","","","","","","IEEE","IEEE Conferences"
"Interchangeable pin routing with application to package layout","Man-Fai Yu; J. Darnauer; W. W. -. Dai","Board of Studies in Comput. Eng., California Univ., Santa Cruz, CA, USA; NA; NA","Proceedings of International Conference on Computer Aided Design","","1996","","","668","673","Many practical routing problems such as BGA, PGA, pin redistribution and test fixture routing involve routing with interchangeable pins. These routing problems, especially package layout, are becoming more difficult to do manually due to increasing speed and I/O. Currently, no commercial or university router is available for this task. In this paper, we unify these different problems as instances of the interchangeable pin routing (IPR) problem, which is NP-complete. By representing the solution space with flows in a triangulated routing network instead of grids, we developed a min-cost max-flow heuristic considering only the most important cuts in the design. The heuristic handles multiple layers, prerouted nets, and all-angle, octilinear or rectilinear wiring styles. Experiments show that the heuristic is very effective on most practical examples. It had been used to route industry designs with thousands of interchangeable pins.","","0-8186-7597","10.1109/ICCAD.1996.571349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=571349","","Routing;Electronics packaging;Pins;Testing;Fixtures;Probes;Geometry;Connectors;Application software;Intellectual property","circuit layout CAD;network routing;circuit optimisation;wiring;integrated circuit packaging;integrated circuit design;application specific integrated circuits","interchangeable pin routing;package layout;routing problems;BGA;PGA;pin redistribution;test fixture routing;speed;input output;NP-complete;triangulated routing network;min-cost max-flow heuristic;multiple layers;prerouted nets;all-angle wiring;rectilinear wiring;octilinear wiring;CAD;ASIC","","15","17","","","","","","IEEE","IEEE Conferences"
"Off-line handwriting recognition from forms","M. D. Garris; J. L. Blue; G. T. Candela; D. L. Dimmick; J. Geist; P. J. Grother; S. A. Janet; C. L. Wilson","Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA","1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century","","1995","3","","2783","2788 vol.3","A public domain optical character recognition (OCR) system has been developed by the National Institute of Standards and Technology (NIST) to provide a baseline of performance on off-line handwriting recognition from forms. The system's source code, training data, and performance assessment tools are all publicly available. The system recognizes the handprint written on handwriting sample forms as distributed on the CD-ROM, NIST Special Database 19. The public domain package contains a number of significant contributions to OCR technology, including an optimized probabilistic neural network classifier that operates a factor of 20 times faster than traditional software implementations of this algorithm. The modular design of the software makes it useful for training and testing set validation, multiple system voting schemes, and component evaluation and comparison. As an example, the OCR results from two versions of the recognition system are presented and analyzed.","","0-7803-2559","10.1109/ICSMC.1995.538203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=538203","","Handwriting recognition;Optical character recognition software;NIST;Character recognition;Training data;CD-ROMs;Distributed databases;Software packages;Packaging;Neural networks","optical character recognition;neural nets;learning systems;image classification;handwriting recognition;public domain software","off-line handwritten character recognition;public domain OCR;National Institute of Standards and Technology;NIST Special Database 19;probabilistic neural network;neural classifier","","1","11","","","","","","IEEE","IEEE Conferences"
"Electromagnetic compatibility of antennas on a mobile board","Ibatoulline","Dept. of Radio Phys., Kazan State Univ., Russia","1997 Proceedings of International Symposium on Electromagnetic Compatibility","","1997","","","235","238","This paper presents solutions to the problem of optimizing the decrease of antenna coupling coefficients on a mobile board for the case of limitations on the coverage for each antenna. To determine the minimum value of the antenna coupling coefficient for one pair of antennas we use the Gauss-Zeidel optimization method or the search method. When two or several antenna pairs have a common antenna the coordinates of this antenna are determined by compromise. The algorithms of optimum antenna placement are supported by computer software and are tested for the case of four antennas.","","0-7803-3608","10.1109/ELMAGC.1997.617131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617131","","Electromagnetic compatibility","electromagnetic compatibility;optimisation;iterative methods;search problems;antenna theory;antenna arrays;mobile antennas","electromagnetic compatibility;mobile board;antenna coupling coefficients;Gauss-Zeidel optimization method;search method;antenna pairs;common antenna;optimum antenna placement","","1","3","","","","","","IEEE","IEEE Conferences"
"Block sorting is hard","W. W. Bein; L. L. Larmore; S. Latifi; I. H. Sudborough","Dept. of Comput. Sci., Nevada Univ., Las Vegas, NV, USA; Dept. of Comput. Sci., Nevada Univ., Las Vegas, NV, USA; NA; NA","Proceedings International Symposium on Parallel Architectures, Algorithms and Networks. I-SPAN'02","","2002","","","349","354","Block sorting is used in connection with optical character recognition (OCR). Recent work has focused on finding good strategies which work in practice. We show that optimizing block sorting is NP-hard. Along with this result, we give new non-trivial lower bounds. These bounds can be computed efficiently. We define the concept of local property algorithms and show that several previously published block sorting algorithms fall into this class.","1087-4089","0-7695-1579","10.1109/ISPAN.2002.1004305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004305","","Sorting;Optical character recognition software;Computer science;Character recognition;Performance evaluation;Testing;Approximation algorithms;Parallel architectures","sorting;optical character recognition;optimisation;directed graphs;computability","block sorting optimization;lower bounds;local property algorithms;directed graph;optical character recognition;OCR;NP-hard","","2","11","","","","","","IEEE","IEEE Conferences"
"Automatic model-driven recovery in distributed systems","K. R. Joshi; M. A. Hiltunen; W. H. Sanders; R. D. Schlichting","Coordinated Sci. Lab., Illinois Univ., Urbana, IL, USA; NA; NA; NA","24th IEEE Symposium on Reliable Distributed Systems (SRDS'05)","","2005","","","25","36","Automatic system monitoring and recovery has the potential to provide a low-cost solution for high availability. However, automating recovery is difficult in practice because of the challenge of accurate fault diagnosis in the presence of low coverage, poor localization ability, and false positives that are inherent in many widely used monitoring techniques. In this paper, we present a holistic model-based approach that overcomes these challenges and enables automatic recovery in distributed systems. To do so, it uses theoretically sound techniques including Bayesian estimation and Markov decision theory to provide controllers that choose good, if not optimal, recovery actions according to a user-defined optimization criteria. By combining monitoring and recovery, the approach realizes benefits that could not have been obtained by using them in isolation. In this paper, we present two recovery algorithms with complementary properties and trade-offs, and validate our algorithms (through simulation) by fault injection on a realistic e-commerce system.","1060-9857","0-7695-2463","10.1109/RELDIS.2005.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541182","","Condition monitoring;Computerized monitoring;Availability;Fault diagnosis;Application software;Redundancy;Bayesian methods;Testing;Decision theory;Optimal control","system recovery;system monitoring;fault diagnosis;fault tolerant computing;distributed processing;Markov processes;Bayes methods;decision theory;optimisation","automatic model-driven recovery;distributed system;automatic system monitoring;automatic system recovery;fault diagnosis;Bayesian estimation;Markov decision theory;optimization;fault injection;e-commerce system","","17","16","","","","","","IEEE","IEEE Conferences"
"A high-level synthesis approach to design of fault-tolerant systems","G. Buonanno; M. Pugassi; M. G. Sami","Dipt. di Elettronica, Politecnico di Milano, Italy; NA; NA","Proceedings. 15th IEEE VLSI Test Symposium (Cat. No.97TB100125)","","1997","","","356","361","Fault-tolerance in embedded systems is a requirement of increasing importance; solutions must achieve a balance between performances and costs that was not usually requested in design of more classical fault-tolerant applications and that involves as a consequence new approaches. A design technique is here proposed supporting fault-tolerance of hardware modules in complex hardware-software systems, fault-tolerance requirements for each hardware-mapped process are specified in terms of time constraints and of relative priorities, and a high-level synthesis methodology allowing to design - for each process - a processor capable of supporting both the nominal execution of the process itself in a fault-free environment and simultaneous execution of a reconfigured pair of processes in a fault-affected environment is defined Performances of the scheduling algorithm, allowing to achieve reconfiguration with minimum resource increase and within the required limits of speed degradation, are evaluated on some relevant instances of algorithms discussed in current literature on high-level synthesis.","1093-0167","0-8186-7810","10.1109/VTEST.1997.600305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=600305","","High level synthesis;Fault tolerant systems;Algorithm design and analysis;Scheduling algorithm;Embedded system;Costs;Hardware;Time factors;Design methodology;Performance evaluation","fault tolerant computing;high level synthesis;real-time systems;reconfigurable architectures","high-level synthesis;design;fault-tolerant system;embedded system;hardware-software system;cost;processor;scheduling algorithm;reconfiguration","","6","10","","","","","","IEEE","IEEE Conferences"
"Shape correspondence through landmark sliding","Song Wang; T. Kubota; T. Richardson","Dept. of Comput. Sci. & Eng., South Carolina Univ., Columbia, SC, USA; Dept. of Comput. Sci. & Eng., South Carolina Univ., Columbia, SC, USA; Dept. of Comput. Sci. & Eng., South Carolina Univ., Columbia, SC, USA","Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.","","2004","1","","I","I","Motivated by improving statistical shape analysis, this paper presents a novel landmark-based method for accurate shape correspondence, where the general goal is to align multiple shape instances by corresponding a set of given landmark points along those shapes. Different from previous methods, we consider both global shape deformation and local geometric features in defining the shape-correspondence cost function to achieve a consistency between the landmark correspondence and the underlying shape correspondence. According to this cost function, we develop a novel landmark-sliding algorithm to achieve optimal landmark-based shape correspondence with preserved shape topology. The proposed method can be applied to correspond various 2D shapes in the forms of single closed curves, single open curves, self-crossing curves, and multiple curves. We also discuss the practical issue of landmark initialization. The proposed method has been tested on various biological shapes arising from medical image analysis and validated in constructing statistical shape models.","1063-6919","0-7695-2158","10.1109/CVPR.2004.1315025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315025","","Shape measurement;Cost function;Topology;Biomedical imaging;Image analysis;Application software;Computer science;Medical tests;Biological system modeling;Computer vision","medical image processing;statistical analysis;iterative methods;computational geometry;optimisation","shape correspondence cost function;landmark sliding algorithm;statistical shape analysis;global shape deformation;local geometric features;landmark correspondence;shape topology;2D shapes;landmark initialization;biological shapes;medical image analysis;statistical shape models;single closed curves;single open curves;self crossing curves;multiple curves;iterative algorithm;optimisation","","11","15","","","","","","IEEE","IEEE Conferences"
"Symbolic-numeric co-simulation of large analogue circuits","S. Dordevic; P. M. Petkovi","Fac. of Electron. Eng., Nis Univ., Yugoslavia; Fac. of Electron. Eng., Nis Univ., Yugoslavia","2002 23rd International Conference on Microelectronics. Proceedings (Cat. No.02TH8595)","","2002","2","","639","642 vol.2","This paper presents a new method for hierarchical analysis of large circuits that combine numeric and symbolic simulation. Symbolic analysis is applied only on subcircuits at the lowest hierarchical level. This approach proves beneficial for symbolic and numeric simulation. The time reduction is given on a benchmark example.","","0-7803-7235","10.1109/MIEL.2002.1003338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003338","","Circuit simulation;Circuit analysis;Analytical models;Numerical simulation;Humans;Particle measurements;Application software;Circuit optimization;Visualization;Frequency","circuit analysis computing;circuit simulation;analogue circuits;numerical analysis;symbol manipulation","large analogue circuits;symbolic-numeric co-simulation;large circuit hierarchical analysis;sub-circuit symbolic analysis;hierarchical levels;numeric simulation;analysis time reduction;benchmark tests","","","8","","","","","","IEEE","IEEE Conferences"
"Data type analysis for hardware synthesis from object-oriented models","M. Radetzki; A. Stammermann; W. Putzke-Roming; W. Nebel","OFFIS Res. Center, Oldenburg, Germany; NA; NA; NA","Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)","","1999","","","491","496","Object-oriented modeling of hardware promises to help deal with design complexity through higher abstraction and better support for reuse. Whereas simulation of such models is rather easy to achieve, synthesis turns out to require the application of quite sophisticated techniques. In this paper, we devise a solution of the foremost problem, optimized synthesis of object-oriented data types. The outlined algorithms have been implemented for an object-oriented dialect of VHDL and may also contribute, possibly in a co-design context, to synthesis from languages such as C++ or Java. We explain our synthesis methods and show their impact with the example of a microprocessor model.","","0-7695-0078","10.1109/DATE.1999.761171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=761171","","Data analysis;Object oriented modeling;Java;Microprocessors;Hardware design languages;Productivity;Pressing;Design methodology;Application software;Software engineering","object-oriented methods;hardware description languages;data analysis","data type analysis;hardware synthesis;object-oriented models;design complexity;VHDL;co-design context;microprocessor model","","1","13","","","","","","IEEE","IEEE Conferences"
"Hardware acceleration of hidden Markov model decoding for person detection","S. A. Fahmy; P. Y. K. Cheung; W. Luk","Dept. of Electr. & Electron. Eng., Imperial Coll. London, UK; Dept. of Electr. & Electron. Eng., Imperial Coll. London, UK; NA","Design, Automation and Test in Europe","","2005","","","8","13 Vol. 3","This paper explores methods for hardware acceleration of hidden Markov model (HMM) decoding for the detection of persons in still images. Our architecture exploits the inherent structure of the HMM trellis to optimise a Viterbi decoder for extracting the state sequence front observation features. Further performance enhancement is obtained by computing the HMM trellis states in parallel. The resulting hardware decoder architecture is mapped onto a field programmable gate array (FPGA). The performance and resource usage of our design is investigated for different levels of parallelism. Performance advantages over software are evaluated. We show how this work contributes to a real-time system for person-tracking in video-sequences.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395785","","Hardware;Acceleration;Hidden Markov models;Decoding;Computer architecture;Field programmable gate arrays;Viterbi algorithm;Concurrent computing;Parallel processing;Software performance","image recognition;feature extraction;hidden Markov models;Viterbi decoding;state estimation;parallel architectures;field programmable gate arrays","person detection hardware acceleration;hidden Markov models;HMM decoding;still image person detection;HMM trellis;Viterbi decoder optimisation;observation feature state sequence extraction;parallel computation;FPGA;real-time video-sequence person-tracking","","8","12","","","","","","IEEE","IEEE Conferences"
"Real-time scheduling of hydro cascaded system using evolutionary computation","R. Golob; T. Stokelj","Fac. of Electr. Eng., Ljubljana Univ., Slovenia; NA","MELECON '98. 9th Mediterranean Electrotechnical Conference. Proceedings (Cat. No.98CH36056)","","1998","2","","983","987 vol.2","In Slovenia, there are three hydro cascaded systems (HCS) accounting for some 35% of the total Slovenian power generation and playing an important role in optimizing the overall power system performance. While hydrothermal scheduling for the entire Slovenian EPS is performed centrally for 24 hours ahead, it is up to local control centers to determine optimal scheduling for each individual hydropower plant (HPP). In this paper, a computational tool for the optimal scheduling of HCS that has been developed as an integral part of the Soske Elektrarne (SENG) local center energy management system (EMS) is presented. The developed software package has been tested against a number of operational scenarios and an important improvement in SoCa river HCS performance has been observed.","","0-7803-3879","10.1109/MELCON.1998.699375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=699375","","Real time systems;Optimal scheduling;Hydroelectric-thermal power generation;Power systems;Processor scheduling;Optimal control;Centralized control;Hydroelectric power generation;Energy management;Medical services","hydroelectric power stations;power generation scheduling;power generation planning;power system simulation;energy management systems;real-time systems;optimisation","hydroelectric cascaded power system;evolutionary computation;real-time generation scheduling;Slovenia;power system performance;local control centers;hydropower plant;energy management system;software package","","2","7","","","","","","IEEE","IEEE Conferences"
"A direct computational method for determining the maximal (A, B)-invariant subspace contained in ker<tex>C</tex>","M. Solak","Electrotechnical Institute, Warsaw, Poland","IEEE Transactions on Automatic Control","","1986","31","4","349","352","A useful method for calculation of the maximal (<tex>A, B</tex>)- invariant subspace contained in<tex>\ker C</tex>is presented. The method demands reduction of certain matrices into echelon form and application of rank test for low-dimensional real matrices. The approach presented allows unification of the approach of Bhattacharyya, Vardulakis, and Wonham.","0018-9286;1558-2523;2334-3303","","10.1109/TAC.1986.1104279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1104279","","Polynomials;Control systems;Computational Intelligence Society;Software algorithms;Software testing;Equations;Automatic control","","Linear systems;Matrices;Topology","","3","10","","","","","","IEEE","IEEE Journals & Magazines"
"Configuring microgenetic algorithms for solving traffic control problems: the case of number of generations","G. Abu-Lebdeh; B. H. Al-Omari","Dept. of Civil & Environ. Eng., Michigan State Univ., East Lansing, MI, USA; NA","Fourth International Symposium on Uncertainty Modeling and Analysis, 2003. ISUMA 2003.","","2003","","","70","77","Efficient and successful use of genetic algorithms (GAs) requires careful selection of several parameter values. One such critical parameter is the processing time (or, number of generations) that is sufficient to ensure suitable convergence. Todate there is only limited guidance on this subject, and in most cases detailed knowledge of the structure and properties of the problem is necessary for such guidance to be useable. For real world problems such knowledge may not be readily available. We describe an experimental approach to establish relationships between time to convergence and problem size of microgenetic algorithms (m-GAs). A discrete time dynamical traffic control problem with different sizes and levels of complexity was used as a test bed. The results showed that upon appropriately sizing the m-GA population, the m-GA can converge to a near-optimal solution in a number of generations equal to the string length. The results also demonstrate that with the selection of appropriate number of generations, it is possible to get most of the worth of the theoretically optimal solution but with only a fraction of the computation cost. The results showed that as the size of the optimization problem grew exponentially, the time requirements of m-GA grew only linearly thus making m-GAs especially suited for optimizing large scale and combinatorial problems for online optimization","","0-7695-1997","10.1109/ISUMA.2003.1236143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1236143","","Traffic control;Computer aided software engineering;Large-scale systems;Communication system traffic control;Genetic algorithms;Intelligent transportation systems;Genetic engineering;Civil engineering;Testing;Cost function","computational complexity;convergence;genetic algorithms;traffic control","microgenetic algorithm;m-GA convergence;discrete time dynamical traffic control;optimal solution;optimization problem;combinatorial problem;online optimization","","","13","","","","","","IEEE","IEEE Conferences"
"Logal: a CHDL for Logic Design and Synthesis of Computers","J. H. Stewart","Sperry-Univac","Computer","","1977","10","6","18","26","A computer hardware description language can be a practical and economical tool for driving a comprehensive automated design facility for a large-scale computer environment. The logic designer must optimize many conflicting requirements, including the following:","0018-9162;1558-0814","","10.1109/C-M.1977.217741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1646516","","Logic design;Algorithm design and analysis;Logic testing;Software algorithms;Design optimization;Hardware design languages;Cost function;Iterative algorithms;Software packages","","","","1","3","","","","","","IEEE","IEEE Journals & Magazines"
"Investigating available instruction level parallelism for stack based machine architectures","Huibin Shi; C. Bailey","Dept. of Comput. Sci., York Univ., UK; Dept. of Comput. Sci., York Univ., UK","Euromicro Symposium on Digital System Design, 2004. DSD 2004.","","2004","","","112","120","Stack architectures have attracted much renewed research in recent years, due largely to the arrival of the JAVA programming language for Internet, and more recently in embedded applications. However, instruction level parallelism (ILP) has not yet received significant attention with respect to such machines. We have developed a stack code viewer/analyzer tool to analyze available ILP in stack assembler-code, together with the UTSA simulator, under unlimited software and hardware resource configurations. Results for basic block analysis reveal marginal speedups of available ILP over the serial execution in the absence of mechanisms for branch speculation or code optimization. Results for superblock and loop unrolling techniques show that more significant improvements can be made to available parallelism where effort is directed. An experiment is also presented to highlight the significance of branch prediction. In all tests, our stack code viewer/analyzer tool can graphically represent the scheduling result of each basic block.","","0-7695-2203","10.1109/DSD.2004.1333266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333266","","Parallel processing;Computer architecture;Java;Computer languages;Internet;Computer science;Application software;Assembly;Analytical models;Software tools","parallel architectures;instruction sets;program diagnostics","instruction level parallelism;stack based machine architectures;stack code viewer tool;stack analyzer tool;stack assembler-code;UTSA simulator;software resource configurations;hardware resource configurations;branch speculation;code optimization;superblock unrolling techniques;loop unrolling techniques;branch prediction","","","11","","","","","","IEEE","IEEE Conferences"
"Comparing evolutionary algorithms on binary constraint satisfaction problems","B. G. W. Craenen; A. E. Eiben; J. I. van Hemert","Vrije Univ. Amsterdam, Netherlands; Vrije Univ. Amsterdam, Netherlands; NA","IEEE Transactions on Evolutionary Computation","","2003","7","5","424","444","Constraint handling is not straightforward in evolutionary algorithms (EAs) since the usual search operators, mutation and recombination, are 'blind' to constraints. Nevertheless, the issue is highly relevant, for many challenging problems involve constraints. Over the last decade, numerous EAs for solving constraint satisfaction problems (CSP) have been introduced and studied on various problems. The diversity of approaches and the variety of problems used to study the resulting algorithms prevents a fair and accurate comparison of these algorithms. This paper aligns related work by presenting a concise overview and an extensive performance comparison of all these EAs on a systematically generated test suite of random binary CSPs. The random problem instance generator is based on a theoretical model that fixes deficiencies of models and respective generators that have been formerly used in the evolutionary computing field.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2003.816584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237162","","Evolutionary computation;Constraint optimization;Genetic mutations;System testing;Guidelines;Mathematics;Computer science;Software algorithms;Software libraries","evolutionary computation;constraint theory;search problems;problem solving;constraint handling","evolutionary algorithms;binary constraint satisfaction problems;constraint handling;evolutionary computing;search operators;mutation;recombination;performance comparison;random binary CSP;random problem instance generator","","43","76","","","","","","IEEE","IEEE Journals & Magazines"
"A heterogeneous solution for improving the return on investment of requirements traceability","J. Cleland-Huang; G. Zemont; W. Lukasik","Sch. of Comput. Sci., Telecommun., & Inf. Syst., DePaul Univ., Chicago, IL, USA; Sch. of Comput. Sci., Telecommun., & Inf. Syst., DePaul Univ., Chicago, IL, USA; Sch. of Comput. Sci., Telecommun., & Inf. Syst., DePaul Univ., Chicago, IL, USA","Proceedings. 12th IEEE International Requirements Engineering Conference, 2004.","","2004","","","230","239","This work describes a best-of-breed approach to traceability, in which the return-on-investment of the requirements traceability effort is maximized through the strategic deployment of a heterogeneous set of traceability techniques. This contrasts with typical traceability practices that tend to utilize a single technique such as a matrix or tool embedded into a requirements management package even though it may not provide the optimal solution for the traceability needs of a diverse set of requirements. The proposed solution, named TraCS (traceability for complex systems), defines project level trace strategies for categories of requirements and establishes links strategically in order to optimize returns of the traceability effort and minimize the risk inherent to software evolution. The paper provides a rationale for heterogeneous traceability, describes an extensible traceability framework, and then defines the process for establishing project level trace strategies. It concludes with an example drawn from a system to control chemical reactions at a catalyst plant.","1090-705X","0-7695-2174","10.1109/ICRE.2004.1335680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1335680","","Investments;Packaging;Costs;Information retrieval;Computer science;Information systems;Control systems;Chemicals;Software engineering;Software testing","systems analysis;large-scale systems;risk management;investment","return-on-investment;requirements traceability;best-of-breed approach;traceability practices;requirements management package;TraCS;complex system traceability;software evolution","","20","31","","","","","","IEEE","IEEE Conferences"
"Using VHDL for board level simulation","S. Habinc; P. Sinander","Eur. Space Res. & Technol. Centre, Noordwijk, Netherlands; Eur. Space Res. & Technol. Centre, Noordwijk, Netherlands","IEEE Design & Test of Computers","","1996","13","3","66","78","Prototyping is necessary for successful development of printed circuit boards built with complex components such as microprocessors, ASICs, and ASSPs. The European Space Agency uses VHDL models for board level simulation, optimizing such models for high functional accuracy and simulation performance.","0740-7475;1558-1918","","10.1109/54.536097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=536097","","Circuit simulation;Computational modeling;Timing;Hardware;Printed circuits;Application specific integrated circuits;Circuit testing;Manufacturing;Computer simulation;Software prototyping","hardware description languages;printed circuits;digital simulation;application specific integrated circuits","VHDL;board level simulation;printed circuit boards;European Space Agency;high functional accuracy;simulation performance","","3","10","","","","","","IEEE","IEEE Journals & Magazines"
"HiBRID-SoC: a multi-core system-on-chip architecture for multimedia signal processing applications","H. -. Stolberg; M. Berekovic; L. Friebe; S. Moch; S. Flugel; Xun Mao; M. B. Kulaczewski; H. Klussmann; P. Pirsch","Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany; Inst. fur Mikroelektronische Syst., Hannover Univ., Germany","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","8","13 suppl.","The HiBRID-SoC multi-core system-on-chip targets a wide range of application fields with particularly high processing demands, including general signal processing applications, video and audio de-/encoding, and a combination of these tasks. For this purpose, the HiBRID-SoC integrates three fully programmable processors cores and various interfaces onto a single chip, all tied to a 64 bit AMBA AHB bus. The processor cores are individually optimized to the particular computational characteristics of different application fields, complementing each other to deliver high performance levels with high flexibility at reduced system cost. The HiBRID-SoC is fabricated in a 0.18 /spl mu/m 6LM standard-cell CMOS technology, occupies about 82 mm/sup 2/, and operates at 145 MHz.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253797","","System-on-a-chip;Multimedia systems;Signal processing;Signal processing algorithms;MPEG 4 Standard;Application software;Computer architecture;Video coding;Testing;Video signal processing","multimedia computing;system-on-chip;digital signal processing chips;integrated circuit design;CMOS integrated circuits;audio coding;video coding;system buses","CMOS;HiBRID-SoC;multi-core system-on-chip architecture;multimedia signal processing applications;video decoding;video coding;audio coding;audio decoding;programmable processors cores;interfaces;system bus;64 bit;0.18 micron;145 MHz","","10","12","","","","","","IEEE","IEEE Conferences"
"A video compression case study on a reconfigurable VLIW architecture","D. Rizzo; O. Colavin","NA; NA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","540","546","In this paper, we investigate the benefits of a flexible, application-specific instruction set by adding a run-time Reconfigurable Functional Unit (RFU) to a VLIW processor. Preliminary results on the motion estimation stage in an MPEG4 video encoder are presented. With the RFU modeled at functional level and under realistic assumptions on execution latency, technology scaling and reconfiguration penalty, we explore different RFU instructions at fine-grain (instruction-level) and coarse-grain (loop-level) granularity to speedup the application execution. The memory bandwidth bottleneck, typical for streaming applications, is alleviated through the combined adoption of custom prefetch pattern instructions and an extent of local memory. Performance evaluations indicate that up to an 8/spl times/ improvement with loop-level optimizations can be achieved under various architectural assumptions.","1530-1591","0-7695-1471","10.1109/DATE.2002.998353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998353","","Video compression;Computer aided software engineering;VLIW;Kernel;Motion estimation;Delay;Bandwidth;MPEG 4 Standard;Hardware;Benchmark testing","reconfigurable architectures;parallel architectures;motion estimation;video coding;instruction sets","reconfigurable VLIW architecture;video compression case study;application-specific instruction set;run-time reconfigurable functional unit;motion estimation stage;MPEG4 video encoder;functional level modeling;execution latency;technology scaling;reconfiguration penalty;fine-grain granularity;coarse-grain granularity;memory bandwidth bottleneck;custom prefetch pattern instructions;local memory;performance evaluations","","3","15","","","","","","IEEE","IEEE Conferences"
"From algorithms to hardware architectures: a comparison of regular and irregular structured IDCT algorithms","C. Schneider; M. Kayss; T. Hollstein; J. Deicke","Siemens Corp. Technol., Munich, Germany; NA; NA; NA","Proceedings Design, Automation and Test in Europe","","1998","","","186","190","The inverse discrete cosine transformation (IDCT) is used in a variety of decoders (e.g. MPEG). On the one hand, highly optimized algorithms that are characterized by an irregular structure and a minimum number of operations are known from software implementations. On the other hand, regular structured architectures are often used in hardware realizations. In this paper a comparison of regular and irregular structured IDCT algorithms for efficient hardware realization is presented. The irregular structured algorithms are discussed with main emphasis on assessment criteria for algorithm selection and high-level synthesis for hardware cost estimation.","","0-8186-8359","10.1109/DATE.1998.655855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655855","","Hardware;Equations;Discrete cosine transforms;Costs;Space exploration;Microelectronics;Registers;Computational efficiency;Performance evaluation;Testing","discrete cosine transforms;high level synthesis;decoding;video coding;telecommunication standards","hardware architectures;structured IDCT algorithms;inverse discrete cosine transformation;decoders;optimized algorithms;assessment criteria;high-level synthesis;algorithm selection","","1","12","","","","","","IEEE","IEEE Conferences"
"Integrated Simulation Software for Outdoor Robots Planning and Coordination","S. Zein-Sabatto; O. Taiwo; P. Koseeyaorn","Tennessee State University; NA; NA","IEEE SoutheastCon, 2004. Proceedings.","","2004","","","215","221","This paper involves the development of optimization software and its associated simulator to compute optimum paths to move a group of mobile robots to a given number of targets in a known environment. Genetic algorithms and the Virtual Reality Modeling Language vrml are used to design the software and the simulator respectively. It is assumed that the robots are located arbitrary at known starting positions and need to be moved to target positions in a known multi-obstacle three-dimensional environment. The factors considered for finding optimum paths for the group of mobile robots are the size and location of obstacles in the environment and the elevations of the environment. The developed software was tested on an aerial picture taken by a satellite imaging device for an outdoor environment. The size of obstacles, elevations present in the environment and starting positions of the robots and target position are all identified on the digital image in a form of grid map. The genetic algorithm takes information about the environment from the grid map, the results of processed pictures, and searches for optimum paths to move a group of mobile robots to specified targets. It is found that genetic algorithms converge and give better solutions to path planning in an environment where results are difficult to obtain.","","0-7803-8368","10.1109/SECON.2004.1287919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287919","","Robot kinematics;Mobile robots;Path planning;Navigation;Simulated annealing;Computational modeling;Genetic algorithms;Orbital robotics;Virtual reality;Software design","","","","1","13","","","","","","IEEE","IEEE Conferences"
"Advanced insulin infusion using a control loop (ADICOL) concept and realization of a control-loop application for the automated delivery of insulin","R. Dudde; T. Vering","NA; NA","4th International IEEE EMBS Special Topic Conference on Information Technology Applications in Biomedicine, 2003.","","2003","","","280","282","Within the ADICOL research project, we have developed a control-loop application for the automated delivery of insulin. The concept is unique in several aspects: a) Modularity: the ADICOL system consists of three elements, insulin pump, glucose monitor and expert-system, which all run as stand alone devices with the patient, but can be interconnected to form a body network. b) Subcutaneous Application: Both, glucose monitor and insulin delivery use the subcutaneous route to the body. c) Insulin Pump: The insulin infusion mimics a natural pancreas by employing a delivery interval of 3 minutes. d) Expert System: The expert system is running two different algorithms in a hand-held computer, taking advantage of the high-resolution insulin delivery. e) Algorithm: The Model-Predictive Algorithm is based on a feedback optimization of the insulin infusion rate and physiological parameters of the patient. f) Glucose Monitor: Our glucose monitor is based on the open-flow microperfusion sampling technique and employs amperometric glucose biosensors. g) Body Network: The interconnection of the 3 elements to form a control-loop system (or automated pancreas) is achieved by modern wireless technology. Various clinical tests have been undertaken to assess the performance of subsets, results will be shown. We present the overall set-up and its performance, and show latest results from various clinical human trials performed at the Universities of Graz, Austria and Perugia, Italy.","","0-7803-7667","10.1109/ITAB.2003.1222532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222532","","Insulin;Automatic control;Sugar;Patient monitoring;Computerized monitoring;Condition monitoring;Pancreas;Expert systems;Application software;Computer networks","medical expert systems;biosensors;feedback;optimisation;closed loop systems;organic compounds;drug delivery systems;pumps;medical control systems","advanced insulin infusion;control-loop application;automated insulin delivery;insulin pump;natural pancreas;glucose monitor;body network;subcutaneous route;clinical human trials;University of Graz;University of Perugia;patient physiological parameters;3 min","","7","3","","","","","","IEEE","IEEE Conferences"
"Integrated reliability analysis, diagnostics and prognostics for critical power systems","F. Tu; M. S. Azam; Y. Shlapak; K. Pattipati; R. Karanam; S. Amin","Dept. of Electr. & Comput. Eng., Connecticut Univ., Storrs, CT, USA; NA; NA; NA; NA; NA","2001 IEEE Autotestcon Proceedings. IEEE Systems Readiness Technology Conference. (Cat. No.01CH37237)","","2001","","","416","440","Critical power systems, such as data centers and communication switching facilities, have very high availability requirements (5 min./year downtime). A data center that consumes electricity at a rate of 3 MW can have a downtime cost of $300,000 an hour. Even a momentary interruption of two seconds may cause a loss of two hours of data processing. Consequently, power quality has emerged as an issue of significant importance in the operation of these systems. In this paper, we address three issues of power quality: real-time detection and diagnosis of power quality problems, reliability and availability evaluation, and capacity margin analysis. The objective of real-time detection and diagnosis is to provide a seamless on-line monitoring and off-line maintenance process. The techniques are being applied to monitor the power quality of a few facilities at the University of Connecticut. Reliability analysis, based on a computationally efficient sum of disjoint products, enables analysts to decide on the optimum levels of redundancy, aids operators in prioritizing the maintenance options within a given budget, and in monitoring the system for capacity margin. Capacity margin analysis helps operators to plan for additional loads and to schedule repair/replacement activities. The resulting analytical and software tool is demonstrated on a sample data center.","1080-7225","0-7803-7094","10.1109/AUTEST.2001.949035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949035","","Power system reliability;Power quality;Monitoring;Availability;Maintenance;Power system analysis computing;Communication switching;Energy consumption;Costs;Data processing","power system reliability;maintenance engineering;condition monitoring;scheduling;redundancy;power system measurement;power system analysis computing;software tools;computerised monitoring","integrated reliability analysis;diagnostics;prognostics;integrated reliability analysis/diagnostics/prognostics;critical power systems;data centers;communication switching facilities;availability requirements;downtime;data center;electricity consumption;downtime cost;momentary interruption;data processing loss;power quality;system operation;real-time detection;real-time diagnosis;power quality problems;reliability evaluation;availability evaluation;capacity margin analysis;on-line monitoring;off-line maintenance process;reliability analysis;computationally efficient disjoint product sum;redundancy;maintenance option priorities;maintenance budget;capacity margin;monitoring;repair/replacement scheduling;software tool","","","13","","","","","","IEEE","IEEE Conferences"
"Native ATM application programmer interface testbed for cluster-based computing","P. W. Dowd; T. M. Carrozzi; F. A. Pellegrino; A. Xin Chen","Dept. of Electr. & Comput. Eng., State Univ. of New York, Buffalo, NY, USA; Dept. of Electr. & Comput. Eng., State Univ. of New York, Buffalo, NY, USA; Dept. of Electr. & Comput. Eng., State Univ. of New York, Buffalo, NY, USA; Dept. of Electr. & Comput. Eng., State Univ. of New York, Buffalo, NY, USA","Proceedings of International Conference on Parallel Processing","","1996","","","843","849","This paper investigates the use of ATM for cluster based computing. The need for a native ATM API is discussed as well as the performance of message passing libraries (MPL) that are written to use such an API to exploit the advantages of a high-speed network for cluster-based computing. The MPLs offer a standard interface, such as PVM or MPI, and interoperate with existing TCP/IP and UDP/IP based versions in addition to the ATM API environment. The interoperability extensions made to two MPLs, MPI and Prowess, are described with a hybrid environment of both ATM and TCP-based legacy network technology. The native ATM API is described in this paper which supports cluster based computing that may be geographically distributed. Furthermore, this API provides a reliable transport interface to the MPL which has been optimized for an ATM environment. The transport protocol is a low-state design that optimizes the performance based on the available bandwidth, buffer constraints, propagation delay characteristics, and security requirements of a particular connection, and will rapidly evolve if the connection characteristics change.","","0-8186-7255","10.1109/IPPS.1996.508190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=508190","","Programming profession;Testing;Multiprotocol label switching;Message passing;Libraries;High-speed networks;Computer networks;High performance computing;TCPIP;Distributed computing","asynchronous transfer mode;local area networks;application program interfaces;software performance evaluation;message passing;software libraries;open systems;transport protocols","ATM application programmer interface;cluster-based computing;ATM;ATM API;performance;message passing libraries;high-speed network;PVM;MPI;TCP/IP;UDP/IP;interoperability;Prowess;legacy network;reliable transport interface;available bandwidth;buffer constraints;propagation delay;security requirements","","2","6","","","","","","IEEE","IEEE Conferences"
"Managing power consumption in networks on chips","T. Simunic; S. P. Boyd; P. Glynn","Hewlett-Packard Labs., Palo Alto, CA, USA; NA; NA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2004","12","1","96","107","In this paper, we present a new methodology for managing power consumption of networks-on-chips (NOCs). A power management problem is formulated for the first time using closed-loop control concepts. We introduce an estimator and a controller that implement our power management methodology. The estimator is capable of very fast and accurate tracking of changes in the system parameters. Parameters estimated are used to form the system model. Our system model combines node and network centric power management decisions. Node centric power management assumes no a priori knowledge of requests coming in from outside the core. Thus, it implements a more traditional dynamic voltage scaling and power management control algorithms. Network-centric power management utilizes interaction with the other system cores regarding the power and the quality of service (QoS) needs. The overall system model is based on Renewal theory and, thus, guarantees globally optimal results. We introduce a fast optimization method that runs multiple orders of magnitude faster than the previous optimization approaches while still having the same accuracy in obtaining the power management control. Finally, our controller implements the results of optimization in either hardware or software. The new methodology for power management of NOCs is tested on a system consisting of four satellite units, each implementing an estimator and a controller capable of both node and network centric power management. Our results show large savings in power with good QoS.","1063-8210;1557-9999","","10.1109/TVLSI.2003.820533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1263561","","Energy management;Energy consumption;Network-on-a-chip;Power system management;Power system modeling;Quality of service;Optimization methods;Parameter estimation;Knowledge management;Dynamic voltage scaling","power consumption;optimisation;stochastic processes;power control;system-on-chip;closed loop systems","power consumption management;NOC;networks-on-chip;node centric power management;network centric power management;closed loop control;estimator;controller;dynamic voltage scaling;power management control algorithms;QoS;quality of service;optimization;satellite units","","30","29","","","","","","IEEE","IEEE Journals & Magazines"
"Performance evaluation of register allocator for the advanced DSP of TMS320C80","Jihong Kim; G. Short","Dept. of Comput. Eng., Seoul Nat. Univ., South Korea; NA","Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)","","1998","5","","3077","3080 vol.5","PPCA is an assembly language-level register allocator and instruction compactor for the advanced DSPs (ADSPs) of the TMS320C80 digital signal processor. It was developed to help the implementation of time-critical ADSP assembly programs which heavily utilize powerful ADSP features optimized for multimedia and image computing applications for maximum efficiency. PPCA takes as an input ADSP assembly operations with symbolic variables. It then allocates the ADSP's physical registers to the symbolic variables and rearranges the operations into a highly-parallelized compact format. In this paper, we have evaluated the performance of a register allocation capability of PPCA using an extensive image computing library for the TMS320C80. We present the basic algorithm of the PPCA's register allocation module and describe the performance evaluation approach used. The result shows that PPCA essentially achieves optimal register allocation for the test cases based on the image computing library functions.","1520-6149","0-7803-4428","10.1109/ICASSP.1998.678176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=678176","","Digital signal processing;Registers;Assembly;Libraries;Digital signal processors;Time factors;Multimedia computing;Computer applications;Signal processing algorithms;Testing","program assemblers;software performance evaluation;shift registers;digital signal processing chips;image processing;software tools","performance evaluation;register allocator;advanced DSP;TMS320C80;PPCA;assembly language-level register allocator;instruction compactor;ADSPs;digital signal processor;time-critical ADSP assembly programs;image computing;multimedia;symbolic variables;physical registers;highly-parallelized compact format;register allocation capability","","1","9","","","","","","IEEE","IEEE Conferences"
"A multi formalisms prototyping approach from formal description to implementation of distributed systems","A. Diagne; F. Kordon","MASI Lab., Paris VI Univ., France; MASI Lab., Paris VI Univ., France","Proceedings Seventh IEEE International Workshop on Rapid System Prototyping. Shortening the Path from Specification to Prototype","","1996","","","102","107","This paper proposes a methodology to build safe distributed systems that considers both conceptual and operational description aspects. At the conceptual level, we focus on the safety properties expected from the system. Such properties are stated and then verified. At the operational level, we focus on properties addressing the optimization of the generated code. Traceability between the two levels is managed in a satisfactory semi-automatic way. It preserves the properties proved at the first level and discards informations that are not relevant for code generation.","1074-6005","0-8186-7603","10.1109/IWRSP.1996.506735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506735","","Prototypes;Petri nets;Design methodology;Safety;System testing;Object oriented modeling;System recovery;Laboratories;Large-scale systems;Performance analysis","software prototyping;Petri nets;formal specification;security of data;distributed processing","multi formalisms prototyping approach;formal description;distributed systems;operational description;conceptual description;safety properties;generated code optimisation","","2","15","","","","","","IEEE","IEEE Conferences"
"A floating point unit for the 68040","S. McCloud; D. Anderson; C. DeWitt; C. Hinds; Y. -. Ho; D. Marquette; E. Quintana","Motorola Inc., Austin, TX, USA; Motorola Inc., Austin, TX, USA; Motorola Inc., Austin, TX, USA; Motorola Inc., Austin, TX, USA; Motorola Inc., Austin, TX, USA; Motorola Inc., Austin, TX, USA; Motorola Inc., Austin, TX, USA","Proceedings., 1990 IEEE International Conference on Computer Design: VLSI in Computers and Processors","","1990","","","187","190","The Motorola 68040 floating point unit (FPU) combines three independent state machines, two data paths, and over 100000 transistors to achieve 8-Mflops peak performance and over 3-Mflops Linpack double-precision performance at the introductory speed of 25 MHz. It is optimized for minimum latency and maximum pipelined performance on frequently used double-precision floating point instructions. The FPU is architecturally divided into three piped stages, the conversion unit (CU), the execution unit (XU), and the normalization unit (NU). The control logic is tested using scan to achieve very high ATPG fault coverage while the data paths are tested using functional patterns.<<ETX>>","","0-8186-2079","10.1109/ICCD.1990.130198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=130198","","Logic testing;Coprocessors;Automatic test pattern generation;ANSI standards;Software standards;Floating-point arithmetic;Pipelines;Computer architecture;Hardware;Read-write memory","computer architecture;microprocessor chips;pipeline processing","floating point unit;Motorola 68040;8-Mflops peak performance;3-Mflops Linpack double-precision performance;minimum latency;maximum pipelined performance;double-precision floating point instructions;conversion unit;execution unit;normalization unit;control logic;ATPG fault coverage;data paths;functional patterns;8 MFLOPS;25 MHz;3 MFLOPS","","2","2","","","","","","IEEE","IEEE Conferences"
"An application of simultaneous approximation in combinatorial optimization","A. Frank; E. Tardos","NA; NA","26th Annual Symposium on Foundations of Computer Science (sfcs 1985)","","1985","","","459","463","We present a preprocessing algorithm to make certain polynomial algorithms strongly polynomial. The running time of some of the known combinatorial optimization algorithms depends on the size of the objective function w. Our preprocessing algorithm replaces w by an integral valued w whose size is polynomially bounded in the size of the combinatorial structure and which yields the same set of optimal solutions as w. As applications we show how existing polynomial algorithms for finding the maximum weight clique in a perfect graph and for the minimum cost submodular flow problem can be made strongly polynomial. The method relies on Lovsz's simultaneous approximation algorithm.","0272-5428","0-8186-0644","10.1109/SFCS.1985.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568171","","Polynomials;Arithmetic;Approximation algorithms;Costs;Ellipsoids;Application software;Computer science;Integral equations;Testing","","","","3","14","","","","","","IEEE","IEEE Conferences"
"A two-level hybrid evolutionary algorithm for modeling one-dimensional dynamic systems by higher-order ODE models","Hong-Qing Cao; Li-Shan Kang; Tao Guo; Yu-Ping Chen; H. de Garis","State Key Lab. of Software Eng., Wuhan Univ., China; NA; NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2000","30","2","351","357","This paper presents a new algorithm for modeling one-dimensional (1-D) dynamic systems by higher-order ordinary differential equation (HODE) models instead of the ARMA models as used in traditional time series analysis. A two-level hybrid evolutionary modeling algorithm (THEMA) is used to approach the modeling problem of HODE's for dynamic systems. The main idea of this modeling algorithm is to embed a genetic algorithm (GA) into genetic programming (GP), where GP is employed to optimize the structure of a model (the upper level), while a GA is employed to optimize the parameters of the model (the lower level). In the GA, we use a novel crossover operator based on a nonconvex linear combination of multiple parents which works efficiently and quickly in parameter optimization tasks. Two practical examples of time series are used to demonstrate the THEMA's effectiveness and advantages.","1083-4419;1941-0492","","10.1109/3477.836383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=836383","","Evolutionary computation;Differential equations;Time series analysis;Genetic programming;Mathematical model;Laboratories;Testing;Algorithm design and analysis;Genetic algorithms;Economic forecasting","evolutionary computation;genetic algorithms;differential equations;large-scale systems","evolutionary algorithm;ODE models;one-dimensional dynamic systems;ordinary differential equation;two-level hybrid evolutionary modeling algorithm;THEMA;genetic algorithm;genetic programming;crossover operator","","10","19","","","","","","IEEE","IEEE Journals & Magazines"
"Path planning for moving sensors in parameter estimation of distributed systems","D. Ucinski; J. Korbicz","Dept. of Robotics and Software Eng., Tech. Univ. of Zielona Gora, Poland; NA","Proceedings of the First Workshop on Robot Motion and Control. RoMoCo'99 (Cat. No.99EX353)","","1999","","","273","278","We develop an effective numerical procedure for optimizing trajectories of moving sensors which take measurements in a given spatial in order to find parameter estimates of a given distributed system. The global design criterion is the Frobenius condition number for the Hessian of the least-squares criterion. The approach converts the problem to an optimal control one in which both the control forces of the sensors and the initial sensor positions are optimized. Its solution is obtained by the use of a gradient algorithm which handles various constraints imposed on sensor motions. Among other things, the method copes efficiently with problems involving state constraints induced by collision-avoidance conditions or admissible distances between the sensors. A simple test problem is presented to illustrate the presented ideas.","","0-7803-5655","10.1109/ROMOCO.1999.791086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791086","","Path planning;Sensor systems;Parameter estimation;Pollution measurement;Atmospheric measurements;Air pollution;Optimal control;Transducers;Instruments;Monitoring","distributed parameter systems;parameter estimation;sensors;optimal control;optimisation;least squares approximations;collision avoidance","path planning;parameter estimation;distributed systems;moving sensors;optimal control;gradient algorithm;collision-avoidance;optimisation","","1","17","","","","","","IEEE","IEEE Conferences"
"An algorithm for optimization of reconfiguration of fault tolerant multiprocessor systems","P. Janik; M. Kotocova","Dept. of Comput. Sci. & Eng., Slovak Univ. of Technol., Bratislava, Slovakia; NA","Proceedings of the Sixth Euromicro Workshop on Parallel and Distributed Processing - PDP '98 -","","1998","","","342","348","We have presented an approach for finding an optimal process allocation for processes from a faulty processor in several previous papers (P. Janik and M. Kotocova, 1997; 1996; 1995; P. Janik, 1996). We have built a coloured Petri net model of a reconfigurable fault tolerant multiprocessor system. The model has been used to run many simulations of reconfiguration under different process and processor parameters. The results of simulations gave us a model of behaviour of performance degradation under different process and processor parameters. On this basis, an algorithm for finding an optimal process allocation has been proposed, with regard to performance degradation. We present the algorithm for finding an optimal process allocation from a faulty processor and results of its testing on a 64 transputer machine. The results have shown that, using this algorithm, it is possible to find an optimal or a near to optimal allocation for processes from a faulty processor, with regard to performance degradation.","","0-8186-8332","10.1109/EMPDP.1998.647218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=647218","","Fault tolerant systems;Multiprocessing systems;Degradation;Testing;Topology;Paper technology;Computer science;Application software;Real time systems;Decision making","multiprocessing systems;processor scheduling;resource allocation;fault tolerant computing;reliability;Petri nets;graph colouring;reconfigurable architectures;transputers;transputer systems","fault tolerant multiprocessor systems reconfiguration;optimal process allocation;coloured Petri net model;reconfigurable fault tolerant multiprocessor system;simulations;processor parameters;performance degradation;64 transputer machine;near optimal allocation;faulty processor","","","14","","","","","","IEEE","IEEE Conferences"
"A scalable, loadable custom programmable logic device for solving Boolean satisfiability problems","M. J. Boyd; T. Larrabee","Dept. of Comput. Eng., California Univ., Santa Cruz, CA, USA; NA","Proceedings 2000 IEEE Symposium on Field-Programmable Custom Computing Machines (Cat. No.PR00871)","","2000","","","13","21","This paper introduces ELVIS, a custom PLD that solves Boolean satisfiability (SAT) problems and presents a significant improvement over previous approaches. SAT is a core computer science problem with important commercial applications, which include timing verification, automated layout, logic minimization and test pattern generation. ELVIS is the first massively parallel SAT-solver to support efficient loading of formulas and on-line clause addition with no instance-specific placement or routing. Furthermore, ELVIS requires significantly less hardware capacity than previous approaches. The design is easily scaled; it requires hardware that grows linearly with formula size. As such, it is the first to guarantee polynomial space and time complexity of formula loading. This avoids the laborious (NP-hard) placement and routing of each formula that has plagued previous approaches. The new approach can efficiently support dynamic clause addition, formula partitioning, implication heuristics and an unbounded number of variables per clause. Large scale implementation of these optimizations and modifying ELVIS to realize a multi-chip board design are the goals of future research.","","0-7695-0871","10.1109/FPGA.2000.903388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=903388","","Programmable logic devices;Routing;Hardware;Computer science;Application software;Timing;Logic devices;Logic testing;Minimization;Test pattern generators","computability;programmable logic devices;parallel architectures","scalable loadable custom programmable logic device;Boolean satisfiability problems;ELVIS;custom PLD;timing verification;automated layout;logic minimization;test pattern generation;massively parallel SAT-solver;online clause addition;time complexity;space complexity;formula loading;dynamic clause addition;formula partitioning;implication heuristics;multi-chip board design","","6","25","","","","","","IEEE","IEEE Conferences"
"General Purpose Enterprise Simulation with Master","W. Bernhard; M. C. Bettoni","Basel Institute of Technology, CIM-Zentrum Muttenz, Switzerland; NA","Proceedings of 1993 Winter Simulation Conference - (WSC '93)","","1993","","","1290","1295","This article describes the concept of a knowledge-based assistant (MASTER) for speeding up simulation projects. It is able to simulate and optimise whole enterprises of the types: discrete part-manufacturing, continuous process-manufacturing and service-companies. It consists of well proven, powerful software-packages, all of them are commercially available. These are the expert-system building tool ""KEE"", the simulation language ""SIMSCRIPT"", the animation-package ""SIMGRAPHICS"" and the spreadsheet-package are ""LOTUS-123"". The packages embedded in a UNIX environment. At present, this system is being developed and carries the name MASTER which stands for ""Muttenzer Assistant for Simulation Tasks with Expert Reasoning"".","","0-7803-1381","10.1109/WSC.1993.718393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=718393","","Power system modeling;Animation;Packaging;Companies;Analytical models;Flow production systems;Computer integrated manufacturing;Software packages;Software testing;Computer languages","","","","2","10","","","","","","IEEE","IEEE Conferences"
"Elastic registration of MRI scans using fast DCT","T. Eric; B. Jean-Yves","Fac. de Med., ERIM, France; NA","Proceedings of the 22nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (Cat. No.00CH37143)","","2000","4","","2854","2856 vol.4","Analysis and comparison of functional SPECT brain scans from different subjects require a common space of study. For this purpose, an elastic matching between a MRI of reference and the MRI of the patient can be performed. A variant of a voxel-based method initially created in SPM software was implemented. A deformation map is defined as a linear combination of basis functions of the discrete cosine transform. The coefficients of this combination are the parameters of the transformation. They are optimized to minimize a cost function which represents the distance between the reference and the deformed volume. A multi-parameters and multi-resolutions scheme, combined with a conjugate gradient method, completes the optimization. A fast discrete cosine transform is used for efficient computations of the cost function and its gradient. This algorithm has been tested with 5 MRI volumes of 1283 voxels, elastically matched on a sixth MRI volume and the results show improvement upon simple nine parameters registration.","1094-687X","0-7803-6465","10.1109/IEMBS.2000.901463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=901463","","Magnetic resonance imaging;Discrete cosine transforms;Cost function;Scanning probe microscopy;Image resolution;Gradient methods;Optimization methods;Testing;Pathology;Alzheimer's disease","biomedical MRI;medical image processing;image registration;discrete cosine transforms;image matching;conjugate gradient methods;brain","MRI scans;elastic registration;fast DCT;elastic matching;voxel-based method;deformation map;linear combination;basis functions;cost function;multiresolutions scheme;multiparameters scheme;conjugate gradient method;nine parameters registration","","3","3","","","","","","IEEE","IEEE Conferences"
"Effective reformulations for task allocation in distributed systems with a large number of communicating tasks","S. Menon","Sch. of Manage., Texas Univ., Dallas, TX, USA","IEEE Transactions on Knowledge and Data Engineering","","2004","16","12","1497","1508","In any distributed processing environment, decisions need to be made concerning the assignment of computational task modules to various processors. Many versions of the task allocation problem have appeared in the literature. Intertask communication makes the assignment decision difficult; capacity limitations at the processors increase the difficulty. This problem is naturally formulated as a nonlinear integer program, but can be linearized to take advantage of commercial integer programming solvers. While traditional approaches to linearizing the problem perform well when only a few tasks communicate, they have considerable difficulty solving problems involving a large number of intercommunicating tasks. This paper introduces new mixed integer formulations for three variations of the task allocation problem. Results from extensive computational tests conducted over real and generated data indicate that the reformulations are particularly efficient when a large number of tasks communicate, solving reasonablylarge problems faster than other exact approaches available.","1041-4347;1558-2191;2326-3865","","10.1109/TKDE.2004.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1350761","Index Terms- Task allocation;nonlinear optimization;integer programming.","Distributed computing;Distributed processing;Peer to peer computing;Linear programming;Costs;Application software;Genetic algorithms;Testing;Computer applications;Banking","resource allocation;peer-to-peer computing;nonlinear programming;integer programming","distributed processing;task allocation;intertask communication;problem solving;nonlinear optimization;integer programming","","8","26","","","","","","IEEE","IEEE Journals & Magazines"
"Extracting isovolumes from three-dimensional torso geometry using PROLOG","J. A. Replogle; D. J. Russomanno; A. L. De Jongh; F. J. Claydon","Dept. of Biomed. Eng., Univ. of Memphis, TN, USA; NA; NA; NA","IEEE Transactions on Information Technology in Biomedicine","","1998","2","1","10","19","Three-dimensional (3D) finite element torso models are widely used to simulate defibrillation field quantities, such as potential, gradient and current density. These quantities are computed at spatial nodes that comprise the torso model. These spatial nodes typically number between 10/sup 5/ and 10/sup 6/, which makes the comprehension of torso defibrillation simulation output difficult. Therefore, the objective of this study is to rapidly prototype software to extract a subset of the geometric model of the torso for visualization in which the nodal information associated with the geometry of the model meets a specified threshold value (e.g., minimum gradient). The data extraction software is implemented in PROLOG, which is used to correlate the coordinate, structural and nodal data of the torso model. A PROLOG-based environment has been developed and is used to rapidly design and test new methods for sorting, collecting and optimizing data extractions from defibrillation simulations in a human torso model for subsequent visualization.","1089-7771;1558-0032","","10.1109/4233.678528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=678528","","Torso;Geometry;Defibrillation;Data mining;Computational modeling;Solid modeling;Data visualization;Finite element methods;Current density;Software prototyping","computational geometry;solid modelling;data visualisation;PROLOG;medical computing;finite element analysis;software prototyping;sorting","isovolume extraction;three dimensional torso geometry;PROLOG;3D finite element torso models;defibrillation field quantities;potential;gradient;current density;spatial nodes;torso defibrillation simulation;rapid prototyping;geometric model;data visualization;threshold value;data extraction software;sorting","Algorithms;Computer Simulation;Humans;Models, Anatomic;Thorax","3","12","","","","","","IEEE","IEEE Journals & Magazines"
"Automation of a mass spectrometer and extraction line for the determination of noble gas isotopes in rocks","R. Singer; J. Curtice; M. Kurz; D. Lott","Woods Hole Oceanogr. Instn., MA, USA; NA; NA; NA","OCEANS 2000 MTS/IEEE Conference and Exhibition. Conference Proceedings (Cat. No.00CH37158)","","2000","1","","119","122 vol.1","This paper describes the automation of an extraction line and mass spectrometer for the determination of noble gas isotope concentrations in rock and mineral samples, housed in the Isotope Geochemistry Facility at Woods Hole Oceanographic Institution (WHOI). The gas extraction occurs in an automated ultra-high-vacuum system designed and fabricated at WHOI. The noble gases of interest are extracted in vacuo by crushing or high temperature fusion, separated using a cryogenic charcoal trap (operated between 70 and 3000 K), and delivered to a Mass Analyzer Products 215-50 mass spectrometer for measurement of isotopic composition. The system is controlled by a PC running under the Windows NT operating system. The software, largely written in Visual Basic and developed at WHOI, provides for synchronization of the extraction process with the mass spectrometer measurements. The extraction line control software is written so that the user can easily modify and customize the procedures, and allows automated operation. The extraction line software also provides a test scheduler, with building blocks which allow control of the system hardware by the computer without programming for modification and extension of the system by the operators. Software on a second PC controls the operation of the mass spectrometer over an IEEE-488 (GPIB) bus. This code was modified at WHOI to provide for the synchronization of the mass spectrometer with the extraction line and to optimize the determination of the measured gas concentrations. The system is primarily used to make helium and neon isotope measurements, in full 24 hour automation mode, but has also been designed and tested for Ar and Xe isotopes. This system is devoted to the measurement of both mantle and cosmic-ray-produced noble gas isotopes.","","0-7803-6551","10.1109/OCEANS.2000.881244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881244","","Automation;Mass spectroscopy;Isotopes;Sea measurements;Automatic control;Control systems;System testing;Minerals;Gases;Ocean temperature","geophysical equipment;geochemistry;isotope relative abundance;mass spectroscopic chemical analysis;inert gases;oceanographic equipment","geophysical equipment;geochemistry;isotope ratio;mass spectroscopy;chemical analysis;automation;rock;mass spectrometer;extraction line;noble gas;inert gas;mineral sample;Isotope Geochemistry Facility;Woods Hole Oceanographic Institution;WHOI;gas extraction;isotopic composition;test scheduler;ocean","","","2","","","","","","IEEE","IEEE Conferences"
"Intelligent, adaptive file system policy selection","T. M. Madhyastha; D. A. Reed","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; NA","Proceedings of 6th Symposium on the Frontiers of Massively Parallel Computation (Frontiers '96)","","1996","","","172","179","Traditionally, maximizing input/output performance has required tailoring application input/output patterns to the idiosyncrasies of specific input/output systems. The authors show that one can achieve high application input/output performance via a low overhead input/output system that automatically recognizes file access patterns and adaptively modifies system policies to match application requirements. This approach reduces the application developer's input/output optimization effort by isolating input/output optimization decisions within a retargetable file system infrastructure. To validate these claims, they have built a lightweight file system policy testbed that uses a trained learning mechanism to recognize access patterns. The file system then uses these access pattern classifications to select appropriate caching strategies, dynamically adapting file system policies to changing input/output demands throughout application execution. The experimental data show dramatic speedups on both benchmarks and input/output intensive scientific applications.","1088-4955","0-8186-7551","10.1109/FMPC.1996.558076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558076","","Intelligent systems;Adaptive systems;File systems;Application software;Pattern recognition;System testing;Contracts;Automatic testing;Computer science;Pattern matching","input-output programs","intelligent adaptive file system policy selection;maximized input/output performance;application input/output patterns;application input/output performance;low overhead input/output system;automatic file access pattern recognition;application requirements;input/output optimization decisions;retargetable file system infrastructure;file system policy testbed;trained learning mechanism;access pattern classifications;caching strategies;dynamically adapted file system policies;changing input/output demands;application execution;speedups;input/output intensive scientific applications;benchmarks","","13","12","","","","","","IEEE","IEEE Conferences"
"On finding the optimal genetic algorithms for robot control problems","J. T. Alander","Inst. of Ind. Autom., Helsinki Univ. of Technol., Espoo, Finland","Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91","","1991","","","1313","1318 vol.3","Describes a C++ package used to analyze a class of genetic algorithms. The parameters of the best genetic algorithms have been searched by a genetic algorithm. The ultimate goal of the work is to find out if it would be possible to utilize genetic algorithm techniques in certain difficult and complex robot control problems, such as task planning, adaptation, error detection, and recovery to create a flexible robot control system. A traveling salesman type problem is used as a test and an example of applications of genetic algorithms. The sequence coding problem is solved by using link sets, which allows a pure genetic algorithm approach. This preserves a clear separation between the problem solved and the genetic algorithm itself.<<ETX>>","","0-7803-0067","10.1109/IROS.1991.174685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=174685","","Genetic algorithms;Robot control;Testing;Neural networks;Packaging;Algorithm design and analysis;Biology computing;Genetic mutations;Problem-solving;Robotics and automation","control system analysis computing;genetic algorithms;robots;software packages","travelling salesman problem;optimisation;software packages;robot control;C++;genetic algorithms;sequence coding;link sets","","7","30","","","","","","IEEE","IEEE Conferences"
"A task space redundancy-based scheme for motion planning","Yixin Chen; J. E. McInroy","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; NA","Proceedings of the 2003 American Control Conference, 2003.","","2003","4","","3435","3441 vol.4","In many applications, the manipulations require only part of the degrees of freedom (DOFs) of the end-effector, or some DOFs are more important than the rest. We name these applications prioritized manipulations. The end-effectors DOFs are divided into those which are critical and must be controlled as precisely as possible, and those which have loose specifications, so their tracking performance can traded-off to achieve other needs. In this paper, we derive a formulation for partitioning the task space into major and secondary task directions and finding the velocity and static force mappings that precisely accomplish the major task and locally optimize some secondary goals. The techniques are tested on a 6-DOF parallel robot performing a 2-DOF tracking task.","0743-1619","0-7803-7896","10.1109/ACC.2003.1244063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244063","","Welding;Manipulators;Cameras;Application software;Redundancy;Trajectory;Computer science;Testing;Parallel robots;Performance evaluation","path planning;redundant manipulators;redundancy;end effectors;tracking;velocity;force","task space redundancy-based scheme;motion planning;degrees of freedom;end-effector DOF;prioritized manipulation;tracking performance;velocity;static force mapping;6-DOF parallel robot;2-DOF tracking task;secondary task direction","","","20","","","","","","IEEE","IEEE Conferences"
"Automating relational operations on data structures","D. Cohen; N. Campbell","USC Inf. Sci. Inst., Marina del Rey, CA, USA; USC Inf. Sci. Inst., Marina del Rey, CA, USA","IEEE Software","","1993","10","3","53","60","An approach to having compilers do most of the implementation detail work in programming that divides a program into two parts is described. The specification part describes what the program should do, but in a way that avoids commitment to implementation details. The annotation part provides implementation instructions that the compiler will carry out. Annotations affect execution efficiency, but not functional behavior. They are very high level and usually very short and hence encourage experimentation. To try out different implementation choices, the programmer simply changes the annotations and recompiles. The implementation details related to data representations are discussed. The testing of compilers that produce Lisp code for iteration, and for adding and deleting tuples of composite relations, is reviewed.<<ETX>>","0740-7459;1937-4194","","10.1109/52.210604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210604","","Data structures;Programming profession;Libraries;Design optimization;Logic programming;Typesetting;Relational databases;Optimizing compilers;Data models","automatic programming;entity-relationship modelling;formal specification;program compilers;relational databases","relational operations;data structures;compilers;programming;specification;annotation;Lisp code;iteration;tuples","","4","4","","","","","","IEEE","IEEE Journals & Magazines"
"Embedded tools for a configurable and customizable DSP architecture","C. Liem; F. Breant; S. Jadhav; R. O'Farrell; R. Ryan; O. Levia","Improv Syst., San Jose, CA, USA; Improv Syst., San Jose, CA, USA; Improv Syst., San Jose, CA, USA; Improv Syst., San Jose, CA, USA; Improv Syst., San Jose, CA, USA; Improv Syst., San Jose, CA, USA","IEEE Design & Test of Computers","","2002","19","6","27","35","This embedded tool suite lets users make architectural changes to a programmable DSP core on three levels and supports designer-defined instructions and computation units. The entire system is based on configurability through a file-based resource description that drives all the design tools.","0740-7475;1558-1918","","10.1109/MDT.2002.1047741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047741","","Digital signal processing;Computer architecture;Computer interfaces;Computer aided instruction;Space exploration;Application software;Debugging;Embedded computing;High level languages;Optimizing compilers","shared memory systems;embedded systems;signal processing","configurable customizable DSP architecture;embedded tool suite;architectural changes;programmable DSP core;designer-defined instructions;computation units;file-based resource description","","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesis of application-specific highly-efficient multi-mode systems for low-power applications","Lih-Yih Chiou; S. Bhunia; K. Roy","Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA; Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA; Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","96","101","We present a novel design methodology for synthesizing multiple configurations (or modes) into a single programmable system. Many DSP and multimedia applications require reconfigurability of a system along with efficiency in terms of power performance and area. FPGAs provide a reconfigurable platform, however, they are slower in speed with significantly higher power consumption than achievable by a customized ASIC. In this work, we have developed techniques to realize an efficient reconfigurable system for a set of user-specified configurations. A data flow graph transformation method coupled with efficient scheduling and allocation are used to automatically synthesize the system from its behavioral level specifications. Experimental results on several applications demonstrate that we can achieve about 60/spl times/ power reduction on average with about 4/spl times/ improvement in performance over corresponding FPGA implementations.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253593","","Field programmable gate arrays;Application specific integrated circuits;Application software;Digital signal processing;Hardware;Design methodology;Multimedia systems;Energy consumption;Design optimization;Logic arrays","reconfigurable architectures;logic design;logic simulation;application specific integrated circuits;integrated circuit design;low-power electronics;graph grammars;processor scheduling","application-specific multimode systems;highly-efficient multi-mode systems;low-power applications;multiple configuration system;DSP applications;multimedia applications;reconfigurable systems;power efficiency;area efficiency;ASIC;data flow graph transformation;scheduling;allocation;behavioral level specifications;power reduction","","2","10","","","","","","IEEE","IEEE Conferences"
"Thermology in the 21st century-the biomedical future of a technology based on defense oriented engineering","M. Anbar","Sch. of Med. & Biomed. Sci., State Univ. of New York, Buffalo, NY, USA","Proceedings of 16th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","1994","1","","A77","A84 vol.1","The development of clinical applications based on infrared telethermometry of human skin has been slower than anticipated from a risk free. Inexpensive technique that is applicable to a very large variety of clinical situations. In addition to expected antagonism from older technologies, there are three major reasons for this. First, in spite of offering quantitative information, most thermological tests have remained qualitative and subjective. Second, it has not been fully realized that abnormal thermal behavior, depicted in thermal imaging, often represents a physiological rather than an anatomical dysfunction. Most of the diagnostic information exists, therefore, in the time domain, rather than in the spatial distribution of temperature. A dynamic approach is needed, therefore, to study thermoregulatory dysfunction. Third, relatively little attention was given to mechanisms of physiological dysfunctions that manifest thermal abnormalities. Once these shortcomings are realized, thermological testing can be substantially improved by optimizing the testing conditions, including the hardware and software used. These points are illustrated by examining the case of breast cancer hyperthermia. It is concluded that breast-cancer-induced hyperthermia involves a thermoregulatory dysfunction rather than hypermetabolism or hypervascularization. Consequently, breast cancer induced hyperthermia is expected to be associated with a characteristic dynamic thermal behavior. To make them more sensitive and specific, screening tests for breast cancer must be substantially changed, including the equipment, the software and the interpretation of the thermal data. Following the same rationale, quantitative and dynamic telethermometry is expected to be extensively used in the diagnosis and management of diabetes mellitus, liver disease, arthritis, dermatology, neonatology, and neurological disorders. All of which involve thermoregulatory dysfunctions, in addition to open heart surgery, kidney transplant vascular and reconstructive surgery, where telethermometry provides real time information on perfusion or reperfusion.<<ETX>>","","0-7803-2050","10.1109/IEMBS.1994.412154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=412154","","Software testing;Breast cancer;Hyperthermia;Surgery;Humans;Skin;Temperature distribution;Hardware;Diabetes;Liver diseases","infrared imaging;hyperthermia;reviews;temperature measurement;skin","21st century thermology;biomedical technology future;defense oriented engineering;infrared telethermometry;human skin;diagnostic information;physiological dysfunctions mechanisms;breast cancer hyperthermia;hypermetabolism;hypervascularization;characteristic dynamic thermal behavior;screening tests;diabetes mellitus;liver disease;open heart surgery;kidney transplant surgery;telethermometry;real time information;perfusion;reperfusion;dermatology;arthritis","","2","26","","","","","","IEEE","IEEE Conferences"
"On the theory of patching","Z. Pap; G. Csopaki; S. Dibuz","Dept. of Telecommun. & Media Informatics, Budapest Univ., Hungary; Dept. of Telecommun. & Media Informatics, Budapest Univ., Hungary; NA","Third IEEE International Conference on Software Engineering and Formal Methods (SEFM'05)","","2005","","","263","271","We study the problem of patching, i.e., modifying the behavior of an existing system. We consider systems modelled as finite state machines (FSMs), and define edit operators for them based on a traditional fault model. We argue that sequences of edit operations can be considered as models of patches defining modifications to an FSM system. We utilize recent results in graph matching theory as mathematical foundations. We introduce a new type of problem which we call the optimal patch or optimal update problem: given an FSM M modeling the behavior of an existing system and an other machine M' modeling a new design, find an optimal patch, i.e., the edit operations changing M to M' that are minimal according to a given cost function associated with the edit operations. We analyze the complexity of the problem, and conclude that it is unlikely to have a polynomial time solution for it. We also show that the problem can be easily transformed to a state-space search problem, for which many heuristic approximation algorithms have been developed.","1551-0255;2160-7656","0-7695-2435","10.1109/SEFM.2005.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575916","finite state machine;patching;update;optimal patch;edit operations;edit distance","Automata;Informatics;Cost function;Polynomials;Search problems;Approximation algorithms;Heuristic algorithms;Computer science;Testing;Computer errors","finite state machines;graph theory;search problems;optimisation;computational complexity;formal specification","patching problem;finite state machines;FSM;edit operation;graph matching theory;optimal update problem;state-space search problem;heuristic approximation algorithms","","2","28","","","","","","IEEE","IEEE Conferences"
"Performance evaluation studies of client-server models using SPEC Web99 benchmarks","A. Godbole; Seung-yun Kim; R. Guzman; W. W. Smari","Electr. & Comput. Eng. Dept., Dayton Univ., OH, USA; Electr. & Comput. Eng. Dept., Dayton Univ., OH, USA; Electr. & Comput. Eng. Dept., Dayton Univ., OH, USA; Electr. & Comput. Eng. Dept., Dayton Univ., OH, USA","Proceedings Fifth IEEE Workshop on Mobile Computing Systems and Applications","","2003","","","406","414","The World Wide Web (WWW) has experienced tremendous growth over the past few years. This increase in use makes server load heavier and leads to more system-bound queries due to newly sought emerging Web techniques, such as information discovery and reuse, mining and fusion. There is an obvious interest in assessing the consequences of this growth and understanding the system's behavior under varying conditions and workloads. The overall performance of the WWW is affected by various criteria in terms of clients, servers, and the network. Measuring performance of a computer system is a complex, difficult and important task. Time and rates are usually the basic measures of system performance. Having more realistic data about system performance will be necessary in the design and development of integrated information systems and systems for reuse. The main objective of this work is to measure the performance of various Web-based computer systems and Web servers using the SPEC Web99 benchmarking toolset. In the experiments that we carried out, the server is set up using freely available Apache server software run on a Sun Sparc-Ultra 80 machine, and six Windows-based clients that are used to load the server using the benchmark suite. First, the effects of varying resources, such as memory sizes and processors, on server's performance are measured and analyzed. Then, we tested and evaluated the effects of six different clients and workload parameters on the server configuration. Amongst the six experiments that were performed, two were set up in order to test the client-side. Some of the parameters varied were memory sizes, number of processors, the workload, and the benchmark parameters. Some interesting results were obtained while others were as expected. The results indicate that good improvements can be achieved if workload and system resources are carefully chosen to optimize metrics of interest.","","0-7803-8242","10.1109/IRI.2003.1251444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251444","","World Wide Web;Web server;System performance;Benchmark testing;Web sites;Network servers;Time measurement;Information systems;Sun;Size measurement","performance evaluation;file servers;Internet;benchmark testing;Web sites;information systems;client-server systems","performance evaluation;client-server models;SPEC Web99 benchmarks;World Wide Web;server load;system-bound queries;Web techniques;information discovery;information reuse;information mining;information fusion;system performance;integrated information systems;Web-based computer systems;Web servers;Apache server software;Sun Sparc-Ultra 80 machine;Windows-based clients;workload parameters;server configuration;memory sizes;Web computing","","1","14","","","","","","IEEE","IEEE Conferences"
"Characterizing efficiency of human robot interaction: a case study of shared-control teleoperation","J. W. Crandall; M. A. Goodrich","Dept. of Comput. Sci., Brigham Young Univ., Provo, UT, USA; Dept. of Comput. Sci., Brigham Young Univ., Provo, UT, USA","IEEE/RSJ International Conference on Intelligent Robots and Systems","","2002","2","","1290","1295 vol.2","Human-robot interaction is becoming an increasingly important research area. In this paper, we present a theoretical characterization of interaction efficiency with an aim towards designing a human-robot system with adjustable robot autonomy. In our approach, we analyze how modifying robot control schemes for a given autonomy mode can increase system performance and decrease workload demands on the human operator. We then perform a case study of the design of a shared-control teleoperation scheme and compare interaction efficiency against a traditional manual-control teleoperation scheme.","","0-7803-7398","10.1109/IRDS.2002.1043932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1043932","","Human robot interaction;Computer aided software engineering;Supervisory control;Jacobian matrices;Computer science;Robot control;Degradation;Communication standards;Delay effects;System testing","telerobotics;man-machine systems;user interfaces;interactive systems;optimisation","human robot interaction;robot autonomy;shared-control;interface loop;teleoperation;optimisation","","49","8","","","","","","IEEE","IEEE Conferences"
"Best practices for a FRACAS implementation","E. J. Hallquist; T. Schick","Relex Software Corp., Greensurgh, PA, USA; Relex Software Corp., Greensurgh, PA, USA","Annual Symposium Reliability and Maintainability, 2004 - RAMS","","2004","","","663","667","Many companies use a FRACAS (failure reporting analysis and corrective action system) process, better known as a closed-loop analysis and correction action process, to track and report problems or failures. Very few companies, however, fully realize all of the possible benefits of a FRACAS process, such as improving quality and productivity while reducing costs. Although many issues may prevent an effective implementation, there are three areas of concern that may cause negative impacts irrespective of the best-intentioned technology. In particular, complex organization interaction, inefficient and ineffective data tracking, and a lack of prioritized goals prevent the dramatic results that can be achieved with a FRACAS. By following the suggested eight step methodology and best practices presented, the potential for implementing a high-performance FRACAS can be greatly increased.","","0-7803-8215","10.1109/RAMS.2004.1285523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285523","","Best practices;Guidelines;Failure analysis;Databases;Testing;Data engineering;Reliability engineering;Productivity;Costs;Product development","failure analysis;closed loop systems","failure reporting analysis;corrective action system;closed-loop analysis;complex organization interaction;data tracking","","","2","","","","","","IEEE","IEEE Conferences"
"A simple genetic algorithm applied to discontinuous regularization","J. B. Jensen; M. Nielsen","Dept. of Comput. Sci., Copenhagen Univ., Denmark; Dept. of Comput. Sci., Copenhagen Univ., Denmark","Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop","","1992","","","69","78","A simple genetic algorithm without mutation has been applied to discontinuous regularization. The relative slope of the energy-to-fitness function has been introduced as a measure of the rate of convergence. The intuitively better rate of convergence (slow in the beginning, faster in the end) has been shown to be superior to an exponential transformation-function in the present case. A probabilistic model of the performance of the algorithm has been introduced. From this model it has been found that a division into subpopulations decreases the performance, unless more than one computer is available.<<ETX>>","","0-7803-0557","10.1109/NNSP.1992.253706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=253706","","Genetic algorithms;Stochastic processes;Simulated annealing;Genetic mutations;Convergence;Testing;Computer science;Energy measurement;Application software;Computational modeling","convergence of numerical methods;genetic algorithms;probability","convergence rate;optimisation;simple genetic algorithm;discontinuous regularization;energy-to-fitness function;probabilistic model","","1","9","","","","","","IEEE","IEEE Conferences"
"Specification criticism via goal-directed envisionment","K. Downing; S. Fickas","Dept. of Comput. Sci., Linkoping Univ., Sweden; NA","Proceedings of the Sixth International Workshop on Software Specification and Design","","1991","","","22","30","Validating a complex system specification is a difficult problem. Generating behaviors and using them to critique a specification is one effective approach. Up until now, symbolic evaluation has been the key technique of behavior generation. Unfortunately, it has drawbacks both in the amount of time it takes to complete a symbolic run, and in the large amount of uninteresting data it produces. The authors propose goal-directed envisionment as an alternative to symbolic evaluation, supplementing the basic envisioning techniques of qualitative physics with behavioral goals. This approach overcomes the problems of symbolic evaluation by generating interpretations in a reasonable amount of time and by exploiting goals to prioritize and analyze the interpretations. The authors describe and evaluate SC, an implemented system which employs goal-directed envisionment to critique specifications.<<ETX>>","","0-8186-2320","10.1109/IWSSD.1991.213080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213080","","Automatic testing;Computer science;Physics;Interconnected systems;Buildings;Robustness;Feathers;Resource management;System testing;Storage automation","formal specification","specification criticism;complex system specification;symbolic evaluation;key technique;behavior generation;symbolic run;uninteresting data;goal-directed envisionment;basic envisioning techniques;qualitative physics;behavioral goals;SC;implemented system","","2","16","","","","","","IEEE","IEEE Conferences"
"Design, fabrication and measurement of an encapsulated inverted F dual band antenna for the gathering of data on seals at sea using SMS over a GSM system","J. Winkle; R. M. Edwards; B. Chambers; B. McConnell; E. Bryant","Univ. of Sheffield, UK; Univ. of Sheffield, UK; Univ. of Sheffield, UK; NA; NA","Twelfth International Conference on Antennas and Propagation, 2003 (ICAP 2003). (Conf. Publ. No. 491)","","2003","2","","739","742 vol.2","In this paper a novel encapsulated dual band planar inverted F antenna is presented designed for the specific application of gathering data on seals at sea. A mobile phone module with the above antenna is encased in epoxy resin and affixed to a seal for transmission of sensor data via the short message service over GSM in both European frequency bands. Results are presented for a prototype both in anechoic facilities and affixed to a seal at sea. Finite element software is used for optimisation.","","0-85296-752","10.1049/cp:20030182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1353751","","","mobile antennas;mobile radio;UHF antennas;microstrip antennas;antenna radiation patterns;finite element analysis;radiotelemetry;marine telemetry;multifrequency antennas;antenna testing;zoology","encapsulated dual band planar inverted F antenna;seals;mobile phone module;sensor data;short message service;GSM;finite element software;telemetry;800 MHz;1800 MHz","","","","","","","","","IET","IET Conferences"
"Enhanced chemical etching and optical observation: a quality analysis technique for industrial SIMOX production","A. Garcia; B. Aspar; J. Margail; C. Pudda","LETI, CEA, Centre d'Etudes Nucleaires, de Grenoble, France; LETI, CEA, Centre d'Etudes Nucleaires, de Grenoble, France; LETI, CEA, Centre d'Etudes Nucleaires, de Grenoble, France; LETI, CEA, Centre d'Etudes Nucleaires, de Grenoble, France","Proceedings of 1993 IEEE International SOI Conference","","1993","","","42","43","SIMOX is a well developed process for producing SOI materials. However, for some applications the top silicon layer still needs crystalline quality improvements. At present the density of threading dislocations on typical SIMOX materials (1.8x10/sup 18/O/sup +/cm/sup -2/) is about 10/sup 5/cm/sup -2/ for single implantation and about 10/sup 4/cm/sup -2/ for multi-implantations. Due to the very small thickness of the top silicon layer a two step etching procedure using SECCO etching and bright field optical observations has been used to determine the dislocation density. A four step procedure which allows the transfer of the dislocation etch pits into the bulk was developed to increase the contrast between dislocation etch pits and the substrate. In this work, we describe an optimized enhanced chemical etching process. It allows etch pit observations using an optical microscope and automatic counting by the use of image processing software. This technique can be used for quality analysis and is ""operator free"".<<ETX>>","","0-7803-1346","10.1109/SOI.1993.344601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=344601","","Chemical analysis;Chemical industry;Etching;Optical microscopy;Silicon on insulator technology;Scanning electron microscopy;Crystalline materials;Optical materials;Image processing;Hafnium","etching;optical microscopy;quality control;SIMOX;integrated circuit manufacture;semiconductor-insulator boundaries;silicon;dislocation density;integrated circuit technology;ion implantation;production testing","enhanced chemical etching;optical observation;quality analysis technique;industrial SIMOX production;SOI materials;crystalline quality;threading dislocations;multi-implantations;two step etching procedure;SECCO etching;dislocation density;dislocation etch pits;optical microscope;image processing software;Si","","","5","","","","","","IEEE","IEEE Conferences"
"Application of PLC for on-line monitoring of power transformers","B. Georges; J. Aubin","GE Syprotec, Pointe-Claire, Que., Canada; NA","2001 IEEE Power Engineering Society Winter Meeting. Conference Proceedings (Cat. No.01CH37194)","","2001","2","","483","486 vol.2","GE Syprotec has designed and manufactured a PLC based transformer monitoring and control system (TMCS). The TMCS is designed for permanent installation on or close to a specific transformer and is intended for monitoring a single unit. It also has the capability to provide cooling system control of the transformer. It may be part of a substation or a plant-wide, multi-unit network system (LAW) that provides input to a central control or DCS system. The TMCS can also give access to a Web page where the current performances of the transformer can be monitored. Through the collection, archiving, processing, analysis and interpretation of the data automatically monitored, the TMCS is designed to manage the operational life and to optimize the performance of the transformer. The processing make use of mathematical models which are computation based on industry recognized calculation methods. The data collected and archived as well as the diagnostics and recommendations resulting from data interpretation are available to end users at selected locations using an appropriate remote monitoring and control software. The TMCS may be used as a part of a global transformer substation monitoring system, or as a fully autonomous transformer monitoring unit consulted by the end user through a telephone line.","","0-7803-6672","10.1109/PESW.2001.916893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916893","","Programmable control;Power transformers;Control systems;Remote monitoring;Substations;Manufacturing;Cooling;Temperature control;Centralized control;Automatic control","power transformer testing;computerised monitoring;programmable controllers;transformer substations","on-line monitoring;power transformers;PLC;GE Syprotec;cooling system control;substation;plant-wide multi-unit network system;Web page access;transformer performance monitoring;operational life management;mathematical models;control software;global transformer substation monitoring system;autonomous transformer monitoring unit","","2","","","","","","","IEEE","IEEE Conferences"
"Evaluation of scheduling techniques on a SPARC-based VLIW testbed","Seongbae Park; SangMin Shim; Soo-Mook Moon","Sch. of Electr. Eng., Seoul Nat. Univ., South Korea; NA; NA","Proceedings of 30th Annual International Symposium on Microarchitecture","","1997","","","104","113","The performance of Very Long Instruction Word (VLIW) microprocessors depends on the close cooperation between the compiler and the architecture. This paper evaluates a set of important compilation techniques and related architectural features for VLIW machines. The evaluation is performed on a SPARC-based VLIW testbed where gcc-generated optimized SPARC code is scheduled into high-performance VLIW code. As a base scheduling compiler, we experiment with three core scheduling techniques including enhanced pipeline scheduling, all-path speculation, and renaming. We analyze the characteristics of the useful and useless ALUs in each cycle to see how many of those ALUs execute non-speculative operations, speculative operations, and copies, respectively. Then, we evaluate the following compilation techniques: software pipelining, loop unrolling, non-greedy enhanced pipeline scheduling, profile-based all-path speculation, trace-based speculation, renaming, restricted speculative loads, and memory disambiguation. Since we experiment on a uniform testbed based on a detailed analysis of ALUs, our evaluation provides an useful insight on the performance impact of these techniques.","1072-4451","0-8186-7977","10.1109/MICRO.1997.645802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=645802","","VLIW;Testing;Pipeline processing;Processor scheduling;Moon;Microprocessors;Performance evaluation;Optimization methods","parallel machines;parallel architectures;instruction sets;scheduling;performance evaluation;program compilers;pipeline processing;program control structures","scheduling techniques;SPARC-based VLIW testbed;performance;Very Long Instruction Word microprocessors;VLIW microprocessors;compiler;computer architecture;gcc-generated optimized SPARC code;high-performance VLIW code;scheduling compiler;copies;all-path speculation;renaming;nonspeculative operations;speculative operations;memory disambiguation;software pipelining;loop unrolling;nongreedy enhanced pipeline scheduling;profile-based all-path speculation;trace-based speculation;restricted speculative loads","","5","16","","","","","","IEEE","IEEE Conferences"
"Power R&amp;D: more watts, but less to pay","M. Drummond; J. d. M. M. Neto; M. P. Meirelles","Cepel, Brazil; Cepel, Brazil; Cepel, Brazil","IEEE Spectrum","","1996","33","7","60","62","Thousands of kilometers separate the hydroelectric sources on the Amazon from main load centers far to the south. If Brazil is to transmit many megawatts of power over such distances, it must develop technology that is more efficient and less costly than what is in use today. For this and other pressing needs in the power field, the country's federally owned holding company in charge of power generation and transmission, Electrobras, turns to the country's leading power R&D center, Cepel. Both organizations are based in Rio de Janeiro. This paper details four of Cepel's many projects which are of particularly broad interest. One is an optimized design of conductor bundles for AC transmission lines, Another is a test station for DC equipment up to 800 kV. Then there is software for the stability analysis of large transmission systems, which may involve thousands of buses and branches. Fourth and last, an inexpensive electronic ampere-hour meter is being developed for customers with low incomes, who seldom use more than 100 kWh per month.","0018-9235;1939-9340","","10.1109/6.526868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526868","","Research and development;Power transmission lines;Voltage;Conductors;Costs;Design optimization;System testing;Electric fields;HVDC transmission;Pressing","power transmission;research initiatives;project engineering;power transmission lines;power system analysis computing;power system stability;transmission networks","Brazil;hydroelectric sources;R&D;research efforts;long-distance power transmission;Cepel;projects;conductor bundles;AC transmission lines;DC equipment;transmission system stability analysis","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Enhanced TCP performance over Mobile IPv6: innovative fragmentation avoidance and adaptive routing techniques","A. D. Pramil; S. Antoine; A. H. Aghvami","Centre for Telecommun. Res., London Univ., UK; Centre for Telecommun. Res., London Univ., UK; Centre for Telecommun. Res., London Univ., UK","First IEEE Consumer Communications and Networking Conference, 2004. CCNC 2004.","","2004","","","175","180","Mobile Internet Protocol version 6 (MIPv6) is a proposal for handling the routing of IPv6 packets to mobile nodes in a foreign network. There are two methods for routing of IPv6 packets from the correspondent node (CN) to the mobile node (MN): triangular routing (TR) and route optimisation (RO). With MIPv6 RO, the CN uses the care-of address (CoA) as the destination address to transfer packets to the MN. Hence the MIPv6 module of the CN adds the mobility routing header (MRH) to store the home address of the MN. This increases packet sizes and may result in fragmentation. Fragmentation degrades transmission control protocol (TCP) performance over MIPv6. The paper discusses the cost of fragmentation and innovative fragmentation avoidance techniques. The cost of fragmentation is studied by carrying out experiments in a real IPv6 backbone (6Bone) where it is found that improved performance can be obtained by implementing innovative fragmentation avoidance techniques in the TCP/MIPv6 protocol stack of the Kame software. The efficiency of fragmentation avoidance techniques is also calculated. The paper also illustrates the impact of MIPv6 TR on TCP performance. A performance comparison between TR and RO is discussed. Based on this performance comparison, a generic adoptive routing technique is proposed.","","0-7803-8145","10.1109/CCNC.2004.1286854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1286854","","Internet;IP networks;Educational institutions;Ethernet networks;Testing;Costs;Throughput;Routing protocols;Proposals;Optimization methods","telecommunication network routing;transport protocols;mobile computing;mobile radio;packet radio networks","Mobile IPv6;Mobile Internet Protocol version 6;MIPv6;TCP;fragmentation avoidance;adaptive routing;foreign network;correspondent node;mobile node;triangular routing;route optimisation;care-of address;destination address;mobility routing header;home address;transmission control protocol;protocol stack;Kame software","","1","11","","","","","","IEEE","IEEE Conferences"
"Dynamic runtime re-scheduling allowing multiple implementations of a task for platform-based designs","Tin-Man Lee; J. Henkel; W. Wolf","Princeton Univ., NJ, USA; NA; NA","Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition","","2002","","","296","301","This paper introduces an extension to the RMS scheduling technique that we call ""hot swapping"". Hot swapping enables a system to choose between various selected implementations of one task on-the-fly and thus to optimize the system's cost (e.g. power savings). The on-the-fly swapping between those implementations requires extra time to save and/or transform states of a certain task implementation. Even if the two steady-state schedules before and after the swapping are feasible, the transient schedule with the additional swapping computation time may exceed the system's capacity. Our technique is an extension to rate monotonic scheduling (RMS). While maintaining and meeting performance requirements, our technique shows an average reduction of 31% in power consumption compared to systems using a pure static scheduling approach (RMS) that cannot make use of task swapping. We have evaluated our algorithm through simulation of five real-world task sets and in addition by use of a large number of generated task sets.","1530-1591","0-7695-1471","10.1109/DATE.2002.998288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998288","","Runtime;Switches;Processor scheduling;Embedded system;Energy consumption;Hardware;Embedded software;National electric code;Cost function;Steady-state","processor scheduling;embedded systems;task analysis;computational complexity","dynamic runtime re-scheduling;multiple task implementations;platform-based designs;RMS scheduling technique;hot swapping;system implementations;system cost optimization;on-the-fly implementation swapping;power savings;task implementation states;steady-state schedules;swapping computation time;transient schedule;system capacity;rate monotonic scheduling extension;performance requirements;power consumption;pure static scheduling;task swapping;real-world task sets;generated task sets;embedded systems","","3","18","","","","","","IEEE","IEEE Conferences"
"Combining ARToolKit with scene graph libraries","M. Haller; W. Hartmann; T. Luckeneder; J. Zauner","Univ. of Appl. Sci., Hagenberg, Austria; NA; NA; NA","The First IEEE International Workshop Agumented Reality Toolkit,","","2002","","","2 pp.","","Many Augmented and Mixed Reality applications are based on two libraries: OpenGL is used for rendering and ARToolKit is used for marker recognition. The ARToolkit library is great for rapid prototyping of AR/MR applications. The library is very easy to use and it hides the complexity of marker recognition. In the AMIRE (Authoring Mixed Reality) project, a European founded AR project, we follow up the aim of ARToolKit consistently: the AMIRE approach is to offer well-established gems (software solutions) and components for a faster prototyping of AR/MR applications. Each content user should be able to develop his/her own AR/MR application without any computer graphics skills. Therefore, one of the primary goals of AMIRE is to find well-established solutions of current AR/MR applications. One of the solution is the ARToolKit library, which is used in numberless AR/MR applications. But which library should be used for rendering? Can we use a high-level graphics AN together with ARToolKit? Which graphic library would be the best for farther AR/MR applications? Should developers use Direct3D/OpenGL or should we propose a high-level graphics API, like Open Inventor, OpenGL Performer, OpenSG, or Open SceneGraph? High-level graphics APIs have a number of advantages as opposed to low-level graphics APIs. They include: A set of loaders (e.g. model and texture loaders); A scenegraph concept; Modern object oriented design; High performance (optimized rendering, view frustum culling, small object culling, Level of Detail nodes, etc.). One problem still remains: How difficult is the usage of ARToolKit, originally based on OpenGL, in combination with a high-level graphics API like OpenSG? For the AMIRE project we tested two different APIs: Open SO and Open SceneGraph.","","0-7803-7680","10.1109/ART.2002.1106978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106978","","Layout;Virtual reality;Application software;Software prototyping;Rendering (computer graphics);Prototypes;Software libraries;Computer graphics;Load modeling;Object oriented modeling","rendering (computer graphics);augmented reality;authoring systems","AMIRE;Authoring Mixed Reality;Augmented Reality;rendering;ARToolKit;marker recognition","","3","6","","","","","","IEEE","IEEE Conferences"
"Taking DCOP to the real world: efficient complete solutions for distributed multi-event scheduling","R. T. Maheswaran; M. Tambe; E. Bowring; J. P. Pearce; P. Varakantham","University of Southern California; NA; NA; NA; NA","Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.","","2004","","","310","317","","","1-58113-864","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1373493","","Constraint optimization;Scheduling algorithm;Permission;Multiagent systems;System testing;Sensor systems;Large-scale systems;Application software;Space vehicles;Software performance","","","","10","14","","","","","","IEEE","IEEE Conferences"
"A novel approach for network on chip emulation","N. Genko; D. Atienza; G. De Micheli; L. Benini; J. M. Mendias; R. Hermida; F. Catthoor","Stanford Univ., Palo Alto, CA, USA; NA; NA; NA; NA; NA; NA","2005 IEEE International Symposium on Circuits and Systems","","2005","","","2365","2368 Vol. 3","Current systems-on-chip execute applications that demand extensive parallel processing. Networks-on-chip (NoC) provide a structured way of realizing interconnections on silicon, and obviate the limitations of bus-based solutions. NoCs can have regular or ad hoc topologies, and functional validation is essential to assess their correctness and performance. In this paper, we present a flexible emulation environment implemented on an FPGA that is suitable to explore, evaluate and compare a wide range of NoC solutions with a very limited effort. Our experimental results show a speed-up of four orders of magnitude with respect to cycle-accurate HDL simulation, while retaining cycle accuracy. With our emulation framework, designers can explore and optimize a range of solutions, as well as characterize quickly performance figures.","0271-4302;2158-1525","0-7803-8834","10.1109/ISCAS.2005.1465100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465100","","Network-on-a-chip;Emulation;Field programmable gate arrays;Switches;Silicon;Network topology;Hardware design languages;Communication switching;Testing;Packet switching","network interfaces;hardware-software codesign;multiprocessor interconnection networks;system-on-chip;telecommunication network topology;field programmable gate arrays;logic testing","network interfaces;HW-SW NoC emulation;network on chip emulation;systems-on-chip;parallel processing;ad hoc topology;functional validation;FPGA;cycle accuracy","","10","15","","","","","","IEEE","IEEE Conferences"
"A Bayesian neural network model for dynamic web document clustering","Jun-Hui Her; Sung-Hae Jun; Jun-Heyog Choi; Jung-Hyun Lee","Dept. of Comput. Sci. & Eng., Inha Univ., Inchon, South Korea; NA; NA; NA","Proceedings of IEEE. IEEE Region 10 Conference. TENCON 99. 'Multimedia Technology for Asia-Pacific Information Infrastructure' (Cat. No.99CH37030)","","1999","2","","1415","1418 vol.2","There has been lots of research to improve the precision of IR system. These research have been studied on the document ranking, user profiles, relevance feedback and the information processing that includes document classification, clustering, routing and filtering. This paper proposes and incarnates method of neural approach about the information processing which makes users can search documents effectively and of the document clustering. In this paper the system calculates entropy between the query, the profile and the each of the web documents each other; and clusters documents using the calculated entropy as the value of the clustering variable through SOM. As the Bayesian Neural Network model has high classification accuracy with a rapid learning speed and clustering, it is possible that dynamic document clustering as it was combined with Bayesian probability model used in real-time document classification. We used KTSET which is a test collection to evaluate Korean IR system for the experiment.","","0-7803-5739","10.1109/TENCON.1999.818696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=818696","","Bayesian methods;Neural networks;Clustering algorithms;Entropy;Information processing;Clustering methods;Unsupervised learning;Computer science;Statistics;Software engineering","Bayes methods;relevance feedback;self-organising feature maps","Bayesian neural network model;dynamic web document clustering;document ranking;neural approach;classification accuracy;dynamic document clustering;real-time document classification","","","10","","","","","","IEEE","IEEE Conferences"
"Rapidly adaptive nulling of interference","Kirsteins; Tufts","Dept. of Electr. Eng., Rhode Island Univ., Kingston, RI, USA; Dept. of Electr. Eng., Rhode Island Univ., Kingston, RI, USA","IEEE 1989 International Conference on Systems Engineering","","1989","","","269","272","A simpler and more general analysis is presented of a previously reported principal component inverse (PCI) method of rapidly adaptive nulling of interference. It is assumed that the interference consists of a strong Gaussian component with a rank deficient covariance matrix plus a weak component of white Gaussian background noise. An approximate beta probability density function of the output signal-to-noise ratio (SNR) for the PCI method is derived. Using the theoretically derived formulas, it is shown that the PCI method requires much less data to produce a given, needed level of SNR with higher probability than the sample matrix inverse (SMI) method based on the inverse of the sample covariance matrix. The approximations and the final probability density function are tested through computer simulation. They accurately explain the experimental results.<<ETX>>","","","10.1109/ICSYSE.1989.48670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48670","","Simulation;Interference suppression;Signal processing;Communication system software","digital simulation;interference suppression;signal processing;telecommunications computing","principal component inverse;PCI;rapidly adaptive nulling;interference;rank deficient covariance matrix;white Gaussian background noise;beta probability density function;signal-to-noise ratio;SNR;matrix inverse;computer simulation","","12","15","","","","","","IEEE","IEEE Conferences"
"About voltage sags and swells analysis","S. M. Deckmann; A. A. Ferrira","Sch. of Electr. & Comput. Eng., UNICAMP, Campinas, Brazil; Sch. of Electr. & Comput. Eng., UNICAMP, Campinas, Brazil","10th International Conference on Harmonics and Quality of Power. Proceedings (Cat. No.02EX630)","","2002","1","","144","148 vol.1","Time at level analysis of discrete events is proposed to characterize voltage sags and swells instead of the simple magnitude vs duration characteristics presently used. Although requiring a little more computer memory and digital processing, the proposed methodology constitutes a significant improvement for the characterization of unpredictable temporary voltage disturbances through simple numerical index. Some potential applications of the new method are: full conformity verification to existent or future standards; direct comparison with load sensitivity curves; implicit data compression for transmission and storage; automatic event classification; events ranking and propagation analysis. An IEEE database of voltage sags and swells was used to test the proposed methodology.","","0-7803-7671","10.1109/ICHQP.2002.1221423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221423","","Power quality;Voltage fluctuations;Protection;Event detection;Databases;Electrical equipment industry;Information analysis;Data mining;Application software;Data processing","power supply quality;data compression;sensitivity analysis;electric potential;signal detection;digital storage","voltage sags;swells analysis;time at level curve;discrete events;computer memory;digital processing;temporary voltage disturbances;simple numerical index;full conformity verification;future standards;direct comparison;load sensitivity curves;implicit data compression;transmission;automatic event classification;events ranking;propagation analysis;IEEE database;magnitude-duration curve;power quality;load sensitivity","","10","5","","","","","","IEEE","IEEE Conferences"
"HW/SW codesign of the MPEG-2 video decoder","M. Verderber; A. Zemva; A. Trost","Fac. of Electr. Eng., Ljubljana Univ., Slovenia; Fac. of Electr. Eng., Ljubljana Univ., Slovenia; Fac. of Electr. Eng., Ljubljana Univ., Slovenia","Proceedings International Parallel and Distributed Processing Symposium","","2003","","","7 pp.","","In this paper, we propose an optimized real-time MPEG-2 video decoder. The decoder has been implemented in one FPGA device as a HW/SW partitioned system. We made timing power-consumption analysis and optimization of the MPEG-2 decoder. On the basis of the achieved results, we decided on hardware implementation of the IDCT and VLD algorithms. The remaining parts were realized in software with a 32-bit RISC processor. The MPEG-2 decoder (RISC processor, IDCT core, VLD core) has been described in high-level Verilog/VHDL hardware description language and implemented in Virtex 1600E FPGA. Finally, the decoder has been tested on an industrial prototyping board.","1530-2075","0-7695-1926","10.1109/IPDPS.2003.1213330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213330","","Decoding;Transform coding;Field programmable gate arrays;Reduced instruction set computing;Image coding;Timing;Hardware design languages;Testing;Video compression;Quantization","field programmable gate arrays;decoding;video coding;code standards;hardware-software codesign;real-time systems;power consumption;discrete cosine transforms;reduced instruction set computing;hardware description languages;transform coding","HW/SW codesign;MPEG-2 video decoder;real-time video decoder;timing power-consumption analysis;optimization;IDCT algorithm;VLD algorithm;hardware implementation;32-bit RISC processor;Verilog;hardware description language;Virtex 1600E FPGA;VHDL","","3","16","","","","","","IEEE","IEEE Conferences"
"Maintenance and Repair Support System (MARSS)","C. D. Bosco; Li Pi Su; H. Girolamo; M. Darty","U.S. Missile Command, Redstone Arsenal, AL, USA; NA; NA; NA","Conference Record. AUTOTESTCON '96","","1996","","","336","343","The MARSS is the first integration of soldier and machine optimized for maintenance. It is a lightweight, open architecture, wearable personal computer with object oriented software that controls and integrates plug-in measurement instrumentation, diagnostics processes, interactive electronic technical manuals, and logistic databases for the soldier. Input and output to the system is by means of a head-mounted microphone and a flat-panel active matrix display. MARSS has wireless Local Area Network (LAN) interaction with the weapon system data bus and members of the maintenance team, allowing transmission of fault or diagnostic data. It contains multimedia repair and replace instructions on small, high-density, removable PCMCIA disks. Flexible board technology provides soldier comfort. The open architecture allows easy upgrading of the hardware and software, including new tools as they become available. New battery technology consists of very small, flat, flexible batteries producing six hours operation.","1088-7725","0-7803-3379","10.1109/AUTEST.1996.547722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547722","","Computer architecture;Batteries;Microcomputers;Lighting control;Software measurement;Instruments;Manuals;Logistics;Object oriented databases;Microphones","maintenance engineering;military systems;weapons;microcomputer applications;fault diagnosis;local area networks;open systems;automatic test equipment;military computing","MARSS;maintenance;batteries;open architecture;personal computer;object oriented software;plug-in measurement instrumentation;diagnostics processes;interactive electronic technical manuals;logistic databases;soldier;head-mounted microphone;flat-panel active matrix display;wireless Local Area Network;LAN;weapon;data bus;transmission of fault;diagnostic data;multimedia;repair;replace instructions;PCMCIA disks","","2","1","","","","","","IEEE","IEEE Conferences"
"Parallel framework for ant-like algorithms","M. Craus; L. Rudeanu","Dept. of Comput. Eng., Tech. Univ. "Gh.Asachi", Iasi, Romania; NA","Third International Symposium on Parallel and Distributed Computing/Third International Workshop on Algorithms, Models and Tools for Parallel Computing on Heterogeneous Networks","","2004","","","36","41","This paper describes the work of an objective framework designed to be used in the parallelization of a set of related algorithms. As a concrete application a parallel ant colony optimization algorithm (ACO) for the travelling salesman problem (TSP) is presented. The idea behind the system we are describing is to have a reusable framework for running several sequential algorithms in a parallel environment. The algorithms that the framework can be used with have several things in common: they have to run in cycles and the work should be possible to be split between several ""processing units"". The parallel framework uses the message-passing communication paradigm and is organized as a master-slave system. The ACO for TSP implemented by means of the parallel framework proves to have good performances: approximately linear speedup and low communication cost.","","0-7695-2210","10.1109/ISPDC.2004.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1372046","","Algorithm design and analysis;Ant colony optimization;Traveling salesman problems;Testing;Switches;Concurrent computing;Design engineering;Concrete;Application software;Master-slave","parallel programming;message passing;travelling salesman problems;sequential machines;parallel algorithms","parallel framework;ant-like algorithms;objective framework;parallel ant colony optimization algorithm;travelling salesman problem;sequential algorithms;parallel environment;processing units;message-passing communication paradigm;master-slave system","","4","9","","","","","","IEEE","IEEE Conferences"
"A static optimization approach to assess dynamic available transfer capability","E. de Tuglie; M. Dicorato; M. La Scala; P. Scarpellini","Politecnio di Barli, Bari Univ., Italy; NA; NA; NA","Proceedings of the 21st International Conference on Power Industry Computer Applications. Connecting Utilities. PICA 99. To the Millennium and Beyond (Cat. No.99CH36351)","","1999","","","269","277","This paper deals with the development of a nonlinear programming methodology for evaluating available transfer capability. The main feature of the approach is the capability to treat static and dynamic security constraints in a unique integrated piece of software. The algorithm has been implemented and tested on an actual power system.","","0-7803-5478","10.1109/PICA.1999.779507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=779507","","Power system dynamics;Power system stability;Power system security;Power system transients;Voltage;Dynamic programming;Nonlinear dynamical systems;Steady-state;Power system reliability;Power system modeling","power system security;nonlinear programming;power system transient stability;power system dynamic stability;power transmission;power system interconnection","static optimization;dynamic available transfer capability;nonlinear programming;dynamic security constraints;static security constraints;integrated software;power system;transient stability;interconnected power systems","","8","17","","","","","","IEEE","IEEE Conferences"
"Microprocessor Control System for Output Power Optimization of a Wind Turbine","M. Eriksson; J. Ottosson; T. Wolpert","Power Supply Department, Telefonaktiebolaget L M Ericsson, 126 25 Stockholm, Sweden; Power Supply Department, Telefonaktiebolaget L M Ericsson, 126 25 Stockholm, Sweden; Power Supply Department, Telefonaktiebolaget L M Ericsson, 126 25 Stockholm, Sweden","INTELEC '83 - Fifth International Telecommunications Energy Conference","","1983","","","158","165","Another paper (1) presented at this INTELEC Conference describes a hybrid power plant for remote sites, ERICSSON SUNWIND, consisting of solar, wind and minidiesel power sources. This power plant includes a microprocessor-based control system for output power optimization of the wind turbine; - this deserves a separate description. The wind turbine used in ERICSSON SUNWIND is a combined Darrieus-Savonius type, in which the Darrieus turbine produces most of the energy, whereas Savonius rotor supplies a starting torque. The particular requirements and difficulties connected with the output power control of a Darrieus turbine are presented and analysed. In order to meet these requirements an adaptive control system based on microcomputer techniques has been developed. This paper describes the principle of operation, the hardware and the software of the control system. An illustration of the practical operation in a test plant is also given.","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4793811","","Microprocessors;Control systems;Power generation;Wind turbines;Wind speed;Wind energy;Torque;Blades;Power supplies;Kinetic theory","","","","","1","","","","","","IEEE","IEEE Conferences"
"Transmission expansion planning: a mixed-integer LP approach","N. Alguacil; A. L. Motto; A. J. Conejo","Dept. of Electr. Eng., Univ. of Castilla-La Mancha, Ciudad Real, Spain; NA; NA","IEEE Transactions on Power Systems","","2003","18","3","1070","1077","This paper presents a mixed-integer LP approach to the solution of the long-term transmission expansion planning problem. In general, this problem is large-scale, mixed-integer, nonlinear, and nonconvex. We derive a mixed-integer linear formulation that considers losses and guarantees convergence to optimality using existing optimization software. The proposed model is applied to Garver's 6-bus system, the IEEE Reliability Test System, and a realistic Brazilian system. Simulation results show the accuracy as well as the efficiency of the proposed solution technique.","0885-8950;1558-0679","","10.1109/TPWRS.2003.814891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1216148","","Costs;Power transmission lines;Power system modeling;Load flow;Investments;Upper bound;Voltage;Large-scale systems;Power system reliability;System testing","power transmission planning;integer programming;linear programming;load flow;losses;power system reliability","transmission expansion planning;mixed-integer LP approach;mixed-integer linear programming;losses;convergence;optimization software;Garver's 6-bus system;IEEE Reliability Test System;Brazilian system;linearized power flow","","236","21","","","","","","IEEE","IEEE Journals & Magazines"
"A multidisciplinary flight control development environment and its application to a helicopter","M. B. Tischler; J. D. Colbourne; M. R. Morel; D. J. Biezad","NASA Ames Res. Center, Moffett Field, CA, USA; NA; NA; NA","IEEE Control Systems Magazine","","1999","19","4","22","33","CONDUIT, a computational facility for aircraft flight control design and evaluation, has been developed and demonstrated. CONDUIT offers a graphical environment for integrating simulation models and control law architectures with design specifications and constraints. This tool provides comprehensive analysis support and design guidance to a knowledgeable control system designer. Combining the easy-to-use graphical interface, the preprogrammed libraries of specifications, and the multiobjective function optimization engine (CONSOL-OPTCAD) in a single environment, CONDUIT offers the potential for significant reduction in time and cost of design, analysis, and flight-test optimization of modern flight control systems. A case study application to a complex rotary-wing flight control problem was presented. The baseline RASCAL UH-60 control system, as provided by the flight control contractor, was evaluated versus the ADS-33D handling-quality specifications. The selectable system gains were optimized to meet all system performance and handling-quality specifications. CONDUIT successfully exploited the tradeoff between forward loop and feedback dynamics to significantly improve the expected handling qualities and stability robustness, while reducing crossover frequency and minimizing actuator activity. The tradeoff studies showed the effect of increasing design margin (overdesign) on closed loop performance and actuator activity. Design margins exceeding 7.5% led to rapidly increasing actuator energy and saturation, resulting in shortened fatigue life of rotor control.","1066-033X;1941-000X","","10.1109/37.777786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777786","","Aerospace control;Actuators;Control systems;Design optimization;Robust stability;Aircraft;Computational modeling;Computer architecture;Libraries;Engines","aircraft control;control system CAD;control system analysis computing;software packages;aerospace computing;helicopters","multidisciplinary flight control development environment;CONDUIT;aircraft flight control design;graphical environment;simulation models;control law architectures;design specifications;analysis support;design guidance;multiobjective function optimization engine;CONSOL-OPTCAD;complex rotary-wing flight control problem;baseline RASCAL UH-60 control system;ADS-33D handling-quality specifications;forward loop;feedback dynamics;stability robustness;crossover frequency;design margin;closed loop performance;actuator activity;fatigue life;rotor control","","23","21","","","","","","IEEE","IEEE Journals & Magazines"
"Diagnosis in Automatic Checkout","S. E. LaMacchia","Systems Engineering Division, Battelle Memorial Institute, Columbus, Ohio.","IRE Transactions on Military Electronics","","1962","MIL-6","3","302","309","This paper summarizes several approaches to the problem of isolating faults in a complex system. The formulation of a diagnostic procedure involves 1) logical analysis techniques for the interpretation of results obtained from testing and 2) optimization techniques to insure that procedures employed in performing tests, selecting or sequencing tests, processing data, etc., are as efficient as possible. A procedure developed at Syracuse University, applicable when deterministic tests are used, is described. A model which extends the logical aspects of this procedure to the case when multiple failures can occur is presented. The problem of applying the figure of merit used in the single error case is discussed briefly. It is pointed out that the extended model is capable of handling a more general class of tests than those usually considered. A simple logic circuit is used to illustrate a test which will fail if two particular element failures occur together, but which will not fail if either one of the two failures occurs alone.","0096-2511;2331-1630","","10.1109/IRET-MIL.1962.5008450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5008450","","Logic testing;Circuit testing;Application software;Contracts;Variable speed drives;Fault diagnosis;Circuit faults;Performance analysis;Performance evaluation;Logic circuits","","","","2","12","","","","","","IEEE","IEEE Journals & Magazines"
"Risk Management and Biomedical Devices","J. Robson; Purdee Yeo; M. Riches; T. Carlisle; N. Kitto","Flinders Biomedical Engineering, Flinders Medical Centre, South Australia; NA; NA; NA; NA","2005 IEEE Engineering in Medicine and Biology 27th Annual Conference","","2005","","","166","169","A method of ranking devices for replacement has been successfully trialled at Flinders Medical Centre (FMC). Software developed by Flinders Biomedical Engineering (see wwwjbe.org.au) has been distributed to a network of device managers throughout FMC for the entry of easily determined parameters which relate to the likelihood of a device problem occurring and to the consequence of the problem. Algorithms developed by empirical methods calculate total risk scores that correlate well with previously used subjective methods of ranking devices for replacement, safety testing, preventative maintenance or repair","1094-687X;1558-4615","0-7803-8741","10.1109/IEMBS.2005.1616368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1616368","","Risk management;Biomedical engineering;Biomedical measurements;Medical diagnostic imaging;Safety devices;Testing;Preventive maintenance;Costs;Calibration;Job shop scheduling","biomedical equipment;preventive maintenance;reliability;risk analysis;safety","risk management;biomedical devices;device problem;empirical methods;device replacement;safety testing;preventative maintenance;repair","","","10","","","","","","IEEE","IEEE Conferences"
"A physics/engineering of failure based analysis and tool for quantifying residual risks in hardware","S. L. Cornford; M. Gibbel; M. Feather; D. Oberhettinger","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA; NA; NA","Annual Reliability and Maintainability Symposium. 2000 Proceedings. International Symposium on Product Quality and Integrity (Cat. No.00CH37055)","","2000","","","382","388","NASA Code Q is supporting efforts to improve the verification and validation and the risk management processes for spaceflight projects. A physics-of-failure based Defect Detection and Prevention (DDP) methodology previously developed has been integrated into a software tool and is currently being implemented on various NASA projects and as part of NASA's new model-based spacecraft development environment. The DDP methodology begins with prioritizing the risks (or failure modes, FMs) relevant to a mission which need to be addressed. These risks can be reduced through the implementation of a set of detection and prevention activities referred to herein as PACTs (preventative measures, analyses, process controls and tests). Each of these PACTs has some effectiveness against one or more FMs but also has an associated resource cost. The FMs can be weighted according to their likelihood of occurrence and their mission impact should they occur. The net effectiveness of various combinations of PACTs can then be evaluated against these weighted FMs to obtain the residual risk for each of these FMs and the associated resource costs to achieve these risk levels. The process thus identifies the project-relevant ""tall pole"" FMs and design drivers and allows real time tailoring with the evolution of the design and technology content. The DDP methodology allows risk management in its truest sense: it identifies and assesses risk, provides options and tools for risk decision making and mitigation and allows for real-time tracking of current risk status.","0149-144X","0-7803-5848","10.1109/RAMS.2000.816338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816338","","Physics;Failure analysis;Flexible manufacturing systems;NASA;Risk management;Costs;Aerospace engineering;Software tools;Space vehicles;Risk analysis","space vehicles;risk management;failure analysis","failure based analysis;hardware residual risks quantification;NASA Code Q;risk management processes;spaceflight projects;physics-of-failure;Defect Detection and Prevention methodology;software tool;model-based spacecraft development environment;risks prioritisation;failure modes;prevention activities;resource cost;mission impact;risk management;risk decision making","","1","7","","","","","","IEEE","IEEE Conferences"
"A modular architecture for hot swappable mobile ad hoc routing algorithms","O. Battenfeld; M. Smith; P. Reinhardt; T. Friese; B. Freisleben","Dept. of Math. & Comput. Sci., Marburg Univ., Germany; Dept. of Math. & Comput. Sci., Marburg Univ., Germany; Dept. of Math. & Comput. Sci., Marburg Univ., Germany; Dept. of Math. & Comput. Sci., Marburg Univ., Germany; Dept. of Math. & Comput. Sci., Marburg Univ., Germany","Second International Conference on Embedded Software and Systems (ICESS'05)","","2005","","","8 pp.","","Routing algorithms for mobile devices equipped with wireless communication capabilities in the field of ubiquitous computing must deal with a highly dynamic environment. No single routing strategy will be capable of handling all possible network situations in a satisfactory manner, since the more optimized the strategy is for a particular situation the less suitable it is for a different scenario. In this paper, we introduce a modular mobile ad hoc routing architecture which features hot-swappable, on-demand loadable routing modules, cross-layer communication facilities und user interaction channels to enable mobile applications to adapt their routing strategy to the current situation on demand. The testbed developed to demonstrate our architecture utilizes the Xen para-virtualization technology to subdivide computer resources to form individual emulated network nodes which may be migrated to other host systems during a live emulation session or interfaced with real-world devices.","","0-7695-2512","10.1109/ICESS.2005.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609899","","Routing;Computer architecture;Mobile computing;Wireless communication;Ubiquitous computing;Mobile communication;Application software;System testing;Computer interfaces;Computer networks","telecommunication network routing;ad hoc networks;mobile radio;ubiquitous computing","hot swappable mobile ad hoc routing algorithm;mobile device;wireless communication;ubiquitous computing;modular mobile ad hoc routing architecture;hot-swappable on-demand loadable routing modules;cross-layer communication;user interaction channels;Xen para-virtualization technology","","5","21","","","","","","IEEE","IEEE Conferences"
"Design of Multimedia Telemedicine System for Inter-hospital Consultation","S. K. Yoo; K. M. Kim; S. M. Jung; K. J. Lee; N. H. Kim","Dept. of Medical Engineering, Center for Emergency Medical Informatics, Yonsei Univ. College of Medicine; NA; NA; NA; NA","The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2004","2","","3109","3111","The telemedicine systems for the decision of patient transfer, and the direction of patient treatment through remote consultation are necessarily required for better patient care in emergency situation. In this paper, the prototype emergency telemedicine system has been designed and implemented. The unified integration of multimedia components, including full-quality video, vital sign signals, radiological images and video conferencing in a single computer, provides an efficient means to investigate the accurate status of emergency patient at the remote location. The software implementation of needed functionality without any externally attached hardware CODEC units enables the compact design with low cost, and ease of operation at the emergency room. Experimental tests at the local networks analyze the technical aspects of implemented systems, and optimize the parameters subjectively to run telemedicine systems with affordable error. Inter-hospital experiments demonstrate the possibility to be effectively used at emergency situation.","","0-7803-8439","10.1109/IEMBS.2004.1403878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403878","Emergency Telemedicine;Multimedia;Real-time","Multimedia systems;Telemedicine;Videoconference;Medical treatment;Software prototyping;Prototypes;Hardware;Codecs;Cost function;System testing","","Emergency Telemedicine;Multimedia;Real-time","","2","7","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of rule grouping on a real-time expert system architecture","Ing-Ray Chen; B. L. Poole","Inst. of Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; NA","IEEE Transactions on Knowledge and Data Engineering","","1994","6","6","883","891","Uses a Markov process to model a real-time expert system architecture characterized by message passing and event-driven scheduling. The model is applied to the performance evaluation of rule grouping for real-time expert systems running on this architecture. An optimizing algorithm based on Kernighan-Lin heuristic graph partitioning for the real-time architecture is developed and a demonstration system based on the model and algorithm has been developed and tested on a portion of the advanced GPS receiver (AGR) and manned manoeuvring unit (MMU) knowledge bases.<<ETX>>","1041-4347;1558-2191;2326-3865","","10.1109/69.334879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=334879","","Real time systems;Expert systems;Switches;Partitioning algorithms;Knowledge based systems;Runtime;Production systems;Pattern matching;Markov processes;Message passing","software performance evaluation;message passing;expert systems;real-time systems;Markov processes;scheduling;optimisation;heuristic programming;graph theory;Global Positioning System;receivers;man-machine systems;telecommunication computing;aerospace computing","performance evaluation;rule grouping;real-time expert system architecture;Markov process;message passing;event-driven scheduling;optimizing algorithm;Kernighan-Lin heuristic graph partitioning;advanced GPS receiver;Global Positioning System;manned manoeuvring unit;knowledge bases","","5","20","","","","","","IEEE","IEEE Journals & Magazines"
"Diagnostics, Control and External Beam Optimization by Computer at the Karlsruhe Cyclotron","W. Kneis; W. Kappel; B. Kogel; C. Lehmann; G. Leinweber; J. Mollenbeck; W. Segnitz; H. Schweickert","Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany; Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany; Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany; Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany; Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany; Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany; Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany; Kernforschungszentrum Karlsruhe GmbH, Institut fr Angewandte Kernphysik II Postfach 3640, D-7500 Karlsruhe, Federal Republic of Germany","IEEE Transactions on Nuclear Science","","1979","26","2","2366","2370","A variety of new procedures and programs have been added to the existing diagnostic system to include new diagnostic and control hardware and to improve existing software. The CAMAC branch has been completely reorganized using a star-like arranqement. For control purposes an extendable FORTRAN multitasking system has been implemented. Actually it allows the continuous surveillance and static control of a variety of parameters for the external beam guiding system. Dynamic control of some beam properties like beam profiles is in the test phase. First results of the optimization of the external beam are available showing that it is necessary to use similar optimization strategies in on-line optimization as in the case of theoretical simulations.","0018-9499;1558-1578","","10.1109/TNS.1979.4329877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4329877","","Cyclotrons;Control systems;CAMAC;Hardware;Multitasking;Surveillance;Testing;Computational modeling;Isotopes;Production systems","","","","1","8","","","","","","IEEE","IEEE Journals & Magazines"
"A combined weight window and biasing approach based on a subspace importance map in the Monte Carlo simulation of a gamma-gamma lithodensity logging tool response with the revised McLDL code","Q. Ao; R. P. Gardner; K. Verghese","Center for Eng. Applications of Radioisotopes, North Carolina State Univ., Raleigh, NC, USA; Center for Eng. Applications of Radioisotopes, North Carolina State Univ., Raleigh, NC, USA; Center for Eng. Applications of Radioisotopes, North Carolina State Univ., Raleigh, NC, USA","Proceedings of 1994 IEEE Nuclear Science Symposium - NSS'94","","1994","2","","919","923 vol.2","A combined weight window and biasing approach has been developed to optimize the specific purpose McLDL code for simulating the spectral response of gamma-gamma lithodensity logging tools. The method is based on the use of photon importance maps which are strongly dependent upon the geometric configuration of the logging tool. A subspace in the formation is taken as an importance region and a one-group importance generator is used in conjunction with the current McLDL code. The splitting of a photon only occurs within the important subspace, and direction biasing and the exponential transform are applied to drive photons into that region or the detector. The approach has been tested on a theoretical benchmark problem and the Atlas Wireline Services' commercial gamma-gamma lithodensity logging tool. Compared to the results from the previous McLDL code, this new approach has eliminated the unacceptably large weight fluctuations and substantially improved the code's performance in computational accuracy and efficiency.<<ETX>>","","0-7803-2544","10.1109/NSSMIC.1994.474469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=474469","","Monte Carlo methods;Well logging;Geometry;Gamma rays;Collimators;Gamma ray detection;Gamma ray detectors;Electromagnetic scattering;Particle scattering;Radioactive materials","geophysical techniques;geophysics computing;geophysical prospecting;software performance evaluation;Monte Carlo methods;optimisation;transforms;fluctuations;gamma-ray detection;solid scintillation detectors","combined weight window/biasing approach;subspace importance map;Monte Carlo simulation;gamma-gamma lithodensity logging tool response;revised McLDL code;optimization;spectral response;photon importance maps;geometric configuration;one-group importance generator;exponential transform;benchmark problem;large weight fluctuations;code performance improvement;computational efficiency;computational accuracy","","","12","","","","","","IEEE","IEEE Conferences"
"L-Band SAR-processor for the Chinese SAR satellite","Ping Wen Jiang; Zhang Yongjun; Zhao Bin; H. M. Braun; B. Fritsch","BRSI, China; BRSI, China; BRSI, China; NA; NA","Proceedings. ICCEA 2004. 2004 3rd International Conference on Computational Electromagnetics and Its Applications, 2004.","","2004","","","399","402","The SAR processor was developed for fast delivery of high-resolution images from the Chinese CRS-1 satellite based on Pentium IV technology. Through testing, all the following requirements were met at an acceptable hardware and software cost: fast delivery time; high resolution; high precision. The development of the SAR processor was a joint project between BRSI (Beijing Remote Sensing Institute), China, and RST (Radar Systemtechnik AG), Switzerland. The paper contains an overview of the architecture used by the SAR processor. Due to the complexity of the SCAN-SAR algorithm, it is described in detail. In strip mode, the standard range of the Doppler algorithm was optimised to meet the SCAN-SAR requirements. The results using a simulated point target are given.","","0-7803-8562","10.1109/ICCEA.2004.1459376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1459376","","L-band;Satellites;Software testing;Hardware;Costs;Synthetic aperture radar;Remote sensing;Doppler radar;Radar remote sensing;Spaceborne radar","synthetic aperture radar;radar imaging;radar resolution;computer architecture;computational complexity;radar computing;multiprocessing systems","L-Band SAR-processor;Chinese SAR satellite;high-resolution images;CRS-1 satellite;Pentium IV technology;resolution;precision;Beijing Remote Sensing Institute;Radar Systemtechnik;Doppler algorithm;multiprocessor;dual processor systems","","","","","","","","","IEEE","IEEE Conferences"
"A resource reservation and scheduling algorithm for learning on-demand system over satellite and cable network","Qinghua Zheng; Yongling Song; Jun Liu; Dehong Yu","Comput. Dept., Xi'an Jiaotong Univ., China; Comput. Dept., Xi'an Jiaotong Univ., China; Comput. Dept., Xi'an Jiaotong Univ., China; Comput. Dept., Xi'an Jiaotong Univ., China","Fourth International Symposium on Multimedia Software Engineering, 2002. Proceedings.","","2002","","","166","173","Distance learning systems over satellite and ground wired network provide a multimedia study environment and unlimited learning opportunities. Learning on demand has become an efficient and popular learning mode. Many learning on demand systems based on IP networks have been implemented and applied, such as real system, media server and IBM Learning Space, etc. However, they neither meet a user's reservation request for education resources, nor can they provide a resource optimization scheduling scheme when many people wish to play the same kind of education resources at the same time. In this paper, we propose a new and practical resource reservation and scheduling algorithm for learning on demand based on satellite and ground IP network (RODS for short). In RODS, users provide a request for resources to the server station via ground Internet, then RODS employs the improved maximal queue length (IMQL) algorithm to schedule reservation requests and then generate an optimized sending scheme of multimedia courseware. RODS sends the corresponding resources to remote sites via satellite transmission. The prototype of RODS has already been implemented and tested on the platforms of satellite and CERNET (China Educational and Research Network), and the algorithm was proved to be efficient, flexible and practical in supporting learning on demand.","","0-7695-1857","10.1109/MMSE.2002.1181609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181609","","Scheduling algorithm;Satellites;IP networks;Network servers;Computer aided instruction;Multimedia systems;Web server;Internet;Courseware;Prototypes","processor scheduling;multimedia computing;courseware;distance learning;resource allocation;Internet;satellite communication;multimedia communication","distance learning system;ground wired network;cable network;multimedia study environment;learning on demand systems;IP Network;reservation request;resource optimization scheduling scheme;RODS;ground Internet;improved maximal queue length algorithm;optimized sending scheme;multimedia courseware;satellite transmission;CERNET;China Educational and Research Network","","","15","","","","","","IEEE","IEEE Conferences"
"Comparing two testbench methods for hierarchical functional verification of a bluetooth baseband adaptor","M. Strum; W. J. Chau; E. L. Romero","University of Sao Paulo; University of Sao Paulo; University of Sao Paulo","2005 Third IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS'05)","","2005","","","327","332","The continuous improvement on the design methodologies and processes has made possible the creation of huge and very complex digital systems. Design verification is one of the main tasks in the design flow, aiming to certify the system functionality has been accomplished accordingly to the specification. A simulation based technique known as functional verification has been followed by the industry. In recent years, several articles in functional verification have been presented, focusing either on specific design verification experiments or on methods to improve and accelerate coverage reaching. In the first category, the majority of the papers are aimed to processors verification, while communication systems experiences were not such commonly reported. In the second category, different authors have proposed methodologies, which need an extensive and complex work by the verification engineer on tuning the acceleration algorithms to the specific design. In the present paper, we present a functional verification methodology applied to a Bluetooth Baseband adaptor core, described in SystemC RTL. Two techniques are considered, one following the traditional framework of applying random stimuli and checking functional coverage aspects; in the second one, a simple acceleration procedure, based on redundant stimuli filtering, is included. For both solutions, a hierarchical approach is adopted. We present several results comparing both solutions, showing the gain obtained in using the acceleration technique. Additionally, we show how results on a real testbench application environment correlate to the hierarchical verification approach taken.","","1-59593-161","10.1145/1084834.1084914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076358","coverage analysis;functional verification;hierarchical verification;optimization;verification strategy","Testing;Bluetooth;Baseband;Acceleration;Continuous improvement;Design methodology;Digital systems;Design engineering;Algorithm design and analysis;Filtering","","","","3","12","","","","","","IEEE","IEEE Conferences"
"AgentP classifier system: self-adjusting vs. gradual approach","Z. V. Zatuchna; A. J. Bagnall","Sch. of Comput. Sci., Univ. of East Anglia Norwich, England, UK; Sch. of Comput. Sci., Univ. of East Anglia Norwich, England, UK","2005 IEEE Congress on Evolutionary Computation","","2005","2","","1196","1203 Vol. 2","Learning classifier systems belong to the class of algorithms based on the principle of self-organization and evolution and have frequently been applied to mazes, an important type of reinforcement learning problem. Mazes may contain aliasing cells, i.e. squares in a different location that look identical to an agent with limited perceptive power. Mazes with aliasing squares present a particular difficult learning problem. As a possible approach to the problem, AgentP, a learning classifier system with associative perception, was recently introduced. AgentP is based on the psychological model of associative perception learning and operates explicitly imprinted images of the environment states. Two types of learning mode are described: the first, self-adjusting AgentP, is more flexible and adapts rapidly to changing information; the second, gradual AgentP, is more conservative in drawing conclusions and rigid when it comes to revising strategy. The performance of both systems is tested on existing and new aliasing environments. The results show that AgentP often outperforms (and always at least matches) the performance of other techniques and, on the large majority of mazes used, learns optimal or near optimal solutions with fewer trials and a smaller classifier population.","1089-778X;1941-0026","0-7803-9363","10.1109/CEC.2005.1554826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554826","","Machine learning;Psychology;Legged locomotion;Animals;Service robots;Intelligent robots;Orbital robotics;Power system modeling;System testing;Navigation","pattern classification;learning systems;self-adjusting systems;learning (artificial intelligence);optimisation;software agents","AgentP classifier system;self-adjusting system;learning classifier systems;self-organization;evolution;reinforcement learning;psychological model;associative perception learning","","","31","","","","","","IEEE","IEEE Conferences"
"Searching for optima in non-stationary environments","K. Trojanowski; Z. Michalewicz","Inst. of Comput. Sci., Polish Acad. of Sci., Warsaw, Poland; NA","Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)","","1999","3","","1843","1850 Vol. 3","Application of evolutionary algorithms to non-stationary problems is the subject of research discussed. We extended evolutionary algorithm by two mechanisms dedicated to non-stationary optimization: redundant genetic memory structures and a particular diversity maintenance technique-random immigrants mechanism. We made experiments with evolutionary optimization employing these two mechanisms (separately and together); the results of experiments are discussed and some observations are made.","","0-7803-5536","10.1109/CEC.1999.785498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=785498","","Evolutionary computation;Genetics;Computer science;Time measurement;Minimization methods;Power system management;Machine learning algorithms;Biological cells;Testing;Application software","evolutionary computation;search problems;redundancy;random processes","optima search;non-stationary environments;evolutionary algorithms;non-stationary problems;non-stationary optimization;redundant genetic memory structures;diversity maintenance technique;random immigrants mechanism;evolutionary optimization","","27","20","","","","","","IEEE","IEEE Conferences"
"Fingerprint registration using genetic algorithms","H. H. Ammar; Y. Tao","Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA; NA","Proceedings 3rd IEEE Symposium on Application-Specific Systems and Software Engineering Technology","","2000","","","148","154","In automated fingerprint identification systems, an efficient and accurate alignment algorithm in the preprocessing stage plays a crucial role in the performance of the whole system. We explore the use of genetic algorithms for optimizing the alignment of a pair of fingerprint images. To test its performance, we compare the implemented genetic algorithm with two other algorithms, namely, 2D and 3D algorithms. Based upon our experiment on 250 pairs of fingerprint images, we find that: genetic algorithms run ten times faster than a 3D algorithm with similar alignment accuracy; and genetic algorithms are 13% more accurate than a 2D algorithm, with the same running time. The conclusion drawn from this study is that a genetic algorithm approach is an efficient and effective approach for fingerprint image registration.","","0-7695-0559","10.1109/ASSET.2000.888069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888069","","Fingerprint recognition;Genetic algorithms;Image matching;Testing;Tellurium;Decision making;Neural networks;Graphics","fingerprint identification;image matching;image registration;genetic algorithms;performance evaluation","fingerprint image registration;genetic algorithms;automated fingerprint identification systems;image alignment;optimization;performance evaluation;2D algorithms;3D algorithms;experiment","","4","19","","","","","","IEEE","IEEE Conferences"
"Design and implementation of a real-time switch for segmented Ethernets","C. Venkatramani; Tzi-cker Chiueh","IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA; NA","Proceedings 1997 International Conference on Network Protocols","","1997","","","152","161","Providing network bandwidth guarantees over an Ethernet requires coordination of the network nodes for traffic prioritization such that real-time data can have deterministic access to the network. We have shown previously how RETHER, a software based token passing protocol can efficiently provide real-time support over a single shared Ethernet segment. This work extends the token passing mechanism into a switched, multi-segment Ethernet environment. This paper describes the detailed design issues, their solutions, and a fully operational switch implementation built into the FreeBSD kernel. By testing the protocol independently and as the underlying network protocol of the Stony Brook Video Server, we have verified that the bandwidth guarantees are successfully provided, with relatively low run-time overhead, for real-time connections crossing multiple Ethernet segments. This paper also provides a comprehensive performance evaluation of the prototype.","1092-1648","0-8186-8061","10.1109/ICNP.1997.643709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643709","","Switches;Ethernet networks;Bandwidth;Telecommunication traffic;Access protocols;Kernel;Testing;Network servers;Runtime;Prototypes","performance evaluation;transport protocols;local area networks;multimedia systems;telecommunication traffic","real-time switch;segmented Ethernets;network bandwidth guarantees;traffic prioritization;RETHER;software based token passing protocol;FreeBSD kernel;Stony Brook Video Server;real-time connections;performance evaluation","","12","17","","","","","","IEEE","IEEE Conferences"
"The Effects of Modeling on Simulator Performance","A. Miczo; D. Mohapatra; S. Perkins; K. Kaufman; K. Huang","Schlumberger Palo Alto Research; Schlumberger Palo Alto Research; Schlumberger Palo Alto Research; Schlumberger Palo Alto Research; Schlumberger Palo Alto Research","IEEE Design & Test of Computers","","1987","4","2","46","54","Gate-level simulators are usually thought of in terms of their benefits to logic designers, while behavioral simulators are considered to be the province of system architects. However, the behavioral modeling capabilities of a multilevel gate/behavioral simulator significantly enhanced the performance and accuracy of what are essentially gate-level simulations. The Behave simulator is a multilevel simulator that can simulate circuits at several levels of abstraction-behavioral level, gate level, or a mixture. Zero delay and rank order capability are also available in Behave and can be used to advantage. For example, in a simulation of an array multiplier involving 10,000 vectors, the time decreased from 16 hours to 38 minutes, simply because the elements were rank ordered. This range of processing is possible because of the flexibility in software for general-purpose CPUs.","0740-7475;1558-1918","","10.1109/MDT.1987.295106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069964","","Circuit simulation;Computer architecture;Computational modeling;Logic design;Delay;Testing;Hardware;Central Processing Unit;Probability;Performance gain","","","","2","7","","","","","","IEEE","IEEE Journals & Magazines"
"The 3D brain topography based on PC","K. H. Kim; J. H. Kwon; D. H. Lee; S. I. Kim","Dept. of Biomed. Eng., Hanyang Univ., Seoul, South Korea; NA; NA; NA","Proceedings of the 19th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 'Magnificent Milestones and Emerging Opportunities in Medical Engineering' (Cat. No.97CH36136)","","1997","4","","1599","1601 vol.4","Brain topographic mapping is a useful technique for visualizing brain activity. It clarifies the spatial and temporal relationships between different cortical areas. However, 3D topographic mapping systems are only available in dedicated image processing and image synthesis workstation environments. In this paper, we aim at a PC-based 3D topographic mapping system, which is simple, low-cost and easy to operate. This system was developed with OpenGL and an optimized 3D barycentric algorithm which is simple, precise and less computationally complex. To prove validity of this algorithm on the PC-based system, we simulated a Windows-based 3D topographic program which was developed for test purposes. The results show that the performance of this system is comparable to that of workstations in terms of speed and precision.","1094-687X","0-7803-4262","10.1109/IEMBS.1997.757020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=757020","","Electroencephalography;Signal processing algorithms;Scalp;Interpolation;Brain modeling;Surface topography;Electrodes;Biomedical engineering;Workstations;Costs","electroencephalography;computational complexity;medical signal processing;microcomputer applications;data visualisation;software performance evaluation","3D brain topography;PC-based 3D topographic mapping system;brain activity visualization;EEG;spatial relationship;temporal relationship;cortical areas;OpenGL;optimized 3D barycentric algorithm;computational complexity;Microsoft Windows;performance;execution speed;precision","","","6","","","","","","IEEE","IEEE Conferences"
"Zero-crossing demodulation for open-loop Sagnac interferometers","A. Tselikov; J. U. De Arruda; J. Blake","Dept. of Electr. Eng., Texas A&M Univ., College Station, TX, USA; NA; NA","Journal of Lightwave Technology","","1998","16","9","1613","1619","This paper describes a new low-cost signal processing scheme for open-loop Sagnac interferometers. An extended computer simulation has been performed in order to determine the optimum parameters of the system providing dynamic range, bias drift, scale factor stability and random noise performances suitable for AHRS grade fiber gyroscope applications and metering grade current sensor applications. The signal processing scheme has been assembled and tested in the laboratory using a commercial gyroscope produced by Honeywell, Inc. Several alternative demodulation circuit implementations have been proposed, built and experimentally investigated.","0733-8724;1558-2213","","10.1109/50.712244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=712244","","Demodulation;Sagnac interferometers;Signal processing;Gyroscopes;Application software;Computer simulation;Dynamic range;Circuit stability;Circuit noise;Optical fiber sensors","fibre optic sensors;light interferometers;optical information processing;optimisation;gyroscopes;demodulation","zero-crossing demodulation;open-loop Sagnac interferometers;low-cost signal processing scheme;extended computer simulation;optimum parameters;dynamic range;bias drift;scale factor stability;random noise performances;AHRS grade fiber gyroscope applications;metering grade current sensor applications;signal processing scheme;laboratory;Honeywell;demodulation circuit;fibre optic sensors","","8","6","","","","","","IEEE","IEEE Journals & Magazines"
"Fault tolerant memory design for HW/SW co-reliability in massively parallel computing systems","M. Choi; N. -. Park; K. M. George; B. Jin; N. Park; Y. B. Kim; F. Lombardi","Dept. of Electr. & Comput. Eng., Missouri Univ., Rolla, MO, USA; NA; NA; NA; NA; NA; NA","Second IEEE International Symposium on Network Computing and Applications, 2003. NCA 2003.","","2003","","","341","348","A highly dependable embedded fault-tolerant memory architecture for high performance massively parallel computing applications and its dependability assurance techniques are proposed and discussed in this paper. The proposed fault tolerant memory provides two distinctive repair mechanisms: the permanent laser redundancy reconfiguration during the wafer probe stage in the factory to enhance its manufacturing yield and the dynamic BIST/BISD/BISR (built-in-self-test-diagnosis-repair)-based reconfiguration of the redundant resources in field to maintain high field reliability. The system reliability which is mainly determined by hardware configuration demanded by software and field reconfiguration/repair utilizing unused processor and memory modules is referred to as HW/SW Co-reliability. Various system configuration options in terms of parallel processing unit size and processor/memory intensity are also introduced and their HW/SW Co-reliability characteristics are discussed. A modeling and assurance technique for HW/SW Co-reliability with emphasis on the dependability assurance techniques based on combinatorial modeling suitable for the proposed memory design is developed and validated by extensive parametric simulations. Thereby, design and Implementation of memory-reliability-optimized and highly reliable fault-tolerant field reconfigurable massively parallel computing systems can be achieved.","","0-7695-1938","10.1109/NCA.2003.1201173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201173","","Fault tolerant systems;Parallel processing;Fault tolerance;Memory architecture;Redundancy;Probes;Production facilities;Manufacturing;Built-in self-test;Maintenance","SRAM chips;built-in self test;fault tolerant computing;parallel architectures;redundancy;parallel memories","highly dependable embedded fault-tolerant memory architecture;high performance massively parallel computing applications;dependability assurance techniques;repair mechanisms;permanent laser redundancy reconfiguration;built-in-self-test-diagnosis-repair-based reconfiguration;system reliability","","","23","","","","","","IEEE","IEEE Conferences"
"A low-power partitioning methodology by maximizing sleep time and minimizing cut nets","P. Ghafari; E. Mirhadi; M. Anis; A. Areibi; M. Elmasry","Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada","Fifth International Workshop on System-on-Chip for Real-Time Applications (IWSOC'05)","","2005","","","368","371","The rising objective in VLSI design is to minimize the average power consumption. Sleep time maximization along with minimization of cut nets are explored as ways to decrease and minimize the power consumption. The major motivation is to deactivate parts of a circuit when they are idle, while simultaneously keeping the cut nets as low as possible. This dual objective problem is separately formulated as two single objectives and then combined into one normalized objective function. The joint problem is shown to be NP-hard, hence heuristic approaches were introduced. A modified version of the genetic algorithm is presented along side with an efficient implementation of a geometric iterative improvement technique using segmented trees. Results are presented for three hypothetical test cases and the results demonstrate more than 40% improvement.","","0-7695-2403","10.1109/IWSOC.2005.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530973","Partitioning;Subthreshold Leakage Power;Genetic Algorithm;Sleep Time;Geometric Iterative;Improvement;Segmented Trees","Sleep;Circuits;Partitioning algorithms;Iterative algorithms;Databases;Data engineering;Application software;Minimization;Linear programming","VLSI;integrated circuit design;circuit optimisation;minimisation of switching nets;trees (mathematics);genetic algorithms;low-power electronics","low-power partitioning methodology;sleep time maximization;cut nets minimization;VLSI design;power consumption;NP-hard problem;genetic algorithm;geometric iterative improvement;segmented trees;subthreshold leakage power","","5","6","","","","","","IEEE","IEEE Conferences"
"Security Constrained Dispatch","R. Lugtu","New York Power Pool","IEEE Transactions on Power Apparatus and Systems","","1979","PAS-98","1","270","274","The power system scheduling process is formulated as an optimization problem with linear inequality constraints. The differential algorithm coupled with the simplex procedure is used to optimize the generation schedules. The constraints considered are machine limitations, transmission considerations and system reserve requirements. Taking advantage of the sparse simplex tableau that results when applied to the dispatch problem yields significant reductions in computer storage requirements. Tests indicate that the method is practical for real time application.","0018-9510","","10.1109/TPAS.1979.319527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4113473","","Costs;Processor scheduling;Application software;Power system security;Constraint optimization;Power system interconnection;Transmission lines;Power markets;Scheduling algorithm;Couplings","","","","26","10","","","","","","IEEE","IEEE Journals & Magazines"
"Finding latent code errors via machine learning over program executions","Y. Brun; M. D. Ernst","Lab. for Molecular Sci., Southern California Univ., Los Angeles, CA, USA; NA","Proceedings. 26th International Conference on Software Engineering","","2004","","","480","490","This paper proposes a technique for identifying program properties that indicate errors. The technique generates machine learning models of program properties known to result from errors, and applies these models to program properties of user-written code to classify and rank properties that may lead the user to errors. Given a set of properties produced by the program analysis, the technique selects a subset of properties that are most likely to reveal an error. An implementation, the fault invariant classifier, demonstrates the efficacy of the technique. The implementation uses dynamic invariant detection to generate program properties. It uses support vector machine and decision tree learning tools to classify those properties. In our experimental evaluation, the technique increases the relevance (the concentration of fault-revealing properties) by a factor of 50 on average for the C programs, and 4.8 for the Java programs. Preliminary experience suggests that most of the fault-revealing properties do lead a programmer to an error.","0270-5257","0-7695-2163","10.1109/ICSE.2004.1317470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317470","","Machine learning;Testing;Programming profession;Computer errors;Support vector machines;Support vector machine classification;Decision trees;Classification tree analysis;Laboratories;Computer science","program diagnostics;learning (artificial intelligence);support vector machines;decision trees;C language;Java;fault diagnosis","machine learning models;program properties;user-written code;program analysis;fault invariant classifier;dynamic invariant detection;support vector machine;decision tree learning tools;C programs;Java programs;fault-revealing properties;latent code errors;program executions","","49","26","","","","","","IEEE","IEEE Conferences"
"Static properties of commercial embedded real-time programs, and their implication for worst-case execution time analysis","J. Engblom","Dept. of Comput. Syst., Uppsala Univ., Sweden","Proceedings of the Fifth IEEE Real-Time Technology and Applications Symposium","","1999","","","46","55","We have used a modified C compiler to analyze a large number of commercial real time and embedded applications written in C for 8- and 18-bit processors. Only static aspects of the programs have been studied i.e., such information that can be obtained from the source code without running the programs. The purpose of the study is to provide guidance for the development of worst-case execution time (WCET) analysis tools, and to increase the knowledge about embedded programs in general. Knowing how real programs are written makes it easier to focus research in relevant areas and set priorities. The conclusion is that real time and embedded programs are not necessarily simple just because they are written for small machines. This indicates that real life WCET analysis tools need to handle advanced programming constructions, including function pointer calls and recursion.","1080-1812","0-7695-0194","10.1109/RTTAS.1999.777660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777660","","Embedded system;Real time systems;Program processors;Software measurement;Benchmark testing;Performance analysis;Embedded computing;Application software;Functional programming;Embedded software","embedded systems;program compilers;C language;system monitoring;programming","static properties;commercial embedded real time programs;modified C compiler;embedded applications;static aspects;source code;worst-case execution time analysis tools;embedded programs;small machines;real life WCET analysis tools;advanced programming constructions;function pointer calls;recursion","","13","21","","","","","","IEEE","IEEE Conferences"
"A systematic tradeoff analysis for conflicting imprecise requirements","J. Yen; W. A. Tiao","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA","Proceedings of ISRE '97: 3rd IEEE International Symposium on Requirements Engineering","","1997","","","87","96","The need to deal with conflicting system requirements has become increasingly important over the past several years. Often, these requirements are elastic in that they can be satisfied to a degree. The overall goal of this research is to develop a formal framework that facilitates the identification and the tradeoff analysis of conflicting requirements by explicitly capturing their elasticity. Based on a fuzzy set theoretic foundation for representing imprecise requirements, we describe a systematic approach for analyzing the tradeoffs between conflicting requirements using the techniques in decision science. The systematic tradeoff analyses are used for three important tasks in the requirement engineering process: (1) for validating the structure used in aggregating prioritized requirements, (2) for identifying the structures and the parameters of the underlying representation of imprecise requirements and (3) for assessing the priorities of conflicting requirements. We illustrate these techniques using the requirements of a conference room scheduling system.","","0-8186-7740","10.1109/ISRE.1997.566845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566845","","Fuzzy logic;Testing;Cost function;Fuzzy sets;Fuzzy systems;Marine vehicles","formal specification;systems analysis","imprecise requirements;tradeoff analysis;fuzzy set theory;requirement engineering process;conference room scheduling system;prioritized requirements","","18","15","","","","","","IEEE","IEEE Conferences"
"A study of concurrency in MPEG-4 video encoder","A. Hamosfakidis; Y. Paker; J. Cosmas","Queen Mary & Westfield Coll., London, UK; NA; NA","Proceedings. IEEE International Conference on Multimedia Computing and Systems (Cat. No.98TB100241)","","1998","","","204","207","The traditional boundaries between the computer, TV/film industries and telecommunications, are blurring. Therein lies the focus of MPEG-4: the convergence of common applications of the above mentioned three industries. MPEG-4 aims to satisfy the new requirements and expectations, by providing an audio visual coding standard allowing for interactivity, high compression and universal accessibility. Moreover coding of video data is an important feature in digital industry and the MPEG-4 video encoder has to be able to encode efficiently visual information and probably in real time conditions. Therefore a lot of effort has been spent in real time video encoding and decoding issues using either hardware or software solutions. This paper addresses these issues by describing a parallel software implementation of the MPEG-4 video encoder using multithread techniques. A scheduling policy is proposed to guarantee via a buffer synchronisation a significant speed up which under some special circumstances reach an optimised load balancing solution. The proposed scheme is tested using the hardware resources of Unix multiprocessor hardware platform (running Solaris 2.0) by rewriting and modifying the current MPEG-4 encoder.","","0-8186-8557","10.1109/MMCS.1998.693641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=693641","","Concurrent computing;MPEG 4 Standard;Video compression;Hardware;Telecommunication computing;TV;Communication industry;Computer industry;Convergence;Application software","video coding;code standards;telecommunication standards;multimedia systems;data compression;real-time systems;scheduling;synchronisation;resource allocation;multiprocessing systems","concurrency;MPEG-4 video encoder;audio visual coding standard;video compression;universal accessibility;real time video encoding;decoding;parallel software;multithread techniques;scheduling;buffer synchronisation;load balancing;Unix;multiprocessor;Solaris 2.0","","4","7","","","","","","IEEE","IEEE Conferences"
"Modeling search in group decision support systems","J. Rees; G. J. Koehler","Krannert Graduate Sch. of Manage., Purdue Univ., West Lafayette, IN, USA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","","2004","34","3","237","244","Groups using group decision support systems (GDSS) to address particular tasks can be viewed as performing a search. Such tasks involve arriving at a solution or decision within the context of a complex search space, warranting the use of computerized decision support tools. The type of search undertaken by the groups appears to be a form of adaptive, rather than enumerative, search. Recently, efforts have been made to incorporate this adaptation into an analytical model of GDSS usage. One possible method for incorporating adaptation into an analytical model is to use an evolutionary algorithm, such as a genetic algorithm (GA), as an analogy for the group problem-solving process. In this paper, a test is made to determine whether GDSS behaves similarly to a GA process utilizing rank selection, uniform crossover, and uniform mutation operators. A Markov model for GAs is used to make this determination. Using GDSS experimental data, the best-fit transition probabilities are estimated and various hypotheses regarding the relation of GA parameters to GDSS functionality are proposed and tested. Implications for researchers in both GAs and group decision support systems are discussed.","1094-6977;1558-2442","","10.1109/TSMCC.2004.829307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310439","","Decision support systems;Analytical models;Evolutionary computation;Decision making;Genetic algorithms;Problem-solving;Testing;Collaborative software;Genetic mutations;Markov processes","group decision support systems;genetic algorithms;Markov processes;probability;search problems;problem solving;decision making","group decision support systems;search space;decision support tools;evolutionary algorithm;genetic algorithm;group problem-solving process;rank selection;uniform crossover;uniform mutation operators;Markov model;best-fit transition probability","","2","10","","","","","","IEEE","IEEE Journals & Magazines"
"Using message semantics for fast-output commit in checkpointing-and-rollback recovery","L. M. Silva; J. G. Silva","Dept. de Engenharia Inf., Coimbra Univ., Portugal; NA","Proceedings of the 32nd Annual Hawaii International Conference on Systems Sciences. 1999. HICSS-32. Abstracts and CD-ROM of Full Papers","","1999","Track8","","10 pp.","","Checkpointing is a very effective technique to ensure the continuity of long running applications in the occurrence of failures. However, one of the handicaps of coordinated checkpointing is the high latency for committing output from the application to the external world. Enhancing the checkpointing scheme, with a message logging protocol is a good solution to reduce the output latency. The idea is to track the sources of non-determinism in order to replay the application in a reproducible way during rollback recovery. We present a new event logging scheme that only logs those messages that may be delivered non deterministically to the application. While other schemes keep track of the arrival order of all the messages we just save the delivery order of some of them. Our scheme exploits the semantics of message passing and is able to reduce considerably the number of receiving events when compared with other existing schemes. We present some performance results that compare the output latency of coordinated checkpointing, pessimistic message logging, optimistic message logging and our event logging scheme.","","0-7695-0001","10.1109/HICSS.1999.772986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772986","","Checkpointing;Printers;Protocols;Humans;Data visualization;Writing","software fault tolerance;program testing;system recovery;message passing","message semantics;fast-output commit;checkpointing-and-rollback recovery;long running applications;failures;coordinated checkpointing;checkpointing scheme;message logging protocol;output latency;non determinism;event logging scheme;arrival order;delivery order;message passing;receiving events;pessimistic message logging;optimistic message logging","","1","20","","","","","","IEEE","IEEE Conferences"
"Person identification using multiple cues","R. Brunelli; D. Falavigna","Istituto per la Ricerca Sci. e Tecnologica, Trento, Italy; Istituto per la Ricerca Sci. e Tecnologica, Trento, Italy","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1995","17","10","955","966","This paper presents a person identification system based on acoustic and visual features. The system is organized as a set of non-homogeneous classifiers whose outputs are integrated after a normalization step. In particular, two classifiers based on acoustic features and three based on visual ones provide data for an integration module whose performance is evaluated. A novel technique for the integration of multiple classifiers at an hybrid rank/measurement level is introduced using HyperBF networks. Two different methods for the rejection of an unknown person are introduced. The performance of the integrated system is shown to be superior to that of the acoustic and visual subsystems. The resulting identification system can be used to log personal access and, with minor modifications, as an identity verification system.<<ETX>>","0162-8828;2160-9292;1939-3539","","10.1109/34.464560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=464560","","Robustness;Face recognition;Speaker recognition;Information security;Identification of persons;Acoustic testing","face recognition;speaker recognition;image recognition;image classification;software performance evaluation;statistical analysis;correlation theory","person identification;multiple cues;acoustic features;visual features;nonhomogeneous classifiers;normalization;integration module;performance evaluation;multiple classifiers;HyperBF networks;unknown person;hybrid rank/measurement level;personal access;identity verification system;template matching;robust statistics;correlation;face recognition;speaker recognition;learning","","273","32","","","","","","IEEE","IEEE Journals & Magazines"
"Human and numerical observer studies of lesion detection in Ga-67 images obtained with MAP-EM reconstructions and anatomical priors","P. P. Bruyant; H. C. Gifford; G. Gindi; P. H. Pretorius; M. A. King","NA; NA; NA; NA; NA","IEEE Symposium Conference Record Nuclear Science 2004.","","2004","7","","4072","4075","Regularization can be implemented in iterative image reconstruction by using an algorithm such as Maximum-A-Posteriori Ordered-Subsets-Expectation-Maximization (MAP OSEM) which favors a smoother image as the solution. One way of controlling the smoothing is to introduce, during the reconstruction process, a prior knowledge about the slice anatomy. In a previous work, we showed using numerical observers that anatomical priors can improve lesion detection accuracy in simulated Ga-67 images of the chest. The goal of this work is to expand and enhance our previous investigations by conducting human-observer localization receiver observer characteristics (LROC) studies and to compare the results to those of a multiclass channelized non-prewhitening (CNPW) model observer. Phantom images were created using the SIMIND Monte Carlo simulation software from the MCAT phantom. The lesion: background contrast was 27.5:1. The anatomical data employed were the structure boundaries from the original, noise-free slices of the MCAT phantom. Images were reconstructed using the DePierro MAP algorithm with surrogate functions. Images were also reconstructed with no priors using the RBI-EM algorithm, with 4 iterations and 4 projections per subset Two weights (0.005 and 0.04) for the prior were tested. The following reconstruction scheme was used to reach convergence for the anatomical priors: The 120 projections were reconstructed successively with 4, 8, 24, 60, and 120 projections per subset with 1, 1, 1, 1, and finally 50 iterations respectively; the result of each reconstruction was used as an initial estimate for the next reconstruction. The human observer areas-under-the-curves (AUC's) agreed with the numerical observer in ranking use of organ and lesion boundaries highest, a slight decrease with tumor boundaries present when no functional tumor was present, and a further slight decrease when just organ boundaries were employed","1082-3654","0-7803-8700-70-7803-8701","10.1109/NSSMIC.2004.1466788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466788","","Humans;Lesions;Image reconstruction;Imaging phantoms;Neoplasms;Iterative algorithms;Smoothing methods;Anatomy;Testing;Convergence","image reconstruction;iterative methods;medical image processing;Monte Carlo methods;phantoms;single photon emission computed tomography;tumours","human-observer localization receiver observer characteristics studies;LROC studies;lesion detection;Ga-67 images;anatomical priors;iterative image reconstruction;Maximum-A-Posteriori Ordered-Subsets-Expectation-Maximization algorithm;MAP OSEM;smoother image;chest;multiclass channelized nonprewhitening model;phantom images;SIMIND Monte Carlo simulation software;MCAT phantom;noise-free slices;DePierro MAP algorithm;tumor boundaries;organ boundaries;SPECT","","4","11","","","","","","IEEE","IEEE Conferences"
"Design and analysis of nonlinear digital controllers-based two-level hierarchy for electric utility industry","A. Rubaai; A. R. Ofoli","Electr. & Comput. Eng. Dept., Howard Univ., Washington, DC, USA; Electr. & Comput. Eng. Dept., Howard Univ., Washington, DC, USA","IEEE Transactions on Industry Applications","","2003","39","2","395","407","This paper suggests a control strategy of coordinating multiple dynamic-braking units during the transients ensuing major disturbances. The control strategy considered in this study is a two-level hierarchy. The proposed two-level structure results from the decomposition of the overall problem into parallel subproblems. This allows the retention of the closed-loop controls associated with each subsystem, which together constitute the lower level (Level I). The central coordinating controller forms the upper level (Level II). The coordination of the local controllers by the central controller accounts for nonlinear terms and interconnections and yields the global optimization of the overall system transient performance. The local controllers are not dependent on one another and are robust to any changes in the network configuration, due to the feedback or closed-loop control formulation inherent in the proposed strategy. To ensure physical realizability of the local controllers, the input was restricted to locally measurable signals. The methodology was implemented in a prototype software program, which was tested on a single machine connected to a very large network approximated by an infinite bus, and then on the IEEE four-generator test system. These studies considered fault-clearing times greater than the critical, assuring an unstable condition. The well-damped optimal state and control trajectories illustrate the successful solution of the problem, indicating that the technique is a valuable tool dealing with transient control problems for large-scale systems.","0093-9994;1939-9367","","10.1109/TIA.2003.809440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189216","","Digital control;Power industry;Industrial control;Centralized control;Control systems;Nonlinear control systems;Software testing;System testing;Optimal control;Robust control","electricity supply industry;power system control;power system transient stability;control system synthesis;hierarchical systems;closed loop systems;optimal control;robust control;nonlinear control systems;feedback;digital control;control system analysis","electric utility industry;two-level hierarchy nonlinear digital controllers;control design;multiple dynamic-braking units;closed-loop controls;feedback;closed-loop control formulation;fault clearing times;well-damped optimal state trajectories;well-damped optimal control trajectories;large-scale systems;transient stability","","4","16","","","","","","IEEE","IEEE Journals & Magazines"
"A framework for data prefetching using off-line training of Markovian predictors","Jinwoo Kim; K. V. Palem; Weng-Fai Wong","Center for Res. on Embedded Syst. & Technol., Georgia Inst. of Technol., Atlanta, GA, USA; Center for Res. on Embedded Syst. & Technol., Georgia Inst. of Technol., Atlanta, GA, USA; NA","Proceedings. IEEE International Conference on Computer Design: VLSI in Computers and Processors","","2002","","","340","347","An important technique for alleviating the memory bottleneck is data prefetching. Data prefetching solutions ranging from pure software approach by inserting prefetch instructions through program analysis to purely hardware mechanisms have been proposed. The degrees of success of those techniques are dependent on the nature of the applications. The need for innovative approach is rapidly growing with the introduction of applications such as object-oriented applications that show dynamically changing memory access behavior In this paper, we propose a novel framework for the use of data prefetchers that are trained off-line using smart learning algorithms to produce prediction models which captures hidden memory access patterns. Once built, those prediction models are loaded into a data prefetching unit in the CPU at the appropriate point during the runtime to drive the prefetching. On average by using table size of about 8KB size, we were able to achieve prediction accuracy of about 68% through our own proposed learning method and performance was boosted about 37% on average on the benchmarks we tested. Furthermore, we believe our proposed framework is amenable to other predictors and can be done as a phase of the profiling-optimizing-compiler.","1063-6404","0-7695-1700","10.1109/ICCD.2002.1106792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106792","","Prefetching;Application software;Object oriented modeling;Predictive models;Hardware;Prediction algorithms;Load modeling;Runtime;Accuracy;Learning systems","computer architecture;storage management;optimising compilers;Markov processes","data prefetching;prefetch instructions;hardware mechanisms;smart learning;prediction models;profiling-optimizing-compiler;Markovian predictors;off-line prediction tables;data cache;off-line trace analysis;microarchitecture","","3","25","","","","","","IEEE","IEEE Conferences"
"A cross-cultural comparison of GSS and non-GSS consensus and satisfaction levels within and between the U.S. and Mexico","R. Mejias; L. Lazenco; A. Rico; A. Torres; D. Vogel; M. Shepherd","Coll. of Bus. Adm., Oklahoma Univ., Norman, OK, USA; NA; NA; NA; NA; NA","Proceedings of HICSS-29: 29th Hawaii International Conference on System Sciences","","1996","3","","408","417 vol.3","Relatively few studies have considered cultural dimensions in their analysis of group support systems (GSS) and even fewer have employed empirical data to test their hypotheses. A cross-cultural field experiment was used to measure the effects of ""national culture"" (U.S. and Mexican) upon group consensus levels and individual satisfaction levels in GSS environments and non-GSS environments. Experimental results indicate that within the U.S culture, there were no significant differences in consensus levels between GSS and manual groups in the ranking of ideas, but that U.S. GSS groups reported greater changes in consensus levels. Results within the Mexican culture, reported higher ranking consensus levels for manual groups, but greater changes in consensus levels for GSS-supported groups. With regard to satisfaction levels, while U.S. groups reported no differences between treatments, Mexican GSS groups reported higher satisfaction levels than Mexican manual groups. While U.S. groups reported no differences perceived participation equity, Mexican GSS groups reported higher participation equity than Mexican manual groups. There were no differences in perceived participation equity between GSS and non-GSS groups reported within either the U.S. or Mexico. However, in comparing U.S. with Mexican groups there were significant differences in satisfaction and perceived participation equity between both cultures across all experimental treatments.","","0-8186-7324","10.1109/HICSS.1996.493236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493236","","Cultural differences;Collaborative software;Educational institutions;System testing;Information technology;Decision making;Springs;Modems;Electronic equipment testing;Power system modeling","groupware;group decision support systems;social aspects of automation;human factors","cross-cultural comparison;Mexico;United States;group support systems;empirical data;national culture;group consensus levels;individual satisfaction levels;non-group support system environments;group support system environments;manual groups;participation equity","","12","36","","","","","","IEEE","IEEE Conferences"
"A caching protocol to improve CORBA performance","S. Wagner; Z. Tari","Dept. of Comput. Sci., R. Melbourne Inst. of Technol., Vic., Australia; NA","Proceedings 11th Australasian Database Conference. ADC 2000 (Cat. No.PR00528)","","2000","","","140","148","For many distributed data intensive applications, the default remote invocation of CORBA objects to a server is not acceptable because of performance degradation. Caching can improve performance and scalability of such applications by increasing the locality of data. This paper proposes a caching approach that optimises the default remote invocation behaviour of CORBA clients. Efficient fine-grained access to remote objects requires objects to be shipped to clients and cached across transaction boundaries. This approach is based on cache consistency via backward validation, generic approach for cache storage, object based data shipping and replication management. These features are introduced without changing the object definitions that the client may already depend upon. An implementation of the proposed caching approach is done on Orbix by extending the smart proxies. We also provide a test with different client workloads. The results demonstrated a significant performance increase, in terms of transactions per second.","","0-7695-0528","10.1109/ADC.2000.819825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=819825","","Protocols;Electrical capacitance tomography;Delay;Bandwidth;Computer science;Australia;Application software;Testing;Sockets;Operating systems","cache storage;memory protocols;distributed object management;remote procedure calls;client-server systems;software performance evaluation;transaction processing;distributed databases;object-oriented databases","caching protocol;CORBA performance;distributed data intensive applications;default remote invocation;performance degradation;scalability;data locality;client server system;fine-grained access;transaction boundaries;cache consistency;backward validation;replication management;Orbix;smart proxies","","1","13","","","","","","IEEE","IEEE Conferences"
"Decentralized transaction management in multidatabase systems","B. Hwang; S. H. Son","Dept. of Comput. Sci., Chonnam Nat. Univ., Kwangju, South Korea; NA","Proceedings of 20th International Computer Software and Applications Conference: COMPSAC '96","","1996","","","192","198","A multidatabase system consists of several heterogeneous local database systems. Most studies of multidatabase transaction management are concerned with the centralized transaction management in which there is only one coordinator. All global transactions are submitted to the coordinator site, and thus it can be overloaded. If the coordinator site fails, the multidatabase systems cannot serve any global transactions. In decentralized transaction management, since the site to which a global transaction is submitted becomes its coordinator, the load is naturally balanced and the multidatabase system is gradually degraded even though a coordinator site fails. This paper proposes a decentralized transaction management algorithm which guarantees global serializability and local autonomy.","0730-3157","0-8186-7579","10.1109/CMPSAC.1996.544162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=544162","","Transaction databases;Scheduling;Computer science;Database systems;System recovery;Degradation;Testing;Optimization methods;Concurrent computing","transaction processing;distributed databases","multidatabase systems;decentralized transaction management;heterogeneous local database systems;global transactions;coordinator site;load balancing;decentralized transaction management algorithm;global serializability;local autonomy","","","15","","","","","","IEEE","IEEE Conferences"
"An original inverse method to determine the approximate electrical contact area between a clip and a bar","F. Bagnon; M. Leclercq; T. Millet; L. Boyer; P. Teste","Schneider Electr. Ind. S.A., Nanterre, France; NA; NA; NA; NA","Proceedings of the Forth-Seventh IEEE Holm Conference on Electrical Contacts (IEEE Cat. No.01CH37192)","","2001","","","265","270","The determination of the electrical contact area between two contact members is a recurrent problem which will probably never be solved exactly. In this paper, we present an original method which allows us to determine the approximate electrical contact area obtained when a branch of an elastic clip is pressed against a flat bar. This method is based on the measurement of the electric potential on the bar, on a U shaped contour located as close as possible to the clip. In the experimental set up, the contour potential V/sub m/(s) is quickly recorded, while the contact is fed by a stabilized current of /spl plusmn/30 A, to rule out thermal drifts and EMFs. The electrical potential is measured between the top of the clip and a metallic probe sliding on the contour, with a constant contact force. Assuming that the contact area is elliptic, the unknown parameters of the problem are the location of the ellipse and its main dimensions. The program used to determine these parameters couples a 2D finite element module with an optimization module of the MATLAB/sup (R)/ software. The algorithm starts with an arbitrary set of values for the parameters, uses the experimental boundary conditions and calculates the potential differences V/sub m/(s) corresponding to the experimental contour. Then the coupled ""optimization module and FEM module"" runs, changing the parameters, in order to make V/sub c/(s) as close as possible to V/sub m/(s). The various tests results of the methodology are very satisfactory.","","0-7803-6667","10.1109/HOLM.2001.953221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953221","","Inverse problems;Contacts;Electric variables measurement;Electric potential;Force measurement;Shape measurement;Probes;Finite element methods;MATLAB;Boundary conditions","contact potential;inverse problems;optimisation;finite element analysis;electrical contacts","inverse method;approximate electrical contact area;clip-to-bar contact;electrical contact area;contact members;elastic clip;flat bar;bar electric potential measurement;U shaped contour;contour potential;stabilized current contact feed;thermal drifts;EMFs;electrical potential;sliding metallic probe;constant contact force;elliptic contact area;ellipse location;2D finite element module;MATLAB optimization module;boundary conditions;potential differences;coupled optimization module/FEM module","","1","8","","","","","","IEEE","IEEE Conferences"
"Design of a new parallel haptic device for desktop applications","F. Gosselin; J. -. Martins; C. Bidard; C. Andriot; J. Brisset","LIST, CEA, Fontenay-aux-Roses, France; LIST, CEA, Fontenay-aux-Roses, France; LIST, CEA, Fontenay-aux-Roses, France; LIST, CEA, Fontenay-aux-Roses, France; LIST, CEA, Fontenay-aux-Roses, France","First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics Conference","","2005","","","189","194","In this paper, we present a new six degrees of freedom haptic device developed at CEA-LIST for desktop applications emphasizing quick and precise manipulation. This device relies on a light parallel architecture connecting the base of the robot to the mobile platform manipulated by the user. It is dimensioned and optimized to fit design requirements associated with computer aided design or virtual sculpting. The design methodology relies on a geometric and static optimization which takes into account technological constraints associated with the main off the shelf components. The control scheme of this device is also described. Finally, feedback obtained from first integration tests are presented.","","0-7695-2310","10.1109/WHC.2005.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1406933","","Haptic interfaces;Design optimization;Application software;Parallel architectures;Joining processes;Mobile robots;Parallel robots;Design methodology;Constraint optimization;Feedback","parallel architectures;haptic interfaces;CAD;interactive devices","parallel haptic device;desktop applications;parallel architecture;computer aided design;CAD","","8","18","","","","","","IEEE","IEEE Conferences"
"Traffic signal controller based on fuzzy logic","J. Niittymaki; V. Kononen","Helsinki Univ. of Technol., Espoo, Finland; NA","Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0","","2000","5","","3578","3581 vol.5","The main goal of this study is to introduce a prototype of a new traffic signal controller based on fuzzy logic. Technical information as well as the basic outline of the software is introduced. The fuzzy inference part of the controller is described in details and used fuzzy methods are introduced in briefly. In the second part of the study, the results of the before-after measurements of the field test are introduced.","1062-922X","0-7803-6583","10.1109/ICSMC.2000.886564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886564","","Fuzzy logic;Traffic control;Fuzzy control;Control systems;Power capacitors;Software prototyping;Vehicles;Delay;Optimization methods;Feedback loop","fuzzy control;traffic control;fuzzy logic","traffic signal controller;fuzzy logic;fuzzy inference;fuzzy methods","","4","9","","","","","","IEEE","IEEE Conferences"
"Optimal harmonic power flow","Ying-Yi Hong","Dept. of Electr. Eng., Chung Yuan Univ., Chung Li, Taiwan","IEEE Transactions on Power Delivery","","1997","12","3","1267","1274","A new optimal harmonic power flow (OHPF) method is proposed in this paper. This new OHPF is designed to minimize the MW losses and the total harmonic voltage distortion while satisfying the power flow equations, harmonic power flow equations, security constraints and harmonic standard, etc. A two-level algorithm is used to solve the problem: the master level determines the settings of compensators and taps; and the slave level includes subproblems which involve cases of fundamental and harmonic frequencies separately. The test results for two power systems show the applicability of the proposed method.","0885-8977;1937-4208","","10.1109/61.637003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637003","","Power system harmonics;Load flow;Power harmonic filters;Frequency;Equations;Reactive power;Harmonic distortion;Voltage;Power system security;Total harmonic distortion","power system analysis computing;power system harmonics;load flow;optimisation;software packages;power system security","optimal harmonic power flow;MW losses;total harmonic voltage distortion;power flow equations;harmonic power flow equations;power systems;security constraints;harmonic standards;computer simulation;two-level algorithm;IDESIGN software","","8","15","","","","","","IEEE","IEEE Journals & Magazines"
"Fuzzy-logic-based power control system for multifield electrostatic precipitators","N. Grass","Electrostatic Precipitation Group, Siemens AG, Erlangen, Germany","IEEE Transactions on Industry Applications","","2002","38","5","1190","1195","The power consumption of large precipitators can be in the range of 1 MW and above. Depending on the dust load properties, the electrical power may be reduced by up to 50% by applying fuzzy logic, without significantly increasing the dust emissions. The new approach uses fuzzy logic for optimization of existing electrostatic precipitators. The software runs on a standard personal computer platform under the Windows NT operating system. The controllers of the electrostatic precipitator power supplies are linked to the personal computer via an industrial network (e.g., PROFIBUS). The system determines online the differentials of emission versus electrical power of each field. This measurement is difficult because of overlaid events in the other zones, and process changes. The long response, time of the resultant dust emission due to electrical power changes in the precipitator is an additional complication. Rules were defined for a coarse, but fast-response power adaptation of all zones. Fine tuning the running system after the coarse optimization increased the accuracy and reliability. When installed on a 4/spl times/5 zone precipitator in a power station, significant results were obtained. The power savings over three months of operation were in the range of 40%-60% depending on the load and fuel characteristics. Data were recorded over the test period of three months. The results are presented.","0093-9994;1939-9367","","10.1109/TIA.2002.802998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035168","","Power control;Electrostatic precipitators;Fuzzy logic;Microcomputers;Energy consumption;Software standards;Operating systems;Computer industry;Electrical equipment industry;Industrial control","electrostatic precipitators;energy conservation;power consumption;fuzzy control;power supplies to apparatus;air pollution control;thermal power stations;dust","fuzzy-logic-based power control system;multifield electrostatic precipitators;power consumption;precipitators;power savings;power supplies;emission control;dust load properties;dust emissions;Windows NT operating system;personal computer platform;controllers;industrial network;PROFIBUS;fast-response power adaptation;coarse optimization;reliability;fuel characteristics;load characteristics","","8","11","","","","","","IEEE","IEEE Journals & Magazines"
"Expert system aided automated design, simulation and controller tuning of AC drive system","S. M. Chhaya; B. K. Bose","Dept. of Electr. Eng., Tennessee Univ., Knoxville, TN, USA; Dept. of Electr. Eng., Tennessee Univ., Knoxville, TN, USA","Proceedings of IECON '95 - 21st Annual Conference on IEEE Industrial Electronics","","1995","1","","712","718 vol.1","Expert systems, a branch of artificial intelligence (AI), are showing tremendous potential for application to power electronics and AC motor drive systems. The paper describes expert system-aided automated design, simulation and controller tuning of an AC motor drive system. It is an extension of earlier work that discussed the design and optimization of power converter systems. Based on the application and performance specifications from the user, the expert system selects a motor drive configuration consisting of the AC machine type, power converter and control topologies. The expert system embeds the design expertise of an experienced AC motor drive system designer to configure, design and fine-tune the AC motor drive system. Next, the expert system supervises an autonomous hybrid simulation where the controller software is fine-tuned in real-time before loading to an experimental AC motor drive system. The paper discusses the design methodology for AC motor drive systems, with particular emphasis on the configuration selection, AC motor sizing and control design and tuning. The expert system based control design is experimentally verified on an indirect vector controlled induction motor drive system with synchronous current control, using a 5 hp IGBT inverter, with the control implemented by a Texas Instruments DSP 320C25. The same principles can be extended to larger and more complex AC motor drive system applications.","","0-7803-3026","10.1109/IECON.1995.483495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=483495","","Expert systems;Automatic control;AC motors;Control systems;Artificial intelligence;Control design;Power electronics;Design optimization;Motor drives;AC machines","AC motor drives;machine control;electric machine analysis computing;electric machine CAD;control system CAD;control system analysis computing;expert systems;hybrid simulation;bipolar transistor switches;power bipolar transistors;power semiconductor switches;insulated gate bipolar transistors;machine testing;DC-AC power convertors;invertors","AC motor drive systems;expert system;artificial intelligence;CAD;computer simulation;controller tuning;power converter;configuration;control topologies;autonomous hybrid simulation;controller software;real-time;design methodology;sizing;control design;indirect vector control;induction motor drive system;synchronous current control;IGBT inverter;Texas Instruments DSP 320C25;5 hp","","2","14","","","","","","IEEE","IEEE Conferences"
"A fixed-structure learning automaton solution to the stochastic static mapping problem","G. Horn; B. J. Oommen","SIMULA Res. Lab., Lysaker, Norway; NA","19th IEEE International Parallel and Distributed Processing Symposium","","2005","","","8 pp.","","This paper considers the problem of distributing the processes of a parallel application onto a set of computing nodes. This problem called the static mapping problem (SMP) is known to be NP-hard, and has been tackled using heuristic solutions. The objective of this paper is to present the first reported learning automaton (LA) based solution to the SMP, generated by the close resemblance of the SMP to the equipartitioning problem. The LA in question is of the so-called fixed-structure family, solution to the equipartitioning problem is then modified to solve the SMP. Several algorithmic variants of this solution have been implemented, and these have all been rigorously tested and evaluated through extensive simulations on randomly generated parallel applications. The focus in this work is to demonstrate the applicability of LA to the SMP, not to optimise and evaluate the performance of the proposed strategy. The results presented here clearly demonstrate that LA provides a promising tool that can effectively solve the mapping problem.","1530-2075","0-7695-2312","10.1109/IPDPS.2005.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420272","Static Mapping;Learning Automata;Task assignment;Parallel Computing","Learning automata;Stochastic processes;Application software;Concurrent computing;Distributed computing;Testing;Parallel processing;Bandwidth;Processor scheduling;Laboratories","learning automata;parallel machines;optimisation;stochastic automata;stochastic programming;scheduling","fixed-structure learning automaton;stochastic static mapping;NP-hard problem;heuristic solution;equipartitioning problem;parallel computing;task assignment","","10","11","","","","","","IEEE","IEEE Conferences"
"The AR-10 family of signal processors","A. Worters","Stein Associates, Inc. Waltham, Massachusetts","ICASSP '77. IEEE International Conference on Acoustics, Speech, and Signal Processing","","1977","2","","490","493","The AR-10 family of Signal Processors developed by Stein Associates is the culmination of over two years of intensive R&amp;D. The processors range from a small single computing element to a large three-section multiprocessor. Among the AR-10 processor unique features are: 1) the AR/BUS, permitting large blocks of data to flow through the AR-10, never slowing down the processing rate; 2) two ""styles"" of processing modules, one optimized for block processing, one optimized for sample-by-sample processing; mix and match to fit the application; and 3) user programmability; hardware and software features combine to make the AR-10 no more difficult or mysterious to code than a typical minicomputer.","","","10.1109/ICASSP.1977.1170233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1170233","","Signal processing;Transmitters;Computer architecture;Testing;Signal processing algorithms;Control systems;Arithmetic;Research and development;Application software;Hardware","","","","2","","","","","","","IEEE","IEEE Conferences"
"Design centering using an approximation to the constraint region","J. M. Wojciechowski; L. J. Opalski; K. Zamlynski","Inst. of Radioelectronics, Warsaw Univ. of Technol., Poland; NA; NA","IEEE Transactions on Circuits and Systems I: Regular Papers","","2004","51","3","598","607","The paper discusses the applicability of the piecewise-ellipsoidal approximation (PEA) to the acceptability region for solution of various design problems. The PEA technique, originally developed and tested for linear discrete circuits described in the frequency domain, is briefly reviewed. It is shown that PEA is a generic mathematical method and its applicability is extended to linear and nonlinear systems (not necessary electrical) described in time or frequency domains. The architecture of a software implementing the technique is introduced and approximations to the acceptability regions for the given design specifications for integrated circuits (CMOS amplifier, clock driver) and multidomain systems (servomechanism) are constructed and their accuracy checked. Then, some standard optimal-design algorithms (i.e., worst case parametric yield maximization, yield versus cost optimization) are redesigned to exploit the PEA properties (e.g., local convexity/concavity) and make them more effective. The algorithms are confronted with design problems, and quality of the resulting designs is assessed.","1549-8328;1558-0806","","10.1109/TCSI.2003.822738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275606","","Circuit testing;Frequency domain analysis;Integrated circuit yield;Algorithm design and analysis;Nonlinear systems;Computer architecture;CMOS integrated circuits;Clocks;Driver circuits;Servomechanisms","analogue integrated circuits;linear network synthesis;frequency-domain synthesis;circuit optimisation;time-domain synthesis;approximation theory","design centering;constraint region approximation;piecewise-ellipsoidal approximation;linear discrete circuits;frequency domain;linear systems;nonlinear systems;circuit architecture;integrated circuit design;CMOS amplifier;clock driver;multidomain systems;servomechanism;optimal-design algorithms;worst case parametric yield maximization;yield versus cost optimization;local convexity-concavity;design problems","","8","22","","","","","","IEEE","IEEE Journals & Magazines"
"Statistical power estimation for FPGAs","E. Todorovich; E. Boemo; F. Angarita; J. Vails","Sch. of Eng., Univ. Autonoma de Madrid, Spain; Sch. of Eng., Univ. Autonoma de Madrid, Spain; NA; NA","International Conference on Field Programmable Logic and Applications, 2005.","","2005","","","515","518","This article presents a power estimation tool integrated with an FPGA design flow. It is able to estimate total and individual-node average power consumption for combinational blocks. The tool is based on the statistical approach, allowing the user to specify the tolerated error and confidence level of the power estimation. An important feature of this software is the short pulse filtration that leads, in other case, to overestimation. Power maps generation is implemented to help both to detect hot-spots, and perform a power optimization. These maps show the power at every physical position in the die. Several circuits have been tested in order to demonstrate the tool features and usability. The estimated values of dynamic power have been compared with physical measurements for Virtex and Virtex-E devices.","1946-147X;1946-1488","0-7803-9362","10.1109/FPL.2005.1515774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515774","","Field programmable gate arrays;Energy consumption;Computational modeling;Power engineering and energy;Circuit testing;Statistics;Probability;Circuit simulation;Filtration;Power generation","field programmable gate arrays;reconfigurable architectures;power consumption;optimisation;integrated circuit design","statistical power estimation;FPGA design flow;individual-node average power consumption;tolerated error;short pulse filtration;overestimation;power maps generation;power optimization;field programmable gate arrays","","6","16","","","","","","IEEE","IEEE Conferences"
"Simulation and performance comparison of four disk scheduling algorithms","M. Y. Javed; I. U. Khan","Dept. of Comput. Eng., Nat. Univ. of Sci. & Technol., Pakistan; NA","2000 TENCON Proceedings. Intelligent Systems and Technologies for the New Millennium (Cat. No.00CH37119)","","2000","2","","10","15 vol.2","Hard disks are being used to store huge amounts of information/data in all modern computers. Disk drives must provide faster access times in order to optimize the speed of I/O operations. In a multitasking system with many processes, disk performance can be improved by incorporating a scheduling algorithm for maintaining several pending requests in the disk queue. This paper describes the development of a simulator which uses four disk scheduling algorithms to measure their performance in terms of total head movement. These algorithms are: FCFS (First-Come, First-Served), SSTF (Shortest-Seek-Time-First), LOOK (Look-ahead) for both upward and downward directions, and C-LOOK (Circular LOOK). Five different types of test samples, containing from 8 to 50 request tracks, have been used to obtain simulation results. The developed simulator runs successfully in a multiprogramming environment and the tabulated results demonstrate that the LOOK (downward direction) algorithm provides the best results for given test samples due to the reduction of a large number of unnecessary head movements. As several wild swings are experienced by the FCFS scheme, it gives the worst scheduling performance. SSTF is much better compared to LOOK (upward direction) and C-LOOK. It has also been noticed that LOOK is more efficient than C-LOOK at all loads, whereas C-LOOK is better at high loads only, as it reduces the starvation problem. The performance of each algorithm, however, depends heavily on the number and type of requests.","","0-7803-6355","10.1109/TENCON.2000.888379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888379","","Scheduling algorithm;Disk drives;Computational modeling;Testing;Delay;Computer simulation;Processor scheduling;Educational institutions;Mechanical engineering;Telephony","hard discs;disc drives;scheduling;software performance evaluation;virtual machines;multiprogramming","simulation;performance comparison;disk scheduling algorithms;disk access times;I/O operation speed optimization;multitasking system;disk queue pending requests;head movement;FCFS algorithm;first-come, first-served;SSTF algorithm;shortest seek time first;LOOK algorithm;look-ahead;upward direction;downward direction;C-LOOK algorithm;circular LOOK;request tracks;multiprogramming environment;efficiency;load;starvation problem","","4","5","","","","","","IEEE","IEEE Conferences"
"A fuzzy multi-objective approach to optimal voltage/reactive power control","Xu Guiguang; Wang Xing; Yu Erkeng","Electr. Power Res. Inst., Beijing, China; NA; NA","POWERCON '98. 1998 International Conference on Power System Technology. Proceedings (Cat. No.98EX151)","","1998","2","","1443","1447 vol.2","The purpose of optimal voltage/reactive power control is to minimize the real power loss and improve the voltage profile. However in practice the security and the number of control movement should be taken into account. In this paper the formulation of the fuzzy multiple objective optimization problem with soft constraints is presented and the SLP algorithm is employed to solve the problem. The tests on the practical network demonstrate the efficiency and flexibility of the proposed approach.","","0-7803-4754","10.1109/ICPST.1998.729326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=729326","","Fuzzy control;Reactive power control;Reactive power;Voltage control;Control systems;Power generation;Software packages;Optimization methods;Power systems;Optimal control","power system control;optimal control;voltage control;reactive power control;fuzzy control;power system security","fuzzy multi-objective approach;optimal voltage/reactive power control;real power loss minimisation;voltage profile improvement;security;SLP algorithm;fuzzy programming","","","6","","","","","","IEEE","IEEE Conferences"
"A Study of the Neon Bulb as a Nonlinear Circuit Element","C. Hendrix","US Naval Ordinance Test Station, China Lake, CA","IRE Transactions on Component Parts","","1956","3","2","44","54","The ordinary NE-2 neon bulb possesses two-valued properties which make it valuable for use in logical gate circuits such as are used in digital computers. The cost of the NE-2 is of the order of $0.05 compared to an average unit cost of $1.00 for germanium diodes. In the logical gate application, circuit performance is relatively insensitive to parameter variations among bulbs. Both ""or"" and ""and"" gates, and combinations thereof can be built, and operation up to 30 kc easily obtained. Statistical studies of bulbs point out that the aging effects to be expected are a rise in voltage drop across the bulb and a reduction in the variability among bulbs. The change appears rather suddenly, after about 1000 milliampere-hours of operation. Equivalent circuits are presented, and simple circuit design techniques have been worked out.","0096-2422;2168-1708","","10.1109/TCP.1956.1135748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1135748","","Nonlinear circuits;Costs;Germanium;Diodes;Application software;Circuit optimization;Aging;Voltage;Equivalent circuits;Circuit synthesis","","","","1","4","","","","","","IEEE","IEEE Journals & Magazines"
"An optimal algorithm for guaranteeing sporadic tasks in hard real-time systems","M. Silly; H. Chetto; N. Elyounsi","Lab. d'Autom. de Nantes, ENSM, Nantes, France; Lab. d'Autom. de Nantes, ENSM, Nantes, France; Lab. d'Autom. de Nantes, ENSM, Nantes, France","Proceedings of the Second IEEE Symposium on Parallel and Distributed Processing 1990","","1990","","","578","585","Guaranteeing that time critical tasks will meet their timing constraints is an important aspect of real-time systems research. Indeed, a real time system is dynamic and consequently requires online and adaptive scheduling strategies. The authors survey and extend results on scheduling hard deadline periodic and sporadic tasks on the monoprocessor model. The periodic tasks are independent, run cyclically and their characteristics are known in advance. The authors allow for the unpredictable arrival of aperiodic tasks said to be sporadic with hard real-time constraints. The problem is to jointly schedule the periodic and sporadic tasks so that the timing requirements for both sets of tasks are met. A schedulability test is presented that decides whether a new occurring task can be accepted. This test is built upon the Earliest Deadline scheduling algorithm which features a high performance and ease of implementation.<<ETX>>","","0-8186-2087","10.1109/SPDP.1990.143607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=143607","","Real time systems;Timing;Dynamic scheduling;Processor scheduling;Process control;Testing;Communication system control;Application software;Adaptive scheduling;Scheduling algorithm","distributed processing;optimisation;real-time systems;scheduling","periodic tasks;dynamic scheduling;distributed systems;optimal algorithm;sporadic tasks;hard real-time systems;timing constraints;sporadic tasks;monoprocessor model;aperiodic tasks;Earliest Deadline scheduling","","8","12","","","","","","IEEE","IEEE Conferences"
"A benchmark for Web service frameworks","N. Wickramage; S. Weerawarana","Dept. of Comput. Sci. & Eng., Univ. of Moratuwa, Sri Lanka; NA","2005 IEEE International Conference on Services Computing (SCC'05) Vol-1","","2005","1","","233","240 vol.1","Considering the facts that existing benchmarks to measure the performance of Web service frameworks simulate only theoretical scenarios such as streaming homogeneous data structures and the computer industry has an established culture of developing performance benchmarks imitating real world scenarios, an effort was made to come up with a benchmark that closely represent the real world business services. The paper concludes that the benchmark represents an unbiased subset of actual scenarios because the ranking and performance patterns of the leading Web services frameworks used in the experiment are consistent with industry wide experiences. Additionally the paper introduces a performance model to analyze Web service frameworks and identifies complexity of the SOAP messages and size of the payloads they carry as two major factors that affect the RTT of the SOAP messages and reveals that a framework that is good at handling complex SOAP messages may not deal with messages that carry larger payloads equally well.","","0-7695-2408","10.1109/SCC.2005.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531259","","Web services;Simple object access protocol;Computer architecture;Service oriented architecture;Testing;Computer industry;Payloads;Grid computing;Space technology;Application software","Internet;communication complexity;protocols;benchmark testing","Web service framework;homogeneous data structure streaming;performance benchmark;real world business services;SOAP message complexity","","11","9","","","","","","IEEE","IEEE Conferences"
"The monitoring facilities of the graphical parallel programming environment TRAPPER","T. Born; W. Obeloer; L. Schafers; C. Scheidler","Paderborn Univ., Germany; Paderborn Univ., Germany; NA; NA","Proceedings Euromicro Workshop on Parallel and Distributed Processing","","1995","","","555","562","TRAPPER is a graphical environment aimed to support the programming of parallel embedded systems. TRAPPER comprises components for the design, mapping, visualization and optimization of parallel applications. One goal of this article is to motivate the requirements for a monitoring system in such a graphical programming environment. Because the existing monitoring system DELTA-T already fulfils a lot of these requirements we decided to integrate it. The result of this integration is shown in detail. The main features are automatic observation of software and hardware events at different granularity, different transport strategies of the monitoring data, and a flexible programmer interface to interact with the monitor. All features may be switched on and off by the graphical user interface of TRAPPER. Measurements show the low overhead of our implementation. The monitoring system is currently being used as part of TRAPPER in several industrial automotive research applications.<<ETX>>","","0-8186-7031","10.1109/EMPDP.1995.389163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=389163","","Parallel programming;Computerized monitoring;Embedded system;Visualization;Design optimization;Programming environments;Hardware;Programming profession;Graphical user interfaces;Automotive engineering","parallel programming;real-time systems;graphical user interfaces;programming environments;program testing;data visualisation","monitoring facilities;graphical parallel programming environment TRAPPER;parallel embedded systems;visualization;mapping;optimization;automatic observation;transport strategies;flexible programmer interface;graphical user interface;industrial automotive research applications","","6","18","","","","","","IEEE","IEEE Conferences"
"Optimized translation of XPath into algebraic expressions parameterized by programs containing navigational primitives","S. Helmer; C. -. Kanne","Mannheim Univ., Germany; Mannheim Univ., Germany","Proceedings of the Third International Conference on Web Information Systems Engineering, 2002. WISE 2002.","","2002","","","215","224","We propose a new approach for the efficient evaluation of XPath expressions. This is important, since XPath is not only used as a simple, stand-alone query language, but is also an essential ingredient of XQuery and XSLT. The main idea of our approach is to translate XPath into algebraic expressions parameterized with programs. These programs are mainly built from navigational primitives like accessing the first child or the next sibling. The goals of the approach are: 1) to enable pipelined evaluation, 2) to avoid producing duplicate (intermediate) result nodes, 3) to visit as few document nodes as possible, and 4) to avoid visiting nodes more than once. This improves the existing approaches, because our method is highly efficient.","","0-7695-1766","10.1109/WISE.2002.1181658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181658","","Navigation;Pipeline processing;Database languages;XML;Performance evaluation;Query processing;Relational databases;Testing;Algebra;Binary trees","hypermedia markup languages;query languages;query processing;multimedia databases;program compilers","XPath translation;algebraic expressions;navigational primitives;query language;XQuery;XML database;multimedia;XSLT;pipelined evaluation;document nodes","","2","17","","","","","","IEEE","IEEE Conferences"
"Accelerator simulation and operation via identical operational interfaces","J. Kewisch; A. Barry; R. Bork; B. Bowling; V. Corker; G. Lahti; K. Nolker; J. Sage; J. Tang","Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA; Continuous Electron Beam Accel. Facility, Newport News, VA, USA","Conference Record of the 1991 IEEE Particle Accelerator Conference","","1991","","","1443","1445 vol.3","The architecture and first results of a control system which allows one the use of identical operation procedures and interfaces for operation of the real accelerator and high-level accelerator simulation programs are presented. The interfaces were developed using TACL (thaumaturgic automated control logic) control software, developed at CEBAF for accelerator control. This setup provides the capability to: (1) test and debug the various operation procedures before the completion of the accelerator, (2) execute machine simulations under realistic environmental conditions, and (3) preview and evaluate the effectiveness of operational procedures during run time. The optimized simulation program adds only two seconds to the normal TACL operational cycle.<<ETX>>","","0-7803-0135","10.1109/PAC.1991.164662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=164662","","Logic arrays;Programmable logic arrays;Electron accelerators;Automatic control;Hardware;Software performance;Computational modeling;CAMAC;Large screen displays;Computer displays","beam handling equipment;beam handling techniques;electron accelerators;linear accelerators;physics computing","control system;identical operation procedures;high-level accelerator simulation programs;thaumaturgic automated control logic;control software;CEBAF;accelerator control;machine simulations","","2","2","","","","","","IEEE","IEEE Conferences"
"Enhancements to Ziv-Lempel data compression","C. Rogers; C. D. Thomborson","Dept. of Comput. Sci., Minnesota Univ., Duluth, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Duluth, MN, USA","[1989] Proceedings of the Thirteenth Annual International Computer Software & Applications Conference","","1989","","","324","330","A description is given of several modifications to the Ziv-Lempel data compression scheme that improve its compression ratio at a moderate cost in run time (J. Ziv, A. Lempel, 1976, 1977, 1978). The best algorithm reduces the length of a typical compressed text file by about 25%. The enhanced coder compresses approximately 2000 bytes of text every second before optimization, making it fast enough for regular use.<<ETX>>","","0-8186-1964","10.1109/CMPSAC.1989.65102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65102","","Data compression;Dictionaries;Testing;Computer science;Costs;Buildings;Data structures;Arithmetic","data compression;programming;word processing","Ziv-Lempel data compression scheme;compression ratio;run time;algorithm;typical compressed text file;enhanced coder;optimization","","2","17","","","","","","IEEE","IEEE Conferences"
"A histogram based adaptive vector filter for color image restoration","Zhonghua Ma; Hong Ren Wu","Sch. of Comput. Sci. & Software Eng., Monash Univ., Clayton, Vic., Australia; Sch. of Comput. Sci. & Software Eng., Monash Univ., Clayton, Vic., Australia","Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint","","2003","1","","81","85 Vol.1","In this paper, a vector distance histogram based adaptive vector filter is proposed for the restoration of color images contaminated by the channel correlated impulse noise. Test results on natural image have shown that the new method is superior in suppressing impulse noise while preserving high image details with a marked performance gain over existing state-of-the-art filters.","","0-7803-8185","10.1109/ICICS.2003.1292417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292417","","Histograms;Adaptive filters;Color;Image restoration;Colored noise;Noise reduction;Additive noise;Image edge detection;Digital images;Pixel","adaptive filters;vectors;image colour analysis;image restoration;impulse noise;filtering theory;image classification;image denoising","adaptive vector filter;color image restoration;vector distance histogram;impulse noise suppression;switch-based filtering;pixel classification;optimization method","","4","18","","","","","","IEEE","IEEE Conferences"
"Rapid architecture prototyper (RAP)","M. Andrews","Space Tech Corp., Fort Collins, CO, USA","[1989] Proceedings of the Twenty-Second Annual Hawaii International Conference on System Sciences. Volume 1: Architecture Track","","1989","1","","5","13 vol.1","A novel hardware prototyper is described for rapid retargeting of hardware and compilers. Architects can directly create a machine organization, using compiler support design tradeoffs. This rapid prototyping tool features relatively simple hosting (on a PC-AT), diverse target architectures, and minimal use of host memory. It provides simple and rapid retargeting through careful utilization of code transformations and expansions internal to the tool. It can be used to test computing hardware by the direct simulation of applications code written in C. Optimization techniques are applied both at the language-dependent level to the intermediate form by shape analysis and during code generation through cost analysis. A major advantage is a significant reduction in microcode development time.<<ETX>>","","0-8186-1911","10.1109/HICSS.1989.47137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=47137","","Prototypes;Hardware;Artificial intelligence;Software prototyping;Architecture;Software tools;Costs;Software design;Very large scale integration;Engines","computer architecture;program compilers","rapid architecture prototyper;optimisation techniques;hardware prototyper;rapid retargeting;hardware;compilers;machine organization;PC-AT;target architectures;code transformations;direct simulation;applications code;C;shape analysis;code generation;cost analysis;microcode development time","","","12","","","","","","IEEE","IEEE Conferences"
"Assessing user experience with CASE tools: an exploratory analysis","G. C. Everest; M. Alanis","Minnesota Univ., Minneapolis, MN, USA; NA","Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences","","1992","iii","","343","352 vol.3","The paper reports on the results of a survey of 19 large organizations in the Twin Cities of Minnesota. The survey gathered data on the experiences, expectations, realizations, and suggestions regarding the use of CASE tools. It focused on what they considered to be the critical success factors in the introduction and use of CASE. Most organizations had strong management support and a high ranking executive to champion the adoption and use of CASE. More organizations had made a commitment to use construction tools than design, and to use design tools than planning tools. Larger organizations were further along in their commitment to use CASE than smaller organizations.<<ETX>>","","0-8186-2420","10.1109/HICSS.1992.183502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=183502","","Computer aided software engineering;Management training;Information systems;Project management;Productivity;Technology management;Cities and towns;System testing;Instruments;Costs","software tools","user experience;CASE tools;success factors;construction tools;design tools;planning tools","","1","32","","","","","","IEEE","IEEE Conferences"
"Proceedings 2003. Design Automation Conference (IEEE Cat. No.03CH37451)","","","Proceedings 2003. Design Automation Conference (IEEE Cat. No.03CH37451)","","2003","","","","","The following topics are dealt with: real challenges and solutions for validating System-on-Chip, reshaping EDA for power, design for manufacturability and global routing, design analysis techniques, embedded hardware design case studies, emerging design and tool challenges in RF and wireless applications, power grid analysis and optimization, low-power embedded system design, cyclic and non-cyclic combinational synthesis, managing leakage power, timing-oriented placement, issues in partitioning and design space exploration for codesign, nanotechnology: design implications and CAD challenges, simulation coverage and generation for verification, tool support for architectural decisions in embedded systems, new topics in logic synthesis, coping with variability: the end of deterministic design, testbench, verification and debugging: practical considerations, delay and noise modeling in the nanometer regime, modeling issues in the design of embedded systems, how application/technology evolutions will shape classical EDA, SAT and BDD algorithms for verification tools, elements of functional and performance analysis, nonlinear model order reduction, novel techniques in high-level synthesis, mixed-signal design and simulation, novel self-test methods, technology mapping, buffering, and bus design, compilation techniques for reconfigurable devices, architectural power estimation and optimization, techniques for reconfigurable logic applications, test and diagnosis for complex designs, highlights of ISSCC: high-speed heterogenous design techniques, highlights of ISSCC and the design of state-of-the-art microprocessors, high frequency interconnect modeling, novel approaches in test cost reduction, retargetable tools for embedded software, ASIC design in nanometer era - dead or alive?, floorplanning and placement, advances in SAT, novel design methodologies and signal integrity, memory optimization for embedded systems, design automation for quantum circuits, energy-aware sys","","1-58113-688","10.1109/DAC.2003.1218765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218765","","Integrated circuit design;Design automation","integrated circuit design;system-on-chip;electronic design automation;circuit layout CAD","system-on-chip;hardware design;RF application;wireless application;power grid analysis;embedded system design;cyclic combinational synthesis;noncyclic combinational synthesis;timing-oriented placement;design space exploration;logic synthesis;EDA;electronic design automation;SAT;satisfiability;BDD algorithm;binary decision diagram;nonlinear model order reduction;self-test method;technology mapping;reconfigurable device;architectural power estimation;reconfigurable logic application;state-of-the-art microprocessor;test cost reduction;retargetable tool;ASIC design;application specific integrated circuit;memory optimization;quantum circuit;interconnect noise avoidance;slew rate prediction","","","","","","","","","IEEE","IEEE Conferences"
"High-speed data paths in host-based routers","S. Walton; A. Hutton; J. Touch","Cinesite Digital Studios, USA; NA; NA","Computer","","1998","31","11","46","52","In networking today, host workstations are increasingly being used as routers. Host based routers offer a number of advantages, but they suffer from inefficient support for high bandwidth interfaces. The authors' work has focused on the technology's major drawback its inefficiency in supporting high bandwidth interfaces. Their approach is to optimize packet processing by applying techniques that transfer packets directly among host interfaces, thus removing an extra data copy. This technique increases data throughput by 45 percent while reducing the host's CPU load. They found that peer DMA forwarding can increase host based router throughput by up to 45 percent, supporting bandwidths of 480 Mbps. Peer DMA host based forwarding requires network interface cards with substantial shared memory resources, because packet queues are stored on the interfaces themselves, rather than in host RAM. The queuing algorithm remains in the host CPU, supporting advanced queue management. Current systems have limited packet processing. A combination of streamlined forwarding algorithms and aggregate interrupt processing should further increase host based capability. Moving some of the IP processing out to the NIC coprocessor may enable this. It is also apparent that as processor speeds increase, the advantages of peer DMA will aid throughput for small packet sizes.","0018-9162;1558-0814","","10.1109/2.730736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730736","","Read-write memory;Network interfaces;Memory management;Random access memory;Workstations;Production;Collaborative software;Software systems;Routing;Software testing","network interfaces;packet switching;queueing theory;performance evaluation;shared memory systems","high speed data paths;host based routers;networking;host workstations;high bandwidth interfaces;packet processing;data copy;peer DMA forwarding;host based router throughput;peer DMA host based forwarding;network interface cards;shared memory resources;packet queues;queuing algorithm;advanced queue management;streamlined forwarding algorithms;aggregate interrupt processing;host based capability;IP processing;NIC coprocessor;processor speeds","","3","9","","","","","","IEEE","IEEE Journals & Magazines"
"Design philosophy for self-repair of electronic systems using the UML","E. A. Coyle; L. P. Maguire; T. M. McGinnity","Intelligent Syst. Eng. Lab., Ulster Univ., Derry, UK; Intelligent Syst. Eng. Lab., Ulster Univ., Derry, UK; Intelligent Syst. Eng. Lab., Ulster Univ., Derry, UK","IEE Proceedings - Software","","2002","149","6","179","186","As electronic systems are entrusted with increasing numbers of critical tasks, it becomes important that these systems exhibit high levels of reliability. The increasing complexity of modern systems, combined with reduced development times, makes the production of fault-free systems extremely difficult. Minimising the effects of such faults is a challenging task. Developing a self-repair capability for electronic systems would provide a means to alleviate the effects of a fault. This approach, inspired by healing in biological systems, offers the capability of repairing faults without user intervention. This paper describes a design methodology targeted at self-repairing systems based on modelling systems using the unified modelling language (UML). The approach increases the flexibility of systems by enabling the transfer of functionality between hardware and software. The system configuration is produced by an intelligent reasoning approach, capable of producing optimised configurations at the design stage, or to repair a fault. This design methodology is demonstrated on an example system and the challenges this presents are discussed.","1462-5970","","10.1049/ip-sen:20020793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167721","","","automatic testing;circuit CAD;fault tolerance;inference mechanisms","Unified Modelling Language;UML;electronic systems;functional repair;circuit CAD;self-repairing systems;functionality transfer;intelligent reasoning;fault tolerance","","","","","","","","","IET","IET Journals & Magazines"
"ARM7100-a high-integration, low-power microcontroller for PDA applications","G. Budd; G. Milne","Adv. RISC Machines Ltd., Cambridge, UK; Adv. RISC Machines Ltd., Cambridge, UK","COMPCON '96. Technologies for the Information Superhighway Digest of Papers","","1996","","","182","187","The ARM7100 is the first of a new generation of highly integrated ARM-based microcontrollers, using modular design techniques based on the Advanced Microcontroller Bus Architecture (AMBA) to simplify design and test while optimizing for lowest power (70 mW) and low die size. The ARM7100, which is targeted at PDA applications, delivers 18.4 MIPS (peak) at 3.3 V and contains an embedded ARM7l0a core (including 8 kByte cache and MMU) with ARM-library peripherals such as an LCD controller, UART and CODEC interface. This paper gives an overview of the ARM7100 microcontroller and describes the architecture used to optimize for low power while maintaining high performance.","1063-6390","0-8186-7414","10.1109/CMPCON.1996.501766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=501766","","Microcontrollers;Clocks;Energy consumption;Design optimization;Testing;System-on-a-chip;Oscillators;Application software;Codecs;Computer architecture","microcontrollers;computer interfaces;power consumption;portable computers","ARM7100;high-integration;low-power microcontroller;PDA applications;highly integrated ARM-based microcontrollers;modular design techniques;Advanced Microcontroller Bus Architecture;embedded ARM7l0a core;LCD controller;UART;CODEC interface","","1","5","","","","","","IEEE","IEEE Conferences"
"High-Level Switching Activity Prediction Through Sampled Monitored Simulation","F. Klein; R. Azevedo; G. Araujo","Institute of Computing State University of Campinas, UNICAMP Campinas, Brazil, klein@ic.unicamp.br; NA; NA","2005 International Symposium on System-on-Chip","","2005","","","161","166","This paper presents a sample-based technique to predict the switching activity of digital circuits. It is an improvement to PowerSC, a SystemC library extension that enables a fast and easy-to-use way of gathering switching activity from SystemC descriptions. The experimental results reveal that it can dramatically reduce the monitoring time of the simulation, with a minimal loss of accuracy with respect to estimates provided by an industrial tool. Several tests realized in a case study with a real-world design obtained reductions in the monitoring time of up to 99% with average errors of no more than 0.05%.","","0-7803-9294","10.1109/ISSOC.2005.1595668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595668","","Predictive models;Circuit simulation;Energy consumption;Computational modeling;Digital circuits;Costs;Design optimization;Condition monitoring;Switching circuits;Software libraries","circuit simulation;hardware description languages;software libraries;switching circuits","high-level switching activity prediction;sampled monitored simulation;PowerSC SystemC library extension;digital circuits","","1","14","","","","","","IEEE","IEEE Conferences"
"Processor/memory co-exploration on multiple abstraction levels","G. Braun; A. Wieferink; O. Schliebusch; R. Leupers; H. Meyr; A. Nohl","Integrated Signal Process. Syst., Aachen, Germany; Integrated Signal Process. Syst., Aachen, Germany; Integrated Signal Process. Syst., Aachen, Germany; Integrated Signal Process. Syst., Aachen, Germany; Integrated Signal Process. Syst., Aachen, Germany; NA","2003 Design, Automation and Test in Europe Conference and Exhibition","","2003","","","966","971","Recently, the evolution of embedded systems has shown a strong trend towards application-specific, single-chip solutions. As a result, application-specific instruction set processors (ASIP) are more and more replacing off-the-shelf processors in such systems-on-chip (SoC). Along with the processor cores, heterogeneous memory architectures play an important role as part of the system. According to last year's ITRS, in 2004 about 70 percent of the chip area will be made up of memories. As such architectures are highly optimized for a particular application domain, processor core and memory subsystem design cannot be apart, but have to merge into an efficient design process. In this paper, we present a unified approach for processor/memory co-exploration using an architecture description language. We show an efficient way, of considering instruction set and memory architecture during the entire exploration process. Finally, we illustrate the feasibility of our approach with a real-world case study.","1530-1591","0-7695-1870","10.1109/DATE.2003.1253730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253730","","Memory architecture;Application specific processors;Hardware design languages;Process design;Architecture description languages;Energy consumption;Microarchitecture;Instruction sets;Signal processing;Embedded system","memory architecture;embedded systems;system-on-chip;microprocessor chips;integrated circuit design;circuit CAD;hardware-software codesign","processor/memory co-exploration;multiple abstraction levels;embedded systems;application-specific instruction set processors;ASIP;systems-on-chip;SoC;architecture description language;heterogeneous memory architectures","","9","19","","","","","","IEEE","IEEE Conferences"
"Life cycle costing for batteries in telecom applications","A. Green","Saft Adv. & Ind. Battery Group, Romainville, France","INTELEC - Twentieth International Telecommunications Energy Conference (Cat. No.98CH36263)","","1998","","","1","7","The economics of a telecom battery system comprises not only the initial cost, but also the cost of installation, maintenance, testing and disposal. In many cases, the initial cost gives a very misleading impression of the total cost of the system during its lifetime. Thus, more and more, in many applications, the total cost of the system over its lifetime, or life cycle cost, needs to be established in order to have a true economic analysis. A computer program, running in MS Windows, has been developed to generate life cycle costing for batteries in general and it has been optimized specifically for the telecom market. The paper describes the general features which are used in the software and gives details and examples of the output possible.","","0-7803-5069","10.1109/INTLEC.1998.793470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=793470","","Costing;Batteries;Telecommunications;Costs;Environmental economics;System testing;Manufacturing;Temperature dependence;Communication industry;Industrial economics","telecommunication power supplies;secondary cells;life cycle costing;economics;power engineering computing;telecommunication computing","telecommunication applications;battery life cycle costing;installation;maintenance;testing;disposal;initial cost;total cost;economic analysis;computer program;MS Windows;computer simulation","","6","3","","","","","","IEEE","IEEE Conferences"
"Influence of electrode geometry on transport and separation efficiency of powders using travelling wave field technique","W. Machowski; W. Balachandran; D. Hu","Dept. of Electron. & Electr. Eng., Surrey Univ., Guildford, UK; Dept. of Electron. & Electr. Eng., Surrey Univ., Guildford, UK; Dept. of Electron. & Electr. Eng., Surrey Univ., Guildford, UK","IAS '95. Conference Record of the 1995 IEEE Industry Applications Conference Thirtieth IAS Annual Meeting","","1995","2","","1508","1513 vol.2","In this paper the separation processes of powders using 3-phase travelling wave field were studied theoretically and experimentally. The separation properties of the nonuniform travelling wave field was investigated by studying the amplitude of its spatial harmonics. The finite element software was used to model the spatial distribution of electric field created by the electrodes with different shapes, It was established that cylindrical, square and stripe electrodes produce the most optimised field distribution. Based on the modelling results several flat-bed travelling wave panels with stripe and cylindrical electrodes were constructed and tested. The separation characteristics of the panels were assessed by examining the direction of transport of a two powder mixture with different size distributions for the frequencies of AC potential up to 300 Hz. It was discovered that for the majority of panels successful powder separation could be achieved for two frequency bands. The position of the bands depended mainly on electrode geometry (i.e. pitch and width). In order to further understand transport phenomena in both separation bands particles trajectories were examined using a CCD camera with telemicroscopic lenses interfaced with computer controlled image grabbing system.","0197-2618","0-7803-3008","10.1109/IAS.1995.530481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=530481","","Electrodes;Geometry;Powders;Frequency;Separation processes;Finite element methods;Shape;Testing;Charge coupled devices;Charge-coupled image sensors","electromagnetic field theory;CCD image sensors;image processing equipment;electrodes;powder technology;finite element analysis","powder transport;powder separation;stripe electrodes;electrode geometry;travelling wave field technique;3-phase;nonuniform travelling wave;spatial harmonics;finite element software;spatial distribution;cylindrical electrodes;CCD camera;telemicroscopic lenses;computer controlled image grabbing system","","3","6","","","","","","IEEE","IEEE Conferences"
"Winging it [Free Flight, air traffic control]","E. A. Bretz","NA","IEEE Spectrum","","2001","38","1","97","99","The FAA believes that the solution to delayed flights is beginning to shape up. The agency is optimistic that its modernization plan and efforts to implement a program called Free Flight will go a long way toward solving the problem. Simply put, Free Flight opens the skies to air traffic, removing many of the enforced air routes that planes were required to follow. The program calls for air traffic controllers to continue to supervise flight operations and monitor safety measures. But air traffic controllers, still using equipment that is decades old and breaks down frequently, are wary of FAA promises. Planned software upgrades are being installed to improve air traffic control systems. Upgrades in about half of the 20 en route centers in the United States, which handle aircraft at higher, cruising altitudes, have gone pretty smoothly, but there have been some mishaps. What will help the situation, and is already being tested in limited circumstances in certain air traffic facilities as prototypes, is better technology that will give controllers more reliable displays, computer systems, decision-support tools, and data-all goals of Free Flight, or at least of Phase I of the program.","0018-9235;1939-9340","","10.1109/6.901154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=901154","","Air traffic control;FAA;Computer displays;Delay;Shape;Monitoring;Air safety;Aerospace safety;Aircraft;System testing","air traffic control;aerospace computing","Free Flight;air traffic control;air traffic controllers;flight operations supervision;safety measures monitoring;software upgrades;air traffic facilities;decision-support tools;reliable displays;computer systems","","","","","","","","","IEEE","IEEE Journals & Magazines"
"E Minor upgrade","M. A. Sinclair; T. F. Bryant","Pulsed Power Group, Atomic Weapons Establ., Reading, UK; Pulsed Power Group, Atomic Weapons Establ., Reading, UK","PPPS-2001 Pulsed Power Plasma Science 2001. 28th IEEE International Conference on Plasma Science and 13th IEEE International Pulsed Power Conference. Digest of Papers (Cat. No.01CH37251)","","2001","2","","1499","1501 vol.2","The E Minor machine at AWE Aldermaston was originally designed and built as a test-bed for the Marx generator of the 10MV, 80ns Mogul E Flash X-Ray machine. For this purpose it had a short (40ns) oil insulated Blumlein, which originally fed a resistive load. After Mogul E was completed, E Minor was converted into a flash X-Ray machine using an existing, surplus insulator stack from another machine on site (EROS). Unfortunately this insulator stack is only capable of holding off 5.5MV, well below the maximum possible output of the E-Minor Marx. To continue the X-ray diode development work required for the proposed Hydrodynamics Research Facility (HRF), E Minor needs to be upgraded so that it can deliver a 10MV pulse. E Minor will be upgraded to full voltage operation by fitting a Mogul-E-style insulator stack, requiring modifications to the front end of the machine and to the building in which it is housed. 2d electrostatic modelling software has been used to design the new front end the machine. The pre-pulse switch has been enlarged for the higher voltage operation, the toroid on the intermediate cylinder of the Blumlein will increase in radius for the same reason and the field shapers that flatten the electric field across the insulator stack have been optimised.","","0-7803-7120","10.1109/PPPS.2001.1001842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1001842","","Voltage;Switches;Testing;Petroleum;Oil insulation;Diodes;Hydrodynamics;Buildings;Electrostatics;Dielectrics and electrical insulation","pulsed power supplies;pulse generators;X-ray apparatus","E Minor upgrade;AWE Aldermaston;test-bed;Marx generator;Mogul E Flash X-Ray machine;oil insulated Blumlein;surplus insulator stack;X-ray diode development;Hydrodynamics Research Facility;insulator stack;10 MV;80 ns;40 ns;5.5 MV","","","2","","","","","","IEEE","IEEE Conferences"
"Non-linear modelling of microwave PIN diode switches for harmonic and intermodulation distortion simulation","V. Cojocaru; F. Sischka","TDK Electron. Ireland Ltd., Dublin, Ireland; NA","IEEE MTT-S International Microwave Symposium Digest, 2003","","2003","2","","655","658 vol.2","The paper presents for the first time a practical and relatively simple non-linear modeling solution for /spl mu/-wave PIN diodes for use in harmonic balance (HB) circuit simulators. It enables very good prediction performance of the very important harmonic and intermodulation characteristics of these devices when used in typical /spl mu/-wave switching applications. Accurate harmonic distortion measurements have been performed on a number of commercial surface mount PIN diodes, over a wide range of relevant bias conditions and RF power levels. Parasitic elements are first extracted from microwave S-parameter data. Separate non-linear circuit models are then used for the ON and OFF states of the PIN diodes, and these are directly optimized on the measured harmonic distortion data using a dedicated modeling platform developed within a commercial modeling software tool. The new models have been implemented and tested in a popular harmonic balance circuit simulator with very good results. The modeling solution described here can be also applied to other active devices for similar large-signal modeling problems.","0149-645X","0-7803-7695","10.1109/MWSYM.2003.1212458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1212458","","Switches;Intermodulation distortion;Circuit simulation;Harmonic distortion;Distortion measurement;Circuit testing;Power measurement;Performance evaluation;Radio frequency;Data mining","p-i-n diodes;semiconductor device models;intermodulation distortion;harmonic distortion;surface mount technology;microwave diodes;semiconductor switches;microwave switches;circuit simulation;nonlinear network analysis;S-parameters;equivalent circuits;semiconductor device measurement","nonlinear modeling;IMD;/spl mu/-wave PIN diodes;harmonic balance circuit simulators;harmonic characteristics;intermodulation characteristics;/spl mu/-wave switching applications;harmonic distortion measurements;surface mount PIN diodes;bias conditions;RF power levels;parasitic elements;microwave S-parameter data;nonlinear circuit models;dedicated modeling platform;commercial modeling software tool;active devices","","1","7","","","","","","IEEE","IEEE Conferences"
"Design-based mission operation","Meemong Lee; R. J. Weidner; Wenwen Lu","Jet Propulsion Lab., Pasadena, CA, USA; NA; NA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","7","","7","3441 vol.7","The Virtual Mission project led by the Mission Simulation and Instrument Modeling Group at JPL has been playing an active role in the NASA-wide information technology infusion programs, such as, Information System Technology, Next-Generation Infrastructure Technology, and Intelligent Synthesis Environment. The goal of the Virtual Mission project is to enable automated design space exploration, progressive design optimization, and lifecycle-wide design validation to ensure mission success. Design-based mission operation has been a major part of the research effort in order to establish system-wide as well as lifecycle-wide impact analysis as an integral part of the mission design process. The design-based mission operation is approached by implementing Virtual Mission Lifecycle (VML), modeling and simulation tools and system engineering processes for building a virtual mission system that can perform a realistic mission operation during the design phase of a mission. As in the real mission lifecycle convention, the VML is composed of design, development, integration and test, and operation phases. This paper describes the four phases of the VML addressing a major challenge per phase, mission model framework, virtual prototyping, agent-based mission system integration, and virtual mission operation.","","0-7803-6599","10.1109/AERO.2001.931421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931421","","Space technology;Process design;Instruments;Information technology;Information systems;Space exploration;Design optimization;Design engineering;Systems engineering and theory;Buildings","aerospace computing;aerospace simulation;digital simulation;CAD;space vehicles;software engineering","Virtual Mission project;JPL;automated design space exploration;progressive design optimization;lifecycle-wide design validation;space missions;NASA technology programs;design-based mission operation;virtual mission lifecycle modeling tools;virtual mission lifecycle simulation tools;system engineering processes;virtual mission system;virtual prototyping;agent-based mission system integration;virtual mission operation","","4","14","","","","","","IEEE","IEEE Conferences"
"VGAUA: The Variable Geometry Automated Universal Array Layout System","D. C. Smith; R. Noto; F. Borgini; S. S. Sharma; J. C. Werbickas","RCA Advanced Technology Laboratories, Camden, NJ; NA; NA; NA; NA","20th Design Automation Conference Proceedings","","1983","","","425","429","This paper presents the Variable Geometry Automated Universal Array (VGAUA) fully automatic physical layout program for gate array LSI which uses a single unique metalization pattern to personalize the array design. This fully automatic layout system minimizes the cost and time required for IC design and can be used over a range of array sizes from 800 to 3,500 gates. Each array size has its own unique, optimized, fixed geometry design for all levels except the programmable metalization level. The paper discusses both the topological design which makes the VGAUA program feasible and the software which makes the system automatic. The key to the success of the system is that the two items are heavily interrelated and highly interdependent.","0738-100X","0-8186-0026","10.1109/DAC.1983.1585687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1585687","","Geometry;Design automation;Logic arrays;Costs;CMOS technology;Large scale integration;Software systems;Routing;CMOS logic circuits;Logic testing","","","","1","4","","","","","","IEEE","IEEE Conferences"
"Library of Digital Circuit Development For Digital System Integration","P. Punugoti; S. V. Wunnava","Florida International University; NA","IEEE SoutheastCon, 2004. Proceedings.","","2004","","","405","409","Present day Digital System Designs have taken a new dimension, especially with the simulation tools such as the ModelSim and synthesis tools such as the Leonardo Spectrum. Unlike the conventional digital system designs, where the logic and functionality are optimized, modern designs call for integration of several digital sub system units for an integrated digital system. Such a scheme needs a collection of digital circuits in a library. It is prohibitive to have very many of different circuits and the associated software coding for each of them. Rather, it is advantageous to identify fundamental digital components and develop the VHDL or Verilog code for these components. Use this code recursively, to develop higher density and more complex circuits.","","0-7803-8368","10.1109/SECON.2004.1287951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287951","","Software libraries;Digital circuits;Digital systems;Hardware design languages;Logic design;Graphics;Logic testing;Digital arithmetic;Logic circuits;Circuit simulation","","","","","5","","","","","","IEEE","IEEE Conferences"
"A case study of a multiobjective recombinative genetic algorithm with coevolutionary sharing","M. Neef; D. Thierens; H. Arciszewski","Cognitive Artificial Intelligence, Utrecht Univ., Netherlands; NA; NA","Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)","","1999","1","","796","803 Vol. 1","We present a multiobjective genetic algorithm that incorporates various genetic algorithm techniques that have been proven to be efficient and robust in their problem domain. More specifically, we integrate rank based selection, adaptive niching through coevolutionary sharing, elitist recombination, and non-dominated sorting into a multiobjective genetic algorithm called ERMOCS. As a proof of concept we test the algorithm on a softkill-scheduling problem.","","0-7803-5536","10.1109/CEC.1999.782014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=782014","","Computer aided software engineering;Genetic algorithms;Testing;Spontaneous emission;Artificial intelligence;Computer science;Computational modeling;Computer simulation;Physics;Laboratories","genetic algorithms;algorithm theory;scheduling","multiobjective recombinative genetic algorithm;coevolutionary sharing;genetic algorithm techniques;rank based selection;adaptive niching;elitist recombination;nondominated sorting;ERMOCS;softkill-scheduling problem","","3","18","","","","","","IEEE","IEEE Conferences"
"Design of autonomous photovoltaic power plant for telecommunication relay station","S. Liu; R. A. Dougal; E. V. Solodovnik","Dept. of Electr. Eng., Univ. of South Carolina, Columbia, SC, USA; Dept. of Electr. Eng., Univ. of South Carolina, Columbia, SC, USA; Dept. of Electr. Eng., Univ. of South Carolina, Columbia, SC, USA","IEE Proceedings - Generation, Transmission and Distribution","","2005","152","6","745","754","The design and prototyping of an autonomous photovoltaic power generation system using the Virtual Test Bed software environment is presented. The comprehensive design of the photovoltaic power plant is very important for achieving the best performance, durability and reliability of the system. The goal of the design is to optimise the system performance by accounting for various data available at the design time. These data may include statistical data of weather conditions at particular geographic locations, the statistics of the load profiles and, finally, the projected outlook of the system use. The design of the autonomous photovoltaic power system presented in this work includes establishing physics-based models for components, prototyping system architectures, implementing control algorithms, testing and evaluating the system performance using the actual weather data at a particular geographical location. The dynamic performance of the photovoltaic power system and its components are revealed in great detail through the system-level simulation. This knowledge is used for verification and improvements of the system design in order to achieve the best system power capability, efficiency and size.","1350-2360","","10.1049/ip-gtd:20045028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1532091","","","photovoltaic power systems;telecommunication power supplies;power engineering computing;statistical analysis","autonomous photovoltaic power plant;telecommunication relay station;Virtual Test Bed software;system reliability;statistical data;weather conditions;geographic locations;load profiles;physics-based models;prototyping system architectures;control algorithms;dynamic performance;system-level simulation","","2","","","","","","","IET","IET Journals & Magazines"
"Developing tactics using low cost, accessible simulations","L. H. McKenna; S. Little","Maritime Warfare Centre, Portsmouth, UK; NA","2000 Winter Simulation Conference Proceedings (Cat. No.00CH37165)","","2000","1","","991","1000 vol.1","The Royal Navy's Maritime Warfare Centre (MWC) is responsible to the UK Commander-in-Chief Fleet (CinCFleet) and was formed with the purpose of developing operational tactics and procedures to optimize the capability of the Fleet's platforms, sensors and weapon systems. Evaluating tactics at sea requires a considerable amount of forward planning and ties up valuable and expensive assets. It is therefore important that the candidate tactics must be developed to a sufficient level of maturity on-shore. This is done through a combination of individual brainpower, paper studies and computer simulation. The computer simulation must be inexpensive, totally flexible, sufficiently accurate, reliable and above all easily available to, and usable by, the individual tactical desk officers. Any simulations developed need to be easily adaptable. Tactical development is not a formally structured process; software development is not easy when there are no formal requirements. The MWC have investigated using the Spreadsheet Excel to form the basis of such simulations. This paper discusses the advantages and disadvantages of this approach, in creating simulations that can be used for developing tactics that have the necessary degree of flexibility, integrity and usability. A specific example of an application to a particular problem will be illustrated.","","0-7803-6579","10.1109/WSC.2000.899902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899902","","Costs;Weapons;Sensor systems;Computational modeling;Computer simulation;Testing;Brain modeling;Programming;Usability;Underwater vehicles","software engineering;digital simulation;military communication;military computing","tactics;accessible simulations;Royal Navy's Maritime Warfare Centre;operational tactics;weapon systems;maturity on-shore;computer simulation;tactical development;software development;Spreadsheet Excel;integrity;usability","","","","","","","","","IEEE","IEEE Conferences"
"Detection of volatile organic compounds using quartz crystal microbalance sensor array and artificial neural network","E. Noorsal; O. Sidek; J. Mohamad-Sateh; M. N. Ahmad","NA; NA; NA; NA","IEEE Conference on Cybernetics and Intelligent Systems, 2004.","","2004","2","","931","936","This paper presents the design and development of an odour sensing system based on a quartz crystal microbalance (QCM) odour-sensor array and an artificial neural network (ANN) for the identification of some of the volatile organic compounds (VOCs) such as acetone, benzene, chloroform, ethanol and methanol. The QCM sensors were developed using PVC-blended lipids as sensing materials. A home-built data acquisition and embedded pattern recognition system were developed using the Xilinx IC and AT89C52 microcontroller. In addition, user interface software was developed to control the vapours flow system, data acquisition, process the acquired data and display the detected vapours using optimised neural network. The performance of this odour sensing system for VOCs emission detection was tested under laboratory conditions to determine its ability to detect single odour compound emission. Simulation and experimental results using an optimised neural network system confirmed that the proposed odour recognition system was effective in identifying the tested VOCs","","0-7803-8643","10.1109/ICCIS.2004.1460713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460713","","Sensor arrays;Gas detectors;Volatile organic compounds;Artificial neural networks;Data acquisition;Neural networks;System testing;Ethanol;Methanol;Lipidomics","data acquisition;electronic noses;microcontrollers;neural nets;pattern recognition;quartz;sensor arrays;user interfaces","volatile organic compound detection;artificial neural network;odour sensing system design;odour sensing system development;quartz crystal microbalance odour sensor array;volatile organic compound identification;PVC-blended lipids;polyvinyl chloride;sensing material;data acquisition;embedded pattern recognition system;Xilinx IC;AT89C52 microcontroller;user interface software;vapour flow system control;detected vapour display;optimised neural network;odour compound emission;odour recognition system","","1","12","","","","","","IEEE","IEEE Conferences"
"Using fuzzy logic to analyze superscript and subscript relations in handwritten mathematical expressions","Ling Zhang; D. Blostein; R. Zanibbi","Sch. of Comput., Queen's Univ., Kingston, Ont., Canada; Sch. of Comput., Queen's Univ., Kingston, Ont., Canada; NA","Eighth International Conference on Document Analysis and Recognition (ICDAR'05)","","2005","","","972","976 Vol. 2","Handwritten mathematical notation contains ambiguities of various kinds. Here we focus on ambiguity in spatial relationships; in particular, we use fuzzy logic to treat ambiguity in subscript-or-inline and inline-or-superscript spatial relationships. We extend an existing system for recognizing handwritten mathematical notation, adding the capability of producing a ranked list of interpretations rather than a single top-choice interpretation. Fuzzy membership values are assigned to each spatial relationship; a given pair of symbols can have non-zero membership in fuzzy sets subscript and inline, or in fuzzy sets inline and superscript. These fuzzy membership values are combined to produce an overall confidence value for the entire interpretation. We have modified the user interface of our system so that a user can quickly view and select from the ranked interpretations when the highest confidence interpretation is incorrect.","1520-5363;2379-2140","0-7695-2420","10.1109/ICDAR.2005.250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575688","","Fuzzy logic;Optical character recognition software;Handwriting recognition;Fuzzy sets;User interfaces;Spatial resolution;Testing;Bars;Text analysis;Stochastic systems","handwritten character recognition;fuzzy logic;fuzzy set theory;computational linguistics","fuzzy logic;handwritten mathematical expressions;subscript-or-inline spatial relationship;inline-or-superscript spatial relationship;handwritten mathematical notation recognition;fuzzy sets","","7","18","","","","","","IEEE","IEEE Conferences"
"Application oriented automatic structuring of time-delay neural networks for high performance character and speech recognition","U. Bodenhausen; A. Waibel","Dept. of Comput. Sci., Karlsruhe Univ., Germany; NA","IEEE International Conference on Neural Networks","","1993","","","1627","1632 vol.3","Highly structured artificial neural networks can be optimized in many ways, and must be optimized for optimal performance. A highly structured approach is the multistate time delay neural network (MSTDNN) which uses shifted input windows and allows the recognition of sequences of ordered events that have to be observed jointly. An automatic structure optimization (ASO) algorithm is proposed and applied to MSTDNN-type networks. The ASO algorithm optimizes all relevant parameters of MSTDNNs automatically and is successfully tested with three different tasks and varying amounts of training data.<<ETX>>","","0-7803-0999","10.1109/ICNN.1993.298800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=298800","","Neural networks;Training data;Speech recognition;Artificial neural networks;Application software;Computer science;Computer architecture;Character recognition;Delay effects;Automatic testing","character recognition;learning (artificial intelligence);neural nets;speech recognition","character recognition;time-delay neural networks;speech recognition;highly structured approach;shifted input windows;ordered events;automatic structure optimization;training data","","3","17","","","","","","IEEE","IEEE Conferences"
"Fusion of three sensory modalities for the multimodal characterization of red wines","M. L. Rodriguez-Mendez; A. A. Arrieta; V. Parra; A. Bernal; A. Vegas; S. Villanueva; R. Gutierrez-Osuna; J. A. de Saja","Dept. of Inorg. Chem., Univ. of Valladolid, Spain; Dept. of Inorg. Chem., Univ. of Valladolid, Spain; Dept. of Inorg. Chem., Univ. of Valladolid, Spain; NA; NA; NA; NA; NA","IEEE Sensors Journal","","2004","4","3","348","354","This work represents the first attempt to develop a sensory system, specifically designed for the characterization of wines, which combines three sensory modalities: an array of gas sensors, an array of electrochemical liquid sensors, and an optical system to measure color by means of CIElab coordinates. This new analytical tool, that has been called ""electronic panel,"" includes not only sensors, but also hardware (injection system and electronics) and the software necessary for fusing information from the three modules. Each of the three sensory modalities (volatiles, liquids, and color) has been designed, tested, and optimized separately. The discrimination capabilities of the system have been evaluated on a database consisting of six red Spanish wines prepared using the same variety of grape (tempranillo) but differing in their geographic origins and aging stages. Sensor signals from each module have been combined and analyzed using pattern recognition techniques. The results of this work show that the discrimination capabilities of the system are significantly improved when signals from each module are combined to form a multimodal feature vector.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2004.824236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1294915","","Sensor arrays;Optical arrays;Gas detectors;Optical sensors;Sensor phenomena and characterization;Sensor systems;Optical design;Coordinate measuring machines;Information analysis;Hardware","gas sensors;electrochemical sensors;optical sensors;pattern recognition;optimisation;sensor fusion","sensory system;sensory modalities;gas sensors;electrochemical liquid sensors;optical system;color;CIElab coordinates;analytical tool;electronic panel;hardware;injection system;electronics;software;fusing information;optimization;discrimination capabilities;red Spanish wines;geographic origins;aging stages;sensor signals;pattern recognition;multimodal feature vector;electronic nose;electronic tongue;multimodal characterization","","64","23","","","","","","IEEE","IEEE Journals & Magazines"
"Building an IP network quality-of-service testbed","D. T. McWherter; J. Sevy; W. C. Regli","Drexel Univ., Philadelphia, PA, USA; NA; NA","IEEE Internet Computing","","2000","4","4","65","73","The Drexel Network Toolkit is a software package for testing various approaches to QoS on IP-based networks. It uses Linux and DiffServ packet-marking primitives to classify and prioritize packets. DNT was used in a project to evaluate satellite based IP delivery for multimedia applications in telemedicine and telemaintenance.","1089-7801;1941-0131","","10.1109/4236.865089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=865089","","IP networks;Quality of service;Testing;Traffic control;Delay;Internet;Routing;Application software;Linux;Satellites","Internet;protocols;quality of service;Unix;packet switching;multimedia systems;telecommunication computing;telemedicine","IP network quality-of-service testbed;Drexel Network Toolkit;software package;QoS;IP based networks;Linux;DiffServ packet-marking primitives;packet classification;satellite based IP delivery;multimedia applications;telemedicine;telemaintenance","","5","13","","","","","","IEEE","IEEE Journals & Magazines"
"Genetic algorithms and multifractal segmentation of cervical cell images","N. Lassouaoui; L. Hamami","Lab. des Syst. Informatiques, Centre de Recherche sur l'Inf. Sci. et Tech., Algiers, Algeria; Lab. des Syst. Informatiques, Centre de Recherche sur l'Inf. Sci. et Tech., Algiers, Algeria","Seventh International Symposium on Signal Processing and Its Applications, 2003. Proceedings.","","2003","2","","1","4 vol.2","This paper deals with the segmentation problem of cervical cell images. Knowing that the malignity criteria appear on the morphology of the core and the cytoplasm of each cell, then, the goal of this segmentation is to separate each cell on its component, that permits to analyze separately their morphology (size and shape) in the recognition step, for deducing decision about the malignity of each cell. For that, we use a multifractal algorithm based on the computation of the singularity exponent on each point of the image. For increasing the quality of the segmentation, we propose to add an optimization step based on genetic algorithms. The proposed processing has been tested on several images. Herein, we present some results obtained by two cervical cell images.","","0-7803-7946","10.1109/ISSPA.2003.1224800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1224800","","Genetic algorithms;Fractals;Image segmentation;Biological cells;Morphology;Genetic mutations;Shape;Testing;Application software;Computer vision","genetic algorithms;cellular biophysics;fractals;image segmentation;mathematical morphology;medical image processing","cervical cell images;genetic algorithms;multifractal algorithm;segmentation;core;cytoplasm;morphology","","6","10","","","","","","IEEE","IEEE Conferences"
"Parameter estimation for two synthetic gene networks: a case study","D. Braun; S. Basu; R. Weiss","Dept. of Molecular Biol., Princeton Univ., NJ, USA; NA; NA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","","2005","5","","v/769","v/772 Vol. 5","In this paper, we use two synthetic gene networks, a transcriptional cascade and a pulse generating network, to study the efficacy of a simple statistical parameter fitting algorithm. The fitting was performed on experimental data and computer-generated data (to test how well the algorithm works under ideal conditions with perfect information). Most of the experimental parameter estimations yielded tight ranges of kinetic values for both gene networks. However, the results using simulated data indicate that the algorithm was able to provide better parameter estimates for the pulse generating network than for the transcriptional cascade. This is likely a result of the larger amount of time-series data available for the pulse generator and its greater level of phenotypical complexity, leading to tighter constraints for optimization. The variation in the magnitudes of the standard deviations between parameter estimates may give an indication of system sensitivity to specific kinetic rate constants. In the future, we also plan to verify the experimental results by constructing network variants and attempting to predict behaviors using values obtained in this study.","1520-6149;2379-190X","0-7803-8874","10.1109/ICASSP.2005.1416417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416417","","Parameter estimation;Computer aided software engineering;Kinetic theory;Pulse generation;Cost function;Computational modeling;Computational biology;Mathematical model;Biological system modeling;Testing","genetics;parameter estimation;statistical analysis","kinetic rate constants;synthetic gene networks;parameter estimation;transcriptional cascade gene network;pulse generating network;statistical parameter fitting;time-series data;phenotypical complexity","","4","10","","","","","","IEEE","IEEE Conferences"
"PROTEUS-Lite project: dedicated to developing a telecommunication-oriented FPGA and its applications","T. Miyazaki; A. Takahara; T. Murooka; M. Katayama; T. Ichimori; K. Shirakawa; A. Tsutsui; K. Fukami","NTT Network Innovation Center, Yokosuka, Japan; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2000","8","4","401","414","This paper describes a project dedicated to developing an improved (in terms of usability) version of our previous telecommunication-oriented field programmable gate array (FPGA), and its applications. To achieve this goal, we adopt several challenging design strategies. First, we determine the new FPGA architecture based on a quantitative evaluation carried out to optimize the interaction between the FPGA and CAD algorithms. In addition, we create a new chip design environment that allows semi-automatic test pattern generation and cross-checking between logic and layout design. Furthermore, a dedicated CAD system is developed based on a consideration of the evaluation results and the characteristics of the FPGA. As a result of these design strategies, the FPGA and CAD system are well-balanced, and even though the FPGA has very rich routing resources, the routing process can be finished quickly without sacrificing application-circuit performance. The FPGA is applied to several reconfigurable systems for telecommunications, and is found to offer the required functions and good performance.","1063-8210;1557-9999","","10.1109/92.863619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=863619","","Field programmable gate arrays;Design automation;Routing;Test pattern generators;Logic design;Hardware;Laboratories;Circuits;Pipelines;Usability","field programmable gate arrays;logic CAD;network routing;circuit layout CAD;integrated circuit layout;reconfigurable architectures;hardware-software codesign","PROTEUS-Lite project;telecommunication-oriented FPGA;design strategies;quantitative evaluation;chip design environment;semi-automatic test pattern generation;dedicated CAD system;FPGA;routing resources;application-circuit performance;reconfigurable systems","","5","27","","","","","","IEEE","IEEE Journals & Magazines"
"Investigation of haptic framework for, quantitative design analysis in a virtual, environment","C. Sharma; T. Kesavadas","Dept. of Mech. & Aerosp. Eng., State Univ. of New York, Buffalo, NY, USA; NA","Proceedings Seventh International Conference on Virtual Systems and Multimedia","","2001","","","844","853","The effect of improved cognition on algorithm performance and evaluation is an exciting area of research. Virtual reality is becoming increasingly popular as a tool for interacting with data and steering the optimization process, rather than simply displaying results. Constantly evolving haptic technology allows people to directly interact with digital objects and data as they do in the real world using their sense of touch., Haptics is revolutionizing the way engineers interact with computers. Hitherto, designers and artists have used haptics to intuitively sculpt models using digital clay and doctors have trained on haptics based virtual surgical simulators Most applications however merely seek to make up for the lack of sense of touch and its effects. The current work investigates the application of the haptics loop for decision support activities in core engineering areas like design optimization and shop floor layout. The simulated annealing algorithm with an adaptive cooling schedule is used to solve the quadratic assignment problem for facility layout The results obtained are interpreted and tested using a combined haptic and graphic environment. The value added due to haptics in virtual reality environments is an open problem. Human factor experiments for a decision-making environment were conducted to measure the value added due to inclusion of haptics. After a comprehensive series of tests, a decision is made with regards to use of haptics for engineering and scientific applications.","","0-7695-1402","10.1109/VSMM.2001.969759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969759","","Haptic interfaces;Virtual reality;Application software;Layout;Testing;Cognition;Data engineering;Surgery;Computational modeling;Design engineering","virtual reality;simulated annealing;quadratic programming;haptic interfaces","haptic framework;quantitative design analysis;virtual environment;cognition;algorithm performance;virtual reality;decision support activities;design optimization;shop floor layout;simulated annealing algorithm;human factor;decision-making environment;quadratic assignment problem","","1","9","","","","","","IEEE","IEEE Conferences"
"The use of the SaGa tool for gathering requirements for future information systems","E. Lewis","Dept. of Comput. Sci., Australian Defence Force Acad., Canberra, ACT, Australia","Proceedings of HICSS-29: 29th Hawaii International Conference on System Sciences","","1996","2","","269","278 vol.2","Scenarios have been used in many different ways to assist planning. They have been used for over 20 years in technological forecasting for corporate plans, in testing for the suitability of human-computer interfaces, in experimental gaming to explore decision-making processes, in preparing case-studies for management education, and for ""surfacing"" assumptions underlying strategic planning. One of the more useful applications of scenarios is generating user requirements for information systems. For this application several scenarios could be generated-showing pessimistic, optimistic, and most likely cases-in order to develop the requirements that are common to all cases, so that a system can be designed for a ""surprise-free"" future. However, preparing scenarios is often too laborious to allow for more than one or two scenarios to be generated. As well, it is very difficult to ensure that the scenarios are consistent and coherent. This paper describes the SaGa (Scenario Generator) computer-based tool that easily generates a number of scenarios containing a coherent set of factors. These scenarios can be used to provoke ideas and judgements from managers about how to achieve the future system that they desire. The paper gives the results of using the tool to determine the requirements for information systems for Australian Government agencies.","","0-8186-7324","10.1109/HICSS.1996.495408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=495408","","Information systems;Computer science;Educational institutions;Drives;Technology forecasting;Testing;Decision making;Technology management;Strategic planning;Management information systems","software tools;systems analysis;information systems;human factors;user interfaces","SaGa tool;requirements;information systems;technological forecasting;corporate plans;program testing;human-computer interfaces;experimental gaming;decision-making;management education;strategic planning;user requirements;system design;Scenario Generator;Australian Government agencies","","1","54","","","","","","IEEE","IEEE Conferences"
"Around the lab in 40 days [indoor robot navigation]","R. Alami; M. Herrb; B. Morisset; R. Chatila; F. Ingrand; P. Moutarlier; S. Fleury; M. Khatib; T. Simeon","Lab. d'Autom. et d'Anal. des Syst., CNRS, Toulouse, France; NA; NA; NA; NA; NA; NA; NA; NA","Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)","","2000","1","","88","94 vol.1","The authors previously (1998) argued that the LAAS architecture is one of the most suitable for mobile robot control. This statement may seem over-optimistic, not to say pretentious and unverifiable. After all, can we compare architectures? can we set up benchmarks? or can we measure how good an architecture is compared to another? An architecture defines organization principles, integration methods and supporting tools. Comparing those tools, methods and principles may sometime end up in sterile controversies. However, we think there are means to measure the overall quality (or interest) of an architecture. Development time is for example one relevant criterion. Basically, using a specific architecture, how long does it take to integrate a complete demonstration, including nontrivial decisional capabilities, from the low level functional modules up to the supervisory level? This may seem a rather weak measure of architecture quality; however, it encompasses properties such as genericity and adaptability, ease of design and programming, extensibility and robustness. In this paper we describe our recent experience in integrating a complete demonstration from scratch in 40 days using the LAAS architecture.","1050-4729","0-7803-5886","10.1109/ROBOT.2000.844044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844044","","Robot control;Robustness;Navigation;Buildings;Motion control;Testing;Mobile robots;Laboratories;Artificial intelligence;Software tools","mobile robots;computerised navigation;software architecture;robot programming","LAAS architecture;mobile robot control;organization principles;integration methods;supporting tools;development time;nontrivial decisional capabilities;genericity;adaptability;extensibility;robustness;indoor robot navigation","","9","12","","","","","","IEEE","IEEE Conferences"
"A robust fine granularity scalability using trellis-based predictive leak","Hsiang-Chun Huang; Chung-Neng Wang; Tihao Chiang","Dept. of Electron. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; NA; NA","IEEE Transactions on Circuits and Systems for Video Technology","","2002","12","6","372","385","Recently, the MPEG-4 committee has approved the MPEG-4 fine granularity scalability (FGS) profile as a streaming video tool. We propose novel techniques to improve further the temporal prediction at the enhancement layer so that coding efficiency is superior to the existing FGS. Our approach utilizes two parameters, the number of bitplanes, /spl beta/ (0/spl les//spl beta//spl les/maximal number of bitplanes), and the amount of predictive leak, /spl alpha/ (0/spl les//spl alpha//spl les/1), to control the construction of the reference frame at the enhancement layer. Parameters /spl alpha/ and /spl beta/ can be selected for each frame to provide tradeoffs between coding efficiency and error drift. Our approach offers a general and flexible framework that allows further optimization. It also includes several well-known motion-compensated FGS techniques as special cases with particular sets of /spl alpha/ and /spl beta/. We analyze the theoretical advantages when /spl alpha/ and /spl beta/ are used, and provide an adaptive technique to select /spl alpha/ and /spl beta/, which yields an improved performance as compared to that of fixed parameters. An identical technique is applied to the base layer for further improvement. Our experimental results show over 4 dB improvements in coding efficiency using the MPEG-4 testing conditions. Removal of error propagation is demonstrated with several typical channel transmission scenarios.","1051-8215;1558-2205","","10.1109/TCSVT.2002.800314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013846","","Robustness;Scalability;MPEG 4 Standard;Streaming media;Video compression;Internet;Multimedia communication;Computer errors;Application software;Decoding","video coding;prediction theory;optimisation;motion compensation;visual communication;mobile radio;Internet;multimedia communication","fine granularity scalability;trellis-based predictive leak;MPEG-4;streaming video tool;enhancement layer;coding efficiency;bitplanes;error drift;optimization;motion compensation;adaptive technique;base layer;error propagation;mobile device;wireless channels;Internet;multimedia communications","","50","12","","","","","","IEEE","IEEE Journals & Magazines"
"Mapping of microbial pathways through constrained mapping of orthologous genes","V. Olman; Hanchuan Peng; Zhengchang Su; Ying Xu","Dept. of Biochem. & Molecular Biol., Georgia Univ., Athens, GA, USA; Dept. of Biochem. & Molecular Biol., Georgia Univ., Athens, GA, USA; Dept. of Biochem. & Molecular Biol., Georgia Univ., Athens, GA, USA; Dept. of Biochem. & Molecular Biol., Georgia Univ., Athens, GA, USA","Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004.","","2004","","","363","370","We present a novel computer algorithm for mapping biological pathways from one prokaryotic genome to another. The algorithm maps genes in a known pathway to their homologous genes (if any) in a target genome that is most consistent with (a) predicted orthologous gene relationship, (b) predicted operon structures, and (c) predicted co-regulation relationship of operons. Mathematically, we have formulated this problem as a constrained minimum spanning tree problem (called a Steiner network problem), and demonstrated that this formulation has the desired property through applications. We have solved this mapping problem using a combinatorial optimization algorithm, with guaranteed global optimality. We have implemented this algorithm as a computer program, called P-MAP. Our test results on pathway mapping are highly encouraging - we have mapped a number of pathways of H. influenzae, B. subtilis, H. pylori, and M. tuberculosis to E. coli using P-MAP, whose homologous pathways in E coli. are known and hence the mapping accuracy could be checked. We have then mapped known E. coli pathways in the EcoCyc database to the newly sequenced organism Synechococcus sp WHS 102, and predicted 158 Synechococcus pathways. Detailed analyses on the predicted pathways indicate that P-MAP's mapping results are consistent with our general knowledge about (local) pathways. We believe that P-MAP will be a useful tool for microbial genome annotation projects and inference of individual microbial pathways.","","0-7695-2194","10.1109/CSB.2004.1332449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1332449","","Bioinformatics;Genomics;Organisms;Computational systems biology;Biology computing;Biological information theory;Biochemistry;Computational biology;Steiner trees;Application software","genetics;molecular biophysics;biology computing;trees (mathematics);combinatorial mathematics;optimisation","microbial pathways;constrained mapping;orthologous genes;prokaryotic genome;orthologous gene relationship;operon structures;constrained minimum spanning tree problem;Steiner network problem;combinatorial optimization algorithm;global optimality;P-MAP;H. influenzae;B. subtilis;H. pylori;M. tuberculosis;E. coli;EcoCyc database;Synechococcus sp WHS 102;microbial genome annotation projects","","","20","","","","","","IEEE","IEEE Conferences"
"Application and modelling of quadrature boosters for the HV transmission system","M. Zhu; A. Dale","Nat. Grid Res. & Dev. Centre, Leatherhead, UK; NA","POWERCON '98. 1998 International Conference on Power System Technology. Proceedings (Cat. No.98EX151)","","1998","2","","923","927 vol.2","Multiple quadrature boosters are used on the transmission network of the UK's National Grid Company plc (NGC) to improve the utilisation of its existing assets. The on-load tap changers installed on the quadrature boosters permit the system operator to adjust the power flow pattern as system conditions change. In operational timescales, the quadrature booster tap positions are optimised to eliminate or minimise any uplift in the total generation cost due to transmission thermal constraints. In planning the development of the transmission system, a frequent objective is to maximise the transfer capability of the existing network and thereby avoid unnecessary reinforcement. NGC has developed in-house software tools to automatically optimise the tap positions of interacting quadrature boosters. These tools are used to assess network capability under specific conditions stipulated by the security standards, and to estimate optimised future operational cost for specified periods, e.g., a year. This paper discusses these recent developments and some of their applications.","","0-7803-4754","10.1109/ICPST.1998.729220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=729220","","Programmable control;Integrated circuit interconnections;Cost function;Power system economics;Power generation economics;Capacity planning;Power system interconnection;Circuit testing;On load tap changers;Load flow","transmission networks;power transmission control;power transmission planning;power transformers;transmission network calculations","HV transmission system;multiple quadrature boosters;transmission network;asset utilisation improvement;on-load tap changers;power flow pattern adjustment;power systems;total generation cost;transmission thermal constraints;transfer capability;in-house software tools;network capability assessment","","5","2","","","","","","IEEE","IEEE Conferences"
"An empirical evaluation of techniques for parallel discrete-event simulation of interconnnection networks","J. Miguel; A. Arruabarrena; R. Beivide; J. A. B. Fortes","Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA; NA; NA; NA","Proceedings of 4th Euromicro Workshop on Parallel and Distributed Processing","","1996","","","219","226","Three parallel discrete-event simulators-synchronous, conservative and optimistic-implemented on an Intel Paragon multicomputer are comparatively evaluated. Parallelism is achieved by model decomposition, distributing the simulation among a set of collaborative logical processes. The three simulators differ in the way those processes synchronize to obey causal restrictions in the simulation of events. Message passing network models are used to study these simulation alternatives. A set of experiments are carried out to understand how model parameters influence simulator performance. Experimental evidence leads to the conclusion that the optimistic simulator is not a viable tool for the analysis of this kind of models. The opposite conclusion applies to the other two: if the workload assigned to each logical process is above a certain threshold then the synchronization overhead is comparatively low and the simulators perform and scale well up to a large number of processors. The performance threshold is influenced by some parameters of the simulated model (size of the network, load level and message length), as well as by the number of processors used by the simulators.","","0-8186-7376","10.1109/EMPDP.1996.500590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=500590","","Discrete event simulation;Computational modeling;Multiprocessor interconnection networks;Analytical models;Concurrent computing;Computer simulation;Parallel processing;Message passing;Testing;Intelligent networks","multiprocessor interconnection networks;discrete event simulation;message passing;synchronisation;software performance evaluation;parallel programming;virtual machines","empirical evaluation;parallel discrete-event simulation;interconnnection networks;synchronous simulation;conservative simulation;optimistic simulation;Intel Paragon multicomputer;model decomposition;collaborative logical processes;process synchronization overhead;causal restrictions;message-passing network models;model parameters;simulator performance;workload;scalability;performance threshold;network size;load level;message length;processor number","","","11","","","","","","IEEE","IEEE Conferences"
"Localized watermarking: methodology and application to template mapping","D. Kirovski; M. Potkonjak","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; NA","2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)","","2000","6","","3235","3238 vol.6","The semiconductor industry has adopted the intellectual property (IP) business model as a dominant system-on-chip development platform. Since copyright fraud has been recognized as the most devastating obstruction to this model, a number of techniques for IP protection have been introduced. Most of them rely on a selection of a global solution to an optimization problem according to a unique user-specific digital signature. Although such techniques may provide convincing proof of authorship with little hardware overhead, they fail to protect design partitions, do not provide an easy procedure for watermark detection, and are not capable of detecting the watermark when the design or its part is augmented in another larger design. Since these demands are of the highest interest for the IP business, we introduce localized watermarking as an IP protection technique which enables these features while satisfying the demand for low-cost and transparency. We have applied the new watermarking technology to template mapping, a behavioral synthesis task. This watermarking method has been tested on a set of real-life benchmarks where high likelihood of authorship has been achieved with negligible overhead in solution quality.","1520-6149","0-7803-6293","10.1109/ICASSP.2000.860089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860089","","Watermarking;Protection;Algorithm design and analysis;Partitioning algorithms;Application software;Digital signatures;Constraint optimization;Design optimization;Computer science;Electronics industry","industrial property;security of data;copy protection;protocols;signal detection;encoding;fraud","localized watermarking;template mapping;semiconductor industry;intellectual property business model;system-on-chip development platform;copyright fraud;intellectual property protection;design partitions;watermark detection;low-cost;transparency;behavioral synthesis;real-life benchmarks;authorship proof;protocols","","4","13","","","","","","IEEE","IEEE Conferences"
"Benchmarking financial database queries on a parallel machine","J. A. Keane; T. N. Franklin; R. Dipper; A. J. Grant; R. Sumner; M. Q. Xu","Dept. of Comput., Univ. of Manchester Inst. of Sci. & Technol., UK; NA; NA; NA; NA; NA","Proceedings of the Twenty-Eighth Annual Hawaii International Conference on System Sciences","","1995","2","","402","411 vol.2","A database benchmark exercise on a parallel machine (a Kendall Square Research KSR1-32 virtual shared memory computer) for a large UK bank is discussed. The benchmark involved 8 queries running on a subset of the bank's database across 10 processors. All queries were of the MIS variety, and included ""real"" marketing queries. Some queries give speedup of over 5 times on 10 processors. As a result of the data distribution, most queries showed speedup for up to 5 processors but then no more speedup until 10 processors were used. Tests suggest that the limit to I/O throughput lies in the disk controllers. The effectiveness of query optimisation and decomposition are also discussed.<<ETX>>","","0-8186-6930","10.1109/HICSS.1995.375518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=375518","","Databases;Parallel machines;Benchmark testing;Throughput;Concurrent computing;Computer science;Indexes","bank data processing;marketing data processing;software performance evaluation;parallel machines;management information systems;distributed databases;query processing","database benchmark;financial database queries;parallel machine;UK bank;MIS;marketing queries;speedup;data distribution;I/O throughput limit;disk controllers;query optimisation;query decomposition;Kendall Square Research KSR1-32;virtual shared memory computer","","","6","","","","","","IEEE","IEEE Conferences"
"Parameter-free, predictive modeling of single event upsets due to protons, neutrons, and pions in terrestrial cosmic rays","G. R. Srinivasan; H. K. Tang; P. C. Murley","Semicond. Res. and Dev. Center, IBM Corp., East Fishkill, NY, USA; Semicond. Res. and Dev. Center, IBM Corp., East Fishkill, NY, USA; Semicond. Res. and Dev. Center, IBM Corp., East Fishkill, NY, USA","IEEE Transactions on Nuclear Science","","1994","41","6","2063","2070","In this paper we present a new approach and a computer software for modeling single event upsets. This model, named Soft Error Monte Carlo Model (SEMM), does not need any experimental inputs or any parameter fitting. It is intended to be a design tool for chip designers when they want to optimize their designs for soft error hardness and performance. The paper focuses on terrestrial cosmic rays that cause single event upsets. Details of the nuclear modeling and of the coupled device-circuit modeling are presented. Also presented are the comparison of SEMM predictions against measurements of single event upset rate in proton beam experiments and in computer main frame field tests performed at high ground elevations. We also present some proton-pion comparisons that are relevant to single event upsets.<<ETX>>","0018-9499;1558-1578","","10.1109/23.340543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=340543","","Predictive models;Single event upset;Computer errors;Software;Monte Carlo methods;Design optimization;Cosmic rays;Semiconductor device measurement;Particle beams;High performance computing","integrated circuit modelling;radiation hardening (electronics);proton effects;neutron effects;cosmic rays;Monte Carlo methods;errors;electronic engineering computing;pions;digital integrated circuits;monolithic integrated circuits","predictive modeling;single event upsets;protons;neutrons;pions;terrestrial cosmic rays;computer software;Soft Error Monte Carlo Model;chip design;soft error hardness;coupled device-circuit modeling;SEU modelling","","35","23","","","","","","IEEE","IEEE Journals & Magazines"
"On optimal pairwise linear classifiers for normal distributions: the two-dimensional case","L. Rueda; B. J. Oommen","Sch. of Comput. Sci., Carleton Univ., Ottawa, Ont., Canada; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2002","24","2","274","280","Optimal Bayesian linear classifiers have been studied in the literature for many decades. We demonstrate that all the known results consider only the scenario when the quadratic polynomial has coincident roots. Indeed, we present a complete analysis of the case when the optimal classifier between two normally distributed classes is pairwise and linear. We focus on some special cases of the normal distribution with nonequal covariance matrices. We determine the conditions that the mean vectors and covariance matrices have to satisfy in order to obtain the optimal pairwise linear classifier. As opposed to the state of the art, in all the cases discussed here, the linear classifier is given by a pair of straight lines, which is a particular case of the general equation of second degree. We also provide some empirical results, using synthetic data for the Minsky's paradox case, and demonstrated that the linear classifier achieves very good performance. Finally, we have tested our approach on real life data obtained from the UCI machine learning repository. The empirical results that we obtained show the superiority of our scheme over the traditional Fisher's discriminant classifier.","0162-8828;2160-9292;1939-3539","","10.1109/34.982905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982905","","Computer aided software engineering;Covariance matrix;Gaussian distribution;Bayesian methods;Pattern recognition;Vectors;Polynomials;Equations;Life testing;Machine learning","pattern classification;Bayes methods;covariance matrices;optimisation;normal distribution","optimal pairwise linear classifiers;normal distributions;optimal Bayesian linear classifiers;quadratic polynomial;coincident roots;nonequal covariance matrices;optimal pairwise linear classifier;Minsky paradox;perceptron;UCI machine learning repository","","10","17","","","","","","IEEE","IEEE Journals & Magazines"
"OCR in a hierarchical feature space","Jaehwa Park; V. Govindaraju; S. N. Srihari","Dept. of Comput. Sci. & Eng., State Univ. of New York, Buffalo, NY, USA; NA; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2000","22","4","400","407","This paper describes hierarchical OCR, a character recognition methodology that achieves high speed and accuracy by using a multiresolution and hierarchical feature space. Features at different resolutions, from coarse to fine-grained, are implemented by means of a recursive classification scheme. Typically, recognizers have to balance the use of features at many resolutions (which yields a high accuracy), with the burden on computational resources in terms of storage space and processing time. We present in this paper, a method that adaptively determines the degree of resolution necessary in order to classify an input pattern. This leads to optimal use of computational resources. The hierarchical OCR dynamically adapts to factors such as the quality of the input pattern, its intrinsic similarities and differences from patterns of other classes it is being compared against, and the processing time available. Furthermore, the finer resolution is accorded to only certain ""zones"" of the input pattern which are deemed important given the classes that are being discriminated. Experimental results support the methodology presented. When tested on standard NIST data sets, the hierarchical OCR proves to be 300 times faster than a traditional K-nearest-neighbor classification method, and 10 times taster than a neural network method. The comparison uses the same feature set for all methods. Recognition rate of about 96 percent is achieved by the hierarchical OCR. This is at par with the other two traditional methods.","0162-8828;2160-9292;1939-3539","","10.1109/34.845383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=845383","","Optical character recognition software;Character recognition;Testing;Pattern recognition;Venus;Senior members;NIST;Neural networks;Feature extraction","optical character recognition;image classification;optimisation;computational complexity","hierarchical OCR;multiresolution hierarchical feature space;recursive classification scheme;computational resource burden;storage space;processing time","","41","14","","","","","","IEEE","IEEE Journals & Magazines"
"Distribution comparison for site-specific regression modeling in agriculture","D. Pokrajac; T. Fiez; D. Obradovic; S. Kwek; Z. Obradovic","Sch. of Electr. Eng. & Comput. Sci., Washington State Univ., Pullman, WA, USA; NA; NA; NA; NA","IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)","","1999","6","","3937","3941 vol.6","A novel method for problem decomposition and for local model selection in a multimodel prediction system is proposed. The proposed method partitions the data into disjoint subsets obtained by the local regression modeling and then it learns the distributions on these sets in order to identify the most appropriate regression model for each test point. The system is applied to a site specific agriculture domain and is shown to provide a substantial improvement in the prediction quality as compared to a global model. Also, some aspects of local learner choice and setting of their parameters are discussed and an overall ability of the proposed model to accurately perform regression is assessed.","1098-7576","0-7803-5529","10.1109/IJCNN.1999.830786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=830786","","Agriculture;Crops;Predictive models;Testing;Production;Training data;Soil;Application software;Radio access networks;Computer science","agriculture;statistical analysis;multilayer perceptrons;optimisation","distribution comparison;site-specific regression modeling;agriculture;problem decomposition;local model selection;multimodel prediction system;data partitioning;disjoint subsets;local regression modeling;site specific agriculture domain;prediction quality;local learner choice;parameter setting;optimal production input level determination","","5","11","","","","","","IEEE","IEEE Conferences"
"Synthetic simulation of mesh-based parallel applications driven by fine-grained profiling","Q. Liu; A. S. Deshmukh; K. A. Tomko","Dept. of Electr. & Comput. Eng. & Comput. Sci., Cincinnati Univ., OH, USA; Dept. of Electr. & Comput. Eng. & Comput. Sci., Cincinnati Univ., OH, USA; Dept. of Electr. & Comput. Eng. & Comput. Sci., Cincinnati Univ., OH, USA","19th IEEE International Parallel and Distributed Processing Symposium","","2005","","","8 pp.","","We are interested in discovering the intrinsic dynamics of parallel applications, which are independent of runtime environment, to aid in the development of appropriate tuning policies, especially dynamic load balancing policies. Based on the novel idea of profiling mesh-based applications at a fine granularity of each mesh element, this paper proposes a synthetic application simulator which is driven by a series of application signatures mapping to the mesh structure. By integrating the ZOLTAN library into the system, our simulator provides a convenient test bed for developing and evaluating load balancing policies.","1530-2075","0-7695-2312","10.1109/IPDPS.2005.417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420221","","Performance analysis;Application software;Load management;Concurrent computing;Distributed computing;Computational modeling;Computer science;Runtime environment;Libraries;System testing","resource allocation;parallel programming;optimising compilers","dynamic load balancing policies;synthetic application simulator;application signatures;ZOLTAN library;mesh-based parallel applications","","","19","","","","","","IEEE","IEEE Conferences"
"Dealing with noise in ant-based clustering","D. Zaharie; F. Zamfirache","Dept. of Comput. Sci., West Univ. of Timisoara, Romania; Dept. of Comput. Sci., West Univ. of Timisoara, Romania","2005 IEEE Congress on Evolutionary Computation","","2005","3","","2395","2401 Vol. 3","Separating the noise from data in a clustering process is an important issue in practical applications. Various algorithms, most of them based on density functions approaches, have been developed lately. The aim of this work is to analyze the ability of an ant-based clustering algorithm (AntClust) to deal with noise. The basic idea of the approach is to extend the information carried by an ant with information concerning the density of data in its neighborhood. Experiments on some synthetic test data suggest that this approach could ensure the separation of noise from data without significantly increasing the algorithm's complexity.","1089-778X;1941-0026","0-7803-9363","10.1109/CEC.2005.1554993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554993","","Density measurement;Clustering algorithms;Density functional theory;Computer science;Algorithm design and analysis;Noise shaping;Application software;Testing;Data analysis;Statistics","pattern clustering;data analysis;sorting;data mining;particle swarm optimisation","ant based clustering;noise separation;density functions","","5","8","","","","","","IEEE","IEEE Conferences"
"Comparing the real-time performance of Windows NT to an NT real-time extension","K. M. Obenland; T. Frazier; J. S. Kim; J. Kowalik","Mitre Corp., McLean, VA, USA; NA; NA; NA","Proceedings of the Fifth IEEE Real-Time Technology and Applications Symposium","","1999","","","142","151","Because of the dominance of Microsoft(R) Windows(R) in the PC market there is a strong interest in using Windows NT(R) as a platform for real-time process and control systems. This type of solution is very cost effective because applications and development tools are widely available. However, Windows NT was designed as a general purpose operating system and optimizes average not worst case performance. We investigate two methods to bring real-time process and control systems to NT based platforms. We first evaluate NT as-is, using a series of real-time benchmarks, and show that NT use in real-time systems is limited to soft real-time systems where there is low system load. The second approach for developing NT based real-time systems is to add a real-time extension to NT. We evaluate one such product, INtime(R) from RadiSys and conclude that, even under a heavy system load hard real-time determinism is possible.","1080-1812","0-7695-0194","10.1109/RTTAS.1999.777669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777669","","Operating systems;Yarn;Real time systems;Sleep;Performance evaluation;Timing;System testing;Jitter;Benchmark testing;Costs","network operating systems;real-time systems;software performance evaluation;interrupts","real-time performance;Windows NT;Microsoft Windows;PC market;cost effective;general purpose operating system;real-time benchmarks;INtime;RadiSys;system load;interrupts","","3","27","","","","","","IEEE","IEEE Conferences"
"Optimal controller for intraaortic balloon pumping","O. Barnea; B. T. Smith; S. Dubin; T. W. Moore; D. Jaron","Sch. of Eng., Tel Aviv Univ., Israel; NA; NA; NA; NA","IEEE Transactions on Biomedical Engineering","","1992","39","6","629","634","An optimal control algorithm was adapted to identify and track the optimal deflation time of the intraaortic balloon pump (IABP). Routines for handling physiologically imposed constraints were added to the algorithm, which was implemented in a computer-controlled system. The system was designed to provide real time optimization for the clinical setting. Proper values for the algorithm parameters were determined and the system was tested in animal experiments. The results indicate that the controlling deflation time relative to the R wave, which precedes the next ejection phase, reduces the time required for optimization when the heart rate varies.<<ETX>>","0018-9294;1558-2531","","10.1109/10.141201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=141201","","Optimal control;Timing;Availability;Oxygen;Hemodynamics;Animals;Performance analysis;Real time systems;Design optimization;Myocardium","biocontrol;haemodynamics;prosthetics","intraaortic balloon pumping;optimal control algorithm;physiologically imposed constraints;computer-controlled system;animal experiments;R wave;ejection phase;heart rate","Algorithms;Animals;Dogs;Intra-Aortic Balloon Pumping;Microcomputers;Software Design;Therapy, Computer-Assisted","8","18","","","","","","IEEE","IEEE Journals & Magazines"
"Skin probability map and its use in face detection","J. D. Brand; J. S. D. Mason","Dept. of Electr. Eng., Univ. of Wales, Swansea, UK; NA","Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)","","2001","1","","1034","1037 vol.1","This paper is in two parts. The first part quantatively assesses an approach to skin segmentation. The second part describes the development and quantitative assessment of an approach to face detection (FD), with the application of content-based image retrieval in mind. Skin detection is introduced as a front-end to an earlier approach to FD by Huang (1994). The baseline approach searches grey scale images only, and is found to be susceptible to variations in lighting conditions and complex backgrounds. It is hypothesised that by integrating colour information into Huang's approach, the number of false faces can be reduced. A skin probability map (SPM) is generated from a large quantity of labeled data (530 images containing faces and 714 images that do not) and is used to pre-process colour test images. Image regions are then ranked in terms of their skin content, thus removing improbable face regions. The performance improvements are shown in terms of false acceptance (FA) and false rejection (FR) scores. As a front-end to Huang's approach, the benefits of skin segmentation can be seen by a reduction in the FA score from 79% to 15% with a negligible impact on FR.","","0-7803-6725","10.1109/ICIP.2001.959225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959225","","Skin;Face detection;Image segmentation;Image retrieval;Computer vision;Application software;Humans;Content based retrieval;Pattern recognition;Gratings","image segmentation;image colour analysis;probability;image retrieval;content-based retrieval;object detection","skin probability map;face detection;skin segmentation;content-based image retrieval;grey scale images;lighting conditions;complex backgrounds;colour information;labeled data;colour test images;image regions;face regions;false acceptance score;false rejection score;front-end","","7","12","","","","","","IEEE","IEEE Conferences"
"Enhancing face detection in colour images using a skin probability map","J. Brand; J. S. Mason; M. Roach; M. Pawlewski","Dept. of Electr. & Electron. Eng., Univ. of Wales, Swansea, UK; NA; NA; NA","Proceedings of 2001 International Symposium on Intelligent Multimedia, Video and Speech Processing. ISIMP 2001 (IEEE Cat. No.01EX489)","","2001","","","344","347","The paper describes the development and quantitative assessment of an approach to face detection (FD), with the application of image classification in mind. The approach adopted is a direct extension of an earlier approach by T.S. Huang and G.Z. Yang (1994). Huang's intensity based approach is found to be susceptible to variations in lighting conditions and complex backgrounds. It is hypothesised that by integrating colour information into Huang's approach, the number of false faces can be reduced. A skin probability map (SPM) is generated from a large quantity of labeled data (530 images containing faces and 714 images that do not) and is used to pre-process colour test images. The SPM allows image regions to be ranked in terms of their skin content, thus removing improbable face regions. The performance improvements are shown in terms of false acceptance (FA) and false rejection (FR) scores. As a front-end to Huang's approach, the benefits of skin segmentation can be seen by a reduction in the FA score from 79% to 15% with a negligible impact on FR.","","962-85766-2","10.1109/ISIMP.2001.925404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925404","","Face detection;Skin;Humans;Scanning probe microscopy;Image segmentation;Application software;Communications technology;Resistance heating;Heat engines;Image classification","face recognition;image classification;image colour analysis;skin;probability","face detection;colour images;skin probability map;quantitative assessment;image classification;intensity based approach;lighting conditions;complex backgrounds;colour information;false faces;SPM;labeled data;colour test images;image regions;skin content;improbable face regions;performance improvements;false acceptance;false rejection;skin segmentation;FA score","","8","11","","","","","","IEEE","IEEE Conferences"
"Cypress: compression and encryption of data and code for embedded multimedia systems","H. Lekatsas; J. Henkel; S. T. Chakradhar; V. Jakkula","NEC Labs., USA; NEC Labs., USA; NEC Labs., USA; NEC Labs., USA","IEEE Design & Test of Computers","","2004","21","5","406","415","Copyright protection of sensitive data plays a significant part in the design of multimedia systems. This article introduces a hardware platform that enables both compression and encryption for data and code in a unified architecture. Besides being parameterizable, the platform features software tools for evaluating and optimizing specific multimedia applications.","0740-7475;1558-1918","","10.1109/MDT.2004.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1341379","","Cryptography;Multimedia systems;Application software;Memory management;Runtime;Image coding;Embedded system;National electric code;Hardware;Computer architecture","data compression;embedded systems;multimedia systems;cryptography","Cypress;data compression;encryption;embedded multimedia systems;copyright protection","","5","12","","","","","","IEEE","IEEE Journals & Magazines"
"Oscillator design using nonlinear CAD","D. Q. Xu; B. P. Kumar; G. R. Branner","Dept. of Electr. & Comput. Eng., California Univ., Davis, CA, USA; NA; NA","Proceedings of 40th Midwest Symposium on Circuits and Systems. Dedicated to the Memory of Professor Mac Van Valkenburg","","1997","1","","581","584 vol.1","Traditionally oscillators are designed using computer aided design (CAD) tools where small signal analysis techniques are employed. The disadvantage of this approach is that the predicted response may not be accurate and much time has to be devoted to tuning the circuits on the workbench. Recent advances in nonlinear computer oriented design software significantly improves the accuracy for design by using harmonic balance techniques. This paper illustrates the design of a low power consumption voltage controlled oscillator (VCO) using nonlinear computer aided design software. Excellent agreement between the simulation and measurement is achieved.","","0-7803-3694","10.1109/MWSCAS.1997.666204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=666204","","Design automation;Voltage-controlled oscillators;Frequency;Circuit simulation;Power system modeling;Circuit optimization;Circuit testing;Computational modeling;Inductance;Soldering","nonlinear network synthesis;circuit CAD;voltage-controlled oscillators;digital simulation","nonlinear CAD;computer aided design;small signal analysis;accuracy;harmonic balance techniques;low power consumption voltage controlled oscillator;VCO","","","4","","","","","","IEEE","IEEE Conferences"
"The Digital Library for Earth System Education: implementing the DLESE Community Plan","M. R. Marlino; D. W. Fulker; G. Horton","UCAR, Boulder, CO, USA; NA; NA","IGARSS 2000. IEEE 2000 International Geoscience and Remote Sensing Symposium. Taking the Pulse of the Planet: The Role of Remote Sensing in Managing the Environment. Proceedings (Cat. No.00CH37120)","","2000","1","","46","48 vol.1","Over the past year, geoscience educators, librarians and information technologists have made substantial progress in initiating the construction ofa Digital Library for Earth System Education (DLESE). Two major efforts, the Portal to the Future Workshop and the Geoscience Digital Library (GDL) project, have established a vision for the library, a governance process to enable community ownership, management, and construction, and have begun development of a testbed collection, discovery system, and user interface. The DLESE Community Plan lays out in detail the need for this facility, a community vision for its goals and priorities, and a strategy for initial construction of the library. From this initial work, two conclusions emerge as paramount in moving forward with the library. First, it is essential that development of the library community and the building of the technological infrastructure for the library go hand in hand. Second, the library will be most effectively built as a highly coordinated, but distributed community effort. In this way, the full range of talents in the community can be leveraged and rapid development of the library is possible. This paper briefly reviews our technical accomplishments to date and outlines their plans for further development. Progress and plans can be charted in the following three areas: 1. Community-centered design and use case development 2. Discovery system, metadata, and collection testbeds 3. System architecture and interoperability.","","0-7803-6359","10.1109/IGARSS.2000.860416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860416","","Software libraries;Earth;Process design;Prototypes;Usability;Geoscience;System testing;User interfaces;Design methodology;Inspection","education;educational technology;teaching;geology;remote sensing;terrain mapping;information science","education;teaching;Earth science;geology;Digital Library for Earth System Education;DLESE;Community Plan;geoscience;Portal to the Future Workshop;Geoscience Digital Library;terrain mapping;remote sensing;land surface;community-centered design;discovery system;metadata;collection testbed;system architecture;interoperability;information science","","","15","","","","","","IEEE","IEEE Conferences"
"ICT in Japanese university language education: a case study","M. H. Field","Waseda Univ., Tokyo, Japan","International Conference on Computers in Education, 2002. Proceedings.","","2002","","","929","933 vol.2","This paper reports part of a study conducted at a university in Japan. The belief that ICT provides students with more opportunities to negotiate target forms has been used to justify its use in language education. This study could not substantiate that ICT will change the way language is used in Japan or that CALL is providing new ways for learning and acquiring a new language. Students were experiencing difficulties in prioritising their learning repertoire between the acquisition of computer skills and language proficiency. Individuals experienced positive and negative coding and de-coding filters when communicating in ICT and this was related to the validity and reliability applied to the text. The value attached to the ICT interaction may influence the degree to which the ICT event influences face-to-face communicative acts.","","0-7695-1509","10.1109/CIE.2002.1186116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1186116","","Natural languages;Computer aided software engineering;Computer science education;Communications technology;Cultural differences;Filters;Communication system control;Boring;Materials testing;Drilling","natural languages;computer aided instruction","Japanese university language education;ICT;Japan;computer aided language learning;learning repertoire;computer skills acquisition;language proficiency;negative de-coding filters;negative coding filters;reliability","","1","18","","","","","","IEEE","IEEE Conferences"
"Improving the efficiency of BDD-based operators by means of partitioning","G. Cabodi; P. Camurati; S. Quer","Dipt. di Autom. e Inf., Politecnico di Torino, Italy; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1999","18","5","545","556","Binary decision diagrams (BDD's) are a state-of-the-art core technique for the symbolic representation and manipulation of Boolean functions, relations and finite sets. Many computer-aided design (CAD) applications resort to them, but size and time efficiency restrict their applicability to medium-small designs. We concentrate on complex operators used in symbolic manipulation. We analyze and optimize their performance by means of new dynamic partitioning strategies. We propose a novel quick algorithm for the estimation of cofactor size, and a technique to choose splitting variables according to their discrimination power, so that their cofactors may be optimized by different variable orderings (tending to the more flexible FBDDs). Furthermore, we analyze time efficiency and the impact of hashing/caching on BDD-based operators. We finally include an experimental observation of memory usage and running time for operators applied in symbolic manipulation.","0278-0070;1937-4151","","10.1109/43.759068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=759068","","Data structures;Boolean functions;Binary decision diagrams;Design automation;Application software;Automata;Reachability analysis;Logic testing;Performance analysis;Partitioning algorithms","binary decision diagrams;logic CAD;logic partitioning;mathematical operators;symbol manipulation;reachability analysis;finite state machines","BDD operator;binary decision diagram;symbolic manipulation;Boolean function;set decomposition;computer-aided design;dynamic partitioning;cofactor estimation;time efficiency;finite state machine;reachability analysis","","7","16","","","","","","IEEE","IEEE Journals & Magazines"
"New implementation techniques of an efficient MPEG advanced audio coder","E. Kurniawati; C. T. Lau; B. Premkumar; J. Absar; S. George","Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore; NA; NA","IEEE Transactions on Consumer Electronics","","2004","50","2","655","665","MPEG-AAC is the current state of the art in audio compression technology. The CD-quality promised at bit rate as low as 64 kbps makes AAC a strong candidate for high quality low bandwidth audio streaming applications over wireless network. Besides this low bit rate requirement, the codec must be able to run on personal wireless handheld devices with its inherent low power characteristics. While the AAC standard is definite enough to ensure that a valid AAC stream is correctly decodable by all AAC decoders, it is flexible enough to accommodate variations in implementation, suited to different resources available and application areas. This paper reviews various implementation techniques of the encoder. We then proposed our method of an optimized software implementation of MPEG-AAC (LC profile). The coder is able to perform encoding task using half the processing power compared to standard implementation without significant degradation in quality as shown by both subjective listening test and an ITU-R compliant quality-testing program (OPERA).","0098-3063;1558-4127","","10.1109/TCE.2004.1309445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309445","","Transform coding;Bit rate;Streaming media;Decoding;Audio compression;Bandwidth;Wireless networks;Codecs;Handheld computers;Application software","audio coding;quantisation (signal);audio acoustics;codecs","MPEG-AAC;advanced audio coder;audio compression technology;CD-quality;bit rate;audio streaming applications;wireless network;codecs;AAC stream;AAC decoders;processing power;subjective listening test;ITU-R compliant quality-testing program;64 kbit/s","","5","24","","","","","","IEEE","IEEE Journals & Magazines"
"Precipitation of inhalable smoke particles using electrostatic ultrasonic nozzle","W. Balachandran; W. Machowski; M. Groemping","Dept. of Manuf. & Eng. Syst., Brunel Univ., Uxbridge, UK; NA; NA","IAS '96. Conference Record of the 1996 IEEE Industry Applications Conference Thirty-First IAS Annual Meeting","","1996","3","","1789","1794 vol.3","The paper presents the result of a research study on precipitation efficiency of inhalable dust particles using electrostatically charged water spray generated by an ultrasonic nozzle. During experiments a commercially available ultrasonic nozzle, LECHLER US1, was used to produce water spray under various operational conditions. The spray generated was charged using induction techniques. The optimal position of the charging electrodes, which create electric field with appropriate spatial distribution required to charge the spray to a desired level of specific charge, was established by applying reverse field modelling technique. A finite element computer software was utilised for this purpose. The shape of the spray cone required optimization of electrode geometry and was examined using a Pulnix TM-765 high speed CCD camera in conjunction with a computerised image grabbing system. The experiments were carried out using cigarette smoke as inhalable test particles with a VMD less than 5 /spl mu/m. The precipitation efficiency was examined using a test chamber and He-Ne laser based sedimentation system. The precipitation efficiency of the smoke was also investigated using bipolar ions created by corona discharge. Based on the experimental results a semi-empirical model has been developed to predict precipitation efficiency for a wider range of spray parameters.","0197-2618","0-7803-3544","10.1109/IAS.1996.559310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559310","","Electrostatics;Spraying;Induction generators;Electrodes;Finite element methods;Software;Shape;Geometrical optics;Computational geometry;Charge coupled devices","smoke;nozzles;ultrasonic applications;electrostatic precipitators;sprays;CCD image sensors;high-speed optical techniques;corona;electric charge;finite element analysis;laser beam applications;sedimentation","inhalable smoke particles precipitation;electrostatic ultrasonic nozzle;precipitation efficiency;electrostatically charged water spray;LECHLER US1;induction charging;optimal charging electrodes position;reverse field modelling technique;finite element computer software;spray cone shape;Pulnix TM-765 high speed CCD camera;computerised image grabbing system;cigarette smoke;test chamber;He-Ne laser based sedimentation system;bipolar ions;corona discharge;spray parameters","","","7","","","","","","IEEE","IEEE Conferences"
"Laser machining of ceramics and silicon for MCM-D applications","V. Glaw; R. Hahn; A. Paredes; U. Hein; O. Ehrmann; H. Reichl","Fraunhofer-Inst. for Reliability & Microintegration Berlin, Germany; NA; NA; NA; NA; NA","Proceedings 3rd International Symposium on Advanced Packaging Materials Processes, Properties and Interfaces","","1997","","","173","176","The ceramics were machined with a computer controlled Nd:YAG slab-laser. The laser parameters as well as the process parameters were optimized to obtain kerfs with good quality and cuts without cracks. Two applications were investigated: ceramics and silicon as substrates for embedded MCM-Ds (Multi Chip Modules, Deposited) and water-cooled heat sinks for single chips, multichip modules or laser diodes. For the embedded MCMs, windows were machined into the substrate. Bare dice and standard passive components (like capacitors or resistors) were inserted into these windows and fixed in their position with an epoxy. AlN, Al/sub 2/O/sub 3/ and Si were tested as substrate materials. The critical parameter was the mechanical stability of the substrate because of the thermal stress introduced by the following thin film process. For the second application, several heat sinks were produced in AlN. Different methods to clean the cooling channels were investigated. Maximum heat dissipation of water-cooled test-devices was determined for various channel geometries.","","0-7803-3818","10.1109/ISAPM.1997.581287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581287","","Machining;Ceramics;Silicon;Substrates;Laser beam cutting;Application software;Heat sinks;Water heating;Thermal stresses;Multichip modules","substrates;laser beam machining;laser materials processing;ceramics;silicon;multichip modules;heat sinks;integrated circuit packaging;semiconductor device packaging;mechanical stability;cooling;alumina","laser machining;ceramics;MCM-D applications;substrates;water-cooled heat sinks;multichip modules;laser diodes;single chips;mechanical stability;thermal stress;cooling channel cleaning;heat dissipation;water-cooled test-devices;channel geometries;Nd:YAG slab-laser;AlN;Al/sub 2/O/sub 3/;Si;YAG:Nd;YAl5O12:Nd","","2","4","","","","","","IEEE","IEEE Conferences"
"Design and realization of a portable data logger for physiological sensing [GSR]","R. Luharuka; R. X. Gao; S. Krishnamurty","Dept. of Mech. & Ind. Eng., Univ. of Massachusetts, Amherst, MA, USA; Dept. of Mech. & Ind. Eng., Univ. of Massachusetts, Amherst, MA, USA; Dept. of Mech. & Ind. Eng., Univ. of Massachusetts, Amherst, MA, USA","IEEE Transactions on Instrumentation and Measurement","","2003","52","4","1289","1295","A microcontroller-based data logger has been designed, prototyped, and field-tested for recording galvanic skin response data and relaying them to a computer for physiological analysis. Focusing on system design issues concerning battery-driven ambulatory applications, this paper presents a special data compression algorithm based on relative encoding to optimize memory utilization and reduce data transfer time. Data flow coordination and timing control are enabled by a PIC microcontroller. A handheld prototype measuring 180/spl times/90/spl times/30 mm was built and tested for ease of use, safety, and reliability.","0018-9456;1557-9662","","10.1109/TIM.2003.816808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232383","","Prototypes;Galvanizing;Skin;Relays;System analysis and design;Application software;Data compression;Encoding;Design optimization;Timing","biomedical measurement;data loggers;microcontrollers;data acquisition;computerised monitoring;data compression;network synthesis","GSR data recording;data acquisition;portable data logger;physiological sensing;microcontroller-based data logger;galvanic skin response data recording;physiological analysis;battery-driven ambulatory applications;data compression;relative encoding;memory utilization optimization;data transfer time;data flow coordination;timing control;PIC microcontroller;autism;30 mm;90 mm;180 mm","","18","14","","","","","","IEEE","IEEE Journals & Magazines"
"Image processing for electronic document storage","A. Cooper; W. Kahari; R. Such","Racal Res. Ltd., Reading, UK; Racal Res. Ltd., Reading, UK; NA","IEE Proceedings E - Computers and Digital Techniques","","1988","135","4","196","201","Reos is a document image processing system which stores images of paper documents as a bit map on optical discs. These images are displayed on a CRT at a lower resolution compared to the stored data. The conversion and display of the stored information must be performed quickly to achieve an acceptable system response time; since even highly optimised software could not process more than a few thousand pixels per second, a hardware solution capable of processing many millions of pixels per second was developed. The mesh offset algorithm which combines image resampling with convolution, was selected for its performance and ease of mapping onto silicon. The hardware of this algorithm option is suitable for more general purposes image processing tasks such as filtering to reduce noise, or contrast enhancement. A high level description of the system has been mapped onto silicon using a number of design options to provide the best balance between speed of design, performance and die area. The resolution converter in 2.5 mu m CMOS double layer metal technology supports scan path test and signature analysis for production test purposes.<<ETX>>","0143-7062","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527","","CMOS integrated circuits;Database management systems;Optical data processing;Image processing;Very-large-scale integration","CMOS integrated circuits;database management systems;optical information processing;picture processing;VLSI","Si;electronic document storage;Reos;document image processing system;bit map;optical discs;CRT;highly optimised software;mesh offset algorithm;filtering;contrast enhancement;high level description;die area;CMOS;signature analysis","","","","","","","","","IET","IET Journals & Magazines"
"A hybrid energy-estimation technique for extensible processors","Y. Fei; S. Ravi; A. Raghunathan; N. K. Jha","Dept. of Electr. Eng., Princeton Univ., NJ, USA; NA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2004","23","5","652","664","In this paper, we present an efficient and accurate methodology for estimating the energy consumption of application programs running on extensible processors. Extensible processors, which are getting increasingly popular in embedded system design, allow a designer to customize a base processor core through instruction set extensions. Existing processor energy macromodeling techniques are not applicable to extensible processors, since they assume that the instruction set architecture as well as the underlying structural description of the micro-architecture remain fixed. Our solution to the above problem is a hybrid energy macromodel suitably parameterized to estimate the energy consumption of an application running on the corresponding application-specific extended processor instance, which incorporates any custom instruction extension. Such a characterization is facilitated by careful selection of macromodel parameters/variables that can capture both the functional and structural aspects of the execution of a program on an extensible processor. Another feature of the proposed energy characterization flow is the use of regression analysis to build the macromodel. Regression analysis allows for in-situ characterization, thus allowing arbitrary test programs to be used during macromodel construction. We validated the proposed methodology by characterizing the energy consumption of a state-of-the-art extensible processor (Tensilica's Xtensa). We used the macromodel to analyze the energy consumption of several benchmark applications with custom instructions. The mean absolute error in the macromodel estimates is only 3.3%, when compared to the energy values obtained by a commercial tool operating on the synthesized register-transfer level (RTL) description of the custom processor. Our approach achieves an average speedup of three orders of magnitude over the commercial RTL energy estimator. Our experiments show that the proposed methodology also achieves good relative accuracy, which is essential in energy optimization studies. Hence, our technique is both efficient and accurate.","0278-0070;1937-4151","","10.1109/TCAD.2004.826546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1291578","","Energy consumption;Embedded system;Hardware;Regression analysis;Application software;Process design;Parameter estimation;Benchmark testing;Optimization methods;Application specific processors","microprocessor chips;application specific integrated circuits;low-power electronics;integrated circuit modelling;statistical analysis","extensible processors;energy estimation;embedded system;base processor core customization;instruction set extensions;energy characterization flow;regression analysis;register-transfer level description;application-specific instruction set processor","","13","31","","","","","","IEEE","IEEE Journals & Magazines"
"Towards the automated design of application specific array processors (ASAPs)","A. P. Marriott; A. W. G. Duller; R. H. Storer; A. R. Thomson; M. R. Pout","Dept. of Electr. Eng. & Electron., Univ. of Manchester, Inst. of Sci. & Technol., UK; NA; NA; NA; NA","[1990] Proceedings of the International Conference on Application Specific Array Processors","","1990","","","414","425","The authors describe the architecture and VLSI design of GLiTCH, an associative processor array chip designed for computer vision applications. The design is built from a library of cells, which can be used in conjunction with high level functional specifications to rapidly design new application specific array processors. The objective is to design a system which will allow application specific associative array processors (ASAPs) to be defined, simulated and then produced in silicon automatically from high level description data. Using such techniques should reduce the design cycle time to the point where processor arrays optimized for a particular problem could be fabricated. The authors describe some of the VLSI design which has been done towards achieving the automatic layout of ASAPs. Specifically, the design decisions and trade-offs made in the implementation of a test chip are described and applied to the problem of producing ASAPs.<<ETX>>","","0-8186-9089","10.1109/ASAP.1990.145477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145477","","Application software;Very large scale integration;Computer architecture;Process design;Computer vision;Libraries;Computational modeling;Silicon;Design optimization;Testing","application specific integrated circuits;circuit CAD;circuit layout CAD;computer vision;computerised picture processing;digital signal processing chips;logic CAD;parallel architectures;VLSI","CAD;DSP;CAM;content addressable memory;automated design;application specific array processors;VLSI design;GLiTCH;associative processor array chip;computer vision applications;high level functional specifications;high level description data;automatic layout","","2","7","","","","","","IEEE","IEEE Conferences"
"Fast, Accurate Microarchitecture Simulation Using Statistical Phase Detection","R. Srinivasan; J. Cook; S. Cooper","NA; NA; NA","IEEE International Symposium on Performance Analysis of Systems and Software, 2005. ISPASS 2005.","","2005","","","147","156","Simulation-based microarchitecture research is often hindered by the slow speed of simulators. In this work, we propose a novel statistical technique to identify highly representative unique behaviors or phases in a benchmark based on its IPC (instructions committed per cycle) trace. By simulating the timing of only the unique phases, the cycle-accurate simulation time for the SPEC suite is reduced from 5 months to 5 days, with a significant retention of the original dynamic behavior. Evaluation across many processor configurations within the same architecture family shows that the algorithm is robust. A cost function is provided that enables users to easily optimize the parameters of the algorithm for either simulation speed or accuracy depending on preference. A new measure is introduced to quantify the ability of a simulation speedup technique to retain behavior realized in the original workload. Unlike a first order statistic such as mean value, the newly introduced measure captures important differences in dynamic behavior between the complete and the sampled simulations","","0-7803-8965","10.1109/ISPASS.2005.1430569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430569","","Microarchitecture;Phase detection;Sampling methods;Timing;Robustness;Cost function;Velocity measurement;Statistics;Space exploration;Algorithm design and analysis","benchmark testing;computer architecture;digital simulation;instruction sets;statistical analysis","microarchitecture simulation;statistical phase detection;instructions committed per cycle;processor configuration","","6","18","","","","","","IEEE","IEEE Conferences"
"A High Performance, Energy Efficient GALS ProcessorMicroarchitecture with Reduced Implementation Complexity","YongKang Zhu; D. H. Albonesi; A. Buyuktosunoglu","NA; NA; NA","IEEE International Symposium on Performance Analysis of Systems and Software, 2005. ISPASS 2005.","","2005","","","42","53","As the costs and challenges of global clock distribution grow with each new microprocessor generation, a globally asynchronous, locally synchronous (GALS) approach becomes an attractive alternative. One proposed GALS approach, called a multiple clock domain (MCD) processor, achieves impressive energy savings for a relatively low performance cost. However, the approach requires separating the processor into four domains, including separating the integer and memory domains which complicates load scheduling, and the implementation of 32 voltage and frequency levels in each domain. In addition, the hardware-based control algorithm, though effective overall, produces a significant performance degradation for some applications. In this paper, we devise modifications to the MCD design that retain many of its benefits while greatly reducing the implementation complexity. We first determine that the synchronization channels that are most responsible for the MCD performance degradation are those involving cache access, and propose merging the integer and memory domains to virtually eliminate this overhead. We further propose significantly reducing the number of voltage levels, separating the reorder buffer into its own domain to permit front-end frequency scaling, separating the L2 cache to permit standard power optimizations to be used, and a new online algorithm that provides consistent results across our benchmark suite. The overall result is a significant reduction in the performance degradation of the original MCD approach and greater energy savings, with a greatly simplified microarchitecture that is much easier to implement","","0-7803-8965","10.1109/ISPASS.2005.1430558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430558","","Energy efficiency;Degradation;Costs;Clocks;Voltage;Microprocessors;Synchronous generators;Processor scheduling;Frequency synchronization;Merging","benchmark testing;cache storage;clocks;computer architecture;microprocessor chips;synchronisation","energy efficient GALS processor;microarchitecture;globally asynchronous locally synchronous approach;multiple clock domain processor;synchronization channels;reorder buffer;L2 cache;benchmark suite;microprocessor chip","","3","13","","","","","","IEEE","IEEE Conferences"
"Genetic selection of biologically inspired receptive fields for computational vision","C. A. Perez; C. Salinas","Dept. of Electr. Eng., Chile Univ., Santiago, Chile; NA","Proceedings of the First Joint BMES/EMBS Conference. 1999 IEEE Engineering in Medicine and Biology 21st Annual Conference and the 1999 Annual Fall Meeting of the Biomedical Engineering Society (Cat. N","","1999","2","","924 vol.2","","The paper presents the genetic selection of biologically inspired receptive fields classifiers to improve pattern recognition in neural networks. A genetic algorithm is employed to select the x and y dimensions of the receptive fields in a two plane per layer configuration with two hidden layers. Networks were ranked based on the fitness criterion: best generalization performance on handwritten digits. Results show a strong correlation between the neural network performance and the receptive field x and y dimensions. The best receptive field configuration results outperformed those of the perceptron based models. Best receptive field configurations consist of a small aspect ratio in x and y direction in each plane of the two hidden layers.","1094-687X","0-7803-5674","10.1109/IEMBS.1999.804078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=804078","","Biology computing;Computer vision;Genetic algorithms;Neural networks;Testing;Biological information theory;Biological system modeling;Databases;Application software;Network topology","computer vision;genetic algorithms;image classification;multilayer perceptrons;handwritten character recognition","genetic algorithm;computational vision;genetic selection;pattern recognition;neural networks;biologically inspired receptive fields classifiers;hidden layers;fitness criterion;best generalization performance;handwritten digits;perceptron based models","","2","5","","","","","","IEEE","IEEE Conferences"
"Hardware implementation of a general multi-way jump mechanism","Soo-Mook Moon; S. D. Carson; A. K. Agrawala","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","[1990] Proceedings of the 23rd Annual Workshop and Symposium@m_MICRO 23: Microprogramming and Microarchitecture","","1990","","","38","45","A VLIW architecture capable of testing multiple conditions in one cycle must support effective multiway (conditional) jumps. In this paper, a hardware-implemented, fast, and space-efficient multi-way jump mechanism is developed that speeds up the execution of multiple conditional jumps and reduces wasted storage. A cluster of multiple conditional jumps packed in an instruction can form an arbitrary rooted DAG (directed acyclic graph), where each node corresponds to a condition. The authors scheme provides a hardware device called an M-unit, which can combinationally produce the next target address using an encoded description of the DAG and the actual test bits. A technique to reduce the number of different configurations is introduced, along with a memory packing scheme that minimizes wasted memory.<<ETX>>","","0-8186-2124","10.1109/MICRO.1990.151425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=151425","","Hardware;Testing;VLIW;Computer science;Optimizing compilers;Computer aided software engineering;Moon;Educational institutions;Computer architecture;Engines","directed graphs;instruction sets;parallel architectures","hardware implementation;general multi-way jump mechanism;VLIW architecture;multiple conditional jumps;instruction;directed acyclic graph;M-unit;memory packing scheme","","6","14","","","","","","IEEE","IEEE Conferences"
"Filtering in Chinese document images based on templates and confidence measure","Chen Jiewei; Xu Weiran; Guo Jun","Sch. of Inf. Eng., Beijing Univ. of Posts & Telecommun., China; Sch. of Inf. Eng., Beijing Univ. of Posts & Telecommun., China; Sch. of Inf. Eng., Beijing Univ. of Posts & Telecommun., China","Proceedings 7th International Conference on Signal Processing, 2004. Proceedings. ICSP '04. 2004.","","2004","2","","1376","1379 vol.2","A fast approach to keyword spotting in Chinese document images based on multiple templates matching and confidence measure is presented. The system generates keyword lexicon of diverse fonts and two-stage feature vectors prior to the procedure of keyword searching. A two-stage retrieval scheme and Boyer-Moore Algorithm is proposed aiming at accelerating the retrieval process. A distance measure between the candidate character and the templates is used to identify and rank similar templates. The performance of new system has been significantly improved when compared to traditional OCR and image-based approach. Experimental results confirmed the robust of the proposed approach over a wide range of degradations.","","0-7803-8406","10.1109/ICOSP.2004.1441582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1441582","","Information filtering;Optical character recognition software;Testing;Degradation;Image recognition;Image segmentation;Image retrieval;Information retrieval;Character recognition;Acceleration","image retrieval;feature extraction;information filtering;document image processing;natural languages;image matching;character recognition","Chinese document image filter;confidence measure;multiple template matching;keyword lexicon;two-stage feature vector;two-stage retrieval scheme;Boyer-Moore algorithm;candidate character;information filtering","","","6","","","","","","IEEE","IEEE Conferences"
"Locality-conscious workload assignment for array-based computations in MPSOC architectures","Feihui Li; M. Kandemir","Comput. Sci. & Eng. Dept., Pennsylvania State Univ., University Park, PA, USA; Comput. Sci. & Eng. Dept., Pennsylvania State Univ., University Park, PA, USA","Proceedings. 42nd Design Automation Conference, 2005.","","2005","","","95","100","While the past research discussed several advantages of multiprocessor-system-on-a-chip (MPSOC) architectures from both area utilization and design verification perspectives over complex single core based systems, compilation issues for these architectures have relatively received less attention. Programming MPSOCs can be challenging as several potentially conflicting issues such as data locality, parallelism and load balance across processors should be considered simultaneously. Most of the compilation techniques discussed in the literature for parallel architectures (not necessarily for MPSOCs) are loop based, i.e., they consider each loop nest in isolation. However, one key problem associated with such loop based techniques is that they fail to capture the interactions between the different loop nests in the application. This paper takes a more global approach to the problem and proposes a compiler-driven data locality optimization strategy in the context of embedded MPSOCs. An important characteristic of the proposed approach is that, in deciding the workloads of the processors (i.e., in parallelizing the application) it considers all the loop nests in the application simultaneously. The authors' experimental evaluation with eight embedded applications showed that the global scheme brings significant power/performance benefits over the conventional loop based scheme.","0738-100X","1-59593-058","10.1109/DAC.2005.193780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510299","","Computer architecture;Programming profession;Application software;Parallel programming;Permission;Logic programming;Frequency synchronization;Computer science;Design engineering;Parallel processing","system-on-chip;benchmark testing;embedded systems;microprocessor chips;microprogramming;parallel architectures","workload assignment;array based computations;MPSOC architectures;multiprocessor system on a chip;data locality;load balance;parallel architectures;locality optimization strategy","","1","19","","","","","","IEEE","IEEE Conferences"
"On the extraction and analysis of prevalent dataflow patterns","P. G. Sassone; D. S. Wills","Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE International Workshop on Workload Characterization, 2004. WWC-7. 2004","","2004","","","11","18","The complexity-effectiveness of modern wire-dominated architectures is heavily influenced by operand movement patterns within workloads. Unfortunately, the study of these common patterns is burdensome given the NP-completeness of the problem and the size of the dataflow graphs in modern applications. In response we present CPX, a fast and memory-efficient tool for the extraction of common dataflow subgraphs from application binaries. Using this tool and a practical metric of pattern popularity, we analyze Media-Bench and Spec2000int benchmarks and present their most frequent communication patterns. Results confirm the intuition of prior research that dependence chains dominate integer code, but more importantly demonstrate that dataflow communication is restricted to a tractable set of templates. A set of only ten small patterns characterizes over 90% of Spec2000int and over 75% of MediaBench dynamic instructions. These common dataflow idioms are amenable to dynamic optimization, more efficient code representations, and reducing the broadcast nature of micro-architectural resources.","","0-7803-8828","10.1109/WWC.2004.1437389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437389","","Data analysis;Pattern analysis;Libraries;Assembly;Frequency;Data mining;Microelectronics;Application software;Broadcasting;Programming profession","data flow graphs;data flow analysis;pattern recognition;communication complexity;benchmark testing;message passing","dataflow pattern extraction;dataflow pattern analysis;complexity-effectiveness;wire-dominated architectures;operand movement patterns;NP-complete problem;dataflow graphs;CPX;dataflow subgraph extraction;application binaries;Media-Bench benchmark;Spec2000int benchmark;communication patterns;dependence chains;integer code;dataflow communication;dynamic instructions;dataflow idioms;dynamic optimization;code representations;microarchitectural resources","","3","16","","","","","","IEEE","IEEE Conferences"
"Integrating remote invocation and distributed shared state","C. Tang; D. Chen; Sandhya Dwarkadas; M. L. Scott","Dept. of Comput. Sci., Rochester Univ., NY, USA; Dept. of Comput. Sci., Rochester Univ., NY, USA; Dept. of Comput. Sci., Rochester Univ., NY, USA; Dept. of Comput. Sci., Rochester Univ., NY, USA","18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.","","2004","","","30","","Summary form only given. Most distributed applications require, at least conceptually, some sort of shared state: information that is nonstatic but mostly read, and needed at more than one site. At the same time, RPC-based systems such as Sun RPC, Java RMl, CORBA, and .NET have become the de facto standards by which distributed applications communicate. As a result, shared state tends to be implemented either through the redundant transmission of deep-copy RPC parameters or through ad-hoc, application-specific caching and coherence protocols. The former option can waste large amounts of bandwidth; the latter significantly complicates program design and maintenance. To overcome these problems, we propose a distributed middleware system that works seamlessly with RPC-based systems, providing them with a global, persistent store that can be accessed using ordinary reads and writes. Relaxed coherence models and aggressive protocol optimizations reduce the bandwidth required to maintain shared state. Integrated support for transactions allows a chain of RPC calls to update shared state atomically. We focus on the implementation challenges involved in combining RPC with shared state and transactions. In particular, we describe a transaction metadata table that allows processes inside a transaction to share data invisible to other processes and to exchange data modifications efficiently. Using microbenchmarks and a large-scale datamining application, we demonstrate how the integration of RPC, transactions, and shared state facilitates the rapid development of robust, maintainable code.","","0-7695-2132","10.1109/IPDPS.2004.1302942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1302942","","Application software;Java;Access protocols;Sun;Bandwidth;Coherence;Grid computing;Peer to peer computing;Programming profession;Computer science","remote procedure calls;middleware;protocols;bandwidth allocation;transaction processing;meta data;benchmark testing;data mining","remote invocation;distributed shared state;distributed applications;RPC-based systems;Sun RPC;Java RMl;CORBA;.NET;de facto standards;deep-copy RPC parameters;ad-hoc application-specific caching;coherence protocols;distributed middleware system;relaxed coherence models;aggressive protocol optimizations;bandwidth allocation;transaction metadata table;microbenchmarks;large-scale datamining application","","3","29","","","","","","IEEE","IEEE Conferences"
"The ALPBench benchmark suite for complex multimedia applications","Man-Lap Li; R. Sasanka; S. V. Adve; Yen-Kuang Chen; E. Debes","Dept. of Comput. Sci., Illinois Univ., Urbana-Champaign, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana-Champaign, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana-Champaign, IL, USA; NA; NA","IEEE International. 2005 Proceedings of the IEEE Workload Characterization Symposium, 2005.","","2005","","","34","45","Multimedia applications are becoming increasingly important for a large class of general-purpose processors. Contemporary media applications are highly complex and demand high performance. A distinctive feature of these applications is that they have significant parallelism, including thread- , data-, and instruction-level parallelism, that is potentially well-aligned with the increasing parallelism supported by emerging multi-core architectures. Designing systems to meet the demands of these applications therefore requires a benchmark suite comprising these complex applications and that exposes the parallelism present in them. This paper makes two contributions. First, it presents ALPBench, a publicly available benchmark suite that pulls together five complex media applications from various sources: speech recognition (CMU Sphinx 3), face recognition (CSU), ray tracing (Tachyon), MPEG-2 encode (MSSG), and MPEG-2 decode (MSSG). We have modified the original applications to expose thread-level and data-level parallelism using POSIX threads and sub-word SIMD (Inters SSE2) instructions respectively. Second, the paper provides a performance characterization of the ALPBench benchmarks, with a focus on parallelism. Such a characterization is useful for architects and compiler writers for designing systems and compiler optimizations for these applications.","","0-7803-9461","10.1109/IISWC.2005.1525999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1525999","","Parallel processing;Application software;Yarn;Speech recognition;Face recognition;Ray tracing;Decoding;Kernel;Videoconference;Computer architecture","multimedia computing;benchmark testing;multi-threading;speech recognition;face recognition;ray tracing;video coding;decoding","ALPBench benchmark suite;complex multimedia applications;thread-level parallelism;data-level parallelism;instruction-level parallelism;speech recognition;CMU Sphinx 3;face recognition;ray tracing;Tachyon;MPEG-2 encode;MPEG-2 decode;POSIX threads;sub-word SIMD;Inters SSE2 instructions;compiler optimizations","","43","35","","","","","","IEEE","IEEE Conferences"
"Conventional benchmarks as a sample of the performance spectrum","J. L. Gustafson; R. Todi","USDOE, Ames, IA, USA; NA","Proceedings of the Thirty-First Hawaii International Conference on System Sciences","","1998","7","","514","523 vol.7","Most benchmarks are smaller than actual application programs. One reason is to improve benchmark universality by demanding resources every computer is likely to have. But users dynamically increase the size of application programs to match the power available, whereas most benchmarks are static and of a size appropriate for computers available when the benchmark was created; this is particularly true for parallel computers. Thus, the benchmark overstates computer performance since smaller problems spend more time in cache. Scalable benchmarks, such as HINT, examine the full spectrum of performance through various memory regimes, and express a superset of the information given by any particular fixed size benchmark. Using 5000 experimental measurements, we have found that performance on the NAS Parallel Benchmarks, SPEC, LINPACK, and other benchmarks is predicted accurately by subsets of HINT performance curve. Correlations are typically better than 0.995, and predicted ranking is often perfect.","","0-8186-8255","10.1109/HICSS.1998.649247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649247","","Laboratories;Concurrent computing;Computer aided manufacturing;Application software;Microcomputers;Workstations;Contracts;Testing;Hardware;Fires","performance evaluation","conventional benchmarks;performance spectrum;application programs;benchmark universality;parallel computers;computer performance;scalable benchmarks;HINT;memory regimes;NAS Parallel Benchmarks;SPEC;LINPACK;predicted ranking","","8","13","","","","","","IEEE","IEEE Conferences"
"A hybrid genetic algorithm for generating optimal synthetic aperture radar target servicing strategies","B. Jackson; J. Norgard","Ball Aerosp. & Technol. Corp., Boulder, CO, USA; NA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","2","","2/709","2/718 vol.2","The purpose of this research was to develop a software tool for generating optimal target servicing strategies for imaging fixed ground targets with a spaceborne SAR. Given a list of targets and their corresponding geographic locations and relative priorities, this tool generates a target servicing strategy that maximizes the overall collection utility based on the number of targets successfully imaged weighted by their relative priorities. This tool is specifically designed to maximize sensor utility in the case of a target-rich environment. For small numbers of targets, a target servicing strategy is unnecessary, and the targets may be imaged in any order without paying any particular attention to geographic proximity or target priority. However, for large, geographically diverse target decks, the order in which targets are serviced is of great importance. The target servicing problem is shown to be of the class NP-hard, and thus cannot be solved to optimality in polynomial time. Therefore, global search techniques such as genetic algorithms are called for. A unique hybrid algorithm that combines genetic algorithms with simulated annealing has been developed to generate optimized target servicing strategies. The performance of this hybrid algorithm was compared against that of three different greedy algorithms in a series of 20 test cases. Preliminary results indicate consistent performance improvements over greedy algorithms for target-rich environments. Over the course of 20 trials, the hybrid optimizing algorithm produced weighted collection scores that were on average 10% higher than the best greedy algorithm.","","0-7803-6599","10.1109/AERO.2001.931250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931250","","Genetic algorithms;Hybrid power systems;Synthetic aperture radar;Greedy algorithms;Spaceborne radar;Image sensors;Software algorithms;Space technology;Business;Springs","synthetic aperture radar;genetic algorithms;target tracking;simulated annealing;radar imaging","hybrid genetic algorithm;synthetic aperture radar;target servicing strategies;fixed ground targets;spaceborne SAR;geographic locations;overall collection utility;sensor utility;target-rich environment;geographic proximity;geographically diverse target decks;NP-hard;global search techniques;simulated annealing;greedy algorithms;target-rich environments;hybrid optimizing algorithm;weighted collection scores","","2","10","","","","","","IEEE","IEEE Conferences"
"Correlation of polarization-sensitive optical coherence tomography with early osteoarthritis and collagen disorganization","W. Drexler; D. Stamper; C. Jesser; X. Li; C. Pitris; K. Saunders; J. G. Fujimot; M. E. Brezinski","Dept. of Electr. Eng. & Comput. Sci., MIT, Cambridge, MA, USA; NA; NA; NA; NA; NA; NA; NA","Conference on Lasers and Electro-Optics (CLEO 2000). Technical Digest. Postconference Edition. TOPS Vol.39 (IEEE Cat. No.00CH37088)","","2000","","","506","","Summary form only given. Osteoarthritis (OA) is a major cause of disability and morbidity. However, conventional imaging methods lack the resolution or are impractical for assessing early OA. Recently, optical coherence tomography (OCT) has demonstrated a feasibility for assessing articular cartilage. In vitro studies have demonstrated that OCT can determine cartilage width at a resolution of 20 /spl mu/m in addition to the identification of fibrillations, fibrosis, and breakdown of the subcondral plate. It was also noted that the normal cartilage was polarization sensitive. In the study, two theories are tested. The first hypothesis is that the origin of polarization sensitivity in normal cartilage is organized collagen. The second hypothesis is that the loss of polarization sensitivity, and therefore collagen disorganization, is a sign of osteoarthritis that precedes cartilage thinning.","","1-55752-634","10.1109/CLEO.2000.907316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=907316","","Optical polarization;Tomography;Osteoarthritis;Optical sensors;Microscopy;Optical character recognition software;Optimized production technology;Aging;Electric breakdown;Joints","optical tomography;proteins;light coherence;biomedical imaging;sensitivity;light polarisation;diseases","polarization-sensitive optical coherence tomography;osteoarthritis;collagen disorganization;disability;morbidity;conventional imaging methods;resolution;optical coherence tomography;feasibility;articular cartilage;in vitro studies;cartilage width;fibrillations;fibrosis;breakdown;subcondral plate;normal cartilage;polarization sensitive cartilage;polarization sensitivity;organized collagen;cartilage thinning","","1","5","","","","","","IEEE","IEEE Conferences"
"Working chips from high level synthesis: a case study from industry","R. W. Hunter; T. E. Fuhrman; D. E. Thomas","Delco Electron. Corp., Kokomo, IN, USA; NA; NA","Proceedings of IEEE Custom Integrated Circuits Conference - CICC '94","","1994","","","144","147","Despite intense activity in the high level synthesis research community over the past several years, to our knowledge no working fabricated chips from a general purpose high level synthesis tool have previously been reported. This paper describes the use of a high level synthesis tool to design two chips for future automotive applications. Both chips have been fabricated and tested and work correctly at speed in their target system. An FFT chip contains 176930 transistors and measures 241 by 245 mils, while a Viterbi chip contains 67790 transistors and measures 165 by 162 mils. Two weeks of combined high level synthesis for both chips replaced 9 man-months of detailed datapath schematic and control logic design.<<ETX>>","","0-7803-1886","10.1109/CICC.1994.379748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=379748","","High level synthesis;Computer aided software engineering;Hardware design languages;Logic design;Design optimization;Semiconductor device measurement;Viterbi algorithm;Algorithm design and analysis;Space exploration;Research and development","high level synthesis;circuit CAD;integrated circuit design;logic design;automotive electronics;fast Fourier transforms;Viterbi decoding","high level synthesis;fabricated chips;automotive applications;FFT chip;Viterbi chip;logic design;162 to 245 mil","","1","3","","","","","","IEEE","IEEE Conferences"
"An intelligent environmental controller for R2000 residential homes","D. Mesher; W. F. S. Poehlman","Department of Electrical and Computer Engineering, McMaster University, Hamilton, Ontario; Department of Computer Science and Systems, McMaster University, Hamilton, Ontario","Canadian Journal of Electrical and Computer Engineering","","1990","15","2","73","82","Presents research into the design and implementation of an environmental controller for energy-efficient passive solar homes. The device is a user-friendly adaptive controller which allows extensive parameterization of the controlled space. Controller software provides capabilities for a seven-day, four-temperature setback thermostat. Control routines allow for the optimization of the passive solar gains experienced during the heating season. The software also includes the capability to avoid dew-point violations on the interior window glass which lead to condensation. Finally, overall indoor air quality is maintained by watchdog software which ensures adherence to ventilation guidelines. The control strategy was implemented using standard VLSI vendor technology and has resulted in a comprehensive, yet user-friendly, system. The unit has been operating in an R2000 test site for two years with highly successful results.","0840-8688","","10.1109/CJECE.1990.6590949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6590949","","Humidity;Heart rate variability;Temperature measurement;Random access memory;Temperature sensors;Heating;Ventilation","controllers;home automation;solar heating;space heating;temperature control","controller software;control routines;intelligent environmental controller;R2000 residential homes;energy-efficient passive solar homes;user-friendly adaptive controller;thermostat;optimization;heating season;indoor air quality;watchdog software;ventilation guidelines","","","","","","","","","IEEE","IEEE Journals & Magazines"
"An FPGA family optimized for high densities and reduced routing delay","M. Ahrens; A. El Gamal; D. Galbraith; J. Greene; S. Kaptanoglu; K. R. Dharmarajan; L. Hutchings; S. Ku; P. McGibney; J. McGowan; A. Sanie; K. Shaw; N. Stiawalt; T. Whitney; T. Wong; W. Wong; B. Wu","Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA; Actel Corp., Sunnyvale, CA, USA","IEEE Proceedings of the Custom Integrated Circuits Conference","","1990","","","31.5/1","31.5/4","The Act-2 family of CMOS field-programmable gate arrays (FPGAs) uses an electrically programmable antifuse and novel architectural and circuit features to obtain higher logic densities while increasing speed and routability. Improvements include: two new logic modules, novel I/O and clock driver circuitry, and more flexible and faster routing paths. New addressing circuitry shortens programming time and speeds complete testing for shorts, opens, and stuck-at faults. Fully automatic placement and complete routing are retrained. Special software tools used for architectural exploration and layout generation are discussed.<<ETX>>","","","10.1109/CICC.1990.124844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=124844","","Field programmable gate arrays;Routing;CMOS logic circuits;Programmable logic arrays;Logic programming;Circuit testing;Logic circuits;Clocks;Driver circuits;Flexible printed circuits","CMOS integrated circuits;logic arrays;VLSI","programmable logic devices;PLD;I/O circuitry;FPGA family;densities;reduced routing delay;Act-2 family;CMOS;field-programmable gate arrays;electrically programmable antifuse;circuit features;routability;logic modules;clock driver circuitry;addressing circuitry;architectural exploration;layout generation","","24","8","","","","","","IEEE","IEEE Conferences"
"Processing of fluxing underfills for flip chip-on-laminate assembly","R. Zhao; R. W. Johnson; G. Jones; E. Yaeger; M. Konarski; P. Krug; L. L. Crane","ECE Dept., Auburn Univ., AL, USA; ECE Dept., Auburn Univ., AL, USA; NA; NA; NA; NA; NA","IEEE Transactions on Electronics Packaging Manufacturing","","2003","26","1","75","83","Fluxing underfill eliminates process steps in the assembly of flip chip-on-laminate (FCOL) when compared to conventional capillary flow underfill processing. In the fluxing underfill process, the underfill is dispensed onto the board prior to die placement. During placement, the underfill flows in a ""squeeze flow"" process until the solder balls contact the pads on the board. The material properties, the dispense pattern and resulting shape, solder mask design pattern, placement force, placement speed, and hold time all impact the placement process and the potential for void formation. A design of experiments was used to optimize the placement process to minimize placement-induced voids. The major factor identified was board design, followed by placement acceleration. During the reflow cycle, the fluxing underfill provides the fluxing action required for good wetting and then cures by the end of the reflow cycle. With small, homogeneous circuit boards it is relatively easy to develop a reflow profile to achieve good solder wetting. However, with complex SMT assemblies involving components with significant thermal mass this is more challenging.","1521-334X;1558-0822","","10.1109/TEPM.2003.813007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1205225","","Assembly;Flip chip;Temperature;Surface-mount technology;Cranes;Electronics packaging;Organic materials;Moisture;Material properties;Shape","flip-chip devices;laminates;chip-on-board packaging;surface mount technology;reflow soldering;circuit reliability;assembling;voids (solid);wetting;delamination;thermal shock;design of experiments;failure analysis;printed circuit manufacture","fluxing underfill process;flip chip-on-laminate assembly;FCOL;die placement;solder balls;dispense pattern;solder mask;placement force;placement speed;void formation;design of experiments;placement-induced voids minimization;placement process optimization;reflow cycle;wetting;SMT assemblies;thermal mass;predictive software tools;reflow profile;liquid-to-liquid thermal shock testing;assembly characteristic life;DOE;reliability testing;complex PWBs;SMT PCBs;-40 to 125 C;5 min;1 min","","7","9","","","","","","IEEE","IEEE Journals & Magazines"
"Hidden Markov modelling for SAR automatic target recognition","C. Nilubol; Q. H. Pham; R. M. Mersereau; M. J. T. Smith; M. A. Clements","Center for Signal & Image Process., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA; NA; NA","Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)","","1998","2","","1061","1064 vol.2","This paper discusses the application of hidden Markov models (HMMs) to solve the translational and rotational invariant automatic target recognition (TRIATR) problem associated with SAR imagery. This approach is based on a cascade of these stages: preprocessing, feature extraction and selection, and classification. Preprocessing and feature extraction and selection involve successive applications of extraction operations from measurements of the Radon transform of target chips. The features which are invariant to changes in rotation, position and shifts, although not to changes in scale are optimized through the use of feature selection techniques. The classification stage successively takes as its inputs the multidimensional multiple observation sequences, parameterizes them statistically using continuous density models to capture target and background appearance variability, and thus results in the TRIATR-HMMs. Experimental results have demonstrated that the recognition rate is as high as 99% over both the training set and the testing set.","1520-6149","0-7803-4428","10.1109/ICASSP.1998.675451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675451","","Hidden Markov models;Target recognition;Discrete Fourier transforms;Speech recognition;Signal processing;Image processing;Application software;Feature extraction;Semiconductor device measurement;Multidimensional systems","synthetic aperture radar;radar imaging;radar target recognition;hidden Markov models;feature extraction;image classification;Radon transforms","hidden Markov modelling;SAR automatic target recognition;hidden Markov models;rotational invariant automatic target recognition;SAR imagery;translational invariant automatic target recognition;preprocessing;feature extraction;feature selection;classification;measurements;Radon transform;target chips;position;shifts;multidimensional multiple observation sequences;continuous density models;background appearance variability;target appearance variability;experimental results;TRIATR-HMM;recognition rate;training set;testing set","","7","5","","","","","","IEEE","IEEE Conferences"
"The Gotland HVDC Light project-experiences from trial and commercial operation","U. Axelsson; A. Holm; C. Liljegren; M. Aberg; K. Eriksson; O. Tollerz","Vattenfall, Sweden; NA; NA; NA; NA; NA","16th International Conference and Exhibition on Electricity Distribution, 2001. Part 1: Contributions. CIRED. (IEE Conf. Publ No. 482)","","2001","1","","5 pp. vol.1","","The electrical system on Gotland, owned by Gotlands Energiverk AB (GEAB) has normally no production except for wind power. The power is provided from the mainland with a peak load of approximately 160 MW, and the frequency is regulated by the HVDC link. The HVDC Light installation on Gotland is the first application where a HVDC link is used as an integral part of an AC system. This combined with the environmental demands and the special conditions in the weak network on the Island of Gotland makes this project a very special one that applies high demands on the performance of both hardware and software. The demand for extensive modeling to evaluate the performance and to optimize the control functions has been met by use of the SIMPOW and EMTDC tools. The HVDC Light Cable design has been proven and the environmental advantages have been fully utilized. The controllability has been used for enhancing the power quality and stabilizing the voltage in fault situations, this ability has been proven by measurements and a staged fault test. Both RI and sound levels have been measured and shown to fulfil the stipulated demands. The HVDC Light link has been in operation since November 1999 and is functioning according to expectations. The project has left the building and construction stage. To further exploit the controllability of the VSC converters a follow up project is formed that studies the more sophisticated ways of enhancing performance and interaction with the complex environment of wind power production in the weak network of southern Gotland.","0537-9989","0-85296-735","10.1049/cp:20010675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942772","","","HVDC power convertors;HVDC power transmission;power transmission control;frequency control;controllability;power supply quality;power system dynamic stability","Gotland HVDC Light project;commercial operation;trial operation;Gotland;Gotlands Energiverk AB;peak load;frequency regulation;environmental demands;weak network;control functions optimisation;EMTDC;SIMPOW;controllability;power quality enhancement;fault situations;voltage stabilization;staged fault test;VSC converter controllability;wind power production","","1","","","","","","","IET","IET Conferences"
"Laser processing for electronic circuit restructuring","W. A. Moreno; R. Lee; N. Saini; O. Acon","Center for Microelectron. Res., South Florida Univ., Tampa, FL, USA; Center for Microelectron. Res., South Florida Univ., Tampa, FL, USA; Center for Microelectron. Res., South Florida Univ., Tampa, FL, USA; Center for Microelectron. Res., South Florida Univ., Tampa, FL, USA","Proceedings of First International Caracas Conference on Devices, Circuits and Systems","","1995","","","14","18","The Center for Microelectronics Research (CMR) at the University of South Florida has pursued the development of high density interconnect technologies for advanced electronic systems. Primary focus has been in the area of Laser created interconnects for quick-turn around prototyping of electronic circuits fabricated using standard Very Large Scale Integration (VLSI) process techniques. The Laser Restructuring of a specific application circuitry at the wafer or packaged chip level is accomplished by creating low electrical resistance links between conductors and cutting conductor lines using an integrated computer controlled Laser system. Laser restructuring of generic electronic circuits, is an excellent technique for low-cost, quick turn around custom programmed circuits. A great effort was put into developing an infrastructure within the Laboratory that will conduct state of the art research while keeping manufacturing capabilities for rapid turn around of restructured circuitry using optimized process parameters. Total system control, flexibility, high data quality, and full documentation are examples of this R&D developed environment. Using Design of Experiment techniques and statistical data analysis, Laser created interconnects were studied. Optimum process parameters were obtained using this methodology. The complete process development will be presented at the conference including among others the design and software development of a stand alone interconnect controller (ICON), testing procedure for packaged, and the statistical data analysis for generating the optimum process parameter.","","0-7803-2672","10.1109/ICCDCS.1995.499109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=499109","","Electronic circuits;Integrated circuit interconnections;Laser beam cutting;Very large scale integration;Packaging;Conductors;Control systems;Data analysis;Microelectronics;Prototypes","integrated circuit interconnections;VLSI;laser materials processing;design of experiments","laser processing;electronic circuit restructuring;high density interconnects;quick-turn around prototyping;VLSI;conductor line cutting;integrated computer controlled laser system;custom programmed circuits;manufacturing;design of experiments;interconnect controller;testing;statistical data analysis;process parameter optimization","","","7","","","","","","IEEE","IEEE Conferences"
"A Monte Carlo simulation of X-ray mammography","G. Spyrou; G. Panayiotakis; G. Tzanakos","Dept. of Phys., Athens Univ., Greece; NA; NA","Proceedings of 18th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","1996","2","","842","843 vol.2","We have developed a model for Monte Carlo simulation of the function of the mammographic unit to be used as a research tool for the optimization of the mammographic procedure. A software phantom of a compressed breast has been designed and all possible physical interactions between X-rays and matter have been taken into account. Preliminary test results are presented. The expectations from this simulation are also discussed.","","0-7803-3811","10.1109/IEMBS.1996.652002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=652002","","Mammography;Breast;Electromagnetic scattering;Particle scattering;Filters;Cancer;Diseases;Image resolution;Photonic integrated circuits;Solids","diagnostic radiography;Monte Carlo methods;medical image processing;physiological models;Compton effect","X-ray mammography;Monte Carlo simulation;optimization;software phantom;compressed breast;physical interactions;X-ray-matter interactions;breast imaging;mean free path;Compton scattering;image quality","","1","3","","","","","","IEEE","IEEE Conferences"
"An update on the CTRON Subproject","T. Wasano; T. Ohkubo","ATR Int. Co., Japan; NA","Proceedings 13th TRON Project International Symposium /TEPS '96","","1996","","","28","32","The paper reports on the current status of the CTRON subproject (part of the main TRON project). The CTRON subproject has many vital activities. We describe all the technical study items. Topics of implementation based on CTRON specification on many kinds of MPU are described. Promotion activities of the CTRON subproject around the world, especially focusing on free software of CTRON kernel, are described. Finally, we describe the future study items of our subproject. Up to now, CTRON interfaces have been tuned to get the maximum real time response and performance from applications running individually on communications equipment. Studies are now moving ahead on new issues that will expand and optimize CTRON to true multimedia network services and distributed systems.","1063-6749","0-8186-7658","10.1109/TRON.1996.566182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566182","","Resource management;Streaming media;Multimedia systems;Prototypes;Kernel;Product development;Turning;Testing;Communication equipment;Telecommunication network reliability","formal specification;human factors;microprocessor chips;operating system kernels;multimedia systems;distributed processing","CTRON Subproject;TRON project;technical study items;CTRON specification;MPU;promotion activities;free software;future study items;CTRON interfaces;maximum real time response;communications equipment;multimedia network services;distributed systems","","","9","","","","","","IEEE","IEEE Conferences"
"A fault-tolerant associative memory with high-speed operation","H. Bergh; J. Eneland; L. -. Lundstrom","Ericsson Telecom AB, Stockholm, Sweden; Ericsson Telecom AB, Stockholm, Sweden; NA","IEEE Journal of Solid-State Circuits","","1990","25","4","912","919","An 8-kb (128-word*64-b) CMOS associative memory with word- and bit-parallel operation is described. The highly parallel and pipelined architecture is optimized for high-speed associative operations. The data processing capability is one word/cycle corresponding to 16 MIPS at a typical cycle time of 60 ns. The memory is fault tolerant under software control. A faulty word location in the memory can be made inaccessible by on-chip circuitry. The device is a complete single-chip associative memory with internally controlled addressing and associative data as output.<<ETX>>","0018-9200;1558-173X","","10.1109/4.58283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=58283","","Fault tolerance;Associative memory;Computer aided manufacturing;CADCAM;Random access memory;Testing;Data processing;Logic devices;Computer architecture;Circuit faults","CMOS integrated circuits;content-addressable storage;fault tolerant computing;integrated memory circuits;memory architecture;parallel architectures;pipeline processing","word parallel operation;parallel architecture;single chip memory;CAM;fault-tolerant associative memory;CMOS;bit-parallel operation;pipelined architecture;high-speed associative operations;software control;on-chip circuitry;internally controlled addressing;8 kbit;60 ns;16 MIPS","","17","4","","","","","","IEEE","IEEE Journals & Magazines"
"Decentralized optimal power pricing: The development of a parallel program","S. Lumetta; L. Murphy; X. Li; D. Culler; I. Khalil","California Univ., Berkeley, CA, USA; California Univ., Berkeley, CA, USA; California Univ., Berkeley, CA, USA; California Univ., Berkeley, CA, USA; California Univ., Berkeley, CA, USA","Supercomputing '93:Proceedings of the 1993 ACM/IEEE Conference on Supercomputing","","1993","","","240","249","Many programming systems available on massively parallel processors (MPPs) today neglect the significance of time spent fixing an algorithm during development. Those systems which do address the fix time commonly demand drastic sacrifices in execution speed. Aiming for the middle ground between these two extremes, the authors have implemented a new algorithm to solve an optimization problem for an electrical power system, a problem large enough to require significant computational resources. To help abstract the communication and layout requirements of the problem away from the main algorithm, they have developed a small object system library. The results are an efficient and easily modifiable solution to the problem and a general approach to solving this class of problems.","1063-9535","0-8186-4340","10.1109/SUPERC.1993.1263450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1263450","","Pricing;Power systems;Acceleration;Kernel;Large-scale systems;Power generation;Workstations;System testing","parallel programming;software libraries;economics;costing;power systems","communication requirements;massively parallel processors;fix time;execution speed;optimization problem;electrical power system;computational resources;layout requirements;object system library","","","9","","","","","","IEEE","IEEE Conferences"
"Can computers really help students understand electromagnetics","J. F. Hoburg","Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Education","","1993","36","1","119","122","Some motivations and experiences of a professor who became active twelve years ago in the development of educational software for use in teaching and learning engineering electromagnetics are described. Categories of educational tools are defined, exemplified, discussed, and ranked in order of effectiveness. The author provides opinions as to what has and has not been accomplished.<<ETX>>","0018-9359;1557-9638","","10.1109/13.204829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=204829","","Electromagnetics;Power engineering computing;Engineering education;Laboratories;Application software;Computer applications;Computer science education;Societies;Testing;Books","computer aided instruction;education;electrical engineering computing;electromagnetism;teaching","CAI;electromagnetism;education;electromagnetics;educational software;teaching;learning","","19","11","","","","","","IEEE","IEEE Journals & Magazines"
"Integration of parallel MM5 with distributed resource manager and performance evaluation","M. Koh; Liang Peng; S. See","Asia Pacific Sci. & Technol. Center, Nanyang Technol. Univ.,, China; Asia Pacific Sci. & Technol. Center, Nanyang Technol. Univ.,, China; Asia Pacific Sci. & Technol. Center, Nanyang Technol. Univ.,, China","Eighth International Conference on High-Performance Computing in Asia-Pacific Region (HPCASIA'05)","","2005","","","8 pp.","298","The PSU/NCAR mesoscale model (known as MM5) is a limited-area, nonhydrostatic, terrain-following sigma-coordinate model designed to simulate or predict mesoscale atmospheric circulation. MM5 is popularly used in numerical weather prediction and each simulation runs can take from several minutes to many hours. In many datacenters today, it is common that distributed resource management (DRM) software like LSF, PBSPro and N1 grid engine are employed to manage and optimize compute resources. In this paper, we described our experience of integrating parallel MM5 with N1 grid engine (N1 GE), and discussed some of its performance characteristics. Our work shows that the integration parallel MM5 with distributed resource managers is feasible and the performance is rather good","","0-7695-2486","10.1109/HPCASIA.2005.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1592281","","Sun;Atmospheric modeling;Engines;Resource management;Distributed computing;Predictive models;Concurrent computing;Scalability;Testing;Asia","atmospheric movements;DP management;grid computing;parallel processing;performance evaluation;resource allocation;weather forecasting","parallel MM5;distributed resource manager;performance evaluation;PSU/NCAR mesoscale model;mesoscale atmospheric circulation;numerical weather prediction;datacenters;distributed resource management software;LSF;PBSPro;N1 grid engine","","","8","","","","","","IEEE","IEEE Conferences"
"Optimal sensor configuration for complex systems","P. Sadegh; J. C. Spall","Dept. of Math. Modeling, Tech. Univ., Lyngby, Denmark; NA","Proceedings of the 1998 American Control Conference. ACC (IEEE Cat. No.98CH36207)","","1998","6","","3575","3579 vol.6","The paper considers the problem of sensor configuration for complex systems with the aim of maximizing the useful information about certain quantities of interest. Our approach involves: 1) definition of an appropriate optimality criterion or performance measure; and 2) description of an efficient and practical algorithm for achieving the optimality objective. The criterion for optimal sensor configuration is based on maximizing the overall sensor response while minimizing the correlation among the sensor outputs, so as to minimize the redundant information being provided by the multiple sensors. The procedure for sensor configuration is based on the simultaneous perturbation stochastic approximation (SPSA) algorithm. SPSA avoids the need for detailed modeling of the sensor response by simply relying on the observed responses obtained by limited experimentation with test sensor configurations. We illustrate the approach with the optimal placement of acoustic sensors for signal detection in structures.","0743-1619","0-7803-4530","10.1109/ACC.1998.703278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=703278","","Sensor systems;Acoustic sensors;Biomedical monitoring;Stochastic processes;Computer simulation;Application software;Biosensors;Mathematical model;Physics;Laboratories","signal detection;sensors;stochastic processes;large-scale systems;approximation theory;optimisation","complex systems;sensor configuration;optimality criterion;simultaneous perturbation stochastic approximation;optimal sensor placement;signal detection;random process;optimisation","","8","23","","","","","","IEEE","IEEE Conferences"
"Deterministic computation of the Frobenius form","A. Storjohann","Ontario Res. Centre for Comput. Algebra, Univ. of Western Ontario, London, Ont., Canada","Proceedings 2001 IEEE International Conference on Cluster Computing","","2001","","","368","377","A deterministic algorithm for computing the Frobenius canonical-form of a matrix over a field is described. A similarity transformation-matrix is recovered in the same time. The algorithm is nearly optimal, requiring about the same number of field operations as required for matrix multiplication. Previously-known reductions to matrix multiplication are probabilistic.","","0-7695-1116","10.1109/SFCS.2001.959911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959911","","Bismuth;Costs;Polynomials;Algorithms;Algebra;Shape;Testing;Application software;Computer science","deterministic algorithms;matrix multiplication;computational complexity;optimisation","deterministic computation;Frobenius form;deterministic algorithm;Frobenius canonical-form;similarity transformation matrix;field operations;matrix multiplication;probabilistic reductions","","5","18","","","","","","IEEE","IEEE Conferences"
"Virtual Fekete point configurations: a case study in perturbing complex systems","R. van Liere; J. Mulder; J. Frank; J. de Swart","Center for Math. & Comput. Sci., CWI, Amsterdam, Netherlands; NA; NA; NA","Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)","","2000","","","189","195","Virtual environments have shown great promise as a research tool in science and engineering. In this paper, we study a classical problem in mathematics: that of approximating globally optimal Fekete point configurations. We found that a highly interactive virtual environment, combined with a time-critical computation, can provide valuable insight into the symmetry and stability of Fekete point configurations. We believe that virtual environments provide more natural interfaces to complex systems, allowing users to perceive, interpret and interact with the problem more rapidly.","1087-8270","0-7695-0478","10.1109/VR.2000.840498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840498","","Computer aided software engineering;Mathematics;Virtual environment;Iron;Time factors;Computer science;Stability;Data mining;Testing;Data visualization","virtual reality;mathematics computing;large-scale systems;symmetry;stability;data visualisation;perturbation techniques;optimisation;engineering graphics","Fekete point configurations;case study;complex system perturbation analysis;interactive virtual environment;globally optimal approximation;time-critical computation;symmetry;stability;natural interface;problem perception;problem interpretation;mathematics;explorative visualization","","1","14","","","","","","IEEE","IEEE Conferences"
"A new built-in self-repair approach to VLSI memory yield enhancement by using neural-type circuits","P. Mazumder; Y. -. Jih","Michigan Univ., Ann Arbor, MI, USA; Michigan Univ., Ann Arbor, MI, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1993","12","1","124","136","It is shown how to represent the objective function of the memory repair problem as a neural-network energy function, and how to exploit the neural network's convergence property for deriving optimal repair solutions. Two algorithms have been developed using a neural network, and their performances are compared with that of the repair most (RM) algorithm. For randomly generated defect patterns, a proposed algorithm with a hill-climbing capability successfully repaired memory arrays in 98% cases, as opposed to RMs 20% cases. It is demonstrated how, by using very small silicon overhead, one can implement this algorithm in hardware within a VLSI chip for built in self repair (BISR) of memory arrays. The proposed auto-repair approach is shown to improve the VLSI chip yield by a significant factor, and it can also improve the life span of the chip by automatically restructuring its memory arrays in the event of sporadic cell failures during the field use.<<ETX>>","0278-0070;1937-4151","","10.1109/43.184849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=184849","","Very large scale integration;Software algorithms;Circuit faults;Neural networks;Built-in self-test;Hip;Neural network hardware;Pulp manufacturing;Virtual manufacturing;Content addressable storage","convergence;integrated memory circuits;neural nets;optimisation;VLSI","built-in self-repair;VLSI memory yield enhancement;neural-type circuits;objective function;memory repair problem;neural-network energy function;convergence property;optimal repair solutions;randomly generated defect patterns;hill-climbing capability;BISR;auto-repair approach;VLSI chip yield","","23","29","","","","","","IEEE","IEEE Journals & Magazines"
"An integer linear programming approach for identifying instruction-set extensions","C. Ozturan; G. Dundar; K. Atasu","Bogazici University, Turkey; Bogazici University, Turkey; Bogazici University, Turkey","2005 Third IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS'05)","","2005","","","172","177","This paper presents an Integer Linear Programming (ILP) approach to the instruction-set extension identification problem. An algorithm that iteratively generates and solves a set of ILP problems in order to generate a set of templates is proposed. A selection algorithm that ranks the generated templates based on isomorphism testing and potential evaluation is described. A Trimaran based framework is used to evaluate the quality of the instructions generated by the technique. Speed-up results of up to 7.5 are observed.","","1-59593-161","10.1145/1084834.1084880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076332","ASIPs;extensible processors;integer linear;programming","Integer linear programming;Computer aided instruction;Computer architecture;Application specific processors;Delay;Embedded computing;Iterative algorithms;Embedded system;Application software;Costs","","","","32","22","","","","","","IEEE","IEEE Conferences"
"Segmentation of textured polarimetric SAR scenes by likelihood approximation","J. -. Beaulieu; R. Touzi","Comput. Sci. & Software Eng. Dept., Laval Univ., Quebec City, Que., Canada; NA","IEEE Transactions on Geoscience and Remote Sensing","","2004","42","10","2063","2072","A hierarchical stepwise optimization process is developed for polarimetric synthetic aperture radar image segmentation. We show that image segmentation can be viewed as a likelihood approximation problem. The likelihood segment merging criteria are derived using the multivariate complex Gaussian, the Wishart distribution, and the K-distribution. In the presence of spatial texture, the Gaussian-Wishart segmentation is not appropriate. The K-distribution segmentation is more effective in textured forested areas. The validity of the product model is also assessed, and a field-adaptable segmentation strategy combining different criteria is examined.","0196-2892;1558-0644","","10.1109/TGRS.2004.835302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1344159","","Layout;Image segmentation;Testing;Synthetic aperture radar;Remote sensing;Speckle;Polarimetric synthetic aperture radar;Merging;Image edge detection;Gaussian processes","remote sensing by radar;geophysical signal processing;maximum likelihood decoding;image segmentation;image texture;terrain mapping;vegetation mapping;radar polarimetry;Gaussian distribution;forestry","image segmentation;textured polarimetric SAR scenes;hierarchical stepwise optimization process;synthetic aperture radar;likelihood approximation problem;likelihood segment merging criteria;multivariate complex Gaussian;Wishart distribution;spatial texture;Gaussian-Wishart segmentation;K-distribution segmentation;textured forested areas;field-adaptable segmentation strategy;maximum likelihood estimation;image texture","","54","39","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal network reconfigurations in distribution systems. II. Solution algorithms and numerical results","H. -. Chiang; R. Jean-Jumeau","Sch. of Electr. Eng., Cornell Univ., Ithaca, NY, USA; Sch. of Electr. Eng., Cornell Univ., Ithaca, NY, USA","IEEE Transactions on Power Delivery","","1990","5","3","1568","1574","Using a two-stage solution methodology and a modified simulated annealing technique, the authors develop a solution algorithm to the network reconfiguration problem, which is a constrained, multiobjective, nondifferentiable, optimization problem. This solution algorithm allows the designer to obtain a desirable, global noninferior point in a reasonable computation time. Also, given a desired number of switch-on/switch-off operations involved in the network configuration, the solution algorithm can identify the most effective operations. In order to reduce the computation time required, the idea of approximate calculations is explored and incorporated into the solution algorithm, where two efficient load-flow methods are employed; one for high temperature and the other for low temperature. The solution algorithm has been implemented in a software package and tested on a 69-bus system with very promising results.<<ETX>>","0885-8977;1937-4208","","10.1109/61.58002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=58002","","Intelligent networks;Load flow;Load flow analysis;Constraint optimization;Load management;Simulated annealing;Temperature;Switches;Power system restoration;Algorithm design and analysis","distribution networks;load flow;power engineering computing","distribution systems;simulated annealing technique;network reconfiguration;optimization;switch-on/switch-off operations;load-flow;high temperature;low temperature;software package","","224","8","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying the fuzzy grey prediction model by genetic algorithms","Yo-Ping Huang; Sheng-Fang Wang","Dept. of Comput. Sci. & Eng., Tatung Inst. of Technol., Taipei, Taiwan; Dept. of Comput. Sci. & Eng., Tatung Inst. of Technol., Taipei, Taiwan","Proceedings of IEEE International Conference on Evolutionary Computation","","1996","","","720","725","The application of genetic algorithms to the design of the fuzzy grey model is investigated. Based on given past data, the next output from an unknown plant can be predicted by the basic grey model. To better improve the accuracy of the prediction model, a fuzzy controller is designed to determine the quantity of compensation for the output from the grey system. Genetic algorithms are used to optimize the roughly-determined fuzzy model. A test pattern is then fed to the well-tuned system to obtain the compensation quantity through a defuzzification process. The procedures for identifying three different types of fuzzy models are presented. Simulation results from a well-known example are shown to demonstrate that simplicity in modeling and applicability to intelligent prediction systems are the merits of the proposed methodology.","","0-7803-2902","10.1109/ICEC.1996.542691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=542691","","Predictive models;Genetic algorithms;Fuzzy systems;Differential equations;Accuracy;Fuzzy set theory;Genetic mutations;Computer science;Genetic engineering;Application software","fuzzy set theory;identification;prediction theory;genetic algorithms;compensation;modelling;fuzzy control;fuzzy systems;predictive control;control system analysis","fuzzy grey prediction model identification;genetic algorithms;unknown plant;next output prediction;prediction model accuracy;fuzzy controller;output compensation;fuzzy model optimization;well-tuned system;defuzzification process;simulation;intelligent prediction systems","","4","12","","","","","","IEEE","IEEE Conferences"
"PoPS: a computational tool for modeling and predicting protease specificity","S. E. Boyd; M. G. de la Banda; R. N. Pike; J. C. Whisstock; G. B. Rudy","Sch. of Comput. Sci. & Software Eng., Monash Univ., Melbourne, Vic., Australia; Sch. of Comput. Sci. & Software Eng., Monash Univ., Melbourne, Vic., Australia; NA; NA; NA","Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004.","","2004","","","372","381","Proteases play a fundamental role in the control of intra- and extracellular processes by binding and cleaving specific amino acid sequences. Identifying these targets is extremely challenging. Current computational attempts to predict cleavage sites are limited, representing these amino acid sequences as patterns or frequency matrices. Here we present PoPS, a publicly accessible bioinformatics tool (http://pops.csse.monash.edu.au/) which provides a novel method for building computational models of protease specificity that, while still being based on these amino acid sequences, can be built from any experimental data or expert knowledge available to the user. PoPS specificity models can be used to predict and rank likely cleavages within a single substrate, and within entire proteomes. Other factors, such as the secondary or tertiary structure of the substrate, can be used to screen unlikely sites. Furthermore, the tool also provides facilities to infer, compare and test models, and to store them in a publicly accessible database.","","0-7695-2194","10.1109/CSB.2004.1332450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1332450","","Computational modeling;Predictive models;Amino acids;Bioinformatics;Bonding;Sequences;Proteins;Biochemistry;Biological control systems;Diseases","enzymes;genetics;molecular biophysics;biology computing;physiological models","PoPS;computational tool;protease specificity;amino acid sequences;cleavage sites;publicly accessible bioinformatics tool;computational models;proteomes","","2","24","","","","","","IEEE","IEEE Conferences"
"Gradient methods in unsupervised neural networks","A. L. Dajani; M. Kamel; M. I. Elmasry","Waterloo Univ., Ont., Canada; Waterloo Univ., Ont., Canada; Waterloo Univ., Ont., Canada","[Proceedings] 1991 IEEE International Joint Conference on Neural Networks","","1991","","","1770","1775 vol.2","The authors discuss the application of gradient methods (steepest descent and conjugate gradients) to an unsupervised neural network that uses potential functions in the output layer as previously reported by the authors (1990). The nature of most unsupervised neural networks makes the gradient step algorithm the only possible way to optimize the network's function. The network reported by the authors in 1990 can make use of other gradient methods. Experiments with the steepest descent/ascent and conjugate gradients algorithms show a potential decrease in the number of iterations. The contours of the network's function showed considerable variation, especially close to the local optima. This nature restricted the line search algorithms that could be used. Simulations confirm this fact by showing how a crude line search algorithm performs better than a semiexact line search algorithm.<<ETX>>","","0-7803-0227","10.1109/IJCNN.1991.170685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=170685","","Gradient methods;Intelligent networks;Neural networks;Clustering algorithms;Neurons;Design engineering;Systems engineering and theory;Application software;Testing;Euclidean distance","iterative methods;neural nets","iterative methods;unsupervised neural networks;gradient methods;steepest descent;conjugate gradients;potential functions;gradient step algorithm;crude line search algorithm;semiexact line search algorithm","","","10","","","","","","IEEE","IEEE Conferences"
"Genetic algorithms for feature selection and weighting, a review and study","F. Hussein; N. Kharma; R. Ward","Dept. of Electr. & Comput. Eng., British Columbia Univ., Vancouver, BC, Canada; NA; NA","Proceedings of Sixth International Conference on Document Analysis and Recognition","","2001","","","1240","1244","Our aim is: a) to present a comprehensive survey of previous attempts at using genetic algorithms (GA) for feature selection in pattern recognition applications, with a special focus on character recognition; and b) to report on work that uses GA to optimize the weights of the classification module of a character recognition system. The main purpose of feature selection is to reduce the number of features, by eliminating irrelevant and redundant features, while simultaneously maintaining or enhancing classification accuracy. Many search algorithms have been used for feature selection. Among those, GA have proven to be an effective computational method, especially in situations where the search space is uncharacterized (mathematically), not fully understood, or/and highly dimensional.","","0-7695-1263","10.1109/ICDAR.2001.953980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953980","","Genetic algorithms;Spatial databases;Testing;Error analysis;Character recognition;Machine learning;Handwriting recognition;Software packages;Packaging;Wheels","genetic algorithms;character recognition;probability;learning (artificial intelligence);search problems;pattern classification","feature selection;weighting;genetic algorithms;pattern recognition applications;character recognition;classification module;classification accuracy;search space","","15","19","","","","","","IEEE","IEEE Conferences"
"An efficient implementation of nested loop control instructions for super scalar processors","V. Andronache; B. Sinclair; R. P. Simpson; N. L. Passos","Dept. of Comput. Sci., Midwestern State Univ., Wichita Falls, TX, USA; NA; NA; NA","1998 Midwest Symposium on Circuits and Systems (Cat. No. 98CB36268)","","1998","","","82","85","This paper presents a technique that makes efficient use of super scalar processor capabilities to optimize the execution of nested loop structures. By creating new global and local execution schedules, the linear dependencies inherent to the regular execution of the loop are removed and the degree of parallelism is increased. New compiler constructs allow the execution of the instructions according to the new schedule directions.","","0-8186-8914","10.1109/MWSCAS.1998.759440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=759440","","Parallel processing;Hardware;Linear algebra;Pipeline processing;Computer science;Processor scheduling;Software tools;Testing;Scheduling algorithm;Computer languages","program control structures;parallel programming;program compilers","nested loop control instructions;super scalar processors;global execution schedules;local execution schedules;regular execution;compiler constructs","","","13","","","","","","IEEE","IEEE Conferences"
"Use of a transputer system for fast 3-D image reconstruction in 3-D PET","S. Barresi; D. Bollini; A. Del Guerra","INFN, Sezione di Pisa, Italy; NA; NA","IEEE Transactions on Nuclear Science","","1990","37","2","812","816","A transputer system was used for fast 3-D image reconstruction in 3-D PET (positron emission tomography). The optimization of the algorithm (written in Occam language) and its parallelization on the transputer system for its use in the HISPET (high spatial resolution PET) project are discussed. It is projected that a 100-transputer machine would suffice in processing process online data at approximately=10 kHz for a matrix volume of 128*128*128 voxels, i.e. at a rate which is one-tenth of the maximum acquisition rate for the HISPET project.<<ETX>>","0018-9499;1558-1578","","10.1109/23.106720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=106720","","Image reconstruction;Positron emission tomography;Solids;Spatial resolution;Software algorithms;Cameras;Radioactive decay;Glass;Testing;Prototypes","computerised picture processing;computerised tomography;medical computing;multiprocessing systems;radioisotope scanning and imaging","PET;positron emission tomography;transputer system;fast 3-D image reconstruction;algorithm;Occam","","20","8","","","","","","IEEE","IEEE Journals & Magazines"
"An improved latching pulse design for dynamic sense amplifiers","J. S. Yuan; J. J. Liou","Dept. of Electr. Eng., Univ. of Central Florida, Orlando, FL, USA; Dept. of Electr. Eng., Univ. of Central Florida, Orlando, FL, USA","IEEE Journal of Solid-State Circuits","","1990","25","5","1294","1299","A generalized optimal latching pulse for dynamic sense-amplifier design has been derived. The model equations account for threshold-voltage imbalance bit-line capacitance imbalance, current-gain imbalance, gate capacitance and intra-bit-line capacitive coupling effect, channel-length modulation, source-body effect, and temperature sensitivity in a unified manner. Computer simulations of the analytical equations, including those effects, are presented. The analytical results provide physical insight into sensing speed and the sensitivity of optimized waveforms in terms of process imbalances, device model parameters, and circuit design variables. A design implementation for fast sense-amplifier operation is also presented to demonstrate the utility of the model equations for practical application.<<ETX>>","0018-9200;1558-173X","","10.1109/4.62155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=62155","","Pulse amplifiers;Error correction;Computer errors;Equations;Computer simulation;Circuit testing;Error correction codes;Solid state circuits;Application software;Capacitance","amplifiers;DRAM chips;equivalent circuits;flip-flops;MOS integrated circuits","MOS flip-flop;latching pulse design;dynamic sense amplifiers;threshold-voltage imbalance bit-line capacitance imbalance;current-gain imbalance;gate capacitance;intra-bit-line capacitive coupling effect;channel-length modulation;source-body effect;temperature sensitivity;device model parameters;circuit design variables","","7","10","","","","","","IEEE","IEEE Journals & Magazines"
"A case study in networks-on-chip design for embedded video","Jiang Xu; W. Wolf; J. Henkel; S. Chakradhar; T. Lv","Dept. of Electr. Eng., Princeton Univ., NJ, USA; Dept. of Electr. Eng., Princeton Univ., NJ, USA; NA; NA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition","","2004","2","","770","775 Vol.2","In this paper we study bus-based and switch-based on-chip networks for an embedded video application, the smart camera SoC (system on chip). We analyze network performance and overall system performance in detail. We explore system performance using crossbars with different sizes, fixed size but different numbers of ports, and different numbers of shared memories. We find that network is a performance bottleneck in our design, and the system using an optimized NoC can outperform one using a bus by 132%. Our simulations are based upon recorded real communication traces, which give more accurate system performance. Our study finds that for the Smart Camera system, a 16-bit/port 3/spl times/3 crossbar with two shared memories shows 85.7% performance improvement over the bus-based model and also has less maximum network throughput than the bus-based model. This design example illustrates a methodology to quickly and accurately estimate the performance of NoC's at architecture level.","1530-1591","0-7695-2085","10.1109/DATE.2004.1268973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268973","","Computer aided software engineering;Intelligent networks;Smart cameras;Computer architecture;Network-on-a-chip;Throughput;System performance;Pipelines;System-on-a-chip;Switches","system-on-chip;shared memory systems;telecommunication networks;embedded systems;cameras","networks-on-chip design;embedded video;bus based on-chip network;switch based on-chip networks;smart camera SoC;smart camera system-on-chip;crossbars;real communication traces;bus based model;shared memory","","8","14","","","","","","IEEE","IEEE Conferences"
"Shared memory implementations of synchronous dataflow specifications","P. K. Murthy; S. S. Bhattacharyya","Angeles Design Syst., San Jose, CA, USA; NA","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","404","410","There has been a proliferation of block-diagram environments for specifying and prototyping DSP systems. These include tools from academia like Ptolemy and GRAPE, and commercial tools like SPW from Cadence Design Systems, Cossap from Synopsys, and the HP ADS tool from HP. The block diagram languages used in these environments are usually based on dataflow semantics because various subsets of dataflow have proven to be good matches for expressing and modeling signal processing systems. In particular synchronous dataflow (SDF) has been found to be a particularly good match for expressing multirate signal processing systems. One of the key problems that arises during synthesis from an SDF specification is scheduling. Past work on scheduling from SDF has focused on optimization of program memory and buffer memory. However, no attempt was made for overlaying or sharing buffers. In this paper we formally tackle the problem of generating optimally compact schedules for SDF graphs, that also attempt to minimize buffering memory under the assumption that buffers will be shared. This will result in schedules whose data memory usage is drastically lower (up to 83%) than methods in the past have achieved.","","0-7695-0537","10.1109/DATE.2000.840303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840303","","Registers;Digital signal processing;Signal processing;Educational institutions;Prototypes;Pipelines;Signal design;Signal synthesis;Software libraries;High level languages","shared memory systems;data flow computing;array signal processing;processor scheduling;buffer storage","shared memory implementations;synchronous dataflow specifications;DSP systems;dataflow semantics;multirate signal processing systems;SDF specification;scheduling;optimally compact schedules;buffering memory","","5","15","","","","","","IEEE","IEEE Conferences"
"System-level design automation tools for digital microfluidic biochips","F. Su; K. Chakrabarty","Duke University, Durham, NC; Duke University, Durham, NC","2005 Third IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS'05)","","2005","","","201","206","Biochips based on digital microfluidics offer a powerful platform for massively parallel biochemical analysis such as clinical diagnosis and DNA sequencing. Current full-custom design techniques for digital microfluidic biochips do not scale well for increasing levels of system integration. Analogous to classical VLSI synthesis, a top-down system-level design automation approach can shorten the biochip design cycle and reduce human effort. We present here an overview of a system-level design methodology that includes architectural synthesis and physical design. The proposed design automation approach is expected to relieve biochip users from the burden of manual optimization of bioassays, time-consuming hardware design, and costly testing and maintenance procedures.","","1-59593-161","10.1145/1084834.1084887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076337","biochip;microfluidics;physical design;synthesis","System-level design;Design automation;Microfluidics;Biochemical analysis;Clinical diagnosis;DNA;Very large scale integration;Humans;Design optimization;Hardware","","","","1","24","","","","","","IEEE","IEEE Conferences"
"Modelling surface-micromachined electrothermal actuators","R. W. Johnstone; M. Parameswaran","NA; NA","Canadian Journal of Electrical and Computer Engineering","","2004","29","3","193","202","A method for the accurate simulation of electrothermal actuators is presented. The model accounts for variations in material parameters that occur over the temperature range at which polysilicon electrothermal actuators are operated. The paper then presents the steps necessary to perform a sequential electrothermal-mechanical analysis using ANSYS modelling software. The model is compared against actuators fabricated using a surface-micromachining process called MUMPs. The literature concerning thematerial parameters associated with this process that are necessary for the simulation is reviewed.","0840-8688","","10.1109/CJECE.2004.1532523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1532523","","Electrothermal effects;Actuators;Thermal conductivity;Boundary conditions;Thermal resistance;Temperature distribution;Testing;Design optimization;Performance analysis;Software performance","","","","15","34","","","","","","IEEE","IEEE Journals & Magazines"
"A practical single image based approach for estimating illumination distribution from shadows","Taeone Kim; Ki-Sang Hong","Dept. of Electron. & Electr. Eng., POSTECH, Pohang, South Korea; Dept. of Electron. & Electr. Eng., POSTECH, Pohang, South Korea","Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1","","2005","1","","266","271 Vol. 1","This paper presents a practical method that estimates illumination distribution from shadows where the shadows are assumed to be cast on a textured, Lambertian surface. Previous methods usually require that the reflectance property of the surface be constant or uniform, or need an additional image to cancel out the effects of varying albedo of the textured surface. We deal with an estimation problem for which surface albedo information is not available. In this case, the estimation problem corresponds to an underdetermined one. We show that combination of regularization by correlation and some user-specified information can be a practical method for solving the problem. In addition, as an optimization tool for solving the problem, we develop a constrained nonnegative quadratic programming (NNQP) technique into which not only regularization but also user-specified information are easily incorporated. We test and validate our method on both synthetic and real images and present some experimental results.","1550-5499;2380-7504","0-7695-2334","10.1109/ICCV.2005.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541266","","Lighting;Surface texture;Layout;Light sources;Reflectivity;Application software;Rendering (computer graphics);Optical reflection;Constraint optimization;Quadratic programming","image texture;quadratic programming","illumination distribution estimation;textured Lambertian surface;optimization tool;constrained nonnegative quadratic programming;synthetic image;real image","","5","9","","","","","","IEEE","IEEE Conferences"
"An extensible genetic algorithm framework for problem solving in a common environment","A. S. Chuang; F. Wu","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; NA","IEEE Transactions on Power Systems","","2000","15","1","269","275","The authors describe an object-oriented framework for solving mathematical power system programs using genetic algorithms (GAs). The advantages of this framework are its extensibility, modular design and accessibility to existing programming code. The framework also incorporates a graphical user interface that may be used to build new GAs as well as run GA simulations. Two power system problems are solved by implementing genetic algorithms using the said framework. The first is a continuous optimization problem and the second an integer programming problem. The authors illustrate the flexibility of the framework as well as its other features on their test problems.","0885-8950;1558-0679","","10.1109/59.852132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852132","","Genetic algorithms;Problem-solving;Power system planning;Optimization methods;Power system restoration;Application software;Power system simulation;Graphical user interfaces;Power system harmonics;Power systems","power system analysis computing;genetic algorithms;integer programming;problem solving;object-oriented methods;graphical user interfaces","power system problems;genetic algorithm framework;problem solving;object-oriented framework;computer simulation;modular design;programming code;graphical user interface;continuous optimization problem;integer programming problem","","12","27","","","","","","IEEE","IEEE Journals & Magazines"
"Two-dimensional recursive filter design--A spectral factorization approach","M. Ekstrom; R. Twogood; J. Woods","University of California, Livermore, CA; NA; NA","IEEE Transactions on Acoustics, Speech, and Signal Processing","","1980","28","1","16","26","This paper concerns development of an efficient method for the design of two-dimensional (2-D) recursive digital filters. The specific design problem addressed is that of obtaining half-plane recursive filters which satisfy prescribed frequency response characteristics. A novel design procedure is presented which incorporates a spectral factorization algorithm into a constrained, nonlinear optimization approach. A computational implementation of the design algorithm is described and its design capabilities demonstrated with several examples.","0096-3518","","10.1109/TASSP.1980.1163355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1163355","","Filters;Algorithm design and analysis;Signal design;Signal processing algorithms;Hardware;Two dimensional displays;Application software;Stability;Testing;Design optimization","","","","50","19","","","","","","IEEE","IEEE Journals & Magazines"
"A survey on NSS adoption intention","J. Lim; B. Gan; Ting-Ting Chang","Sch. of Comput., Nat. Univ. of Singapore, Singapore; NA; NA","Proceedings of the 35th Annual Hawaii International Conference on System Sciences","","2002","","","399","408","This paper reports on a survey of Singapore companies' intentions to adopt negotiation support systems (NSSs). The data collected were tested based on two theoretical models: the theory of planned behavior (TPB) and the technology acceptance model (TAM). Preliminary findings showed that the TPB provides a better prediction of the intention to adopt a NSS than the TAM does, with subjective norm and perceived behavioral control being the significant determinants of intention. The implications of these results are discussed.","","0-7695-1435","10.1109/HICSS.2002.993888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=993888","","Decision support systems;Optimized production technology;Collaborative software;Information processing;Gallium nitride;Testing;Packaging;Management information systems;Psychology;Humans","negotiation support systems;reviews;planning;information technology;DP management;psychology","survey;Singapore companies;negotiation support systems;technology adoption intention;planned behavior;technology acceptance model;subjective norm;perceived behavioral control","","5","66","","","","","","IEEE","IEEE Conferences"
"Digital predistortion linearizes wireless power amplifiers","Wan-Jong Kim; S. P. Stapleton; Jong Heon Kim; C. Edelman","Simon Fraser Univ. in Burnaby, BC, Canada; Simon Fraser Univ. in Burnaby, BC, Canada; NA; NA","IEEE Microwave Magazine","","2005","6","3","54","61","When compared to other linearization methods, adaptive DP provides sufficient linearization with less complex RF hardware by depending primarily upon DSP rather than analog manipulation. The adaptive DP system may be implemented in EDA software along with interconnected test equipment (arbitrary RF signal source and vector signal analyzer). Through the use of a training signal, a simple adaptive algorithm may be employed to update the LUT coefficients until an optimum setting is achieved and used to predistort the input to the amplifier. This ""connected solution"" approach can provide key information to design engineers for optimizing the DSP architecture of a PA. For future work, other issues are taken into account, such as temperature and electrical memory effects.","1527-3342;1557-9581","","10.1109/MMW.2005.1511914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1511914","","Predistortion;Power amplifiers;Radiofrequency amplifiers;Digital signal processing;Radio frequency;Hardware;Adaptive systems;Electronic design automation and methodology;Power system interconnection;Test equipment","microwave power amplifiers;linearisation techniques;electronic design automation;signal processing;table lookup","wireless power amplifiers;digital predistortion;linearization method;adaptive DP;RF hardware;DSP;analog manipulation;DP system;EDA software;interconnected test equipment;RF signal source;vector signal analyzer;adaptive algorithm;LUT coefficient","","34","6","","","","","","IEEE","IEEE Journals & Magazines"
"Hardware-in-the-loop-simulation of a vehicle climate controller with a combined HVAC and passenger compartment model","D. Michalek; C. Gehsat; R. Trapp; T. Bertram","NA; NA; NA; NA","Proceedings, 2005 IEEE/ASME International Conference on Advanced Intelligent Mechatronics.","","2005","","","1065","1070","A good example for a complex mechatronic system is a modern vehicle. It covers a lot of subsystems which solve different tasks but are often coupled. The automatic air conditioning system is one of those subsystems. It consists, especially in vehicles of higher categories, of a high number of distributed mechatronic actuators and sensors and a central controller. The aim of such a system is to achieve a desired climatic condition in the vehicle interior which leads to a higher comfort for the passengers and therefore it also increases indirect the safety in road traffic. The climatic condition is defined by the interior temperature, its layering, the humidity and the velocity of the airflow blowing through the passenger compartment. For the adjustment of the desired interior climate with the available air conditioning components complex control algorithms are necessary. This control task is performed by the climate controller, which includes all control strategies in form of software programs. Beneath the construction of all system parts, a further challenge is the design of the control which includes the design of the controller hardware as well as the control strategies. By applying modern structured development methods for mechatronic systems like the modified V-model [VDI 2206, 2004] the development time and the costs can be reduced whereas the quality can be improved by using a model based approach. Fundamental part of this strategy are the X-in-the-loop-simulation (XIL-simulation) methods, where all system parts, which can be pure virtual in form of models or real elements, are considered in interaction. This is done for verification and validation of the system parts and the software. By using this technique in the concrete case of the climate controller, its hardware as well as the applied control strategies can be tested and optimized in conjunction with a real time computer that is connected to the controller via appropriate interfaces. Models running on the real time computer simulate the air conditioning system. Thus it is possible to close the control loops in a partially virtual system. In contrast to this approach, conventional development strategies, where most control loops can be considered for the first time at the assembled system, are very time consuming and cost intensive because real test drives have to be carried out for the verification and validation processes. This is especially essential for tests under different climatic conditions like summer and winter tests. In case of the software- and hardware-in-the-loop-simulation (SIL- and HIL-simulation) for an air conditioning controller, models of the heating, ventilation and air conditioning elements (HVAC) and the passenger compartment are necessary. A main demand for the HIL-Simulation is the real time capability. Therefore it is a challenge to create simple but accurate models, that enable a fast calculation of the wanted values. In this paper both models, the HVAC and the single zone passenger compartment model and their interfaces would be presented. Further results of both models interacting in a HIL-simulation on a commercial hardware platform connected via interfaces to the climate controller would be shown","2159-6247;2159-6255","0-7803-9047","10.1109/AIM.2005.1511151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1511151","","Air conditioning;Control systems;Mechatronics;Hardware;Testing;Automatic control;Vehicle safety;Road safety;Costs;Actuators","automotive engineering;control engineering computing;digital simulation;HVAC;road safety;road traffic;road vehicles","hardware-in-the-loop-simulation;vehicle climate controller;HVAC;passenger compartment model;automatic air conditioning system;road traffic;road safety;software-in-the-loop-simulation","","7","6","","","","","","IEEE","IEEE Conferences"
"Mixed. PTL/static logic synthesis using genetic algorithms for low-power applications","Geun Rae Cho; T. Chen","Dept. of Electr. & Comput. Eng., Colorado State Univ., Fort Collins, CO, USA; Dept. of Electr. & Comput. Eng., Colorado State Univ., Fort Collins, CO, USA","Proceedings International Symposium on Quality Electronic Design","","2002","","","458","463","We present a new mixed pass-transistor logic (PTL) and static CMOS logic synthesis method based on a genetic search. The proposed synthesis method first performs a search for possible matches between a logic structure and a set of predefined PTL/CMOS logic gates using BDDs. The unique contribution of our approach is the use of a genetic algorithm to determine the best mixture of PTL and static cells based on area and power. Our experimental results demonstrate that circuits synthesized using the proposed mixed PTL/CMOS synthesis method outperforms their static counterparts in delay or power consumption or both in a 0.25 /spl mu/m CMOS process. The average area, power consumption, and power-delay product of ISCAS85 and MCNC91 benchmark circuits using the proposed method are 25%, 40%, and 45% better than their static counterparts, respectively.","","0-7695-1561","10.1109/ISQED.2002.996788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996788","","Genetic algorithms;Circuit synthesis;CMOS logic circuits;Energy consumption;Logic functions;Application software;Data structures;Boolean functions;CMOS process;Very large scale integration","CMOS logic circuits;low-power electronics;genetic algorithms;binary decision diagrams;Boolean functions;logic CAD;circuit CAD;integrated circuit design;circuit optimisation;delays","mixed PTL/CMOS synthesis method;mixed PTL/static logic synthesis;pass-transistor logic;static CMOS logic;genetic search;logic structure;predefined logic gates;BDDs;genetic algorithm;static cells;delay;power consumption;power-delay product;0.25 micron","","1","20","","","","","","IEEE","IEEE Conferences"
"Adopting Cleanroom software engineering with a phased approach","P. A. Hausler; R. C. Linger; C. J. Trammell","IBM Corporation, 6710 Rockledge Drive, Bethesda, Maryland 20817, USA; IBM Corporation, 6710 Rockledge Drive, Bethesda, Maryland 20817, USA; Department of Computer Science, 107 Ayres Hall, University of Tennessee, Knoxville, Tennessee 37996, USA","IBM Systems Journal","","1994","33","1","89","109","Cleanroom software engineering is a theory-based, team-oriented engineering process for developing very high quality software under statistical quality control. The Cleanroom process combines formal methods of object-based box structure specification and design, function-theoretic correctness verification, and statistical usage testing for reliability certification to produce software approaching zero defects. Management of the Cleanroom process is based on a life cycle of development and certification of a pipeline of user-function increments that accumulate into the final product. Teams in IBM and other organizations that use the process are achieving remarkable quality results with high productivity. A phased implementation of the Cleanroom process enables quality and productivity improvements with an increased control of change. An introductory implementation involves the application of Cleanroom principles without the full formality of the process; full implementation involves the comprehensive use of formal Cleanroom methods; and advanced implementation optimizes the process through additional formal methods, reuse, and continual improvement. The AOEXPERT/MVS project, the largest IBM Cleanroom effort to date, successfully applied an introductory level of implementation. This paper presents both the implementation strategy and the project results.","0018-8670","","10.1147/sj.331.0089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387350","","","","","","12","","","","","","","IBM","IBM Journals & Magazines"
"A shape analysis model with applications to a character recognition system","J. Rocha; T. Pavlidis","Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA; Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA","[1992] Proceedings IEEE Workshop on Applications of Computer Vision","","1992","","","182","189","A method for the recognition of multifont printed characters is proposed, giving emphasis to the identification of structural descriptions of character shapes using prototypes. Noise and shape variations are modeled as series of transformations from groups of features in the data to features in each prototype. Thus, the method manages systematically the relative distortion between a candidate shape and its prototype, accomplishing robustness to noise with less than two prototypes per class, on the average. Our method uses a flexible matching between components and a flexible grouping of the individual components to be matched. A number of shape transformations are defined. Also, a measure of the amount of distortion that these transformations cause is given. The problem of classification of character shapes is defined as a problem of optimization among the possible transformations that map an input shape into prototypical shapes. Some tests with hand printed numerals confirmed the method's high robustness level.<<ETX>>","","0-8186-2840","10.1109/ACV.1992.240313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=240313","","Shape;Character recognition;Prototypes;Optical character recognition software;Noise robustness;Noise shaping;Working environment noise;Application software;Computer science;Multi-stage noise shaping","optical character recognition","shape analysis model;character recognition system;multifont printed characters;identification;structural descriptions;character shapes;shape variations;relative distortion;candidate shape;flexible grouping;shape transformations;classification;optimization;hand printed numerals;robustness","","4","17","","","","","","IEEE","IEEE Conferences"
"A static optimization approach to assess dynamic available transfer capability","E. De Tuglie; M. Dicorato; M. La Scala; P. Scarpellini","Poltecnico di Bari, Italy; NA; NA; NA","IEEE Transactions on Power Systems","","2000","15","3","1069","1076","This paper deals with the development of a nonlinear programming methodology for evaluating available transfer capability. The main feature of the approach is the capability to treat static and dynamic security constraints in a unique integrated piece of software. The algorithm has been implemented and tested on an actual power system.","0885-8950;1558-0679","","10.1109/59.871735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=871735","","Power system dynamics;Power system stability;Power system security;Power system transients;Dynamic programming;Nonlinear dynamical systems;Steady-state;Power system reliability;Dynamic voltage scaling;Power system modeling","power system security;power transmission;nonlinear programming;power system transient stability","static optimization;dynamic available transfer capability;nonlinear programming methodology;dynamic security constraints;static security constraints;transient stability","","52","17","","","","","","IEEE","IEEE Journals & Magazines"
"Fast decoding of tagged message formats","T. Blackwell","Div. of Appl. Sci., Harvard Univ., Cambridge, MA, USA","Proceedings of IEEE INFOCOM '96. Conference on Computer Communications","","1996","1","","224","231 vol.1","Many important protocols, such as Q.2931 or any protocol based on the ASN.1 basic encoding rules, are transmitted using tagged message formats, in which a message can be considered as a sequence of interleaved tag and data fields, where tag fields define the meaning of subsequent fields. These messages are computationally expensive to decode, partly because decoding each data field requires resting one or more tag fields. Evidence suggests that in some applications, although the potential space of message encodings may be very large, only a small number of message layouts are seen frequently, and thus some of the work required in decoding can be amortized over many messages. This paper analyzes the use of run-time code generation to generate optimized decoding instruction sequences for received messages matching previously observed layouts, and describes a prototype system that applies the techniques to decoding the Q.2931 and ASN.1 BER protocols. In the average case, substantial performance gains are seen.","0743-166X","0-8186-7293","10.1109/INFCOM.1996.497897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497897","","Decoding;Protocols;Encoding;Bit error rate;Runtime;Prototypes;Costs;Performance gain;Software testing;Software algorithms","protocols;decoding;asynchronous transfer mode","fast decoding;tagged message formats;protocols;Q.2931;ASN.1 basic encoding rules;message encodings;message layouts;run-time code generation;decoding instruction sequences;received message;BER protocols","","","34","","","","","","IEEE","IEEE Conferences"
"The role of practice in videodisc-based procedural instructions","P. Bagget","Sch. of Educ., Michigan Univ., Ann Arbor, MI, USA","IEEE Transactions on Systems, Man, and Cybernetics","","1988","18","4","487","496","The author reports a procedure for developing multimedia instructions that are 'optimized' according to certain criteria. She describes the design of interactive videodisc-based assembly instruction using IBM's Info Window system. Research comparing interactive videodisc-based assembly instructions and passive videotape instructions is presented. Performance in building from memory an 80-piece object is assessed for six different groups. The main comparison is between groups which have interactive instructions and are allowed to build during training, and groups that have the same instructions but are not followed to build during training. The 'build' groups never perform better from memory than the 'no build' groups (on structure or efficiency) and sometimes perform significantly worse. An explanation is offered using a framework for multimedia concepts in memory. It is concluded that, when practice is mixed with simultaneous audiovisual instruction, the motoric elements created by practice do not become an integrated part of the concept of building the object formed from the audiovisual information. Instead, motoric elements remain 'outside' the concept.<<ETX>>","0018-9472;2168-2909","","10.1109/21.17366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=17366","","Assembly systems;Buildings;Electric breakdown;Pulleys;Multimedia systems;Software systems;Software testing;Computer aided instruction;Image segmentation","educational aids;interactive systems;psychology;training;video and audio discs","video discs;video applications;educational aids;psychology;procedural instructions;multimedia instructions;interactive videodisc-based assembly instruction;Info Window system;training;audiovisual instruction;motoric elements","","5","12","","","","","","IEEE","IEEE Journals & Magazines"
"OCR parameters tuning by means of evolution strategies for aircraft's tail number recognition","A. Berlanga; J. Garcia-Herrero; J. M. Molina; J. Besada; J. Portillo","ETSI Telecomunicacion, Univ. Politecnica de Madrid, Spain; NA; NA; NA; NA","Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)","","2002","1","","902","907 vol.1","This paper describes the optimisation of some parameters of an optical character recognition system (OCR). The optimisation is performed by means of evolution strategies (ES) in order to maximize the pattern discrimination. The pattern set is a vectorial representation of the character set. The OCR is applied to identify the tail number of an aircraft moving on the airfield runway. The proposed approach is discussed together with some results obtained on a benchmark data set of aircraft tail numbers.","","0-7803-7282","10.1109/CEC.2002.1007045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007045","","Optical character recognition software;Aircraft;Tail;Telecommunications;Character recognition;Airports;Testing;Expert systems;Aerospace control;Surface morphology","optical character recognition;evolutionary computation;aerospace computing;aircraft","OCR;parameter tuning;evolution strategies;aircraft tail number recognition;optimisation;image-based aircraft identification system;optical character recognition system;pattern discrimination;vectorial representation;character set;benchmark data set","","","8","","","","","","IEEE","IEEE Conferences"
"Delay-Centric Link Quality Aware OLSR","Keun Jae Lee; Min Soo Kim; Song Yean Cho; Byung In Mun","Samsung Electronics Co., LTD; NA; NA; NA","The IEEE Conference on Local Computer Networks 30th Anniversary (LCN'05)l","","2005","","","690","696","This paper introduces a delay-centric link quality aware routing protocol, LQOLSR (link quality aware optimized link state routing). The LQOLSR chooses find fast and high quality routes in mobile ad hoc networks (MANET). LQOLSR predicts a packet transmission delay according to multiple transmission rates in IEEE 802.11 and selects the fastest route from source to destination by estimating relative transmission delay between nodes. We implement a LQOLSR protocol by modifying the basic OLSR (optimized link state routing) protocol. We evaluate and analyze the performance in a real testbed established in an office building","0742-1303","0-7695-2421","10.1109/LCN.2005.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550948","","Routing protocols;Delay estimation;Mobile ad hoc networks;Probes;Performance analysis;Testing;Buildings;Network topology;State estimation;Software quality","ad hoc networks;mobile radio;routing protocols;wireless LAN","delay-centric link quality aware routing protocol;mobile ad hoc networks;IEEE 802.11;relative transmission delay;optimized link state routing protocols","","","11","","","","","","IEEE","IEEE Conferences"
"Applications of square-root information filtering and smoothing in spacecraft orbit determination","Tseng-Chan Wang; J. B. Collier; J. E. Ekelund; P. J. Breckheimer","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA","Proceedings of the 27th IEEE Conference on Decision and Control","","1988","","","825","830 vol.1","The JPL (Jet Propulsion Laboratory) orbit determination software system is a set of computer programs developed for the primary purpose of determining the flight path of deep-space mission spacecraft in NASA's planetary program and highly elliptical orbiting spacecraft in Earth orbit. The filtering processes available within the JPL orbit determination software are discussed, and several examples are presented. In particular, solutions obtained by the square root information filter (SRIF) using Bierman's estimation subroutine library (ESL) are discussed and compared with the solutions obtained by the singular value decomposition (SVD) technique. It is concluded that the SRIF filtering and smoothing algorithms are efficient and numerically stable for well-conditioned systems. The use of Bierman's ESL simplifies the task of maintaining the orbit determination software by providing efficient, tested filtering tools. For solving a large well-conditioned system (rank >120), SRIF is approximately four times faster than SVD; however, for solving an ill-conditioned system, SVD is recommended.<<ETX>>","","","10.1109/CDC.1988.194427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=194427","","Information filtering;Smoothing methods;Information filters;Planetary orbits;Space vehicles;Application software;Propulsion;Laboratories;Software systems;Earth","aerospace computing;computerised navigation;filtering and prediction theory;space vehicles","Bierman;aerospace computing;square-root information filtering;spacecraft orbit determination;JPL;Jet Propulsion Laboratory;flight path;deep-space mission;NASA's planetary program;estimation subroutine library;singular value decomposition;smoothing algorithms","","2","12","","","","","","IEEE","IEEE Conferences"
"Multiple subclass pattern recognition: A maximin correlation approach","H. I. Avi-Itzhak; J. A. Van Mieghem; L. Rub","Canon Res. Centre America, Palo Alto, CA, USA; NA; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1995","17","4","418","431","This paper addresses a correlation based nearest neighbor pattern recognition problem where each class is given as a collection of subclass templates. The recognition is performed in two stages. In the first stage the class is determined. Templates for this stage are created using the subclass templates. Assignment into subclasses occurs in the second stage. This two stage approach may be used to accelerate template matching. In particular, the second stage may be omitted when only the class needs to be determined. The authors present a method for optimal aggregation of subclass templates into class templates. For each class, the new template is optimal in that it maximizes the worst case (i.e. minimum) correlation with its subclass templates. An algorithm which solves this maximin optimization problem is presented and its correctness is proved. In addition, test results are provided, indicating that the algorithm's execution time is polynomial in the number of subclass templates. The authors show tight bounds on the maximin correlation. The bounds are functions only of the number of original subclass templates and the minimum element in their correlation matrix. The algorithm is demonstrated on a multifont optical character recognition problem.<<ETX>>","0162-8828;2160-9292;1939-3539","","10.1109/34.385977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=385977","","Pattern recognition;Character recognition;Nearest neighbor searches;Optical character recognition software;Optical noise;Acceleration;Testing;Polynomials;Clustering algorithms;Minimax techniques","pattern recognition;minimax techniques;optical character recognition;correlation methods","multiple subclass pattern recognition;maximin correlation approach;template matching;maximin optimization problem;correctness;multifont optical character recognition problem","","15","27","","","","","","IEEE","IEEE Journals & Magazines"
"A min-max cut algorithm for graph partitioning and data clustering","C. H. Q. Ding; Xiaofeng He; Hongyuan Zha; Ming Gu; H. D. Simon","NERSC Div., Lawrence Berkeley Lab., CA, USA; NA; NA; NA; NA","Proceedings 2001 IEEE International Conference on Data Mining","","2001","","","107","114","An important application of graph partitioning is data clustering using a graph model - the pairwise similarities between all data objects form a weighted graph adjacency matrix that contains all necessary information for clustering. In this paper, we propose a new algorithm for graph partitioning with an objective function that follows the min-max clustering principle. The relaxed version of the optimization of the min-max cut objective function leads to the Fiedler vector in spectral graph partitioning. Theoretical analyses of min-max cut indicate that it leads to balanced partitions, and lower bounds are derived. The min-max cut algorithm is tested on newsgroup data sets and is found to out-perform other current popular partitioning/clustering methods. The linkage-based refinements to the algorithm further improve the quality of clustering substantially. We also demonstrate that a linearized search order based on linkage differential is better than that based on the Fiedler vector, providing another effective partitioning method.","","0-7695-1119","10.1109/ICDM.2001.989507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989507","","Clustering algorithms;Partitioning algorithms;Helium;Laboratories;Computer science;Mathematics;Application software;Mathematical model;Testing;Clustering methods","minimax techniques;graph theory;pattern clustering;data mining;vectors","min-max cut algorithm;graph partitioning;data clustering;graph model;data object pairwise similarities;weighted graph adjacency matrix;objective function;relaxed optimization;Fiedler vector;spectral graph partition;balanced partitions;lower bounds;newsgroup data sets;algorithm performance;linkage-based refinements;clustering quality;linearized search order;linkage differential","","226","24","","","","","","IEEE","IEEE Conferences"
"Hybrid real-coded genetic algorithms with female and male differentiation","C. Garcia-Martinez; M. Lozano","Comput. Sci. & AI Dept., Univ. of Granada, Spain; Comput. Sci. & AI Dept., Univ. of Granada, Spain","2005 IEEE Congress on Evolutionary Computation","","2005","1","","896","903 Vol.1","Parent-centric real-parameter crossover operators create the offspring in the neighborhood of one of the parents, the female parent, using a probability distribution. The other parent, the male one, defines the range of this probability distribution. The female and male differentiation process determines the individuals in the population that may become female or/and male parents. An important property of this process is that it makes possible the design of two kinds of real-coded genetic algorithms: ones that promote global search and ones that are effective local searchers. In this paper, we study the performance of a hybridization of these real-coded genetic algorithms when tackling the test problems proposed for the Special Session on Real-Parameter Optimization of the IEEE Congress on Evolutionary Computation 2005","1089-778X;1941-0026","0-7803-9363","10.1109/CEC.2005.1554778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554778","","Genetic algorithms;Probability distribution;Testing;Biological cells;Noise measurement;Computer science;Evolutionary computation;Algorithm design and analysis;Sampling methods;Application software","genetic algorithms;probability;search problems","hybrid real-coded genetic algorithm;female differentiation;male differentiation;parent-centric real-parameter crossover operator;probability distribution;search problem;optimization;evolutionary computation","","21","10","","","","","","IEEE","IEEE Conferences"
"Temperature considerations in solar arrays","Min-Jung Wu; E. J. Timpson; S. E. Watkins","Dept. of Electr. & Comput. Eng., Missouri Univ., Rolla, MO, USA; Dept. of Electr. & Comput. Eng., Missouri Univ., Rolla, MO, USA; Dept. of Electr. & Comput. Eng., Missouri Univ., Rolla, MO, USA","Region 5 Conference: Annual Technical and Leadership Workshop, 2004","","2004","","","1","9","Temperature is an important consideration in the operation of photovoltaic (PV) arrays. In particular, daily and seasonal temperature variations are a limitation on the application of solar power to homes. At lower temperatures, PV systems produce more power. For higher temperatures, optimum operation requires modification of electrical load and removal of excess heat. Several technologies and approaches are available. To pursue this system optimization, PV cells were investigated at different temperatures. These investigations are compared with simulated theoretical results to draw more specific conclusions that can be applied to a solar house. A temperature reduction of 60/spl deg/C improved the power by up to twenty-seven percent with the current test cell. The simulations matched this conclusion and can be applied to the PV array used on a house. The University of Missouri-Rolla (UMR) and Rolla Technical Institute (RTI) jointly built a solar house for the 2002 National Solar Decathlon Competition. This house is the motivation and testbed for our research. The first application is to cool the cell; then compare the additional amount of power produced with the amount of power required to cool the cell. The feasibility of cooling the array is discussed. This paper first gives a description of the UMR/RTI solar house, a literature review, and overview. Temperature-dependence theory and experiments is given next. The third portion shows simulations including current-voltage curves and an analysis of load lines and temperature. The direct application of this research to the solar house and proposals for design considerations are summarized.","","0-7803-8217","10.1109/REG5.2004.1300152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1300152","","Solar energy;Educational technology;Temperature dependence;Photovoltaic systems;Solar power generation;Resistance heating;Testing;Application software;Cooling;Analytical models","solar cell arrays;solar power;photovoltaic power systems;temperature distribution","photovoltaic arrays;solar arrays;temperature considerations;daily temperature variations;seasonal temperature variations;solar power;system optimization;solar house;cooling feasibility;current-voltage curves;load lines","","4","6","","","","","","IEEE","IEEE Conferences"
"Current results from a rover science data analysis system","R. Castano; M. Judd; T. Estlin; R. C. Anderson; D. Gaines; A. Castano; B. Bornstein; T. Stough; K. Wagstaff","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA","2005 IEEE Aerospace Conference","","2005","","","356","365","The Onboard Autonomous Science Investigation System (OASIS) evaluates geologic data gathered by a planetary rover. This analysis is used to prioritize the data for transmission, so that the data with the highest science value is transmitted to Earth. In addition, the onboard analysis results are used to identify science opportunities. A planning and scheduling component of the system enables the rover to take advantage of the identified science opportunity. OASIS is a NASA-funded research project that is currently being tested on the FIDO rover at JPL for use on future missions. In this paper, we provide a brief overview of the OASIS system, and then describe our recent successes in integrating with and using rover hardware. OASIS currently works in a closed loop fashion with onboard control software (e.g., navigation and vision) and has the ability to autonomously perform the following sequence of steps: analyze gray scale images to find rocks, extract the properties of the rocks, identify rocks of interest, retask the rover to take additional imagery of the identified target and then allow the rover to continue on its original mission. We also describe the early 2004 ground test validation of specific OASIS components on selected Mars exploration rover (MER) images. These components include the rock-finding algorithm, RockIT, and the rock size feature extraction code. Our team also developed the RockIT GUI, an interface that allows users to easily visualize and modify the rock-finder results. This interface has allowed us to conduct preliminary testing and validation of the rock-finder's performance.","1095-323X","0-7803-8870","10.1109/AERO.2005.1559328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559328","","Data analysis;Testing;Geology;Geoscience;Hardware;Software performance;Navigation;Image analysis;Image sequence analysis;Performance analysis","planetary rovers;feature extraction;Mars;geophysical signal processing;geophysical techniques;image processing;rocks;graphical user interfaces","rover science data analysis system;Onboard Autonomous Science Investigation System;OASIS;planetary rover;onboard analysis;NASA;FIDO rover;Mars exploration rover images;rock-finding algorithm;feature extraction code;RockIT GUI","","11","4","","","","","","IEEE","IEEE Conferences"
"Neural fuzzy agents that learn a user's preference map","S. Mitaim; B. Kosko","Signal & Image Process. Inst., Univ. of Southern California, Los Angeles, CA, USA; NA","Proceedings of ADL '97 Forum on Research and Technology. Advances in Digital Libraries","","1997","","","25","35","This paper models an intelligent agent that helps a user search or filter images in the database. Database search depends on the user's profile of likes and dislikes and how the user ranks similar images. A neural fuzzy system can help learn an agent profile of a user. The fuzzy system uses if-then rules that store and compress the agent's knowledge of the user's likes and dislikes. A neural system uses training data to form and tune the rules. The profile is a preference map or a bumpy utility surface over the space of search objects. Rules define fuzzy patches that cover the bumps as learning unfolds and as the fuzzy agent system gives a finer approximation of the profile. The agent system searches for preferred objects with the learned profile and a new fuzzy measure of similarity. We derive a new supervised learning law that tunes this matching measure with new sample data. Then we test the fuzzy agent profile system on object spaces of flowers and sunsets and test the fuzzy agent matching system on an object space of sunset images.","1092-9959","0-8186-8010","10.1109/ADL.1997.601197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601197","","Fuzzy systems;Fuzzy sets;Image databases;Filters;Intelligent agent;System testing;Signal processing;Image processing;Image coding;Training data","visual databases;software agents;learning (artificial intelligence);fuzzy neural nets;image matching","fuzzy agents;preference map;intelligent agent;fuzzy patches;bumpy utility surface;learned profile;finer approximation;matching measure;images","","","21","","","","","","IEEE","IEEE Conferences"
"A general prognostic tracking algorithm for predictive maintenance","D. C. Swanson","Appl. Res. Lab., Pennsylvania State Univ., University Park, PA, USA","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","6","","2971","2977 vol.6","Prognostic health management (PHIM) is a technology that uses objective measurements of condition and failure hazard to adaptively optimize a combination of availability, reliability, and total cost of ownership of a particular asset. Prognostic utility for the signature features are determined by transitional failure experiments. Such experiments provide evidence for the failure alert threshold and of the likely advance warning one can expect by tracking the feature(s) continuously. Kalman filters are used to track changes in features like vibration levels, mode frequencies, or other waveform signature features. This information is then functionally associated with load conditions using fuzzy logic and expert human knowledge of the physics and the underlying mechanical systems. Herein is the greatest challenge to engineering. However, it is straightforward to track the progress of relevant features over time using techniques such as Kalman filtering. Using the predicted states, one can then estimate the future failure hazard, probability of survival, and remaining useful life in an automated and objective methodology.","","0-7803-6599","10.1109/AERO.2001.931317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931317","","Prediction algorithms;Predictive maintenance;Hazards;Asset management;Technology management;Particle measurements;Availability;Cost function;Vibrations;Frequency","maintenance engineering;Kalman filters;tracking filters;diagnostic expert systems;diagnostic reasoning;machine testing;reliability;fault diagnosis;condition monitoring;computerised monitoring;fuzzy logic;software agents;feature extraction;sensor fusion","predictive maintenance;general prognostic tracking algorithm;prognostic health management;failure hazard;objective measurements;availability;reliability;total cost of ownership;signature features;failure alert threshold;Kalman filters;fuzzy logic;expert human knowledge;future failure hazard;probability of survival;reliability centered maintenance;automated logistics;machine faults;automated reasoning;intelligent agents;intelligent sensors","","42","7","","","","","","IEEE","IEEE Conferences"
"Query-driven petri net reduction for analysis in Ada tasking","S. Tu; Y. -. Wang; M. E. Mathews","Dept. of Comput. Sci., New Orleans Univ., LA, USA; Dept. of Comput. Sci., New Orleans Univ., LA, USA; Dept. of Comput. Sci., New Orleans Univ., LA, USA","Proceedings International Phoenix Conference on Computers and Communications","","1995","","","334","340","We have illustrated methods to address three types of problems in static analysis for Ada tasking: quantitative questions, safety problems and MAY-happen event problems. We have applied a two-phase methodology to automate analysis: first deriving a semantically rich model independent of any specific analysis issue, that is the original Ada nets, and then manipulating this model with algorithms that are designed for the specific analysis issue of concern. We call such a methodology the query-driven net reduction. The philosophy behind this methodology is that different analyses demand different aspects of information from the system. An optimized analysis model should only contain the necessary information. In addition to reachability graph generation, the linear algebraic method is also investigated as a follow-up analysis technique. Experiments show that the net reduction technique substantially enhances the analysis ability of both state space generation approaches and linear algebraic methods.<<ETX>>","","0-7803-2492","10.1109/PCCC.1995.472472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472472","","Safety;Algorithm design and analysis;System recovery;State-space methods;Information analysis;Computer industry;Concurrent computing;Computer science;Computer languages;Design methodology","Ada;Petri nets;program testing;reachability analysis;safety-critical software","Ada tasking;query-driven petri net reduction;quantitative questions;MAY-happen event problems;safety problems;two-phase methodology;semantically rich model;reachability graph generation;linear algebraic method;follow-up analysis technique;state space generation approaches","","","22","","","","","","IEEE","IEEE Conferences"
"A perspective on computational structures technology","A. K. Noor; S. L. Venneri","Virginia Univ., VA, USA; NA","Computer","","1993","26","10","38","46","Computational structures technology (CST), which blends structural modeling with computational methods and is an outgrowth of matrix methods used to analyze the dynamic and static responses of structures, is reviewed. The authors summarize CST's history and status, emphasizing software development, goals, and promising research areas. Example applications of CST from the aerospace field are presented.<<ETX>>","0018-9162;1558-0814","","10.1109/2.237442","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=237442","","Finite element methods;Space technology;Power engineering computing;Design optimization;Computational modeling;NASA;Design engineering;Predictive models;Vehicle crash testing;Aerospace engineering","structural engineering computing","computational structures technology;structural modeling;computational methods;matrix methods;software development;goals;research areas;aerospace","","3","10","","","","","","IEEE","IEEE Journals & Magazines"
"Performance measurement intrusion and perturbation analysis","A. D. Malony; D. A. Reed; H. A. G. Wijshoff","Dept. of Comput. & Inf. Sci., Oregon Univ., Eugene, OR, USA; NA; NA","IEEE Transactions on Parallel and Distributed Systems","","1992","3","4","433","450","The authors study the instrumentation perturbations of software event tracing on the Alliant FX/80 vector multiprocessor in sequential, vector, concurrent, and vector-concurrent modes. Based on experimental data, they derive a perturbation model that can approximate true performance from instrumented execution. They analyze the effects of instrumentation coverage, (i.e., the ratio of instrumented to executed statements), source level instrumentation, and hardware interactions. The results show that perturbations in execution times for complete trace instrumentations can exceed three orders of magnitude. With appropriate models of performance perturbation, these perturbations in execution time can be reduced to less than 20% while retaining the additional information from detailed traces. In general, it is concluded that it is possible to characterize perturbations through simple models. This permits more detailed, accurate instrumentation than traditionally believed possible.<<ETX>>","1045-9219;1558-2183;2161-9883","","10.1109/71.149962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=149962","","Performance analysis;Instruments;Hardware;Testing;Computer science;Physics;Optimizing compilers;Time measurement;Registers;Military computing","parallel programming;performance evaluation;perturbation theory","performance measurement;perturbation analysis;instrumentation perturbations;software event tracing;Alliant FX/80 vector multiprocessor;true performance;instrumented execution;instrumentation coverage;source level instrumentation;hardware interactions","","36","17","","","","","","IEEE","IEEE Journals & Magazines"
"Simulation of design dependent failure exposure levels for CMOS ICs","N. Kaul; B. L. Bhuva; V. Rangavajjhala; H. van der Molen; S. E. Kerns","Dept. of Electr. Eng., Vanderbilt Univ., Nashville, TN, USA; Dept. of Electr. Eng., Vanderbilt Univ., Nashville, TN, USA; Dept. of Electr. Eng., Vanderbilt Univ., Nashville, TN, USA; Dept. of Electr. Eng., Vanderbilt Univ., Nashville, TN, USA; Dept. of Electr. Eng., Vanderbilt Univ., Nashville, TN, USA","IEEE Transactions on Nuclear Science","","1990","37","6","2097","2103","The effects of design and bias on the radiation tolerance of ICs are studied, and an automated design tool is described that produces different designs for a logic function and presents important parameters of each design to a circuit designer for tradeoff analysis. It was shown by simulation and experimentally verified that the logic implementation of a circuit and the bias applied during irradiation are significant in determining the radiation tolerance of ICs. The software package aids designers in designing radiation-hard integrated circuits.<<ETX>>","0018-9499;1558-1578","","10.1109/23.101235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=101235","","Delay;Circuit testing;Logic devices;Degradation;Voltage;Leakage current;Failure analysis;Radiation hardening;Logic circuits;Design optimization","circuit layout CAD;CMOS integrated circuits;integrated circuit technology;integrated logic circuits;logic CAD;radiation hardening (electronics)","bias effects;logic ICs;design software package;radiation-hard integrated circuits design;design dependent failure exposure levels;CMOS ICs;effects of design;radiation tolerance;automated design tool;tradeoff analysis;logic implementation","","5","7","","","","","","IEEE","IEEE Journals & Magazines"
"Java virtual-machine support for portable worst-case execution-time analysis","L. Bate; G. Bernat; P. Puschner","Dept. of Comput. Sci., York Univ., UK; Dept. of Comput. Sci., York Univ., UK; NA","Proceedings Fifth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing. ISIRC 2002","","2002","","","83","90","The current trend towards the usage of Java in real-time, supported by two specifications (Real-Time Java and Real-Time Core extensions for the Java platform) requires adequate schedulability analysis, and consequently, worst case execution time (WCET) analysis techniques for the Java platform. This paper proposes a framework for providing portable WCET analysis for the Java platform. Portability means that the analysis is language and hardware independent. It is achieved by, separating the WCET analysis process in three stages and by analysing the Java byte code, not the high-level source code, thus enabling the analysis of programs written in other languages (such as Ada and compiled for the Java virtual machine). The three stages are: a Java virtual machine platform dependent (low-level) analysis, a software dependent (high-level) analysis and an on-line integration step.","","0-7695-1558","10.1109/ISORC.2002.1003664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003664","","Java;Timing;Testing;Virtual machining;Virtual manufacturing;Computer science;Real time systems;Processor scheduling;Hardware;Optimizing compilers","Java;real-time systems;processor scheduling","Java virtual machine support;portable worst-case execution-time analysis;Real-Time Java;Real-Time Core extensions;schedulability analysis;portability;Java byte code;software dependent analysis;on-line integration step","","8","9","","","","","","IEEE","IEEE Conferences"
"Applications of robust failure detection algorithms to power systems","K. J. Drake; S. L. Campbell; I. V. Andjelkovic; B. L. Hannas; K. A. Sweetingham","Carderock Div., Naval Surface Warfare Center, Philadelphia, PA, USA; NA; NA; NA; NA","Proceedings of the 13th International Conference on, Intelligent Systems Application to Power Systems","","2005","","","6 pp.","","As modelling and simulation become increasingly popular in the design process and as an alternative to expensive testing, fault detection methods based on model identification algorithms become more reliable as well as less expensive and easier to implement. In this paper we discuss the application of two active fault detection algorithms based on model identification to power systems. The algorithms are similar in theory though differ in implementation. The first is a direct optimization approach that handles more general systems and more varied constraints. It requires more sophisticated software but it's easily adapted to more than two models. The second algorithm is a constrained control approach that can be implemented on common math software, such as Matlab or Scilab, and handles model uncertainty. In both cases, the algorithms are free of false alarms depending upon the quality of the models used","","1-59975-174","10.1109/ISAP.2005.1599275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1599275","","Robustness;Detection algorithms;Power systems;Power system modeling;Mathematical model;Power system reliability;Power system faults;Power system simulation;Electrical fault detection;Fault diagnosis","fault diagnosis;optimisation;power system identification;power system reliability","fault detection;model identification;turbine model;robust failure detection;power systems;direct optimization;constrained control;model uncertainty handling","","1","8","","","","","","IEEE","IEEE Conferences"
"Streaming real-time audio and video data with transformation-based error concealment and reconstruction","B. W. Wah; Dong Lin; Xiao Su","Dept. of Electr. & Comput. Eng., Illinois Univ., Urbana, IL, USA; NA; NA","Proceedings of the First International Conference on Web Information Systems Engineering","","2000","1","","2","11 vol.1","One fundamental problem with streaming audio and video data over unreliable IP networks is that packets may be dropped or arrive too late for playback. Traditional error control schemes are not attractive because they either add redundant information that may worsen network traffic, or rely solely on the inadequate capability of the decoder to do error concealment. The authors propose a simple yet efficient transformation based algorithm in order to conceal network losses in streaming real time audio and video data over the Internet. In the receiver side, we adopt a simple reconstruction algorithm based on interpolation, as sophisticated concealment techniques cannot be employed in software based real time playback. In the sender side, we design a linear transformation with the objective of minimizing the mean squared error, assuming that some of the descriptions are lost and that the missing information is reconstructed by simple averaging at the destination. We further integrate the transformations in case of video streaming in the discrete cosine transform (DCT) to produce an optimized reconstruction based DCT. Experimental results show that our proposed algorithm performs well in real Internet tests.","","0-7695-0577","10.1109/WISE.2000.882369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=882369","","Streaming media;Discrete cosine transforms;IP networks;Error correction;Computer errors;Communication system traffic control;Decoding;Reconstruction algorithms;Interpolation;Performance evaluation","audio-visual systems;multimedia communication;packet switching;real-time systems;Internet;mean square error methods;discrete cosine transforms","real time audio/video data streaming;transformation based error concealment;unreliable IP networks;error control schemes;redundant information;network traffic;decoder;transformation based algorithm;network losses;Internet;receiver side;reconstruction algorithm;interpolation;software based real time playback;linear transformation;mean squared error;missing information;simple averaging;video streaming;discrete cosine transform;optimized reconstruction based DCT;real Internet tests","","2","16","","","","","","IEEE","IEEE Conferences"
"Integrated Product Policy and distributed supplier structures: SME and sound LCA data in conflict","F. Mandorli; M. Germani; H. E. Otto","Department of Mechanical Engineering, The University of Ancona, Via Brecce Bianche, Ancona I-60131 ITALY, f.mandorli@univpm.it; NA; NA","2005 4th International Symposium on Environmentally Conscious Design and Inverse Manufacturing","","2005","","","430","437","The sustainable development of our societies is one of the priorities of the European Commission. Through its new Integrated Product Policy (IPP), the European Commission is developing a series of measures that influence the supply and demand of environmentally sound products. Some IPP tools are based on product and process self-declarations, while others require the performance of a Life-Cycle Assessment (LCA). Life-Cycle Inventory (LCI) data availability is the fundamental premise in order to be able to perform an LCA. In this paper, the work is to investigate the diffusion of required LCA data along the supplier chain with the aim of identifying strategies to increase the awareness of Small and Medium Enterprises (SMEs) in respect to LCA, to suggest methodologies to facilitate the collection of sound LCI data and to test available low-cost software tools to support LCA, with particular reference to the production phase is reported","","1-4244-0081","10.1109/ECODIM.2005.1619261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1619261","Environmental Product Declaration;Life-Cycle Assessment data;Small and Medium Enterprise","Production;Acoustical engineering;Data engineering;Certification;Availability;Sustainable development;Environmental economics;ISO;Supply and demand;Supply chains","legislation;product development;product life cycle management;small-to-medium enterprises;societies;software tools;supply and demand;supply chains;sustainable development","integrated product policy tools;life-cycle assessment data;small-to-medium enterprises;sustainable development;societies;European Commission;supply and demand;environmental products;life-cycle inventory data;supply chain;software tools","","","8","","","","","","IEEE","IEEE Conferences"
"Coarse-grain parallel genetic algorithms: categorization and new approach","Shyh-Chang Lin; W. F. Punch; E. D. Goodman","Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA","Proceedings of 1994 6th IEEE Symposium on Parallel and Distributed Processing","","1994","","","28","37","This paper describes a number of different coarse-grain GA's, including various migration strategies and connectivity schemes to address the premature convergence problem. These approaches are evaluated on a graph partitioning problem. Our experiments showed, first, that the sequential GA's used are not as effective as parallel GA's for this graph partition problem. Second, for coarse-grain GA's, the results indicate that using a large number of nodes and exchanging individuals asynchronously among them is very effective. Third, GA's that exchange solutions based on population similarity instead of a fixed connection topology get better results without any degradation in speed. Finally, we propose a new coarse-grained GA architecture, the Injection Island GA (iiGA). The preliminary results of iiGA's show them to be a promising new approach to coarse-grain GA's.<<ETX>>","","0-8186-6427","10.1109/SPDP.1994.346184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=346184","","Genetic algorithms;Convergence;Genetic mutations;Adaptive systems;Circuit testing;Application software;Computer science;Topology;Optimization methods;Image recognition","genetic algorithms;parallel algorithms","coarse-grain parallel genetic algorithms;categorization;migration strategies;connectivity schemes;premature convergence problem;graph partitioning problem;connection topology;Injection Island","","28","27","","","","","","IEEE","IEEE Conferences"
"A Practical Environment for Scientific Programming","Carle; Cooper; Hood; Kennedy; Torczon; Warren","Rice University; NA; NA; NA; NA; NA","Computer","","1987","20","11","75","89","","0018-9162;1558-0814","","10.1109/MC.1987.1663418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1663418","","Algorithms;Atmospheric modeling;Programming profession;Optimizing compilers;Software maintenance;Testing;Monitoring;Hydrocarbon reservoirs;Weather forecasting;Ground support","","","","21","9","","","","","","IEEE","IEEE Journals & Magazines"
"Model-checking of correctness conditions for concurrent objects","R. Alur; K. McMillan; D. Peled","AT&T Bell Labs., USA; NA; NA","Proceedings 11th Annual IEEE Symposium on Logic in Computer Science","","1996","","","219","228","The notions of serializability, linearizability and sequential consistency are used in the specification of concurrent systems. We show that the model checking problem for each of these properties can be cast in terms of the containment of one regular language in another regular language shuffled using a semi-commutative alphabet. The three model checking problems are shown to be, respectively, in PSPACE, in EXPSPACE, and undecidable.","1043-6871","0-8186-7463","10.1109/LICS.1996.561322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=561322","","Protocols;History;Transaction databases;Application software;Computer bugs;Design optimization;Delay;System recovery;Testing;Automata","decidability","concurrent objects;serializability;linearizability;specification;model checking problem;regular language;semi-commutative alphabet;PSPACE;EXPSPACE;undecidable","","13","10","","","","","","IEEE","IEEE Conferences"
"Traveling through Dakota: experiences with an object-oriented program analysis system","M. Hind; A. Pioli","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA","Proceedings. 34th International Conference on Technology of Object-Oriented Languages and Systems - TOOLS 34","","2000","","","49","60","The paper describes experiences with the design and implementation of the NPIC program analysis system. We describe how the object oriented design of the intermediate representation (Dakota) provides front end and analysis independence using the abstract factory pattern, and illustrate how using multiple inheritance allows it to be extended to support program analysis. We also describe how the intermediate representation can be serialized to and from a file. The techniques described in the article provide useful insight into the construction of an object oriented program analysis system.","1530-2067","0-7695-0774","10.1109/TOOLS.2000.868958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868958","","Pattern analysis;Performance analysis;Flow graphs;Production facilities;Runtime;Optimizing compilers;Software testing;Buildings;Prototypes;Program processors","object-oriented programming;inheritance;data structures;data flow analysis;program control structures","object oriented program analysis system;NPIC program analysis system;object oriented design;intermediate representation;analysis independence;abstract factory pattern;multiple inheritance;program analysis;C++ program;Dakota","","","30","","","","","","IEEE","IEEE Conferences"
"Partitioning using second-order information and stochastic-gain functions","S. Dutt; H. Arslan; H. Theny","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1999","18","4","421","435","A probability-based partitioning algorithm, PROP, was introduced in [8] that achieved large improvements over traditional ""deterministic"" iterative-improvement techniques like Fidducia-Mattheyses (FM) and Krishnamurthy's look-ahead (LA) algorithm. While PROP's gain function has a greater futuristic component than PM or LA, it incorporates spatially local information-only information on the removal probabilities of adjacent nets of a cell is used in its gain computation. This prevents a higher-level view of nonlocal structures. Also, giving uniform weights to all nets, results in an inability to differentiate between the futuristic benefit of removing one net from another. This paper investigates for the first time the issues of using nonlocal structural information in gain functions and variable net weights based on the futuristic (stochastic) benefit of moving them from the cutset. The result is a more sophisticated partitioner DEEP-PROP that performs better for circuits with large complexities by incorporating more nonlocal (second order) structural information than PROP. The second-order information is incorporated into cell gains as well as variable net weights-the latter helps to focus future cell moves in the ""right"" cluster around the currently moved cell and, thus, better utilizes the information that led to its selection as the best move. A lower complexity version, variable weight PROP (VAR-PROP), that also uses dynamically assigned variable net weights, but based on first-order information, has also been developed. Both versions yield significant improvements over PROP on the ACM/SIGDA benchmark suite. DEEP-PROP yields mincut improvements of as much as 39% and an average of 20% for large circuits (10-K to 25-K cells) and an average of 14% over all circuits. DEEP PROP is about a factor of 2.8 times slower than PROP, which is very fast. VAR-PROP, which has a much lower computational complexity than DEEP-PROP, yields for large circuits, maximum and average mincut improvements over PROP of 27% and 18%, respectively, and an average of 12.6% improvement over all circuits. It is only about 14% slower than PROP, For the only very large circuit golem3 in the suite (>100 K cells), the improvements produced by DEEP-PROP and VAR-PROP over PROP are 15.6% and 11.5%, respectively. We also compare DEEP-PROP to FM, PROP and hMetis for a subset of the newer 1SPD98 benchmark circuits, and demonstrate significant improvements over FM and PROP, and comparable mincuts (within 2%) to hMetis, one of the best multilevel partitioners.","0278-0070;1937-4151","","10.1109/43.752926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=752926","","Partitioning algorithms;Iterative algorithms;Very large scale integration;Circuit testing;Gain;Stochastic processes;Computational complexity;Large scale integration;Application software;Optimization methods","VLSI;integrated circuit layout;circuit layout CAD;logic partitioning;iterative methods;computational complexity;stochastic processes","second-order information;stochastic-gain functions;probability-based partitioning algorithm;PROP;spatially local information;removal probabilities;adjacent nets;gain computation;nonlocal structures;uniform weights;gain functions;variable net weights;dynamically assigned variable net weights;ACM/SIGDA benchmark suite;computational complexity;VAR-PROP;mincut improvements;ISPD98 benchmark circuits;multilevel partitioners;VLSI","","1","17","","","","","","IEEE","IEEE Journals & Magazines"
"Concealment of whole-frame losses for wireless low bit-rate video based on multiframe optical flow estimation","S. Belfiore; M. Grangetto; E. Magli; G. Olmo","Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy; Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy; Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy; Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy","IEEE Transactions on Multimedia","","2005","7","2","316","329","In low bit-rate packet-based video communications, video frames may have very small size, so that each frame fills the payload of a single network packet; thus, packet losses correspond to whole-frame losses, to which the existing error concealment algorithms are badly suited and generally not applicable. In this paper, we deal with the problem of concealment of whole frame-losses, and propose a novel technique which is capable of handling this very critical case. The proposed technique presents other two major innovations with respect to the state-of-the-art: i) it is based on optical flow estimation applied to error concealment and ii) it performs multiframe estimation, thus optimally exploiting the multiple reference frame buffer featured by the most modern video coders such as H.263+ and H.264. If data partitioning is employed, by e.g., sending headers, motion vectors, and coding modes in prioritized packets as can be done in the DiffServ network model, the algorithm is capable of exploiting the motion vectors to improve the error concealment results. The algorithm has been embedded in the H.264 test model software, and tested under both independent and correlated packet loss models with parameters typical of the wireless environment. Results show that the proposed algorithm significantly outperforms other techniques by several dBs in peak signal-to-noise ratio (PSNR), provides good visual quality, and has a rather low complexity, which makes it possible to perform real-time operation with reasonable computational resources.","1520-9210;1941-0077","","10.1109/TMM.2005.843347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407904","Optical flow estimation;packet-based video transmission;video error concealment;whole-frame losses;wireless communications","Optical losses;Image motion analysis;Optical buffering;State estimation;Partitioning algorithms;Software testing;PSNR;Payloads;Technological innovation;Estimation error","video coding;video streaming;video databases;DiffServ networks;image sequences;wireless LAN;packet switching;error statistics","multiframe optical flow estimation;error concealment algorithms;multiple reference frame buffer;video coders;data partition;DiffServ network model;packet loss models;wireless communications;signal-to-noise ratio;visual quality;packet-based video transmission;video error concealment;whole-frame losses","","66","37","","","","","","IEEE","IEEE Journals & Magazines"
"A real experience on configuring a wafer scale 2D array of monobit processors","A. Boubekeur; J. L. Patry; M. Slimane-Kadi; G. Saucier; J. Trilhe","Inst. Nat. Polytech. de Grenoble, France; Inst. Nat. Polytech. de Grenoble, France; Inst. Nat. Polytech. de Grenoble, France; Inst. Nat. Polytech. de Grenoble, France; NA","IEEE Transactions on Components, Hybrids, and Manufacturing Technology","","1993","16","7","637","645","Presents hardware and software techniques for configuring a wafer scale 2D array. A switching network independent of the processing elements (PEs) has been designed and implemented. Two algorithms find and program an optimized target array in a reversible or irreversible manner. This paper is based on a wafer scale design for low-level image processing.<<ETX>>","0148-6411;1558-3082","","10.1109/33.257871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257871","","Switches;Registers;Wafer scale integration;Testing;Image processing;Multiplexing;Hardware;Proposals;Pixel","digital signal processing chips;image processing equipment;parallel architectures;VLSI","wafer scale 2D array;monobit processors;hardware techniques;software techniques;switching network;processing elements;optimized target array;reversible manner;irreversible manner;low-level image processing","","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic locomotion pattern generation for modular robots","A. Kamimura; H. Kurokawa; E. Toshida; K. Tomita; S. Murata; S. Kokaji","Nat. Inst. of Adv. Ind. Sci. & Technol., Ibaraki, Japan; Nat. Inst. of Adv. Ind. Sci. & Technol., Ibaraki, Japan; Nat. Inst. of Adv. Ind. Sci. & Technol., Ibaraki, Japan; Nat. Inst. of Adv. Ind. Sci. & Technol., Ibaraki, Japan; NA; NA","2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)","","2003","1","","714","720 vol.1","Locomotion, one of the most basic robotic functions, has been widely studied for several types of robots. As for self-reconfigurable modular robots, there are two types of locomotion; one type is realized as a series of self-reconfiguration and the other is realized as a whole body motion such as walking and crawling. Even for the latter type of locomotion, designing control method is more difficult than ordinary robots. This is because the module configuration includes many degrees of freedom and there are a wide variety of possible configurations. We propose an offline method to generate a locomotion pattern automatically for a modular robot in an arbitrary module configuration, which utilizes a neural oscillator as a controller of the joint motor and evolutionary computation method for optimization of the neural oscillator network, which determines the performance of locomotion. We confirm the validity of the method by software simulation and hardware experiments.","1050-4729","0-7803-7736","10.1109/ROBOT.2003.1241678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241678","","Robotics and automation;Legged locomotion;Oscillators;Design methodology;Automatic control;Automatic generation control;Evolutionary computation;Optimization methods;Computational modeling;Hardware","automatic test pattern generation;legged locomotion;neurocontrollers;oscillators;control system synthesis","automatic locomotion pattern generation;modular robot;walking;crawling;designing control method;degrees of freedom;offline method;arbitrary module configuration;neural oscillator controller;joint motor;evolutionary computation method;neural oscillator network","","32","26","","","","","","IEEE","IEEE Conferences"
"How to get an optimal neural network for a given application","R. L. Smeliansky; N. A. Umnov","Dept. of Comput. Sci., Moscow State Univ., Russia; Dept. of Comput. Sci., Moscow State Univ., Russia","The Second International Symposium on Neuroinformatics and Neurocomputers","","1995","","","329","336","The paper presents a problem of optimal neural network (NN) design for a given application. The application is described as a mapping R/sup N//spl rarr/R. The design process consists of two stages: selection of NN models which are able to approximate the given application, and an optimisation criterion estimation for each selected model with the further choosing of the best NN. The structure of an instrumental system for optimal NN design is proposed.","","0-7803-2512","10.1109/ISNINC.1995.480877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=480877","","Neural networks;Neurons;Application software;Process design;Instruments;Cellular neural networks;Transfer functions;State-space methods;Testing;Computer networks","neural nets;graph theory;learning (artificial intelligence);minimisation","optimal neural network;design process;optimisation criterion estimation","","","12","","","","","","IEEE","IEEE Conferences"
"Tools to speed FPGA development","B. K. Fawcett","Xilinx Inc., San Jose, CA, USA","IEEE Spectrum","","1994","31","11","88","94","Users of field-programmable gate arrays (FPGA) can now find easy-to-use development tools with powerful verification and optimization capabilities. The author discusses the features of these design tools. The author discusses logic synthesis and instantiation. The way in which such tools are used is also discussed.<<ETX>>","0018-9235;1939-9340","","10.1109/6.328732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=328732","","Field programmable gate arrays;Logic design;Electronic design automation and methodology;Logic arrays;Programmable logic arrays;Logic functions;Software tools;System testing;Graphics;Libraries","field programmable gate arrays;logic arrays;logic design;electronic engineering computing","field-programmable gate arrays;verification capability;optimization capability;design tools;logic synthesis;instantiation","","7","","","","","","","IEEE","IEEE Journals & Magazines"
"C++ exception handling","C. de Dinechin","Hewlett-Packard Co., Cupertino, CA, USA","IEEE Concurrency","","2000","8","4","72","79","The author describes how Hewlett-Packard implemented C++ exception handling in its IA-64 processor architecture. Researchers at the IA-64 Foundation Laboratory studied the performance impact of various solutions before finding a solution that leaves room for future optimizations.","1092-3063;1558-0849","","10.1109/4434.895109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895109","","Computer languages;Robustness;Software performance;Automatic control;Velocity measurement;Size measurement;Degradation;Testing;Resource management","C++ language;exception handling;Hewlett Packard computers;computer architecture;performance evaluation;microprocessor chips","C++ exception handling;Hewlett-Packard IA-64 processor architecture;performance impact;future optimizations;operating systems","","3","7","","","","","","IEEE","IEEE Journals & Magazines"
"Coordination in markets with nonconvexities as a mathematical program with equilibrium constraints-Part II: case studies","A. L. Motto; F. D. Galiana","Dept. of Electr. & Comput. Eng., McGill Univ., Montreal, Canada; Dept. of Electr. & Comput. Eng., McGill Univ., Montreal, Canada","IEEE Transactions on Power Systems","","2004","19","1","317","324","This paper is the second of a two-paper series. It is concerned with the numerical study of the solution procedure derived in to solve the coordination problem that arises in a new equilibrium model , which for the purpose of this presentation applies to a static (no-time coupling costs or constraints) electricity pool market with price inelastic demand and no network. The new equilibrium model has the following main properties: i) every scheduled generator satisfies its minimum surplus (or bid profit) condition; ii) the energy price is a system marginal cost (a Lagrange multiplier associated with the power balance constraint in the related economic dispatch problem where all of the discrete variables are fixed to their optimal values); iii) the power balance and all of the generators' technical constraints are satisfied. We present some numerical results based on three test systems: a simple three-generating unit system that can be solved by hand, a 32-generating unit system that consists of piecewise linear offer curves, and a large system of 768 generating units with monotone and nonmonotone, piecewise linear offer curves, some of which are set as must-run units. The results demonstrate that the proposed procedure is more efficient than a heuristic approach, both in terms of solution quality and computational efficiency.","0885-8950;1558-0679","","10.1109/TPWRS.2003.820709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1266584","","Computer aided software engineering;Production;Power generation;Power generation economics;Power system modeling;Lagrangian functions;Piecewise linear techniques;Cost function;Power system economics;System testing","power markets;power generation economics;power generation dispatch;piecewise linear techniques;duality (mathematics);integer programming;mathematical programming","market coordination;nonconvexities;static electricity pool markets;no-time coupling costs;price inelastic demand;scheduled generator;bid profit;energy price;Lagrange multiplier;power balance constraint;economic dispatch problem;discrete variables;three-generating unit system;nonmonotone piecewise linear offer curves;computation al efficiency;duality gap;integer programming;mathematical program;equilibrium constraints;optimization methods;power generation economics;power generation scheduling;incremental costs;must-run units","","10","23","","","","","","IEEE","IEEE Journals & Magazines"
"Static-dynamic mapping in heterogeneous computing environment","G. Martinovic; L. Budin; Z. Hocenski","Fac. of Electr. Eng., Univ. of Osijek, Croatia; NA; NA","IEEE International Symposium on Virtual Environments, Human-Computer Interfaces and Measurement Systems, 2003. VECIMS '03. 2003","","2003","","","32","37","Appropriate mapping procedures make heterogeneous computing environment especially suitable for the execution of various computer-intensive applications. The paper shows a static dynamic mapping procedure relying on the multicriteria model. Model parameters describe the application, platform, mapping and user. They are divided into static and dynamic ones with three weight levels assigned to them. The described approach ensures optimal initial mapping, but also a quick response to changes in the environment, which is supported by prediction. The described approach is a simplified environment shows improvements in the execution of the test application. Further adjustments are directed towards the implementation in the computational grid.","","0-7803-7785","10.1109/VECIMS.2003.1227026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227026","","Grid computing;Distributed computing;Application software;Processor scheduling;Computer applications;Resource management;Predictive models;Job shop scheduling;Testing;Hardware","distributed processing;resource allocation;programming environments;dynamic programming","static-dynamic mapping;heterogeneous computing environment;mapping procedure;application execution;computer-intensive application;multicriteria model;model parameter;optimal initial mapping;prediction;computational grid;high performance machine;task processing;resource allocation;NP-hard problem;combinatorial optimization;distributed processing;time variability","","1","9","","","","","","IEEE","IEEE Conferences"
"A technique of linear system identification using correlating filters","W. Lichtenberger","University of Illinois, Urbana, IL, USA","IRE Transactions on Automatic Control","","1961","6","2","183","199","A technique for measuring the impulse response of linear processes while they are on line is described. Such an identification of process dynamics is necessary in process-adaptive control systems. A testing signal and correlating filter are employed after the manner of Turin. Such a procedure requires no multiplier, and the output of the filter is the impulse response as a continuous function of real time. To reduce accompanying output noise, the method of adding coherently the results of a number of tests made in succession is proposed. This idea is applied to the measurement of a member of an ensemble of slowly varying impulse responses. Optimum design of both the correlating filter and the necessary test signal is determined on the basis of minimum mean-square error of the resulting estimate. The optimization of the number of tests to be included in a measurement is described. The general results are applied to the case of a single, slowly time-varying process. In addition to optimum design, normalized curves showing the optimum number of tests for a particular mode of variation are included. A second application is made to the problem of measuring a member of an ensemble of fixed processes. The results of a digital computer simulation of this case are given.","0096-199X;1558-3651","","10.1109/TAC.1961.1105194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1105194","","Linear systems;Nonlinear filters;Testing;Process control;Control systems;Noise reduction;Signal design;Application software;Electronic switching systems;Computer simulation","","","","7","9","","","","","","IEEE","IEEE Journals & Magazines"
"NUMA-aware Java heaps for server applications","M. M. Tikir; J. K. Hollingsworth","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","19th IEEE International Parallel and Distributed Processing Symposium","","2005","","","10 pp.","","We introduce a set of techniques to both measure and optimize memory access locality of Java applications running on cc-NUMA servers. These techniques work at the object level and use information gathered from embedded hardware performance monitors. We propose a new NUMA-aware Java heap layout. In addition, we propose using dynamic object migration during garbage collection to move objects local to the processors accessing them most. Our optimization technique reduced the number of non-local memory accesses in Java workloads generated from actual runs of the SPECjbb2000 benchmark by up to 41%, and also resulted in 40% reduction in workload execution time.","1530-2075","0-7695-2312","10.1109/IPDPS.2005.299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419934","","Java;Hardware;Sun;Fires;Application software;File servers;Delay;Distributed computing;Computer science;Educational institutions","Java;storage management;file servers;embedded systems;benchmark testing","Java heap layout;cc-NUMA servers;embedded hardware performance monitors;dynamic object migration;garbage collection;SPECjbb2000 benchmark","","8","23","","","","","","IEEE","IEEE Conferences"
"Sensor and data fusion design and evaluation with a virtual environment simulator","K. A. Redmill; J. I. Martin; U. Ozguner; K. Tamura","Dept. of Electr. Eng., Ohio State Univ., Columbus, OH, USA; NA; NA; NA","Proceedings of the IEEE Intelligent Vehicles Symposium 2000 (Cat. No.00TH8511)","","2000","","","668","674","A virtual environment simulation is described, which is useful for the evaluation and testing of on-vehicle ITS sensors and control suites. It is capable of 3D environment simulation and scene generation for use as input to image processing systems, the simulation of complex six degree of freedom (X, Y, Z, roll, pitch, yaw) vehicle models, the generation of ground truth and simulated sensor outputs (i.e., inertial sensors, GPS, vehicle speed), and closed loop testing. The simulator can accept road geometry and terrain data as derived from GIS databases. It is implemented as a modular and extensible software simulation environment using distributed, multiprocessing, networked software modules. Use of the simulator is illustrated by the parameter optimization and evaluation of a sensor fusion algorithm involving GPS, inertial sensors, and a map database.","","0-7803-6363","10.1109/IVS.2000.898425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898425","","Sensor fusion;Land vehicles;Road vehicles;Image sensors;Sensor systems;Global Positioning System;Spatial databases;Virtual environment;Testing;Sensor phenomena and characterization","traffic engineering computing;virtual reality;digital simulation;sensor fusion;distributed processing;road traffic;geographic information systems;Global Positioning System;visual databases","virtual environment simulator;data fusion;ITS sensors;scene generation;closed loop testing;road geometry;terrain data;GIS databases;distributed processing;GPS;inertial sensors;map database","","5","2","","","","","","IEEE","IEEE Conferences"
"PANDDA: a program for advanced network design: digital and analogue","R. K. Henderson; Li Ping; J. I. Sewell","Dept. of Electron. & Electr. Eng., Glasgow Univ., UK; Dept. of Electron. & Electr. Eng., Glasgow Univ., UK; Dept. of Electron. & Electr. Eng., Glasgow Univ., UK","IEE 1988 Saraga Colloquium on Electronic Filters","","1988","","","4/1","4/8","New design techniques and filter applications call for more advanced facilities to be offered to the designer. Important examples are: 1. the design of arbitrarily weighted frequency responses, 2. easy construction and comparison of many different filter structures, especially to allow new structures to compete with the popular, well-tested ones, 3. optimisation methods to improve the performance of circuits within physical constraints. These topics form the motivation for PANDDA, a new prototype filter design system. PANDDA is a highly modular program, with well-defined interfaces between the main stages, permitting convenient access at all levels. The software is distinguished by its use of iterative algorithms, providing good accuracy and efficiency while avoiding the complexity of traditional approaches. Special attention has been given to the problems of obtaining good initial values and convergency. Matrix methods are applied (6) as a compact and flexible approach to filter network derivation, greatly facilitating the incorporation of new designs. An optimisation procedure has been developed which makes use of the efficiency of the design process. The program will produce a network description for a number of the popular circuit simulation programs. An example of a practical filter design is given and optimisation of the circuit is illustrated.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=209269","","Active filters;Design automation","active filters;circuit CAD","filter CAD;advanced network design;design techniques;arbitrarily weighted frequency responses;comparison of many different filter structures;optimisation methods;PANDDA;prototype filter design system;modular program;well-defined interfaces;iterative algorithms;obtaining good initial values;filter network derivation;incorporation of new designs;optimisation procedure;example;practical filter design","","","","","","","","","IET","IET Conferences"
"Architectural and multiprocessor design verification of the PowerPC 604 data cache","G. Z. N. Cai","Somerset Design Center, Motorola Inc., USA","Proceedings International Phoenix Conference on Computers and Communications","","1995","","","383","388","The PowerPC 604 microprocessor has high performance 32-bit implementation, which is optimized to produce compact code while adhering to RISC philosophy. The PowerPC 604 microprocessor can sustain a maximum issue rate of 4 instructions per cycle. The data cache of the 604 is a 16 KB four-way set-associative non-blocking cache which contains MESI states (M: Modified, E: Exclusive-unmodified, S: Shared, I: Invalid), a reservation bit with its reservation address register, an independent snoop port, WIMG (W: cache write policy, I: cacheability, M: coherency mode, G: protection against speculative access) support logic, and parity bits. The 604 has an on-chip phase-locked loop to provide different Processor/Bus clock ratios to simplify the system design while using a 100 MHz processor clock. The data cache to BIU (Bus Interface Unit) interface can handle different Processor/Bus clock ratios. The architecture and multiprocessor verification for the PowerPC 604 data cache systematically checks the data cache architecture, logic, and implementation correctness and provides the assurance that the PowerPC 604 microprocessor's aggressive hardware and software implementation is carried out correctly in the uniprocessor and multiprocessor environment.<<ETX>>","","0-7803-2492","10.1109/PCCC.1995.472464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472464","","Microprocessors;Clocks;Logic;Computer architecture;Reduced instruction set computing;Registers;Protection;System-on-a-chip;Phase locked loops;Hardware","cache storage;formal verification;logic testing;microprocessor chips","multiprocessor design verification;PowerPC 604 data cache;32-bit implementation;multiprocessor verification;data cache architecture;implementation correctness","","2","6","","","","","","IEEE","IEEE Conferences"
"Modeling SPECT acquisition and processing of changing radiopharmaceutical distributions","Ji Chen; J. R. Galt; J. D. Valentine; T. L. Faber; E. V. Garcia","George W. Woodruff Sch. of Mech. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA; NA; NA","2001 IEEE Nuclear Science Symposium Conference Record (Cat. No.01CH37310)","","2001","3","","1366","1370 vol.3","The accuracy of SPECT images is compromised and artifacts may be produced when the radiopharmaceutical distribution changes during image acquisition. Optimization of SPECT acquisition protocols for changing tracer distributions can be difficult not only in patient studies (undesirability of performing repeat studies on the same patient) but also in phantom studies (difficulty of emulating the changing distributions). This study proposes a simulation that allows computer modeling of both tracer kinetics and different acquisition schemes. /sup 99m/Tc Teboroxime (Bracco Diagnostics) is used as a model. SPECT acquisition of a software phantom (NCAT, UNC Chapel Hill) is simulated with photon attenuation, collimator resolution, Compton scatter, Poisson noise, and changing tracer distribution. Short-axis uniformity is used to assess the severity of artifacts in the myocardium. The simulation produces similar artifacts to those found in patient studies with /sup 99m/Tc Teboroxime. This simulation methodology can provide a valuable tool for testing novel acquisition and processing techniques and to facilitate the optimization of SPECT images of changing tracer distributions. Summed fanning (back and forth) acquisitions have been tested and artifact reduced short-axis images obtained. Image restoration techniques are proposed to further improve the image quality. Furthermore, the simulated studies can be compared to the simulations with assigned low liver uptake and no tracer clearance from the myocardium to detect and resolve artifacts through variations in the acquisition and processing schemes.","1082-3654","0-7803-7324","10.1109/NSSMIC.2001.1008591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008591","","Imaging phantoms;Computational modeling;Myocardium;Testing;Protocols;Computer simulation;Kinetic theory;Single photon emission computed tomography;Attenuation;Optical collimators","single photon emission computed tomography","SPECT images;radiopharmaceutical distribution;image acquisition;SPECT acquisition protocols;tracer kinetics;Teboroxime;Bracco diagnostics;software phantom;photon attenuation;Poisson noise;Compton scatter;collimator resolution;tracer distributions;liver uptake;tracer clearance;myocardium;/sup 99/Tc/sup m/;Tc","","1","10","","","","","","IEEE","IEEE Conferences"
"Improved contingency measures for operation and planning applications","R. A. Schlueter; J. E. Sekerke; K. L. Burnett; A. G. Costi","Michigan State Univ., East Lansing, MI, USA; NA; NA; NA","IEEE Transactions on Power Systems","","1989","4","4","1430","1437","Three new contingency measures are proposed and tested based on a large power system peak load base case. The Type II contingency measure is zero for all noncritical contingencies, has no misclassification or false alarm problems, and ranks contingencies according to their largest thermal limit violation. It should be used when operating conditions are known exactly. The Type III contingency measure has very small or zero contingency measure for noncritical contingencies, has no perceptible misclassification or false alarm problems, and appears to rank contingencies by their worst thermal limit violation. The Type III contingency measure is very useful for selecting contingencies where the precise operating conditions are unknown. A Type IV contingency measure sets branch weights to zero on all branches that do not experience thermal overloads for a set of contingencies. The Type IV contingency measure is useful for selecting single contingencies that when taken in combination could produce critical multiple contingencies.<<ETX>>","0885-8950;1558-0679","","10.1109/59.41694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=41694","","Security;Voltage;Power system planning;Performance evaluation;System testing;Particle measurements;Weight measurement;Production;Software packages;Packaging","power system planning","contingency measures;operation;planning;power system peak load;Type II contingency measure;worst thermal limit violation;Type III contingency measure;Type IV contingency measure","","4","15","","","","","","IEEE","IEEE Journals & Magazines"
"A neural network implementation of real-time fuzzy predictive control","J. M. Sousa; L. F. Baptista; L. J. Nunes; J. M. G. S da Costa","Technical University of Lisbon, Instituto Superior Tcnico, Department of Mechanical Engineering, GCAR, Av. Rovisco Pais, 1049-001 Lisboa, Portugal; Escola Nutica Infante D. Henrique, Department of Marine Engineering, Av. Eng. Bonneville Franco, Pao de Arcos, Portugal; Technical University of Lisbon, Instituto Superior Tcnico, Department of Mechanical Engineering, GCAR, Av. Rovisco Pais, 1049-001 Lisboa, Portugal; Technical University of Lisbon, Instituto Superior Tcnico, Department of Mechanical Engineering, GCAR, Av. Rovisco Pais, 1049-001 Lisboa, Portugal","2001 European Control Conference (ECC)","","2001","","","3288","3293","Fuzzy predictive controllers have been applied to several applications with good control performance. However, this methodology often leads to nonconvex optimization problems, which are difficult to solve for fast processes, i.e. processes with small sampling times. This paper proposes a new methodology to apply a fuzzy predictive controller in real-time by using a neural network architecture, which receives data from the process and computes the control actions. Thus, the neural network is learned off-line, and its final structure guarantees that control actions are computed very rapidly. An internal model control structure is used to cope with model-plant mismatches and disturbances. The proposed methodology is tested in a realistic simulation of an experimental robot manipulator, where force and position are both controlled. The proposed scheme reveals very good control performance.","","978-3-9524173-6","10.23919/ECC.2001.7076440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076440","Algorithms and Software for Real-time Control;Fuzzy Systems;Neural Networks;Predictive Control;Robot Applications","Computational modeling;Predictive control;Force;Real-time systems;Optimization;Robots;Neural networks","fuzzy control;neurocontrollers;predictive control;real-time systems","real-time fuzzy predictive control;neural network architecture;internal model control structure;robot manipulator;force control;position control","","","12","","","","","","IEEE","IEEE Conferences"
"A comparison of interior-point codes for medium-term hydro-thermal coordination","J. Medina; V. H. Quintana; A. J. Conejo; F. P. Thoden","Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; NA; NA; NA","IEEE Transactions on Power Systems","","1998","13","3","836","843","This paper studies the performance of newly developed and under development interior-point optimization codes as applied to the solution of medium-term hydro-thermal coordination (MTHTC) problems. We compare commercial and research codes, and their main advantages and drawbacks are pointed out. The codes that we study are: CPLEX 3.0-barrier (the latest version of OB1 by Lustig), HOPDM by Gondzio, LOQO by Vanderbei, PCx by Mehrotra's group, LIPSOL by Zhang, and LPA1 (a code being developed by the authors). All codes have been tested on the Spanish hydrothermal system.","0885-8950;1558-0679","","10.1109/59.708701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=708701","","Job shop scheduling;Lagrangian functions;Genetic algorithms;System testing;Production systems;Costs;Security;Thermal decomposition;Thermal engineering;Power systems","hydrothermal power systems;power system analysis computing;software packages","interior-point codes;medium-term hydro-thermal coordination;research codes;commercial codes;CPLEX 3.0-barrier;HOPDM;LOQO;PCx;LIPSOL;LPA1;Spanish hydrothermal system","","22","27","","","","","","IEEE","IEEE Journals & Magazines"
"Autonomous adaptive agents for distributed control of the electric power grid in a competitive electric power industry","A. M. Wildberger","Electr. Power Res. Inst., Palo Alto, CA, USA","Proceedings of 1st International Conference on Conventional and Knowledge Based Intelligent Electronic Systems. KES '97","","1997","1","","2","11 vol.1","A generic model of a complete electric power grid (including generation, transmission, distribution and loads), based on multiple adaptive, intelligent agents, is being developed by the Electric Power Research Institute (EPRT). Its ultimate purpose is to model the computational intelligence required for distributed control of a geographically dispersed but globally interconnected power network. The current worldwide trend toward free competition in electric power, combined with the new availability of very high voltage active control devices, generates the requirement and provides the means for totally distributed sensing, computation and control. The multiple agents-based model and simulation being developed will also serve as a ""scenario-free"" testbed for ""what if"" studies and computer experiments to provide insight into the evolution of the electric enterprise in response to various economic pressures and technological advances. Early work in this area has emphasized modeling market forces and strategies limited only by the physics of electricity and the topology of the grid. Ultimately, this model will test whether any central authority is required, or even desirable, and whether free economic cooperation and competition can, by itself, optimize the efficiency and security of network operation for the mutual benefit of all.","","0-7803-3755","10.1109/KES.1997.616845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=616845","","Programmable control;Adaptive control;Distributed control;Power systems;Power generation;Computational modeling;Voltage control;Testing;Power generation economics;Mesh generation","distributed control;software agents;electricity supply industry;distribution networks;power system control;intelligent control","adaptive agents;distributed control;electric power grid;competitive electric power industry;intelligent agents;computational intelligence;multiple agents-based model;scenario-free","","3","28","","","","","","IEEE","IEEE Conferences"
"An autonomous system-based distribution system for web search","Zhang Xiaohui; Wang Huayong; Chang Guiran; Zhao Hong","Software Center, Northeastern Univ., Shenyang, China; NA; NA; NA","2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)","","2001","1","","435","440 vol.1","The tremendous growth in computer networks and storage has fueled the explosive growth of the web. The amount of information accessible from the web has dramatically increased by several orders of magnitude in the last few years, and shows no signs of abating. Developers of web search engines are confronted with the consequent information overload problem. Although many algorithms are widely used to optimize the performance of web search engines, it is still difficult to decrease the updating cycles of the web data for a traditional general-purpose search and indexing system. In this paper, an autonomous system (AS) based solution is presented. One of its main merits is to limit the network load inside an AS. Since AS is the element of the Internet interconnection and a uniformed route strategy is adopted inside an AS, it will be helpful to solve global network overload problem. At the other side, updating cycles are greatly decreased since the distributed system is working in a parallel way at different ASes. Some key algorithms and techniques are put forward and tested for management and synchronization of the whole system. The effectiveness of the solution is verified in laboratory simulation.","1062-922X","0-7803-7087","10.1109/ICSMC.2001.969851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969851","","Web search;Search engines;Internet;Uniform resource locators;IP networks;System testing;Packaging;Robots;Network servers;Web server","search engines;information networks","Web search;computer networks;search engines;autonomous system;distributed system;load-balanced algorithm","","","25","","","","","","IEEE","IEEE Conferences"
"Development of a continuous respiration olfactometer for odorant delivery synchronous with natural respiration during recordings of brain electrical activity","C. M. Owen; J. Patterson; D. G. Simpson","Sensory Neurosci. Lab., Swinbume Univ. of Technol., Victoria, Australia; NA; NA","IEEE Transactions on Biomedical Engineering","","2002","49","8","852","858","The continuous respiration olfactometer (CRO) was designed as a respiration-synchronous method for delivering odorants during recordings of brain electrical activity, providing control and monitoring of the timing of the delivery as well as the quantities of odorant involved. The CRO incorporates a purpose-built electronic system designed with very specific temporal and quantitative characteristics, and is composed of four main parts: the respiratory monitoring apparatus, the odorant/air delivery system, the serial interface device and the respiratory monitoring software. Tests were undertaken to determine the performance of the system with reference to the accuracy and precision of timing and control of odorant delivery. Tests were also undertaken to determine the effects of variations in natural respiration between subjects on the capability of the respiratory monitoring system, using a group of 50 subjects, to test the success of a variable gain control to optimize the range of the digitized respiratory output. The delivery system was able to provide information concerning quantities of air or odorant delivered, and the stimulus timing information required for integration with neurophysiological recording techniques.","0018-9294;1558-2531","","10.1109/TBME.2002.800765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019449","","Monitoring;Olfactory;Food technology;Australia;System testing;Brain;Timing;Food manufacturing;Packaging;Control systems","chemioception;computerised monitoring;patient monitoring;biomedical electronics;bioelectric potentials;pneumodynamics;electroencephalography;biocontrol;biomedical equipment;medical diagnostic computing","continuous respiration olfactometer;respiration-synchronous method;odorant delivery;natural respiration;brain electrical activity recordings;purpose-built electronic system;respiratory monitoring apparatus;serial interface device;respiratory monitoring software;timing precision;delivery control;digitized respiratory output;chemosensory event-related potentials;continuous electroencephalogram;peak inspiratory air flow;odorant perception;pneumotachometer;neurophysiological recording","1-Butanol;Adult;Drug Delivery Systems;Drug Delivery Systems;Electroencephalography;Equipment Design;Equipment Failure Analysis;Evoked Potentials;Female;Humans;Male;Middle Aged;Odors;Reproducibility of Results;Respiration;Respiratory Physiology;Sensitivity and Specificity;Sensory Thresholds;Smell;Stimulation, Chemical","2","21","","","","","","IEEE","IEEE Journals & Magazines"
"A new type of intelligent navigation node of hypertext","Xu Chuanyu","Dept. of Math., Hangzhou Inst. of Commerce, Zeijiang, China","1997 IEEE International Conference on Intelligent Processing Systems (Cat. No.97TH8335)","","1997","1","","910","914 vol.1","To solve the problem that is difficult for some current navigation devices to discover navigation knowledge automatically, this paper adds the mechanism of machine learning to hypertext, and proposes a new type of intelligent navigation node of hypertext, INNGRL, in which a genetic-based reinforcement learning system is embedded. In a class of process control systems with a hypertext structure, INNGRL can in real-time and automatically discover rules, optimize them, correct mistaken ones, and can evaluate the utility of these rules. In the simulation of a process control, it has discovered 130 rules. The test of the quality of these rules shows that the cost of the simulation process is lower than that of the practical process by 13.4%.","","0-7803-4253","10.1109/ICIPS.1997.672963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=672963","","Navigation;Machine learning;Learning systems;Engines;User interfaces;Process control;Expert systems;Computer architecture;Mathematics;Business","learning (artificial intelligence);process control;navigation;hypermedia;software agents","intelligent navigation node;hypertext;navigation devices;machine learning;INNGRL;genetic-based reinforcement learning system;process control systems","","","7","","","","","","IEEE","IEEE Conferences"
"A performance comparison of communication APIs on Solaris and Windows operating systems","S. Zeadally; Jia Lu","Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA; Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA","Proceedings ITCC 2003. International Conference on Information Technology: Coding and Computing","","2003","","","336","340","Communication application programming interfaces (API) constitute an important component of many network-based applications. They play a central role in the end-to-end performance ultimately delivered by networked applications. Most network architectures exploit the underlying networking API in their designs. In this paper, we conduct an empirical performance evaluation on the PC platform of some of the most popular networking API which include: Winsock/BSD, Java, and RMI. To explore the impact of the underlying operating system and Java virtual machine (JVM) architecture, we conducted performance tests on two operating systems namely, Windows NT 4.0 and Solaris 8. We found that on both operating system platforms, Winsock and BSD sockets yield about 1.8 times better throughput than Java sockets, and Java sockets in turn yield twice the throughput of that obtained using remote method invocation (RMI). We also obtained about 1.3 times higher latency overheads with Java compared to either Winsock or BSD as well as with RMI when compared to Java sockets on both Windows NT and Solaris operating systems. We hope that our results will be useful to application designers and developers in better optimizing end-to-end application performance.","","0-7695-1916","10.1109/ITCC.2003.1197551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1197551","","Operating systems;Information technology","application program interfaces;network operating systems;software performance evaluation;Java;delays","performance comparison;operating systems;communication API;application programming interfaces;PC platform;Winsock/BSD;RMI;JVM architecture;Windows NT 4.0;Solaris 8;Java sockets;throughput;remote method invocation;latency overheads;Java virtual machine","","","13","","","","","","IEEE","IEEE Conferences"
"Experience with Vsystem in a wide variety of applications","P. N. Clout; R. T. Westervelt","Vista Control Syst. Inc., Los Alamos, NM, USA; NA","Proceedings of the 1997 Particle Accelerator Conference (Cat. No.97CH36167)","","1997","2","","2532","2534 vol.2","Vsystem has been developing rapidly with the demands of industrial and research users as well as with promising research into smart controls. A significant development has been the fielding of V3.0 on VxWorks, Digital UNIX, Concurrent power/UX, and Solaris as well as our traditional platforms. One application running is with Vaccess on all three of VxWorks, VAX/OpenVMS, and Alpha/OpenVMS in one system. Other applications use Digital UNIX and Concurrent Power/UX. Our new version of the logger has been running in a power plant for over a year, acting as a black-box flight recorder, recording data at 33 Hz and 100 Hz. We are also installing a power-plant training system emulating the operator's home system with Vdraw and Vaccess running under Digital UNIX. For a new contract for an equipment vendor, we have finished the port of the full Vsystem package to Windows 95 and Windows NT. We made the first tests of charged-particle beam line automation and optimization with a generic automation package. This package has been developed against realistic models and exemplifies on-going research and development at our company.","","0-7803-4376","10.1109/PAC.1997.751264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=751264","","Power generation;Metals industry;Milling machines;Databases;Emulation;Keyboards;Video recording;Web sites;Internet;Interactive systems","distributed processing;intelligent control;software architecture;industrial control;nuclear engineering computing;accelerator control systems;control engineering computing","Vsystem;smart controls;VxWorks;Digital UNIX;Concurrent power/UX;Solaris;Vaccess;VAX/OpenVMS;Alpha/OpenVMS;power-plant training system;Windows 95;Windows NT;charged-particle beam line automation;generic automation package;33 Hz;100 Hz","","1","8","","","","","","IEEE","IEEE Conferences"
"CHT: A Digital Computer Package for Solving Short Term Hydro-Thermal Coordination and Unit Commitment Problems","R. Nieva; A. Inda; J. Frausto","IIE. Cuernavaca, Mor., Mxico; IIE. Cuernavaca, Mor., Mxico; IIE. Cuernavaca, Mor., Mxico","IEEE Transactions on Power Systems","","1986","1","3","168","174","The main characteristics of a digital computer package for solving the complex problem of Short-Term hydro-thermal unit commitment in medium and large size power systems (over 150 units) are presented. The over-all system design criteria, including Man Machine Interface (MMI), the software tools that support the analysis of the large volume of information required for unit commitment planning, and the optimization algorithms are discussed in the text. The package has been tested on a medium-size power system model.","0885-8950;1558-0679","","10.1109/TPWRS.1986.4334977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4334977","","Packaging machines;Power system analysis computing;Power system planning;Power system modeling;System analysis and design;Man machine systems;Software tools;Information analysis;Algorithm design and analysis;Design optimization","","","","11","17","","","","","","IEEE","IEEE Journals & Magazines"
"Quality assured disassembly of electronic components for reuse","I. Stobbe; H. Griese; H. Potter; H. Reichl; L. Stobbe","Res. Center Microperipherics, Technische Univ. Berlin, Germany; NA; NA; NA; NA","Conference Record 2002 IEEE International Symposium on Electronics and the Environment (Cat. No.02CH37273)","","2002","","","299","305","Nowadays, some producers consider the reuse of electronic components which were rejected from production processes. These components can be recycled and used as spare parts or as substitution parts in new products. Up to now, there has been no commercially applicable method and technology available for the automated reverse manufacturing process and the quality control of recovered components. This paper describes technological solutions for an economical recovery of electronic components as a basis for reuse. The main tasks in this respect are the development of an automated desoldering system consisting of commercially available standard components of automation engineering (hardware) as well as industrial image processing (software), the optimization of a desoldering process to guarantee the quality of the regained components and the quality testing of the extracted parts by analysis of their geometric parameters. As a result, an automated process was developed, which includes modules like pre-recognition and selection, quality assured desoldering, handling and visual quality inspection of form and measurement criteria.","1095-2020","0-7803-7214","10.1109/ISEE.2002.1003284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003284","","Electronic components;Manufacturing processes;Quality control;Optimized production technology;Software standards;Standards development;Manufacturing automation;Hardware;Computer industry;Electrical equipment industry","recycling;reverse engineering;electronics industry;quality control;process control;computer vision","electronic components quality assured disassembly;component reuse;automated reverse manufacturing process;recovered components;industrial image processing;automation engineering;desoldering process automation;visual quality inspection;measurement criteria;AOI","","7","17","","","","","","IEEE","IEEE Conferences"
"On-line system setup in a cellar of a flotation plant","W. X. Wang; O. Stephansson; S. C. Wan","Div. of Eng. Geol., R. Inst. of Technol., Stockholm, Sweden; NA; NA","Proceedings 15th International Conference on Pattern Recognition. ICPR-2000","","2000","4","","791","794 vol.4","This paper describes an online system for froth images of a flotation cellar in mineral processing. The aim of the system setup is to obtain optical information of froth images. Information will then be used for optimization of mineral flotation procedures. In this study, we have designed an illumination system for grabbing froth image with a constant illumination, studied image quality evaluation and classification algorithms. They are used for selecting right types of images for further processing. Special segmentation algorithms for bubbles, and about 40 parameters for froth image analysis are presented. Both DOS software and Windows software have been developed and tested.","1051-4651","0-7695-0750","10.1109/ICPR.2000.903036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=903036","","Minerals;Image segmentation;Image edge detection;Image quality;Image color analysis;Image sequence analysis;Lighting;Laboratories;Image processing;Image analysis","mineral processing industry;online operation;foams;image classification;image segmentation;process control","online system setup;flotation plant cellar;froth images;mineral processing;image quality evaluation;image classification algorithms;bubble segmentation algorithms;DOS software;Windows software","","1","6","","","","","","IEEE","IEEE Conferences"
"Combining feature sets with support vector machines: application to speaker recognition","A. O. Hatch; A. Stolcke; B. Peskin","Int. Comput. Sci. Inst., Berkeley, CA, USA; Int. Comput. Sci. Inst., Berkeley, CA, USA; Int. Comput. Sci. Inst., Berkeley, CA, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","","2005","","","75","79","In this paper, we describe a general technique for optimizing the relative weights of feature sets in a support vector machine (SVM) and show how it can be applied to the field of speaker recognition. Our training procedure uses an objective function that maps the relative weights of the feature sets directly to a classification metric (e.g. equal-error rate (EER)) measured on a set of training data. The objective function is optimized in an iterative fashion with respect to both the feature weights and the SVM parameters (i.e. the support vector weights and the bias values). In this paper, we use this procedure to optimize the relative weights of various subsets of features in two SVM-based speaker recognition systems: a system that uses transform coefficients obtained from maximum likelihood linear regression (MLLR) as features (A. Stolcke, et al., 2005) and another that uses relative frequencies of phone n-grams (W. M. Campbell, et al., 2003), (A. Hatch, et al., 2005). In all cases, the training procedure yields significant improvements in both EER and minimum DCF (i.e. decision cost function), as measured on various test corpora","","0-7803-9478-X0-7803-9479","10.1109/ASRU.2005.1566508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566508","","Support vector machines;Speaker recognition;Kernel;Maximum likelihood linear regression;Support vector machine classification;Application software;Training data;Frequency;Testing;Cepstral analysis","error statistics;maximum likelihood estimation;regression analysis;speaker recognition;support vector machines","support vector machines;speaker recognition;feature sets;equal-error rate;maximum likelihood linear regression","","5","9","","","","","","IEEE","IEEE Conferences"
"Value-based reliability for short term operational planning","J. A. Momoh; M. Elfayoumy; W. Mittelstadt","Dept. of Electr. Eng., Howard Univ., Washington, DC, USA; NA; NA","IEEE Transactions on Power Systems","","1999","14","4","1533","1542","The paper presents a scheme for evaluating and optimally enhancing power system reliability for a short-time horizon operation of one week. Power system reliability can be efficiently enhanced by applying appropriate remedial actions (RAs). This paper investigates the impact of remedial actions on system reliability, the corresponding associated cost and the amount of unserved energy using the EPRI Transmission Reliability Evaluation for Large Scale System (TRELSS) program. The proposed scheme has been tested on a 27-bus example taken from the Northwest (NW) system. The method has the potential to be used for assessing the risks associated with near-term scheduled outages and other operational issues.","0885-8950;1558-0679","","10.1109/59.801953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=801953","","Costs;Power system reliability;Power system planning;Power industry;Electricity supply industry;Shunt (electrical);Senior members;Student members;Large-scale systems;System testing","power system planning;power system reliability;power system economics;value engineering;power system analysis computing","short-term operational planning;value-based reliability;power systems;reliability enhancement optimisation;remedial actions;TRELSS software;unserved energy;computer simulation","","13","16","","","","","","IEEE","IEEE Journals & Magazines"
"Providing absolute differentiated services for real-time applications in static-priority scheduling networks","Shengquan Wang; Dong Xuan; Riccardo Bettati; Wei Zhao","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA; NA; NA","Proceedings IEEE INFOCOM 2001. Conference on Computer Communications. Twentieth Annual Joint Conference of the IEEE Computer and Communications Society (Cat. No.01CH37213)","","2001","2","","669","678 vol.2","We propose and analyze a methodology for providing absolute differentiated services for real-time applications in networks that use static-priority schedulers. We extend previous work on worst-case delay analysis and develop a method that can be used to derive delay bounds without specific information on flow population. With this new method, we are able to successfully employ a utilization-based admission control approach for flow admission. This approach does not require explicit delay computation at admission time and hence is scalable to large systems. We assume the underlying network to use static-priority schedulers. We design and analyze several priority assignment algorithms, and investigate their ability to achieve higher utilization bounds. Traditionally, schedulers in differentiated services networks assign priorities on a class-by-class basis, with the same priority for each class on each router. We show that relaxing this requirement, that is, allowing different routers to assign different priorities to classes, achieves significantly higher utilization bounds.","0743-166X","0-7803-7016","10.1109/INFCOM.2001.916255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916255","","Admission control;Job shop scheduling;Diffserv networks;Delay effects;Processor scheduling;Algorithm design and analysis;Bandwidth;Application software;System testing;Intelligent networks","Internet;telecommunication congestion control;delays;telecommunication network routing;quality of service;telecommunication traffic;performance evaluation","absolute differentiated services;real-time applications;static-priority scheduling networks;worst-case delay analysis;delay bounds;utilization-based admission control;flow admission;static-priority schedulers;priority assignment algorithms;utilization bounds;differentiated services networks;router;Internet;heuristic algorithms;QoS architecture;performance evaluation;traffic model","","15","23","","","","","","IEEE","IEEE Conferences"
"Statistically controlled activation weight initialization (SCAWI)","G. P. Drago; S. Ridella","Istituto per i Circuiti Elettronici, CNR, Genova, Italy; NA","IEEE Transactions on Neural Networks","","1992","3","4","627","631","An optimum weight initialization which strongly improves the performance of the back propagation (BP) algorithm is suggested. By statistical analysis, the scale factor, R (which is proportional to the maximum magnitude of the weights), is obtained as a function of the paralyzed neuron percentage (PNP). Also, by computer simulation, the performances on the convergence speed have been related to PNP. An optimum range for R is shown to exist in order to minimize the time needed to reach the minimum of the cost function. Normalization factors are properly defined, which leads to a distribution of the activations independent of the neurons, and to a single nondimensional quantity, R, the value of which can be quickly found by computer simulation.<<ETX>>","1045-9227;1941-0093","","10.1109/72.143378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=143378","","Weight control;Neurons;Computer simulation;Convergence;Neural networks;Statistical analysis;Cost function;Application software;Feedforward neural networks;Testing","convergence of numerical methods;minimisation;neural nets;statistical analysis","neural nets;backpropagation;optimisation;statistically controlled activation weight initialisation;scale factor;paralyzed neuron percentage;convergence;cost function","","72","10","","","","","","IEEE","IEEE Journals & Magazines"
"DPlan: a case study on the cooperation between university and industry","L. A. Jorge; E. Quaresma; F. Mira; A. Fonseca; P. Rosa; L. M. F. Barruncho; L. A. F. M. Ferreira; P. M. S. Carvalho; S. N. C. Grave","EDP Distribuicao Energia, Portugal; NA; NA; NA; NA; NA; NA; NA; NA","2001 IEEE Porto Power Tech Proceedings (Cat. No.01EX502)","","2001","1","","6 pp. vol.1","","The aim of this paper is to explain how the research interests of a university group have been matched with the real needs of a distribution company team working in the field. It describes a case study of technology transfer, from research, through prototyping, to the market. This paper also addresses the formulation and the main evolutionary computation techniques used to deal with the problem of optimal distribution planning. The integration of all systems currently in operation is a priority of EDP distribution. The strategy is to arrive at a situation where those systems become modules of an integration platform. The infrastructure for this integration platform is a geographic information system. In the paper we describe the integration of DPlan in this platform.","","0-7803-7139","10.1109/PTC.2001.964653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964653","","Computer aided software engineering;Geographic Information Systems;Testing;Team working;Technology transfer;Prototypes;Evolutionary computation;Artificial intelligence;Helium;Packaging","electricity supply industry;technology transfer;geographic information systems;evolutionary computation;distribution networks;power engineering computing","DPlan;university and industry cooperation;research;university group;distribution company;technology transfer;prototyping;evolutionary computation techniques;optimal distribution planning;EDP distribution;geographic information system;distribution optimization;artificial intelligence","","1","12","","","","","","IEEE","IEEE Conferences"
"Fitness landscape analysis and memetic algorithms for the quadratic assignment problem","P. Merz; B. Freisleben","Dept. of Electr. Eng. & Comput. Sci., Siegen Univ., Germany; NA","IEEE Transactions on Evolutionary Computation","","2000","4","4","337","352","In this paper, a fitness landscape analysis for several instances of the quadratic assignment problem (QAP) is performed, and the results are used to classify problem instances according to their hardness for local search heuristics and meta-heuristics based on local search. The local properties of the fitness landscape are studied by performing an autocorrelation analysis, while the global structure is investigated by employing a fitness distance correlation analysis. It is shown that epistasis, as expressed by the dominance of the flow and distance matrices of a QAP instance, the landscape ruggedness in terms of the correlation length of a landscape, and the correlation between fitness and distance of local optima in the landscape together are useful for predicting the performance of memetic algorithms-evolutionary algorithms incorporating local search (to a certain extent). Thus, based on these properties, a favorable choice of recombination and/or mutation operators can be found. Experiments comparing three different evolutionary operators for a memetic algorithm are presented.","1089-778X;1089-778X;1941-0026","","10.1109/4235.887234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=887234","","Algorithm design and analysis;Performance analysis;Autocorrelation;Simulated annealing;Genetic mutations;Testing;Cost function;Application software;Wiring","facility location;genetic algorithms;search problems;correlation methods","fitness landscape analysis;memetic algorithms;quadratic assignment problem;local search space;heuristics;autocorrelation;evolutionary algorithms;genetic algorithm;optimisation","","233","85","","","","","","IEEE","IEEE Journals & Magazines"
"A PC-based computer program for teaching the design and analysis of dry-type transformers","W. M. Grady; R. Chan; M. J. Samotyj; R. J. Ferraro; J. L. Bierschenk","Texas Univ., Austin, TX, USA; Texas Univ., Austin, TX, USA; NA; NA; NA","IEEE Transactions on Power Systems","","1992","7","2","709","717","The authors illustrate the use of a new PC-based interactive computer program that has been developed specifically for the design of dry-type transformers. The BASIC language executes on a DOS-based PC with 512 kbyte of RAM and a CGA display. No additional software is needed. Simulation of a design case requires approximately 10 s of execution time on a 4.7 MHz PC/XT. The program enables transformer designers to test, modify, and optimize their designs. Once a base design is established, sensitivity to operating conditions can be easily determined by changing the appropriate input data and re-executing the program. This program can be a useful training tool in the classroom and an effective production design tool in the power electronics industry.<<ETX>>","0885-8950;1558-0679","","10.1109/59.141777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=141777","","Education;Transformers;Industrial training;Read-write memory;Computer displays;Computational modeling;Testing;Design optimization;Production;Power electronics","CAD;computer aided instruction;education;microcomputer applications;power engineering computing;power transformers;teaching;training","PC;CAD;power engineering computing;CAI;power transformers;computer program;teaching;dry-type transformers;BASIC;RAM;CGA display;training;power electronics;512 kB;10 s;4.7 MHz","","13","9","","","","","","IEEE","IEEE Journals & Magazines"
"Thermal and electrical imaging of surface properties with high lateral resolution","E. Oesterschulze; R. Kassing","Inst. of Tech. Phys., Kassel Univ., Germany; NA","XVI ICT '97. Proceedings ICT'97. 16th International Conference on Thermoelectrics (Cat. No.97TH8291)","","1997","","","719","725","The demand to increase the bandwidth of very large scale integrated circuits for future applications in computer and communication electronics has led to device structure dimensions in the sub-micrometer range. The ability of the device materials to withstand stress due to the growing electrical and thermal loading will be of particular importance. The design optimization as well as the material analysis thus demand analytical tools with high lateral resolution, high bandwidth capability, and high sensitivity. In this paper we demonstrate that scanning probe microscopy (SPM) employing integrated probes is a promising candidate for not only imaging topography but simultaneously electrical and thermal device properties. For voltage contrast imaging a coplanar waveguide cantilever was designed and fabricated to be used in high frequency scanning electrical force microscopy (HFSEFM). Additionally it is demonstrated that an in a tip of a cantilever probe integrated Schottky diode is useful for thermal surface imaging with highest temperature resolution in scanning thermal microscopy (SThM). The batch fabrication processes of both probes will be briefly introduced. Fabricated probes are characterized and first results obtained with these novel probes are presented.","1094-2734","0-7803-4057","10.1109/ICT.1997.667631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=667631","","High-resolution imaging;Surface topography;Bandwidth;Thermal stresses;Scanning probe microscopy;Very large scale integration;Application specific integrated circuits;Application software;Computer applications;Thermal loading","scanning probe microscopy;integrated circuit measurement;integrated circuit testing;micromachining;microsensors;temperature distribution;surface potential;VLSI;Schottky diodes;coplanar waveguides;photothermal effects","scanning probe microscopy;integrated probes;topography imaging;electrical imaging;thermal surface imaging;surface properties;high lateral resolution;voltage contrast imaging;coplanar waveguide cantilever;HF scanning electrical force microscopy;cantilever probe integrated Schottky diode;scanning thermal microscopy;batch fabrication processes;high bandwidth capability;high sensitivity;VLSI;Coulomb force;micromachining","","","25","","","","","","IEEE","IEEE Conferences"
"NpBench: a benchmark suite for control plane and data plane applications for network processors","B. K. Lee; L. K. John","Dept. of Electr. & Comput. Eng., Texas Univ., Austin, TX, USA; Dept. of Electr. & Comput. Eng., Texas Univ., Austin, TX, USA","Proceedings 21st International Conference on Computer Design","","2003","","","226","233","Modern network interfaces demand highly intelligent traffic management in addition to the basic requirement of wire speed packet forwarding. Several vendors are releasing network processors in order to handle these demands. Network workloads can be classified into data plane and control plane workloads, however most network processors are optimized for data plane. Also, existing benchmark suites for network processors primarily contain data plane workloads, which perform packet processing for a forwarding function. We present a set of benchmarks, called NpBench, targeted towards control plane (e.g., traffic management, quality of service, etc.) as well as data plane workloads. The characteristics of NpBench workloads, such as instruction mix, parallelism, cache behavior and required processing capability per packet, are presented and compared with CommBench, an existing network processor benchmark suite [T. Wolf et. al., (2000)]. We also discuss the architectural characteristics of the benchmarks having control plane functions, their implications to designing network processors and the significance of instruction level parallelism (ILP) in network processors.","1063-6404","0-7695-2025","10.1109/ICCD.2003.1240899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240899","","Protocols;Application software;Network interfaces;Intelligent networks;Wire;Process design;Bandwidth;Application specific processors;Communication system traffic control;Quality of service","network interfaces;benchmark testing;telecommunication traffic;microprocessor chips","network interface;network traffic management;wire speed packet forwarding;network processor;data plane workload;control plane workload;NpBench workload characteristics;CommBench network processor benchmark suite;instruction level parallelism;ILP","","26","35","","","","","","IEEE","IEEE Conferences"
"Taking evolutionary circuit design from experimentation to implementation: some useful techniques and a silicon demonstration","A. Stoica; R. S. Zebulum; X. Guo; D. Keymeulen; M. I. Ferguson; V. Duong","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA; NA; NA; NA","IEE Proceedings - Computers and Digital Techniques","","2004","151","4","295","300","Current techniques in evolutionary synthesis of analogue and digital circuits designed at transistor level have focused on achieving the desired functional response, without paying sufficient attention to issues needed for a practical implementation of the resulting solution. No silicon fabrication of circuits with topologies designed by evolution has been done before, leaving open questions on the feasibility of the evolutionary circuit design approach, as well as on how high-performance, robust, or portable such designs could be when implemented in hardware. It is argued that moving from evolutionary 'design-for-experimentation' to 'design-for-implementation' requires, beyond inclusion in the fitness function of measures indicative of circuit evaluation factors such as power consumption and robustness to temperature variations, the addition of certain evaluation techniques that are not common in conventional design. Several such techniques that were found to be useful in evolving designs for implementation are presented; some are general, and some are particular to the problem domain of transistor-level logic design, used here as a target application. The example used here is a multifunction NAND/NOR logic gate circuit, for which evolution obtained a creative circuit topology more compact than what has been achieved by multiplexing a NAND and a NOR gate. The circuit was fabricated in a 0.5 /spl mu/m CMOS technology and silicon tests showed good correspondence with the simulations.","1350-2387","","10.1049/ip-cdt:20040503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318864","","","data compression;genetic algorithms;image coding;ISO standards","lossless image compression;high resolution bi-level images;compression method;image data;evolvable hardware chip;bi-level image coding;JBIG2;genetic algorithm;software execution;ISO standardisation;optimisation","","33","","","","","","","IET","IET Journals & Magazines"
"Accelerating Statistical Texture Analysis with an FPGA-DSP Hybrid Architecture","F. I. Pico; S. C. Asensi; V. Corcoles","Universidad de Alicante; NA; NA","The 9th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM'01)","","2001","","","289","290","Nowadays, most image processing systems are implemented using either MMX-optimized software libraries or, when time requirements are limited, expensive high performance DSP-based boards. In this paper we present a texture analysis co-processor concept that permits the efficient hardware implementation of statistical feature extraction, and hardware-software codesign to achieve high-performance low-cost solutions. We propose a hybrid architecture based on FPGA chips, for massive data processing, and digital signal processor (DSP) for floating-point computations. In our preliminary trials with test images, we achieved sufficient performance improvements to handle a wide range of real-time applications.","","0-7695-2667","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420936","","Acceleration;Image texture analysis;Computer architecture;Image processing;Software libraries;Coprocessors;Hardware;Feature extraction;Field programmable gate arrays;Data processing","","","","","2","","","","","","IEEE","IEEE Conferences"
"Real time speech processing to eliminate slamdowns in digital voice systems","C. G. M. Harrison; M. A. Javed; P. Wolanski","Manchester Univ., UK; Manchester Univ., UK; Manchester Univ., UK","1995 Fourth International Conference on Artificial Neural Networks","","1995","","","19","23","Digital voice messaging systems are being used extensively by mobile phone users to store and pass on messages to other users. However with the customer base increasing at an alarming rate, the number of stored calls is increasing dramatically and the number of empty messages, or slamdowns as they are known in the trade, is becoming problematic. A method of detecting these empty messages so that they can be deleted before they are stored was required. Algorithms were developed by using real data recorded on a 'live' system and assessing their performance. Having extracted suitable coefficients from segments of the data using standard digital processing techniques, a multiperceptron artificial neural network was used to produce a logical speech/nospeech result. The network was optimised by investigating aspects such as the training data, the test data, number of hidden nodes, the number of passes and number of segment features being used. The results were biased in favour of speech and the measured success rate for unseen data is in excess of 95%. The software based solution is suitable for the current digital voice messaging system but it was decided to develop a stand alone ASIC based solution for future products. It has been necessary to further optimise the parameter extraction routines and the neural network to simplify the hardware functions required. The final solution uses integer arithmetic and an EPROM to store the neural network coefficients and a sigmoid function lookup table.","0537-9989","0-85296-641","10.1049/cp:19950522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497784","","Speech processing;Voice mail;Communication system software;Real time systems;Multilayer perceptrons","speech processing;voice mail;telecommunication computing;real-time systems;multilayer perceptrons","real time speech processing;digital voice messaging systems;mobile phone users;slamdowns;empty messages;real data;standard digital processing techniques;multiperceptron artificial neural network;logical speech/nospeech result;measured success rate;unseen data;stand alone ASIC based solution;parameter extraction routines;integer arithmetic;EPROM;neural network coefficients;sigmoid function lookup table","","","","","","","","","IET","IET Conferences"
"A discrete event simulation for the crew assignment process in North American freight railroads","R. Guttkuhn; T. Dawson; U. Trutschel","Circadian Technol. Inc., Lexington, MA, USA; Circadian Technol. Inc., Lexington, MA, USA; Circadian Technol. Inc., Lexington, MA, USA","Proceedings of the 2003 Winter Simulation Conference, 2003.","","2003","2","","1686","1692 vol.2","This paper introduces a discrete event simulation for crew assignments and crew movements as a result of train traffic, labor rules, government regulations and optional crew schedules. The software is part of a schedule development system, FRCOS (Freight Rail Crew Optimization System), that was co-developed by Canadian National (CN) Rail and Circadian Technologies, Inc. The simulation allows verification of the impact of changes to trainflow, labor rules or government regulations on the overall operational efficiency of how crews are called to work. The system helps to evaluate changes to current crew assignments and can test new crew assignment scenarios such as crew schedules. Potential problems can be detected before the actual implementation, saving unnecessary costs. The software is also used to assess the impact of traffic changes on existing crew schedules in order to implement reactive corrections to these schedules.","","0-7803-8131","10.1109/WSC.2003.1261620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261620","","Rails;Discrete event simulation;Government;Traffic control;Computational modeling;System testing;Costs;Transportation;Flexible manufacturing systems;Loading","discrete event simulation;freight handling;railways;labour resources;rail traffic","discrete event simulation;crew assignments;train traffic schedule development system;Freight Rail Crew Optimization System;labor rules;government regulation","","3","3","","","","","","IEEE","IEEE Conferences"
"Ascending frequency ordered prefix-tree: efficient mining of frequent patterns","Guimei Liu; Hongjun Lu; Yabo Xu; J. X. Yu","Dept. of Comput. Sci., Hong Kong Univ. of Sci. & Technol., China; Dept. of Comput. Sci., Hong Kong Univ. of Sci. & Technol., China; NA; NA","Eighth International Conference on Database Systems for Advanced Applications, 2003. (DASFAA 2003). Proceedings.","","2003","","","65","72","Mining frequent patterns is a fundamental and important problem in many data mining applications. Many of the algorithms adopt the pattern growth approach, which is shown to be superior to the candidate generate-and-test approach significantly. We identify the key factors that influence the performance of the pattern growth approach, and optimize them to further improve the performance. Our algorithm uses a simple while compact data structure-ascending frequency ordered prefixtree (AFOPT) to organize the conditional databases, in which we use arrays to store single branches to further save space. We traverse our prefix-tree structure using a top-down strategy. Our experiment results show that the combination of the top-down traversal strategy and the ascending frequency item ordering method achieves significant performance improvement over previous works.","","0-7695-1895","10.1109/DASFAA.2003.1192369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192369","","Frequency;Itemsets;Data mining;Transaction databases;Space exploration;Computer science;Spatial databases;Councils;Costs","tree data structures;data mining;very large databases;software performance evaluation;tree searching","ascending frequency ordered prefix-tree;frequent pattern mining;data mining applications;pattern growth approach;data mining;candidate generate-and-test approach;tree data structure;conditional databases;arrays;very large databases;top-down strategy;experiment results;performance improvement","","9","18","","","","","","IEEE","IEEE Conferences"
"A practical resource scheduling with OPF constraints","K. H. Abdul-Rahman; S. M. Shahidehpour; M. Aganagic; S. Mokhtari","Dept. of Electr. & Comput. Eng., Illinois Inst. of Technol., Chicago, IL, USA; Dept. of Electr. & Comput. Eng., Illinois Inst. of Technol., Chicago, IL, USA; NA; NA","Proceedings of Power Industry Computer Applications Conference","","1995","","","92","97","This paper presents an efficient approach to short term power system resource scheduling based on the augmented Lagrangian relaxation method. The problem is divided into two stages, the commitment stage and the constrained economic dispatch stage. The proposed mathematical model incorporates optimal power flow (OPF) constraints in the unit commitment stage. By OPF constrains, the authors refer to the relevant active power constraints that are incorporated in the constrained economic dispatch stage (i.e. transmission capacity constraints, fuel and various regulated emission requirements). The inclusion of OPF constraints in the commitment stage will improve the feasibility of the constrained economic dispatch solution. Other unit commitment constraints such as spinning and operating reserve requirements, power balance as well as other relevant local constraints (i.e. unit ramping rates, upper and lower generation limits, minimum up and down times) are taken into account in the proposed model. As a larger number of constraints are dealt with, a more rigorous method is introduced for updating Lagrange multipliers to improve the solution convergence. A software package which addresses energy management systems requirements is developed and tested.","","0-7803-2663","10.1109/PICA.1995.515170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=515170","","Power generation economics;Fuel economy;Power system modeling;Lagrangian functions;Relaxation methods;Power system economics;Mathematical model;Load flow;Spinning;Power generation","power system planning;optimisation;energy resources;load flow;power system analysis computing;scheduling;load dispatching;load distribution;digital simulation;convergence of numerical methods","short term power system resource scheduling;augmented Lagrangian relaxation method;commitment stage;constrained economic dispatch stage;mathematical model;optimal power flow constraints;unit commitment;transmission capacity;computer simulation;regulated emission;operating reserve;ramping rates;generation limits;spinning reserve;down times;software package;energy management systems;planning","","8","10","","","","","","IEEE","IEEE Conferences"
"Visual Object Categorization using Distance-Based Discriminant Analysis","S. Kosinov; S. Marchand-Maillet; T. Pun","University of Geneva, Switzerland; NA; NA","2004 Conference on Computer Vision and Pattern Recognition Workshop","","2004","","","145","145","This paper formulates the problem of object categorization in the discriminant analysis framework focusing on transforming visual feature data so as to make it conform to the compactness hypothesis in order to improve categorization accuracy. The sought transformation, in turn, is found as a solution to an optimization problem formulated in terms of inter-observation distances only, using the technique of iterative majorization. The proposed approach is suitable for both binary and multiple-class categorization problems, and can be applied as a dimensionality reduction technique. In the latter case, the number of discriminative features is determined automatically since the process of feature extraction is fully embedded in the optimization procedure. Performance tests validate our method on a number of benchmark data sets from the UCI repository, while the experiments in the application of visual object and content-based image categorization demonstrate very competitive results, asserting the method's capability of producing semantically relevant matches that share the same or synonymous vocabulary with the query category and allowing multiple pertinent category assignment.","","","10.1109/CVPR.2004.475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1384942","","Feature extraction;Computer vision;Data mining;Neural networks;Benchmark testing;Application software;Vocabulary;Focusing;Object detection;Linear discriminant analysis","","","","1","","","","","","","IEEE","IEEE Conferences"
"Handwritten character classification using nearest neighbor in large databases","S. J. Smith; M. O. Bourgoin; K. Sims; H. L. Voorhees","Thinking Machines Corp., Cambridge, MA, USA; Thinking Machines Corp., Cambridge, MA, USA; Thinking Machines Corp., Cambridge, MA, USA; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1994","16","9","915","919","Shows that systems built on a simple statistical technique and a large training database can be automatically optimized to produce classification accuracies of 99% in the domain of handwritten digits. It is also shown that the performance of these systems scale consistently with the size of the training database, where the error rate is cut by more than half for every tenfold increase in the size of the training set from 10 to 100,000 examples. Three distance metrics for the standard nearest neighbor classification system are investigated: a simple Hamming distance metric, a pixel distance metric, and a metric based on the extraction of penstroke features. Systems employing these metrics were trained and tested on a standard, publicly available, database of nearly 225,000 digits provided by the National Institute of Standards and Technology. Additionally, a confidence metric is both introduced by the authors and also discovered and optimized by the system. The new confidence measure proves to be superior to the commonly used nearest neighbor distance.<<ETX>>","0162-8828;2160-9292;1939-3539","","10.1109/34.310689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=310689","","Nearest neighbor searches;Databases;NIST;System testing;Error analysis;Optical character recognition software;Artificial intelligence;Character recognition;Machine learning;Neural networks","optical character recognition;learning (artificial intelligence);computer vision","handwritten character classification;nearest neighbor distance;simple statistical technique;large training database;classification accuracies;handwritten digits;error rate;distance metrics;standard nearest neighbor classification system;Hamming distance metric;pixel distance metric;penstroke features extraction;National Institute of Standards and Technology;confidence metric","","31","22","","","","","","IEEE","IEEE Journals & Magazines"
"Uliss G, a fully integrated 'all-in-one' and 'all-in-view' inertia-GPS unit","L. Camberlein; B. Capit","SAGEM, Paris, France; SAGEM, Paris, France","IEEE Symposium on Position Location and Navigation. A Decade of Excellence in the Navigation Sciences","","1990","","","399","406","An eight-channel GPS (Global Positioning System) receiver (500 cm/sup 3/, 0.5 kg, 9 W) has been physically embedded into an existing Uliss inertial system, resulting in a small, light, all-in-one, all-in-view unit functionally optimized through tight coupling of GPS and INS (inertial navigation system) for performance and robustness. Flight tests have been conducted on a Mirage III at the French Official Flight Test Center of Bretigny, and excellent performance has been demonstrated.<<ETX>>","","","10.1109/PLANS.1990.66206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=66206","","Global Positioning System;Aircraft navigation;Costs;Hardware;Space technology;Software testing;Jamming;Robust stability;Aerospace electronics;Radio frequency","aircraft control;aircraft instrumentation;inertial navigation;military equipment;radionavigation;satellite relay systems","Uliss G;GPS;Global Positioning System;inertial navigation system;robustness;Mirage III","","","11","","","","","","IEEE","IEEE Conferences"
"An Air Bearing Rotating Coil Magnetic Measurement System","S. C. Gottschalk; K. Kangas; D. J. Taylor; W. Thayer","STI, WA, scg@stioptronics.com; NA; NA; NA","Proceedings of the 2005 Particle Accelerator Conference","","2005","","","2038","2040","This paper describes a rotating coil magnetic measurement system supported on air bearings. The design is optimized for measurements of 0.1micron magnetic centerline changes on long, small aperture quadrupoles. Graphite impregnated epoxy resin is used for the coil holder and coil winding forms. Coil holder diameter is 11 mm with a length between supports of 750mm. A pair of coils is used to permit quadrupole bucking during centerline measurements. Coil length is 616mm, inner radius 1.82mm, outer radius 4.74mm. The key features of the mechanical system are simplicity; air bearings for accurate, repeatable measurements without needing warm up time and a vibration isolated stand that uses a steel-topped Newport optical table with air suspension. Coil rotation is achieved by a low noise servo motor controlled by a standalone Ethernet servo board running custom servo software. Coil calibration procedures that correct wire placement errors, tests for mechanical resonances, and other system checks will also be discussed.","1944-4680;2152-9582","0-7803-8859","10.1109/PAC.2005.1591001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1591001","","Coils;Magnetic variables measurement;Servomechanisms;Vibration measurement;Optical noise;Design optimization;Magnetic levitation;Apertures;Epoxy resins;Mechanical systems","","","","1","4","","","","","","IEEE","IEEE Conferences"
"A reconfigurable FSS antenna for a second generation of the Spanish domestic satellite (HISPASAT-II)","L. de Haro; M. Calvo; M. Malagon; J. C. Vargas; M. J. Martin; G. Crone","Univ. Politecnica de Madrid, Spain; Univ. Politecnica de Madrid, Spain; Univ. Politecnica de Madrid, Spain; Univ. Politecnica de Madrid, Spain; NA; NA","IEEE Antennas and Propagation Society International Symposium 1992 Digest","","1992","","","1328","1331 vol.3","The studies of a reconfigurable antenna system to fulfil the FSS (frequency selective surface) antenna requirements for the next generation of the Spanish communication satellite system are presented. The scenario, the antenna design tools developed, and the antenna design, performed under an ESA contract named the ASYRIO project, are described. Software to optimize the reconfigurable contoured beam antenna has been developed and tested with the electrical design of the FSS HISPASAT-II antenna.<<ETX>>","","0-7803-0730","10.1109/APS.1992.221752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=221752","","Frequency selective surfaces;Satellite antennas;Satellite broadcasting;Contracts;Space technology;Europe;Artificial satellites;Power generation economics;Antenna accessories;Design optimization","CAD;directive antennas;satellite antennas;satellite relay systems","reconfigurable FSS antenna;Spanish domestic satellite;HISPASAT-II;frequency selective surface;communication satellite system;antenna design tools;ASYRIO project;contoured beam antenna","","","3","","","","","","IEEE","IEEE Conferences"
"A Production Parylene Coating Process for Hybrid Microcircuits","V. Kale; T. Riley","Teledyne Microelectric Division,Los Angeles; NA","IEEE Transactions on Parts, Hybrids, and Packaging","","1977","13","3","273","279","Application of Parylene for protecting microelectronic circuits from loose particles and external environment has been visualized for many years. With a joint effort by NASA and TMD, a process has now been qualified to perform Parylene deposition on hybrid circuits on a production basis, for the Centaur inertial guidance computer. The Parylene coating process developed during this program consists of a) obtaining a hybrid cover with a hole in it, b) sealing of the circuit with a hole in the cover, c) Parylene coating through the hole with the external leads protected from Parylene by appropriate fixturing, and d) sealing of the hole by soldering a pretinned Kovax tab. Development of the above process required optimization of the Parylene coater parameters to obtain a uniform consistent coating which could offer adequate protection to the circuits, fixture design for packages of various types, determination of the size of the deposition hole, the amount of dimer charge per run, a process to hermetically seal the deposition holes and establishment of quality control techniques or acceptance criteria for the deposited film. Several experimental runs were made on test circuits as well as actual production circuits to determine the effect of Parylene coating on active components, thin film resistors, and wire bonds under various conditions. Tests were also made to determine if Parylene indeed protected circuits from loose particles and external environment. After these experiments, Parylene coating acceptance standards were established and a long and rigorous qualification program was completed in order to prove the feasibility of this process. The results of the qualification program will be reported in a future publication. It is concluded that Parylene offers excellent protection against loose particles and a degree of protection from some environmental conditions. It is expected that the fraction of hybrids being coated with Parylene will continue to increase in the microelectronic industry.","0361-1000;1558-2469","","10.1109/TPHP.1977.1135214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1135214","","Production;Coatings;Hybrid integrated circuits;Protection;Circuit testing;Microelectronics;Fixtures;Qualifications;Application software;Visualization","","Hybrid integrated circuit packaging;Manufacturing","","18","5","","","","","","IEEE","IEEE Journals & Magazines"
"Asynchronous programming model for the concurrent solution of the security constrained optimal power flow problem","M. Rodrigues; O. R. Saavedra; A. Monticelli","DSEE-FEE-Unicamp, Campinas, Brazil; DSEE-FEE-Unicamp, Campinas, Brazil; DSEE-FEE-Unicamp, Campinas, Brazil","IEEE Transactions on Power Systems","","1994","9","4","2021","2027","This paper presents an algorithm for the parallel solution of the security constrained optimal-power flow (SCOPF) problem using an asynchronous programming model. In addition to the increased efficiency, the proposed model allows for the development of applications that can be ported among different parallel computer architectures in a nearly transparent way and without significant loss of computing efficiency. The initial implementation of the approach was made on a 9-processor shared-memory parallel computer; subsequently, the system has been ported to a 64-processor distributed-memory parallel machine. The paper summarizes the results obtained in tests performed with two real-life systems: a network formed by 735 buses, 1212 branches and 76 adjustable power generators (900 contingencies); and a network with 1663 buses, 2349 branches and 99 adjustable generators (1555 contingencies).<<ETX>>","0885-8950;1558-0679","","10.1109/59.331464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=331464","","Load flow;Concurrent computing;Parallel machines;Parallel programming;Application software;Power generation;Security;Power engineering computing;Power engineering and energy;Power system modeling","power system analysis computing;load flow;power system security;digital simulation;optimisation;parallel processing;shared memory systems;distributed memory systems","security constrained optimal power flow problem;concurrent solution;asynchronous programming model;algorithm;efficiency;computer simulation;parallel computer architectures;computing efficiency;transparency;shared-memory;distributed-memory;buses;branches;adjustable generators;contingencies","","17","12","","","","","","IEEE","IEEE Journals & Magazines"
"Solving the protein threading problem in parallel","N. Yanev; R. Andonov","Sofia Univ., Bulgaria; NA","Proceedings International Parallel and Distributed Processing Symposium","","2003","","","8 pp.","","We propose a network flow formulation for protein threading and show its equivalence with the shortest path problem on a graph with a very particular structure. The underlying mixed integer programming (MIP) model proves to be very appropriate for the protein threading problem - huge real-life instances have been solved in a reasonable time by using only a mixed integer optimizer instead of a special-purpose branch and bound algorithm. The properties of the MIP model allow decomposition of the main problem on a large number of subproblems (tasks). We show in this paper that a branch and bound-like algorithm can be efficiently applied to solving in parallel these tasks, which leads to a significant reduction in the total running time. Computational experiments with huge problem instances are presented.","1530-2075","0-7695-1926","10.1109/IPDPS.2003.1213295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213295","","Proteins;Sequences;Shape;Intelligent networks;Shortest path problem;Linear programming;Biology computing;Computational biology;Testing;Polynomials","proteins;integer programming;tree searching;parallel algorithms;software performance evaluation;biology computing","protein threading problem;network flow formulation;shortest path problem;mixed integer programming;MIP model;branch and bound-like algorithm;parallel algorithm;running time;huge problem instances","","2","13","","","","","","IEEE","IEEE Conferences"
"Multigrain automatic parallelization in Japanese Millennium Project IT21. Advanced Parallelizing Compiler","H. Kasahara; M. Obata; K. Ishizaka; K. Kimura; H. Kaminaga; H. Nakano; K. Nagasawa; A. Murai; H. Itagaki; J. Shirako","Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan; Japanese Millennium Project IT21 Advancea Parallelizing Compiler, Waseda Univ., Tokyo, Japan","Proceedings. International Conference on Parallel Computing in Electrical Engineering","","2002","","","105","111","This paper describes OSCAR multigrain parallelizing compiler which has been developed in Japanese Millennium Project IT21 ""Advanced Parallelizing Compiler"" project and its performance on SMP machines. The compiler realizes multigrain parallelization for chip-multiprocessors to high-end servers. It hierarchically exploits coarse grain task parallelism among loops, subroutines and basic blocks and near fine grain parallelism among statements inside a basic block in addition to loop parallelism. Also, it globally optimizes cache use over different loops, or coarse grain tasks, based on the data localization technique to reduce memory access overhead. Current performance of OSCAR compiler for SPEC95fp is evaluated on different SMPs. For example, it gives us 3.7 times speedup for HYDRO2D, 1.8 times for SWIM, 1.7 times for SU2COR, 2.0 times for MGRID, 3.3 times for TURB3D on 8 processor IBM RS6000, against XL Fortran compiler ver 7.1 and 4.2 times speedup for SWIM and 2.2 times speedup for TURB3D on 4 processor Sun Ultra80 workstation against Forte6 update 2.","","0-7695-1730","10.1109/PCEE.2002.1115213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115213","","Parallel processing;Multiprocessing systems;Algorithms;Testing;Workstations;Memory architecture;Usability;Data analysis;Program processors;Government","parallelising compilers;parallel programming;subroutines;program control structures;cache storage;software performance evaluation;shared memory systems;scheduling","multigrain automatic parallelization;Japanese Millennium Project IT21;Advanced Parallelizing Compiler;OSCAR;SMP machines;chip-multiprocessors;high-end servers;coarse grain task parallelism;subroutines;near fine grain parallelism;cache use;data localization technique;SPEC95fp;HYDRO2D;TURB3D;IBM RS6000;SWIM;SU2COR;MGRID;XL Fortran compiler;Sun Ultra80 workstation","","2","26","","","","","","IEEE","IEEE Conferences"
"Efficiently solving dynamic Markov random fields using graph cuts","P. Kohli; P. H. S. Torr","Dept. of Comput., Oxford Brookes Univ., AK, UK; Dept. of Comput., Oxford Brookes Univ., AK, UK","Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1","","2005","2","","922","929 Vol. 2","In this paper, we present a fast new fully dynamic algorithm for the st-mincut/max-flow problem. We show how this algorithm can be used to efficiently compute MAP estimates for dynamically changing MRF models of labeling problems in computer vision, such as image segmentation. Specifically, given the solution of the max-flow problem on a graph, we show how to efficiently compute the maximum flow in a modified version of the graph. Our experiments showed that the time taken by our algorithm is roughly proportional to the number of edges whose weights were different in the two graphs. We test the performance of our algorithm on one particular problem: the object-background segmentation problem for video and compare it with the best known st-mincut algorithm. The results show that the dynamic graph cut algorithm is much faster than its static counterpart and enables real time image segmentation. It should be noted that our method is generic and can be used to yield similar improvements in many other cases that involve dynamic change in the graph","1550-5499;2380-7504","0-7695-2334","10.1109/ICCV.2005.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544820","","Markov random fields;Heuristic algorithms;Image segmentation;Computer vision;Inference algorithms;Computational geometry;Tree graphs;Belief propagation;Performance analysis;Application software","computer vision;graph theory;image segmentation;Markov processes;maximum likelihood estimation;optimisation;random processes","dynamic Markov random fields;dynamic algorithm;st-mincut algorithm;max-flow algorithm;maximum a-posteriori estimates;computer vision;object-background segmentation;dynamic graph cut algorithm;real time image segmentation","","76","16","","","","","","IEEE","IEEE Conferences"
"Minimum effective dimension for mixtures of subspaces: a robust GPCA algorithm and its applications","Kun Huang; Yi Ma; R. Vidal","Dept. of Electr. & Comput. Eng., Illinois Univ., Urbana, IL, USA; Dept. of Electr. & Comput. Eng., Illinois Univ., Urbana, IL, USA; NA","Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.","","2004","2","","II","II","We propose a robust model selection criterion for mixtures of subspaces called minimum effective dimension (MED). Previous information-theoretic model selection criteria typically assume that data can be modelled with a parametric model of certain (possibly differing) dimension and a known error distribution. However, for mixtures of subspaces with different dimensions, a generalized notion of dimensionality is needed and hence introduced in this paper. The proposed MED criterion minimizes this geometric dimension subject to a given error tolerance (regardless of the noise distribution). Furthermore, combined with a purely algebraic approach to clustering mixtures of subspaces, namely the generalized PCA (GPCA), the MED is designed to also respect the global algebraic and geometric structure of the data. The result is a non-iterative algorithm called robust GPCA that estimates from noisy data an unknown number of subspaces with unknown and possibly different dimensions subject to a maximum error bound. We test the algorithm on synthetic noisy data and in applications such as motion/image/video segmentation.","1063-6919","0-7695-2158","10.1109/CVPR.2004.1315223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315223","","Image segmentation;Principal component analysis;Application software;Noise robustness;Computer vision;Maximum likelihood estimation;Solid modeling;Biomedical engineering;Parametric statistics;Clustering algorithms","optimisation;principal component analysis;pattern clustering;image segmentation;algebra","minimum effective dimension;robust model selection criteria;error distribution;generalized PCA;global algebraic;geometric structure;data segmentation","","15","16","","","","","","IEEE","IEEE Conferences"
"How to select fair improving directions in a negotiation model over continuous issues","H. Ehtamo; M. Verkama; R. P. Hamalainen","Syst. Anal. Lab., Helsinki Univ. of Technol., Espoo, Finland; NA; NA","1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation","","1997","4","","3466","3471 vol.4","We present a new constructive approach for finding Pareto optimal agreements in two-party negotiations over continuous issues. The procedure results in Pareto optimal agreements under fairly general assumptions, and it uses a mediator, e.g. a person or software, who assists the decision makers in choosing jointly beneficial compromises. The procedure has the appealing feature that the decision makers are only required to answer relatively simple questions, and that their individual utility functions need not be identified completely.","1062-922X","0-7803-4053","10.1109/ICSMC.1997.633188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633188","","Convergence;Search methods;Costs;Testing;Resource management;Proposals;Decision making;Mathematical analysis;Mechanical factors","negotiation support systems;distributed decision making;optimisation;mathematical analysis","fair improving direction selection;negotiation model;continuous issues;constructive approach;Pareto optimal agreements;two-party negotiations;general assumptions;mediator;decision makers;decision support system;utility functions;mathematical analysis","","3","17","","","","","","IEEE","IEEE Conferences"
"Design and implementation of a process control language for an automated pager manufacturing system","Keon Young Yi; Se Joong Jeon","Dept. of Electr. Eng., Kwangwoon Univ., Seoul, South Korea; NA","Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C)","","1999","2","","1132","1137 vol.2","A process control language for an automated pager manufacturing system and its implementation are dealt with. The pager manufacturing process consists of the tuning process in the tune TEM-cell and the examination process in the exam TEM-cell. Three capacitor trimmers are tuned to optimize the state of the pager in the shielding box, we call it the TEM-cell, using small motors, pneumatic actuators and sensors. In the exam TEM-cell equipped with a CCD camera and pneumatic actuators, message receiving status with an arbitrary message calling is tested using a vision system. For both tuning and examination process, 37 process control commands are defined by analyzing the manual pager manufacturing process. Project editors are used to make a process description command file, we call it the project file, are also presented. The system is composed of a desktop personal computer(PC586) and two TEM-cells, two Flex pager testers, an oscilloscope, a camera sensor and actuators controlled by 5 Lonwork controllers. Visual Basic running on the Windows 95 is used to implement the control software which has the GUI function. The effectiveness of the completed control program is shown by the field test with the machine developed at KITECH, Korea, which is now ready for production.","1050-4729","0-7803-5180","10.1109/ROBOT.1999.772514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772514","","Process control;Manufacturing processes;Pneumatic actuators;System testing;Control systems;Manufacturing systems;Capacitors;Capacitive sensors;Charge coupled devices;Charge-coupled image sensors","process control;automatic optical inspection;CCD image sensors;tuning;computer vision;controllers;graphical user interfaces","process control language;automated pager manufacturing system;tuning process;tune TEM-cell;examination process;exam TEM-cell;capacitor trimmers;shielding box;small motors;pneumatic actuators;pneumatic sensors;CCD camera;message receiving status;vision system;project editors;Flex pager testers;Lonwork controllers;Visual Basic;KITECH","","","4","","","","","","IEEE","IEEE Conferences"
"Functional verification of SMP, MPP, and vector-register supercomputers through controlled randomness","J. T. Wunderlich","NA","IEEE SoutheastCon, 2003. Proceedings.","","2003","","","117","122","Prototype supercomputer functionality can be verified by comparing simulated hardware execution with actual hardware test-program runs where each successive test-program run includes randomly changing machine-states, operating scenarios, and data. Increased verification is achieved through repeated program execution. In both multiprocessor and vector-register systems, a ""controlled randomness"" can be used to verify the functionality of simultaneously executing processors or functional units. We discuss the selection and combining of random number generators such that a ""degree-of-randomness"" between successive or parallel program runs is controlled. This allows computer engineers to simulate the execution of actual software (application or system-level) in which successive or parallel program runs may or may not involve uncorrelated tasks. Additionally, random number generators are selected to maximize execution speed and cycle-length, ensure reproducibility, and when desired, best produce a random source of numbers (i.e., to better approximate an independent, identically-distributed source). Generators can also be chosen for ease of implementation, the ability to run backwards, and the ability to split the generator's cycle into uncorrelated segments. ""Backward multipliers"" to allow generators to be run in reverse can also be easily found for some types of generators; reversibility is critical for functional verification so that code execution can be traced backwards to find scenarios that led to detected hardware failures. When generators are carefully selected and combined, the verification process can be optimized. By using this methodology, functional verification of SMP, MPP and vector-register supercomputers can be achieved.","","0-7803-7856","10.1109/SECON.2003.1268440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268440","","Supercomputers;Hardware;Testing;Random number generation;Virtual prototyping;Control systems;Diversity reception;Control engineering computing;Concurrent computing;Computational modeling","parallel machines;random number generation;parallel programming;digital simulation;program verification;vector processor systems","functional verification;symmetric multiprocessing;parallel programs;massively parallel processing;vector-register supercomputers;controlled randomness;random number generators","","","16","","","","","","IEEE","IEEE Conferences"
"Case Study in Active Circuit Design (FET Amplifier)","P. G. Debois","Bell Telephone Mfg Cy, Antwerpen, Belgium","1984 14th European Microwave Conference","","1984","","","738","739","The different steps of the CAD design process of a microwave active circuit are described. It is not the intention to repeat the instruction manual of a commercially available CAD software package. The CAD approach presented, does not require the use of a synthesis program for cost considerations. The active device selection, the approximate design method (necessary in case no synthesis program is available), the computer aided optimisation, the manufacturing and the testing of the hardware are discussed for both examples. Two microwave FET amplifiers with different specification requirement are discussed. The design of ann Intermediate Power Amplifier (IPA) in the 5.8 to 6.5 GHz is presented. In view of the application in SCPC (Single Channel Per Carrier) Satellite Ground Station, a stringent linearity requirement is imposed. A flatness of 0.25 dB over any 36 MHz transponder bandwidth is also required.","","","10.1109/EUMA.1984.333430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4132119","","Active circuits;Design automation;Microwave FETs;Microwave devices;Circuit synthesis;Computer aided manufacturing;Process design;Manuals;Software packages;Costs","","","","","8","","","","","","IEEE","IEEE Conferences"
"Stepped helical antenna modeling using nec and validation by measurements","N. Sultan; T. Pellerin; C. W. Trueman; S. J. Kubina","Canadian Space Agency St-Hubert, Canada; Canadian Space Agency St-Hubert, Canada; University of Concordia Montreal, Canada; University of Concordia Montreal, Canada","1996 Symposium on Antenna Technology and Applied Electromagnetics","","1996","","","681","684","This paper investigates the validation of a modeling software for helical antennas. One of the benefits of the stepped helical antenna, which consists of two or more cylindrical helices of different diameters, is its capability, under some conditions, of providing a large bandwidth. It is also simpler and cheaper than a tapered helix. For spacecraft applications, a theoretical design of such an antenna can allow the flexibility for optimization of the antenna size and weight. This paper presents an analysis and modeling of stepped helical antennas. It provides theoretical performance prediction. Corresponding helices were built and tested to validate the theoretical prediction. The experimental performances are in good agreement with the theoretical results.","","978-0-9692563-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863992","","Helical antennas;Antenna measurements;Broadband antennas;Wires;Software;Antenna theory;Gain","","","","","","","","","","","IEEE","IEEE Conferences"
"Intelligent scheduling based on start time adjustment for advanced sequential control systems","T. Aizono; T. Kikuno","Syst. Dev. Lab., Hitachi Ltd., Kawasaki, Japan; NA","Proceedings Third IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC 2000) (Cat. No. PR00607)","","2000","","","138","145","High productivity is required for industrial systems that are to produce large amounts of low-price products. Most industrial systems are sequentially controlled because materials and parts are processed and constructed step by step. A new method of sequential control is needed to increase productivity, and a new technique to schedule the start time of each process is also needed because of frequent adjustment of production lines and their equipment. Intelligent devices such as intelligent sensors and actuators used in the latest industrial systems have embedded high-performance processors and contain software modules. These devices cooperate and control processing robots and production lines. Advanced sequential control is proposed for sequential control systems consisting of distributed software modules. Processing time can be minimized and productivity increased by intelligent scheduling for advanced sequential control, and this paper describes a new method for adjusting the starting times of production processes automatically. This method shortens the time margin and decreases the total processing time. This paper also describes a processing flow diagram that makes it easy to adjust the starting time of each process. The effectiveness of this intelligent scheduling and start-time-adjustment has been demonstrated by using them to optimize the operation of a system for testing disk plates.","","0-7695-0607","10.1109/ISORC.2000.839521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839521","","Job shop scheduling;Automatic control;Productivity;Electrical equipment industry;Control systems;Production;Intelligent sensors;Intelligent actuators;Processor scheduling;Intelligent robots","scheduling;production engineering computing;production control;control systems;object-oriented programming;distributed programming;intelligent sensors;intelligent actuators;control engineering computing","intelligent scheduling;start time adjustment;advanced sequential control systems;productivity;production lines;intelligent sensors;intelligent actuators;embedded high-performance processors;robots;distributed software modules;processing time;processing flow diagram;disk plate testing","","","10","","","","","","IEEE","IEEE Conferences"
"An adaptable connectionist text retrieval system with relevance feedback","M. R. Azimi-Sadjadi; J. Salazar; S. Srinivasan; S. Sheedvash","Dept. of Electr. & Comput. Eng., Colorado State Univ., Fort Collins, CO, USA; Dept. of Electr. & Comput. Eng., Colorado State Univ., Fort Collins, CO, USA; Dept. of Electr. & Comput. Eng., Colorado State Univ., Fort Collins, CO, USA; NA","2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)","","2004","1","","309","314","This paper introduces a new connectionist network for large-scale text retrieval applications. A learning mechanism is proposed to optimally map the original query using relevance feedback from multiple expert users. The query mapping not only meets the requirements of the expert users but also preserves the positions and ranks of other relevant documents. An updating algorithm is also proposed to incorporate new documents (or delete the obsolete ones) into the system either one-by-one or in a batch mode without requiring to retrain the system. The algorithms are successfully tested on a large database and for a large number of most commonly used single-term or multi-terms queries.","1098-7576","0-7803-8359","10.1109/IJCNN.2004.1379919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1379919","","Databases;Neural networks;State feedback;Learning systems;Information retrieval;Neurofeedback;Large-scale systems;Application software;Testing;Feedforward systems","feedback;relevance feedback;text analysis;neural nets;learning (artificial intelligence)","adaptable connectionist text retrieval system;relevance feedback;learning mechanism;query mapping","","2","7","","","","","","IEEE","IEEE Conferences"
"Next best view system in a 3D object modeling task","L. M. Wong; C. Dumont; M. A. Abidi","Dept. of Electr. & Comput. Eng., IRIS Lab., Knoxville, TN, USA; NA; NA","Proceedings 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA'99 (Cat. No.99EX375)","","1999","","","306","311","Sensor placement for 3D modeling is a growing area of computer vision and robotics. The objective of a sensor placement system is to make task-directed decisions for optimal pose selection. We propose a next best view solution to the sensor placement problem. Our algorithm computes the next best view by optimizing an objective function that measures the quantity of unknown information in each of a group of potential viewpoints. The potential views are either placed uniformly around the object or are calculated from the surface normals of the occupancy grid model. To initiate the collection of new data, the optimal pose is selected from the objective function calculation. The model is incrementally updated from the information acquired in each new view. This process terminates when the number of recovered voxels ceases to increase, yielding the final model. We tested two different algorithms on 8 objects of various complexity, including objects with simple concave, simple hole, and complex hole self-occlusions.","","0-7803-5806","10.1109/CIRA.1999.810066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810066","","Cameras;System software;Image resolution;Optimization methods;Shape;Computational modeling;Computer simulation;Pixel","computer vision;stereo image processing;solid modelling;sensor fusion","3D object modeling;computer vision;sensor placement;next best view;occupancy grid model;objective function;sensor planning;sensor fusion","","12","10","","","","","","IEEE","IEEE Conferences"
"H/sub /spl infin// control design for an industrial boiler","Wen Tan; H. J. Marquez; Tongwen Chen; R. K. Gooden","Dept. of Electr. & Comput. Eng, Alberta Univ., Edmonton, Alta., Canada; NA; NA; NA","Proceedings of the 2001 American Control Conference. (Cat. No.01CH37148)","","2001","4","","2537","2542 vol.4","In the control of an industrial boiler system, multi-loop (decentralized) PI control is used because of its simplicity and the existence of simple online tuning rules. We show that such control schemes sacrifices robustness and performance of the overall system. In particular, under normal boiler operating conditions, we design a robust multivariable controller using the H/sub /spl infin// loop-shaping technique. For reason of implementation, we then reduce this controller to a multivariable PI structure. Both the H/sub /spl infin// controller and its PI approximation are tested extensively in the frequency domain as well as in the time domain, using a complex nonlinear simulation software; the results obtained show that the designed controllers are superior in robustness and performance compared to the existing multi-loop controller.","0743-1619","0-7803-6495","10.1109/ACC.2001.946245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=946245","","Control design;Industrial control;Boilers;Control systems;Robust control;Design optimization;Optimal control;Testing;Distributed power generation;Power generation","boilers;H/sup /spl infin// control;robust control;two-term control;multivariable control systems;decentralised control;control system synthesis","H/sub /spl infin// control;industrial boiler;decentralized control;robust control;PI control;multivariable control systems;loop-shaping","","3","7","","","","","","IEEE","IEEE Conferences"
"Fast and accurate extraction of capacitance parameters for the Statz MESFET model","S. Van den Bosch; L. Martens","Dept. of Inf. Technol., Univ. of Gent, Belgium; NA","IEEE Transactions on Microwave Theory and Techniques","","1997","45","8","1247","1249","A modified approach to S-parameter fitting of measured MESFET data to the Statz's model equations is presented. The technique only requires one sweep of measured S-parameters versus frequency at a single bias point. In addition, there is a reduced need for optimization during the extraction procedure. The additional information in the data-fitting code reduces problems with measurement noise and calibration error. The presented method was developed and tested in HP-ICCAP, a software tool for parameter extraction. All measurements are controlled by this software.","0018-9480;1557-9670","","10.1109/22.618414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618414","","Capacitance;Data mining;Scattering parameters;MESFETs;Equations;Frequency measurement;Noise measurement;Noise reduction;Calibration;Computer errors","Schottky gate field effect transistors;semiconductor device models;equivalent circuits;capacitance;microwave field effect transistors","capacitance parameters extraction;MESFET model;S-parameter fitting;measured MESFET data;Statz model equations;HP-ICCAP;software tool","","1","9","","","","","","IEEE","IEEE Journals & Magazines"
"Optimised microelectronic power control of an AC drive for an electric vehicle","W. K. Loh; A. C. Renfrew","Singapore Polytechnic, Department of Electrical Engineering, Singapore, Singapore; University of Manchester Institute of Science and Technology, Department of Electrical Engineering and Electronics, Manchester, UK","IEE Proceedings B - Electric Power Applications","","1986","133","4","291","298","A control strategy for a battery powered electric vehicle employing a 3-phase induction motor drive is described. To maximise efficiency and minimise weight, a software based strategy has been developed to minimise energy loss in the battery, inverter and motor. Laboratory tests show an advantage over `traditional control methods.","0143-7038","","10.1049/ip-b.1986.0039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4644210","","","electric drives;electric vehicles;induction motors;optimal control;power control","three phase induction motor;microelectronic power control;AC drive;battery powered electric vehicle;3-phase induction motor drive;energy loss;inverter","","2","","","","","","","IET","IET Journals & Magazines"
"An integrated high power factor three-phase AC-DC-AC converter for AC-machines implemented in one microcontroller","F. Blaabjerg; J. K. Pedersen","Inst. of Energy Technol., Aalborg Univ., Denmark; Inst. of Energy Technol., Aalborg Univ., Denmark","Proceedings of IEEE Power Electronics Specialist Conference - PESC '93","","1993","","","285","292","A high power factor three-phase AC-DC-AC power converter for AC machines has been designed and tested. The AC-DC-AC converter is controlled by one single 16-b microcontroller. The converter has two PWM-VSI bridges with common snubber- and drive-circuit topologies. The high power factor rectifier is space vector controlled and the inverter controls an AC-induction machine by an energy optimized strategy. Common software tasks are utilized in order to minimize memory demands in the microcontroller. Measurements show a high power factor of the converter, and, in addition, the converter can very rapidly change from motor to generator operation during reversing of the AC machine. It is concluded that the AC-DC-AC converter works well, can be designed to be very compact, and represents some of the state-of-the-art integration and performance in three-phase AC-DC-AC converters.<<ETX>>","","0-7803-1243","10.1109/PESC.1993.471915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=471915","","Reactive power;Analog-digital conversion;AC machines;Microcontrollers;Testing;Pulse width modulation;Pulse width modulation converters;Bridges;Topology;Rectifiers","DC-AC power convertors;AC-DC power convertors;AC machines;PWM power convertors;bridge circuits;PWM invertors;power factor;rectifiers;rectifying circuits;microcontrollers;machine control;digital control;snubbers;driver circuits;optimal control;AC-AC power convertors","snubber circuit;space vector control;energy optimal control;power factor;microcontroller;three-phase AC-DC-AC power converter;AC machines;PWM-VSI bridges;drive-circuit;rectifier;inverter;software;memory demands;generator operation;reversing;state-of-the-art;performance;16 bit","","32","10","","","","","","IEEE","IEEE Conferences"
"PuMa, the first fully digital pulsar machine","P. C. van Haren; J. L. L. Voute; T. D. Beijaard; D. Driesens; M. L. A. Kouwenhoven; J. J. Langerak","Instrum. Groep Fysica, Utrecht Univ., Netherlands; NA; NA; NA; NA; NA","1999 IEEE Conference on Real-Time Computer Applications in Nuclear Particle and Plasma Physics. 11th IEEE NPSS Real Time Conference. Conference Record (Cat. No.99EX295)","","1999","","","55","61","Pulsars are neutron stars, rapidly rotating remains of supernova explosions, emitting bundles of broadband electromagnetic radiation. Researching these signals yields tests for fundamental physics theories and insight in the evolution of stars. Observing pulsars suffers from two hurdles. Typically, the signal-to-noise ratio is poor, requiring long observations and large bandwidths. Next there is dispersion, causing the pulsating signals to smear out and calls for narrow signal bands. Using many parallel narrow signal bands resolves this dilemma. Traditionally, pulsar machines use tens of parallel (analog) heterodyne receivers. Though impractical, it is desirable to have many more receivers. PuMa, the first Dutch pulsar machine, uses digital signal processing to split the incoming signal into thousands of narrow bands. The processor based design also increases loading the appropriate software into the signal processors. In total 192 SHARC processors (ADSP 21062) deliver the processing capacity. For PuMa a general purpose 6-processor SHARC board was developed, optimized for concurrent use of data busses. Other parts are commercially available components and all this equipment is connected in a VME environment. In mid-1998 PuMa was installed at the Westerbork Synthesis Radio Telescope in the Netherlands and its commissioning is completed.","","0-7803-5463","10.1109/RTCON.1999.842563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842563","","Signal processing;Neutrons;Explosions;Electromagnetic radiation;Testing;Physics;Signal to noise ratio;Bandwidth;Signal resolution;Digital signal processing","radiotelescopes;pulsars;radioastronomy;astronomy computing","PuMa;digital pulsar machine;rapidly rotating neutron stars;broadband EM radiation;signal-to-noise ratio;dispersion;pulsating signals;narrow signal bands;digital signal processing;software;signal processors;SHARC processors;ADSP 21062;data busses;VME environment;Westerbork Synthesis Radio Telescope;Netherlands","","","8","","","","","","IEEE","IEEE Conferences"
"Variable frequency closed-loop discrete pulse modulation for induction motor control","R. D. Horn; J. D. Birdwell","Dept. of Electr. & Comput. Eng., Tennessee Univ., Knoxville, TN, USA; Dept. of Electr. & Comput. Eng., Tennessee Univ., Knoxville, TN, USA","29th IEEE Conference on Decision and Control","","1990","","","3039","3044 vol.6","A variable-frequency closed-loop discrete pulse modulation system for variable-speed induction motor control has been implemented. An important component of this system is the decision tree IPWM (integral pulse width modulation) controller. The decision tree controller was designed to mimic a nonlinear optimization program by its construction from a training set composed of solutions to the optimization problem. The performance of this type of system was tested using a program which simulates the operation of a three-phase induction motor. Simulation results are presented and discussed.<<ETX>>","","","10.1109/CDC.1990.203348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=203348","","Frequency;Pulse modulation;Induction motors;Inverters;Decision trees;Application software;RLC circuits;Resonance;Circuit synthesis;Automatic generation control","closed loop systems;discrete systems;induction motors;machine control;pulse width modulation;variable speed gear","integral pulse width modulation;variable-frequency closed-loop discrete pulse modulation system;variable-speed induction motor control;decision tree controller;nonlinear optimization;three-phase induction motor","","","10","","","","","","IEEE","IEEE Conferences"
"Evolving binary classifiers through parallel computation of multiple fitness cases","S. Cagnoni; F. Bergenti; M. Mordonini; G. Adorni","Dipt. di Ingegneria dell'Informazione, Univ. of Parma, Italy; Dipt. di Ingegneria dell'Informazione, Univ. of Parma, Italy; Dipt. di Ingegneria dell'Informazione, Univ. of Parma, Italy; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2005","35","3","548","555","This paper describes two versions of a novel approach to developing binary classifiers, based on two evolutionary computation paradigms: cellular programming and genetic programming. Such an approach achieves high computation efficiency both during evolution and at runtime. Evolution speed is optimized by allowing multiple solutions to be computed in parallel. Runtime performance is optimized explicitly using parallel computation in the case of cellular programming or implicitly taking advantage of the intrinsic parallelism of bitwise operators on standard sequential architectures in the case of genetic programming. The approach was tested on a digit recognition problem and compared with a reference classifier.","1083-4419;1941-0492","","10.1109/TSMCB.2005.846671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430838","Cellular programming;genetic programming;multiple classifiers;pattern recognition","Concurrent computing;Computer aided software engineering;Genetic programming;Evolutionary computation;Parallel programming;Pattern recognition;Space exploration;Programming profession;Runtime;Parallel processing","pattern classification;genetic algorithms;cellular automata","binary classifiers;multiple fitness case parallel computation;evolutionary computation paradigms;cellular programming;genetic programming;evolution speed optimization;digit recognition problem;pattern recognition;cellular automata","Algorithms;Artificial Intelligence;Cluster Analysis;Computer Graphics;Computing Methodologies;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Logistic Models;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Signal Processing, Computer-Assisted;Subtraction Technique","7","26","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic detection and removal of inactive clauses in SAT with application in image computation","A. Gupta; Z. Yang; P. Ashar","CCRL, NEC Res. Inst., Princeton, NJ, USA; NA; NA","Proceedings of the 38th Design Automation Conference (IEEE Cat. No.01CH37232)","","2001","","","536","541","In this paper, we present a new technique for the efficient dynamic detection and removal of inactive clauses, i.e. clauses that do not affect the solutions of interest of a Boolean satisfiability (SAT) problem. The algorithm is based on the extraction of gate connectivity information during generation of the Boolean formula from the circuit, and its use in the inner loop of a branch-and-bound SAT algorithm. The motivation for this optimization is to exploit the circuit structure information, which can be used to find unobservable gates at circuit outputs under dynamic conditions. It has the potential to speed up all applications of SAT in which the SAT formula is derived from a logic circuit. In particular, we find that it has considerable impact on an image computation algorithm based on SAT. We present practical results for benchmark circuits which show that the use of this optimization consistently improves the performance for reachability analysis, in some cases enabling the prototype tool to reach more states than otherwise possible.","0738-100X","1-58113-297","10.1145/378239.379018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935566","","Computer applications;Automatic test pattern generation;Reachability analysis;Very large scale integration;Application software;National electric code;Logic circuits;Permission;Decision making;Logic gates","Boolean functions;computability;tree searching;logic gates;VLSI;circuit CAD;logic CAD;reachability analysis;formal verification","inactive clauses;SAT;Boolean satisfiability problem;gate connectivity information;branch-and-bound SAT algorithm;circuit structure information;unobservable gates;circuit outputs;logic circuit;image computation algorithm;benchmark circuits;reachability analysis;dynamic detection","","12","22","","","","","","IEEE","IEEE Conferences"
"Dynamic split: flexible border between instruction and data cache","P. Trancoso","Dept. of Comput. Sci., Univ. of Cyprus, Nicosia, Cyprus","8th Euromicro Conference on Digital System Design (DSD'05)","","2005","","","476","483","Current microprocessors are optimized for the average use. Nevertheless, it is known that different applications impose different demands on the system. This work focuses on the reconfiguration of the first-level caches. In order to achieve good performance, the first-level cache is split physically into two parts, one for instruction and one for data. This separation has the benefit of avoiding interference between instructions and data. Nevertheless, this separation is strict and determined at design-time. In this work we show a cache design that is able to change the split dynamically at runtime. The proposed design was tested using simulation of a variety of benchmark applications from the MiBench suite on two baseline architectures: embeddedXScale and high-end PowerPC. The results show that, while the average misses rate reduction may seem small; certain applications show a benefit larger than 90%. For miss rate reduction, the dynamic split cache seems to be more relevant for the cache with the smaller associativity (PowerPC). Lastly, the dynamic split cache was also used to reduce the energy consumption without loss of performance. This feature resulted in a significant energy reduction and the results showed that it has a bigger impact for the caches with larger associativity (42% energy reduction for the XScale and 28% for the PowerPC for a large data set size).","","0-7695-2433","10.1109/DSD.2005.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559842","","Energy consumption;Microprocessors;Computer science;Application software;Interference;Runtime;Benchmark testing;Performance loss;Cache memory;Energy efficiency","microprocessor chips;cache storage;embedded systems;computer architecture","microprocessors;first-level caches design;MiBench suite;baseline architecture;embeddedXScale;high-end PowerPC;dynamic split cache;energy consumption","","1","9","","","","","","IEEE","IEEE Conferences"
"'Defensive programming' in the rapid development of a parallel scientific program","D. Y. Cheng; J. T. Deutsch; R. W. Dutton","Dept. of Electr. Eng., Stanford Univ., CA, USA; Dept. of Electr. Eng., Stanford Univ., CA, USA; Dept. of Electr. Eng., Stanford Univ., CA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1990","9","6","665","669","A set of programming techniques which can reduce the time required to develop scientific programs is presented. Subroutines are designed to allow every execution path to be tested independently. Checksums are used to compress arrays and groups of variables without losing meaningful debugging information. Invariance assertions on checksums are used to detect incorrect behavior of a program at a place near the source of the error. Traces of checksums of groups of variables are used to monitor the time evolution of intermediate results. By comparing the checksums produced in tracing the program and the checksums produced in tracing a reference copy of the program, errors can be rapidly localized when a program is modified, ported, put in parallel, or optimized for a particular machine.<<ETX>>","0278-0070;1937-4151","","10.1109/43.55196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=55196","","Parallel programming;Debugging;Hardware;Laboratories;Computer bugs;Application software;Hypercubes;Switches;Algorithms;Testing","programming","subroutines;checksums;parallel scientific program;programming techniques","","1","41","","","","","","IEEE","IEEE Journals & Magazines"
"A reconfigurable VLSI coprocessing system for the block matching algorithm","A. Bugeja; W. Yang","Dept. of Electr. & Comput. Eng., Illinois Univ., Urbana, IL, USA; NA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","1997","5","3","329","337","Several VLSI architectures for the full-search block matching algorithm have been proposed in recent years due to its computation and I/O-intensive nature and its importance in various computer vision and image processing applications. This paper presents a new coarse grained reconfigurable coprocessor which is suitable for integration with general purpose microprocessors. The 180000 transistor custom VLSI design was implemented in 0.6 /spl mu/m CMOS on a 4.12 min/spl times/2.59 mm die and has been fully tested up to 33 MHz. For a typical image database search application, a sample system consisting of four coprocessors interfaced through a 33 MHz PCI bus will provide a speedup of 320/spl times/ over an 80486 DX2/66 MHz and 64/spl times/ over a 150-MHz Pentium running fully optimized assembly code.","1063-8210;1557-9999","","10.1109/92.609876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=609876","","Very large scale integration;Computer vision;Coprocessors;Computer architecture;Image processing;Application software;Microprocessors;Testing;Image databases;Assembly systems","reconfigurable architectures;VLSI;coprocessors;image matching;computer vision;application specific integrated circuits","reconfigurable VLSI coprocessing system;block matching algorithm;VLSI architectures;image processing applications;computer vision;coarse grained reconfigurable coprocessor;custom VLSI design;image database search application;0.6 micron;33 MHz","","11","10","","","","","","IEEE","IEEE Journals & Magazines"
"An environment for evaluating architectures for spatially mapped computation: System architecture and initial results","M. C. Herbordt; C. C. Weems","Dept. of Comput. Sci., Massachusetts Univ., MA, USA; NA","1993 Computer Architectures for Machine Perception","","1993","","","191","201","An environment which addresses several problems in evaluating massively parallel array architectures is described. A realistic workload including a series of applications currently being used as building blocks in vision research has been constructed. Both flexibility in architectural parameter selection and simulation efficiency are maintained by combining virtual machine emulation with trace driven simulation. The tradeoff between fairness to diverse target architectures and programmability of the test programs is addressed through the use of operator and application libraries. Initial results are presented indicating the appropriate balance between register file and cache to optimize performance under varying levels of processor element virtualization.","","0-8186-5420","10.1109/CAMP.1993.622473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622473","","Computer architecture;Benchmark testing;Computer vision;Virtual machining;Concurrent computing;Computer science;Application software;Military computing;Contracts;Machine vision","parallel architectures","spatially mapped computation;massively parallel array architectures;realistic workload;vision research;architectural parameter selection;simulation efficiency;virtual machine emulation;trace driven simulation;diverse target architectures;application libraries;register file;cache;processor element virtualization","","3","25","","","","","","IEEE","IEEE Conferences"
"The performance realities of massively parallel processors: a case study","O. M. Lubeck; M. L. Simmons; H. J. Wasserman","Los Alamos Nat. Lab., NM, USA; Los Alamos Nat. Lab., NM, USA; Los Alamos Nat. Lab., NM, USA","Supercomputing '92:Proceedings of the 1992 ACM/IEEE Conference on Supercomputing","","1992","","","403","412","The authors present the results of an architectural comparison of SIMD (single-instruction multiple-data) massive parallelism, as implemented in the Thinking Machines Corp. CM-2, and vector or concurrent-vector processing, as implemented in the Cray Research Inc., Y-MP/8. The comparison is based primarily upon three application codes taken from the LANL (Los Alamos National Laboratory) CM-2 workload. Tests were run by porting CM Fortran codes to the Y-MP, so that nearly the same level of optimization was obtained on both machines. The results for fully configured systems, using measured data rather than scaled data from smaller configurations, show that the Y-MP/8 is faster than the 64 k CM-2 for all three codes. A simple model that accounts for the relative characteristic computational speeds of the two machines, and reduction in overall CM-2 performance due to communication or SIMD conditional execution, accurately predicts the performance of two of the three codes. The authors show the similarity of the CM-2 and Y-MP programming models and comment on selected future massively parallel processor designs.<<ETX>>","","0-8186-2630","10.1109/SUPERC.1992.236663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=236663","","Computer aided software engineering;Supercomputers;Concurrent computing;Bandwidth;Computer architecture;Laboratories;Parallel processing;Testing;Predictive models;Government","parallel processing;performance evaluation;vector processor systems","vector processing;performance;massively parallel processors;architectural comparison;SIMD;single-instruction multiple-data;massive parallelism;CM-2;concurrent-vector processing;Y-MP/8;CM Fortran codes;fully configured systems;measured data;scaled data;programming models","","6","30","","","","","","IEEE","IEEE Conferences"
"Reactive power control by least squares minimization","R. R. Shoults; Mo-Shing Chen","University Computing Company, Dallas, Texas; NA","IEEE Transactions on Power Apparatus and Systems","","1976","95","1","325","334","An effective method is presented for performing reactive power control in a bulk power supply system by optimally rescheduling generator terminal voltage magnitudes and/or transformer tap settings. The optimization procedure is the familar least squares minimization technique and is suitable for on-line application in energy control center computers. Sensitivity factors provide the basis for the control function and are developed from a linearized reactive power model. Important features of this method include a model approximating the reactive power flow through a load-tap-changing (LTC) transformer and a Norton's equivalent circuit model approximating the reactive power output of a regulating generator. As a result of the linear models developed, optimally ordered triangular factorization is fully exploited to enhance computational speed and efficiency. Test results are presented to demonstrate the effectiveness of the method. A linearized active power model is briefly presented to illustrate its dual relationship to the linearized reactive power model. This is an important relationship, enabling both active and reactive power control by application of the same general model.","0018-9510","","10.1109/T-PAS.1976.32109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1601711","","Reactive power control;Least squares methods;Reactive power;Minimization;Power generation;Least squares approximation;Power supplies;Application software;Equivalent circuits;Circuit testing","","","","16","15","","","","","","IEEE","IEEE Journals & Magazines"
"Parameterized Module Generator for an FPGA-Based Electronic Cochlea","M. P. Leong; C. T. Jin; P. H. W. Leong","Chinese University of Hong Kong; NA; NA","The 9th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM'01)","","2001","","","21","30","An FPGA-based implementation of Lyon and Mead's electronic cochlea filter and its application to a real-time cochleagram display are presented. Compared with analog VLSI implementations, an FPGA implementation offers shorter design time, improved dynamic range, higher accuracy and a simpler computer interface. The FPGA cochlea filter is generated by a tool which takes filter coefficients to compile an application-optimized design with arbitrary precision. In the process of compilation, the tool can use simulation test vectors in order to determine the appropriate scaling for each filter. The resulting model can be used as an accelerator for cochlea model research or as the front end for embedded auditory signal processing systems.","","0-7695-2667","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420898","","Filters;Field programmable gate arrays;Application software;Computer displays;Very large scale integration;Dynamic range;Computer interfaces;Computational modeling;Testing;Signal processing","","","","","23","","","","","","IEEE","IEEE Conferences"
"Minimal loss reconfiguration based on simulated annealing meta-heuristic","L. G. Santander; F. A. Chacra; H. Opazo; E. Lopez","Dept. of Electr. Eng., Univ. of Concepcion, Chile; NA; NA; NA","15th International Conference on Electronics, Communications and Computers (CONIELECOMP'05)","","2005","","","95","99","A method for minimal losses in primary distribution systems by changing topology is presented The loss minimization in distribution systems is based on simulated annealing (SA) and radial power flow. SA's major advantage over other methods is an ability to avoid becoming trapped at local minima. The SA algorithm does not require or deduce derivative information, it merely needs to be supplied with an objective function for each trial solution it generates. Thus, the evaluation of the problem functions is essentially a 'black box' operation as far as the optimization algorithm is concerned, which is the most convenient with large real systems such as electrical networks. The model is useful for planning and operating in primary distribution systems. Through different test network applications, the method was validated and compared to other proposed methods.","","0-7695-2283","10.1109/CONIEL.2005.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488542","","Simulated annealing;Load flow;Switches;Piecewise linear approximation;Voltage;Topology;Electron traps;Testing;Application software;Computer networks","simulated annealing;minimisation;power distribution planning;network topology","minimal loss reconfiguration;simulated annealing meta-heuristic;primary distribution systems;topology;loss minimization;radial power flow;planning","","1","34","","","","","","IEEE","IEEE Conferences"
"Affine reconstruction of curved surfaces from uncalibrated views of apparent contours","J. Sato; R. Cipolla","Dept. of Eng., Cambridge Univ., UK; NA","Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)","","1998","","","715","720","In this paper, we show that even if the camera is uncalibrated, and its translational motion is unknown, curved surfaces can be reconstructed from their apparent contours up to a 3D affine ambiguity. Furthermore, we show that even if the reconstruction is nonmetric (non-Euclidean), we can still extract useful information for many computer vision applications just from the apparent contours. We first show that if the camera undergoes pure translation (unknown direction and magnitude), the epipolar geometry can be recovered from the apparent contours without using any search or optimisation process. The extracted epipolar geometry is next used for reconstructing curved surfaces from the deformations of the apparent contours viewed from uncalibrated cameras. The result is applied to distinguishing curved surfaces from fixed features in images. It is also shown that the time-to-contact to the curved surfaces can be computed from simple measurements of the apparent contours. The proposed method is implemented and tested on real images of curved surfaces.","","81-7319-221","10.1109/ICCV.1998.710796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=710796","","Surface reconstruction;Cameras;Image reconstruction;Geometry;Computer vision;Application software;Testing;Calibration","computer vision;image reconstruction","affine reconstruction;curved surfaces;uncalibrated views;apparent contours;3D affine ambiguity;computer vision;epipolar geometry","","2","18","","","","","","IEEE","IEEE Conferences"
"Discriminative training based on frame level likelihood normalization and its application for speech and speaker recognition","K. P. Markov; K. Hanai; S. Nakagawa","Res. Eng. Dept., ATR-I, Kyoto, Japan; NA; NA","IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)","","1999","1","","532","537 vol.1","We present a method for discriminative estimation of parameters of Gaussian distribution based classifiers and its application to speech and speaker recognition. The objective of this method is to maximize the normalized likelihood of the design samples. In contrast to other discriminative algorithms such as minimum classification error/generalized probabilistic descent (MCE/GPD) and maximum mutual information (MMI), the objective function is optimized using a modified expectation-maximization (EM) algorithm. The evaluation experiments using both clean and telephone speech showed improvement of the recognition rates compared to the maximum likelihood estimation (MLE) training method, especially when the mismatch between the training and testing conditions is significant. Compared with the MCE/GPD discriminative method, our algorithm showed better performance in both the speech and speaker recognition tasks.","1062-922X","0-7803-5731","10.1109/ICSMC.1999.814148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=814148","","Speech recognition;Speaker recognition;Application software;Maximum likelihood estimation;Mutual information;Parameter estimation;Telephony;Speech analysis;Testing;Probability","Gaussian distribution;speaker recognition;maximum likelihood estimation","discriminative training;frame level likelihood normalization;discriminative estimation;Gaussian distribution based classifiers;normalized likelihood;minimum classification error;generalized probabilistic descent;maximum mutual information;objective function;modified expectation-maximization algorithm;telephone speech;clean speech;recognition rates;maximum likelihood estimation","","","11","","","","","","IEEE","IEEE Conferences"
"Computer-based learning environment for the teaching of logic synthesis","N. L. K. Lester; E. L. Dagless","Dept. of Electr. Eng., Bristol Univ., UK; NA","IEE Colloquium on Digital System Design Using Synthesis Techniques (Digest No: 1996-029)","","1996","","","4/1","4/4","The Teaching and Learning Technology Project (TLTP) Electronic Design Education Consortium is a group of 8 Universities whose aim is to develop computer based learning (CBL) material (courseware) to support the education of Electronic Engineers and Computer Scientists in the design of electronic circuits and systems. The use of CBL allows use of colour graphics and animation to enhance the material. It allows the demonstration of principles which would otherwise be difficult to teach. There are three important aims regarding the production of this material: it should enhance the learning experience of the student, increase the number of students that a lecturer can support, and improve the use of the industry standard CAD software that is available under the ECAD and EUROCHIP initiatives. The work was divided into 4 themes, namely, Electronic Circuit Design, Digital Design, System and High Level Design and Testing and Design for Test. The System and High Level Design theme is known as Theme C which is divided into topics relating to high-level design, specification, synthesis, logic optimization and FSM synthesis. This paper will mainly concentrate on the production of logic optimization and technology mapping CBL material. This paper will discuss the environment, how courseware is developed and how CAD tools are integrated into the system.","","","10.1049/ic:19960165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=578432","","Courseware","courseware","computer-based learning;logic synthesis;Electronic Design Education Consortium;courseware;colour graphic;animation;technology mapping;logic optimization;CAD tools","","1","","","","","","","IET","IET Conferences"
"Web-technology of university's evaluation rating","I. A. Botygin; I. I. Kalyatsky","Tomsk Polytech. Univ., Russia; Tomsk Polytech. Univ., Russia","7th Korea-Russia International Symposium on Science and Technology, Proceedings KORUS 2003. (IEEE Cat. No.03EX737)","","2003","2","","410","415 vol.2","At present there is no a unified standard system of both qualitative and quantitative criteria to estimate objectively the conformity of the University activities and its infrastructure with the requirements of the public state attestation and accreditation. So the use of heuristic and expert procedures is considered to be necessary. In the suggested model of the University (institute, faculty, department) rating, the fragments of the models used by the Ministry of Education of Russia, the Association for Technical Universities of Russia, the Independent Accreditation Center, the advanced technical universities of Russia, as well as long-term developments of the Tomsk Polytechnic University in quality control of the University activities and its departments, modern tendencies and priorities of the universities developments, are accepted as the basic ones. 55 indices have been determined. Numerical values of indices are normalized on the maximum value achieved in the objects under consideration that provides a commensurability of indices by their essential spread in various objects. The data convolution by sections and subsections is carried out linearly taking into account their importance. The value of the index is determined by an expert method taking into account its relative importance within the limits of each section (subsection). The numerical value of each rating index is determined by its relation to the ""base"". The ""base"" is determined as the on-budget salary fund of the University (institute, faculty, department) staff, providing educational process, and the on-budget salary fund of the staff carrying out state budgetary research. The software for the collective interactive remote input of the indices, as well as for the centralized calculation and operative analysis of the universities rating has been developed. The server Apache, the database server MySQL, and the scripting language PHP have been used for the above work to be carried out.","","89-7868-617","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222647","","Accreditation;State estimation;Remuneration;Convolution;Databases;Region 8;Engineering education;Educational institutions;Statistics;Testing","Web sites;quality control;educational institutions;budgeting data processing;operations research","web-technology;qualitative criteria;quantitative criteria;university activities;long-term developments;quality control;data convolution;educational process;on-budget salary;state budgetary research;rating index;centralized calculation;operative analysis;scripting language","","","","","","","","","IEEE","IEEE Conferences"
"Analysis of preemptive periodic real-time systems using the (max, plus) algebra with applications in robotics","F. Baccelli; B. Gaujal; D. Simon","ENS, Paris, France; NA; NA","IEEE Transactions on Control Systems Technology","","2002","10","3","368","380","We present the model of a system of periodic real-time tasks with fixed priorities, preemption and synchronization, performed by a robot controller, using marked graphs. Then, with the help of the (max, plus) algebra, we derive simple tests to check real-time constraints on those tasks such as response times and the respect of deadlines. This method takes into account the precedence and synchronization constraints and is not limited to a particular scheduling policy.","1063-6536;1558-0865;2374-0159","","10.1109/87.998024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998024","","Real time systems;Algebra;Delay;Synchronization;Robot control;Computational complexity;Performance analysis;Testing;Clocks;Time measurement","robots;synchronisation;periodic control;real-time systems;control system analysis computing;graph theory","fixed priority preemption;marked graphs;max plus algebra;periodic systems;real-time systems;synchronization;robotics;ORCCAD software;response times","","6","19","","","","","","IEEE","IEEE Journals & Magazines"
"Teenagers identify causes of violence in schools and develop strategies to eliminate violence using GroupSystems electronic meeting system (EMS)","B. F. Marsh","Comput. Sci. Corp., Huntsville, AL, USA","Proceedings of the 32nd Annual Hawaii International Conference on Systems Sciences. 1999. HICSS-32. Abstracts and CD-ROM of Full Papers","","1999","Track1","","11 pp.","","Is it possible to engage teenagers in a serious effort to identify the root causes of school violence and to develop strategies to deal with it? If so, will computer-aided group decision support tools add value to the process? Those are the questions we addressed with the 1998 Teen Think Tank on School Violence. While this was neither a formal nor a scientific treatment of the subject, the results of the initial experiment were overwhelmingly impressive. Using the GroupSystems Electronic Meeting System (EMS), sixteen teenagers grappled with the issue of school violence and generated more than 800 different ways to predict, prevent, avoid, protect, react, eliminate, or cope with youth violence. After brainstorming for ideas, they also used EMS to categorize, prioritize, and to reach consensus about their best ideas. Then they developed teen recommendations for students, parents, teachers, school administrators, and law enforcement officers. This was all accomplished in two EMS sessions; and none of the students had any prior knowledge or experience with EMS. This report presents a synopsis of their findings and a brief description of the EMS process.","","0-7695-0001","10.1109/HICSS.1999.772733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772733","","Medical services;Educational institutions;Protection;Law enforcement;Software systems;Productivity;Testing;Meetings","group decision support systems;social aspects of automation;teleconferencing","GroupSystems electronic meeting system;school violence;computer-aided group decision support tools;brainstorming","","","","","","","","","IEEE","IEEE Conferences"
"Wavelet analysis to fabric defects detection in weaving processes","Sungshin Kim; Man Hung Lee; Kwang-Bang Woo","Dept. of Electr. Eng., Pusan Nat. Univ., South Korea; NA; NA","ISIE '99. Proceedings of the IEEE International Symposium on Industrial Electronics (Cat. No.99TH8465)","","1999","3","","1406","1409 vol.3","The current procedure for the determination of fabric defects in the textile industry is performed by humans in the offline stage. The advantage of an online inspection system is not only defect detection and identification, but also quality improvement by a feedback control loop to adjust setpoints. This paper introduces a vision-based online fabric inspection methodology of woven textile fabrics. The proposed inspection system consists of hardware and software components. The hardware components consist of CCD array cameras, a frame grabber and appropriate illumination. The software routines capitalize upon vertical and horizontal scanning algorithms to reduce the 2-D image into a stream of 1-D data. The wavelet transform is used next to extract features that are characteristic of a particular defect. The signal to noise ratio (SNR) calculation based on the results of the wavelet transform is performed to measure any defects. The defect declaration is carried out by employing SNRs and scanning methods. Learning routines are called upon to optimize the wavelet coefficients. Test results from different types of defect and different styles of fabric demonstrate the effectiveness of the proposed inspection system.","","0-7803-5662","10.1109/ISIE.1999.796918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=796918","","Wavelet analysis;Fabrics;Weaving;Inspection;Hardware;Wavelet transforms;Signal to noise ratio;Textile industry;Humans;Feedback control","textile industry;automatic optical inspection;feature extraction;feedback;process control;computer vision;wavelet transforms;learning (artificial intelligence)","weaving processes;fabric defects detection;wavelet analysis;textile industry;online inspection system;defect identification;CCD array cameras;frame grabber;software routines;wavelet transform;signal to noise ratio;SNRs;scanning methods;learning routines;scanning algorithms;feature extraction","","11","7","","","","","","IEEE","IEEE Conferences"
"Research of 2-DOF planar parallel high speed/high accuracy robot","Chu ZhongYi; Qu DongSheng; Sun LiNing; Cui Jing","Robotics Inst., Harbin Inst. of Technol., China; Robotics Inst., Harbin Inst. of Technol., China; Robotics Inst., Harbin Inst. of Technol., China; Robotics Inst., Harbin Inst. of Technol., China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","","2004","6","","4715","4719 Vol.6","Reports a novel high-speed/high-accuracy robot: two permanent, linear, DC servomotors drive a 2-DOF planar parallel mechanism to realize high-speed/high-accuracy planar motion. In the design of the planar parallel mechanism, considering the impacted structure and suppressing residual vibration, we separately adopt simulated annealing algorithm and finite element analysis software ANSYS to optimize mechanism's geometrical dimension and section parameter. Test presents that the planar parallel mechanism being optimized includes high stiffness and the robot's settling time is decided by linear motors. At last, in order to reduce influence of nonlinear factors (force ripple, load's disturbance and so on) on the settling time of linear motor's set-point control, the paper proposes a novel fuzzy self-tuning PID controller. Experiments show that linear motor's settling time has been shortened significantly after the novel fuzzy self-tuning PID controller is applied in the system.","","0-7803-8273","10.1109/WCICA.2004.1343533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1343533","","Parallel robots;Algorithm design and analysis;Force control;Fuzzy control;Fuzzy sets;Three-term control;Servomotors;Solid modeling;Analytical models;Simulated annealing","servomotors;linear motors;simulated annealing;finite element analysis;control engineering computing;machine control;three-term control;self-adjusting systems;fuzzy control;DC motor drives;vibration control;control nonlinearities;control system synthesis;manipulators","2-DOF planar parallel high speed robot;high accuracy robot;permanent linear DC servomotors;planar parallel mechanism design;impacted structure;suppressing residual vibration;simulated annealing algorithm;finite element analysis software;ANSYS;linear motor set-point control;fuzzy self-tuning PID controller","","4","8","","","","","","IEEE","IEEE Conferences"
"A second generation microprocessor line protection relay","D. M. Peck; F. Engler; I. De Mesmaeker","NA; NA; NA","1989 Fourth International Conference on Developments in Power Protection","","1989","","","200","204","The authors describe a line protection with distance relay and auxiliary functions, having an optimized hardware and software structure, for power transmission and sub-transmission as well as distribution applications. The development of this relay has evolved from an earlier prototype line protection equipment which was based on a single Intel 8086 16-bit microprocessor hardware and programmed in assembler language. The protection algorithms for the new line protection relay have thereby been thoroughly proven through many successful laboratory and field tests. The firmware and menu-driven man-machine interface (MMI) concepts are the same as for a new bus-oriented multi-processor generator protection system. Since the line and generator protection equipment share a common firmware concept, all available protection or measurement functions are compatible with both equipments. Through the use of continuous self-monitoring and self-testing, any hardware failures can be immediately reported, thereby reducing the probability of relay maloperation to virtually zero.<<ETX>>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=20642","","Microcomputer applications;Power system protection;Protective relaying","microcomputer applications;power engineering computing;power system protection;relay protection","microprocessor line protection relay;distance relay;protection algorithms;firmware;menu-driven man-machine interface;bus-oriented multi-processor generator protection system;continuous self-monitoring;self-testing;hardware failures","","","","","","","","","IET","IET Conferences"
"Detecting cartoons: a case study in automatic video-genre classification","T. I. Ianeva; A. P. de Vries; H. Rohrig","Dept. de Informatica, Valencia Univ., Spain; NA; NA","2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)","","2003","1","","I","449","This paper presents a new approach for classifying individual video frames as being a 'cartoon' or a 'photographic image'. The task arose from experiments performed at the TREC-2002 video retrieval benchmark: 'cartoons' are returned unexpectedly at high ranks even if the query gave only 'photographic' image examples. Distinguishing between the two genres has proved difficult because of their large intra-class variation. In addition to image metrics used in prior cartoon-classification work, we introduce novel metrics like ones based on the pattern spectrum of parabolic size distributions derived from parabolic granulometries and the complexity of the image signal approximated by its compression ratio. We evaluate the effectiveness of the proposed feature set for classification (using support vector machines) on a large set of keyframes from the TREC-2002 video track collection and a set of Web images. The paper reports the identification error rates against the number of images used as training set. The system is compared with one that classifies Web images as photographs or graphics and its superior performance is evident.","","0-7803-7965","10.1109/ICME.2003.1220951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1220951","","Computer aided software engineering;Image retrieval;Error analysis;Support vector machines;Support vector machine classification;Graphics;Testing;World Wide Web;Large-scale systems;Information retrieval","video signal processing;image retrieval;support vector machines;feature extraction","video frames;cartoon;photographic image;video retrieval benchmark;support vector machines;parabolic granulometries;video track collection;Web images;image signal","","23","19","","","","","","IEEE","IEEE Conferences"
"A methodology for comparing fault tolerant computers","L. S. DeBrunner; F. G. Gray","Sch. of Electr. Eng., Oklahoma Univ., Norman, OK, USA; NA","[1992] Conference Record of the Twenty-Sixth Asilomar Conference on Signals, Systems & Computers","","1992","","","999","1003 vol.2","A general methodology for comparing fault tolerant computers using a hierarchical taxonomy of pertinent issues is described. The uppermost level taxonomy consists of reliability, specialized fault detection, performance, flexibility, and communication issues. Each class is further decomposed into more specific topics. The methodology consists of studying each issue in turn to determine which candidate architecture(s) best support that issue. The results are then combined using a user defined function to rank the architectures. A design space technique is described to aid in drawing the comparisons. The technique is illustrated by considering the fault tolerant capabilities of the JPL hypercube and the Encore Multimax.<<ETX>>","1058-6393","0-8186-3160","10.1109/ACSSC.1992.269070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=269070","","Fault tolerance;Redundancy;Hardware;Fault tolerant systems;System testing;Taxonomy;Computer architecture;Fault diagnosis;Software algorithms;Fault detection","fault tolerant computing;hypercube networks;performance evaluation","fault tolerant computers;hierarchical taxonomy;reliability;fault detection;user defined function;design space technique;JPL hypercube;Encore Multimax","","","8","","","","","","IEEE","IEEE Conferences"
"Multielement CdTe stack detectors for gamma-ray spectroscopy","R. Redus; A. Huber; J. Pantazis; T. Pantazis; T. Takahashi; S. Woolf","Amptek. Inc., Bedford, MA, USA; Amptek. Inc., Bedford, MA, USA; Amptek. Inc., Bedford, MA, USA; Amptek. Inc., Bedford, MA, USA; NA; NA","2003 IEEE Nuclear Science Symposium. Conference Record (IEEE Cat. No.03CH37515)","","2003","5","","3361","3365 Vol.5","This paper describes the development of a field portable gamma-ray spectroscopy system based on multielement CdTe detectors. A stack of multiple planar detector elements is used, with each individual element thin enough for good charge transport but with the outputs summed so that the full volume is used. The detector elements are CdTe diodes mounted on a thermoelectric cooler, providing low noise and good charge transport with relatively simple electronics. It is anticipated that the detector will have a resolution of < 1% FWHM at 662 keV with a volume of at least 0.5 cm/sup 3/ and that the approach will be suitable for production. To optimize the detector configuration, simulation software was developed and test devices fabricated. This paper will present simulation results and experimental data obtained using multielement detectors.","1082-3654","0-7803-8257","10.1109/NSSMIC.2003.1352626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1352626","","Gamma ray detection;Gamma ray detectors;Spectroscopy;Instruments;Thermoelectricity;Production;Radiation detectors;Packaging;Semiconductor diodes;Semiconductor device noise","semiconductor counters;gamma-ray spectroscopy;semiconductor diodes;circuit noise;nuclear electronics;gamma-ray apparatus","multielement CdTe stack detectors;gamma-ray spectroscopy;field portable gamma-ray spectroscopy system;multiple planar detector elements;charge transport;summed outputs;CdTe diodes;thermoelectric cooler;low noise;electronics;detector configuration;simulation software development;test device fabrication;662 keV","","","12","","","","","","IEEE","IEEE Conferences"
"High-level synthesis of control and memory intensive communication systems","A. Hemani; B. Svantesson; P. Ellervee; A. Postula; J. Oberg; A. Jantsch; H. Tenhunen","Dept. of Electron., R. Inst. of Technol., Kista, Sweden; NA; NA; NA; NA; NA; NA","Proceedings of Eighth International Application Specific Integrated Circuits Conference","","1995","","","185","191","Communication sub-systems that deal with switching, routing and protocol implementation often have their functionality dominated by control logic and interaction with memory. Synthesis of such Control and Memory Intensive Systems (hereafter abbreviated to CMISTs) poses demands that in the past have not been met satisfactorily by general purpose high-level synthesis (HLS) tools and have led to several research efforts to address these demands. In this paper we: characterise CMISTs from the synthesis viewpoint; present a synthesis methodology adapted for CMISTs; present the Operation and Maintenance (OAM) Protocol of the ATM, its modelling in VHDL and synthesis aspects of the VHDL model; present the results of applying the synthesis methodology to the OAM as a test case-the results are compared to that obtained using the not adapted general purpose High-level synthesis tool; prove the efficacy of the proposed synthesis methodology by applying it to an industrial design and comparing our results to the results from two commercial HLS tools and to the results obtained by designing manually at register-transfer level.","1063-0988","0-7803-2707","10.1109/ASIC.1995.580711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=580711","","High level synthesis;Communication system control;Logic design;Control system synthesis;Multiplexing;Design optimization;Australia;Communication switching;Routing protocols;Application software","high level synthesis","high-level synthesis;control and memory intensive communication systems;ASICs;switching;routing;protocols;CMISTs;OAM Protocol;VHDL model;industrial design;register-transfer level;ATM","","3","20","","","","","","IEEE","IEEE Conferences"
"Comparative evaluation of visualization and experimental results using image comparison metrics","Hualin Zhou; Min Chen; M. F. Webster","Univ. of Wales, Swansea, UK; Univ. of Wales, Swansea, UK; Univ. of Wales, Swansea, UK","IEEE Visualization, 2002. VIS 2002.","","2002","","","315","322","Comparative evaluation of visualization and experimental results is a critical step in computational steering. In this paper, we present a study of image comparison metrics for quantifying the magnitude of difference between visualization of a computer simulation and a photographic image captured from an experiment. We examined eleven metrics, including three spatial domain, four spatial-frequency domain and four HVS (human-vision system) metrics. Among these metrics, a spatial-frequency domain metric called 2nd-order Fourier comparison was proposed specifically for this work. Our study consisted of two stages: base cases and field trials. The former is a general study on a controlled comparison space using purposely selected data, and the latter involves imagery results from computational fluid dynamics and a rheological experiment. This study has introduced a methodological framework for analyzing image-level methods used in comparative visualization. For the eleven metrics considered, it has offered a set of informative indicators as to the strengths and weaknesses of each metric. In particular, we have identified three image comparison metrics that are effective in separating ""similar"" and ""different"" image groups. Our 2nd-order Fourier comparison metric has compared favorably with others in two of the three tests, and has shown its potential to be used for steering computer simulation quantitatively.","","0-7803-7498","10.1109/VISUAL.2002.1183790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183790","","Visualization;Computer simulation;Rheology;Application software;Humans;Computer graphics;Image processing;Computer vision;Design optimization;Lighting","data visualisation;digital simulation;computational fluid dynamics;rheology;image processing","visualization;comparative evaluation;computational steering;image comparison metrics;computer simulation;photographic image;spatial domain metrics;spatial-frequency domain metrics;human vision system metrics;2nd-order Fourier comparison;base cases;field trials;controlled comparison space;imagery;computational fluid dynamics;rheological experiment","","12","26","","","","","","IEEE","IEEE Conferences"
"On-chip traffic modeling and synthesis for MPEG-2 video applications","G. V. Varatkar; R. Marculescu","Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA; Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2004","12","1","108","119","The objective of this paper is to introduce self-similarity as a fundamental property exhibited by the bursty traffic between on-chip modules in typical MPEG-2 video applications. Statistical tests performed on relevant traces extracted from common video clips establish unequivocally the existence of self-similarity in video traffic. Using a generic tile-based communication architecture, we discuss the implications of our findings on on-chip buffer space allocation and present quantitative evaluations for typical video streams. We also describe a technique for synthetically generating traces having statistical properties similar to those obtained from real video clips. Our proposed technique speeds up buffer simulations, allows media system designers to explore architectures rapidly and use large media data benchmarks more efficiently. We believe that our findings open new directions of research with deep implications on some fundamental issues in on-chip networks design for multimedia applications.","1063-8210;1557-9999","","10.1109/TVLSI.2003.820523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1263562","","Traffic control;Network-on-a-chip;System-level design;Multimedia systems;Telecommunication traffic;System-on-a-chip;Home appliances;Design optimization;Application software","multimedia communication;multimedia systems;telecommunication traffic;video coding;modelling","on-chip traffic modeling;MPEG-2 video applications;bursty traffic;on-chip modules;video clips;video traffic;tile based communication architecture;on-chip buffer space allocation;buffer simulations;media system designers;media data benchmarks;on-chip networks design;multimedia applications","","98","36","","","","","","IEEE","IEEE Journals & Magazines"
"The DASH prototype: Logic overhead and performance","D. Lenoski; J. Laudon; T. Joe; D. Nakahira; L. Stevens; A. Gupta; J. Hennessy","Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA","IEEE Transactions on Parallel and Distributed Systems","","1993","4","1","41","61","The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multiprocessors with hardware cache coherence. The hardware overhead of directory-based cache coherence in a 48-processor is examined. The data show that the overhead is only about 10-15%, which appears to be a small cost for the ease of programming offered by coherent caches and the potential for higher performance. The performance of the system is discussed, and the speedups obtained by a variety of parallel applications running on the prototype are shown. Using a sophisticated hardware performance monitor, the effectiveness of coherent caches and the relationship between an application's reference behavior and its speedup are characterized. The optimizations incorporated in the DASH protocol are evaluated in terms of their effectiveness on parallel applications and on atomic tests that stress the memory system.<<ETX>>","1045-9219;1558-2183;2161-9883","","10.1109/71.205652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205652","","Prototypes;Logic;Large-scale systems;Hardware;Parallel architectures;Software prototyping;Costs;Protocols;Parallel programming;Scalability","buffer storage;parallel programming;performance evaluation;shared memory systems;storage management","DASH project;large-scale shared-memory multiprocessors;directory-based cache coherence;coherent caches;hardware performance monitor;reference behavior;DASH protocol;atomic tests","","63","31","","","","","","IEEE","IEEE Journals & Magazines"
"A novel design technology for next generation ubiquitous computing architectures","C. Nitsch; C. Lara; U. Kebschull","Dept. of Comput. Sci., Leipzig Univ., Germany; Dept. of Comput. Sci., Leipzig Univ., Germany; Dept. of Comput. Sci., Leipzig Univ., Germany","Proceedings International Parallel and Distributed Processing Symposium","","2003","","","8 pp.","","Modem applications for mobile computing require high performance architectures. On the other hand, there are restrictions such as storage or power consumption. The use of reconfigurable logic allows to design low power yet potent architectures. Storage restrictions of small devices can be solved by using external servers to store and manage data. We have developed and tested a novel versatile digital architecture (VDA) to design inexpensive, but very flexible devices for mobile ubiquitous computing. A single VDA compliant device can be used as a mobile phone, video player, database, etc. In contrast to state of the art architectures, our approach offers new key facilities such as application download on demand and on-the-fly update of hardware acceleration components. These updates can be controlled by an external server or by the client itself. Furthermore we are able to monitor any internal signal of hardware components remotely for error detection and performance optimization. Finally our architecture offers a powerful server side layer for management of hardware and software components and user data.","1530-2075","0-7695-1926","10.1109/IPDPS.2003.1213355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213355","","Ubiquitous computing;Computer architecture;Hardware;Mobile computing;Modems;Computer applications;High performance computing;Pervasive computing;Energy consumption;Reconfigurable logic","embedded systems;ubiquitous computing;mobile computing;reconfigurable architectures","design technology;ubiquitous computing architectures;mobile computing;high performance architectures;reconfigurable logic;potent architectures;versatile digital architecture;hardware acceleration components;error detection;performance optimization;software components;hardware components","","1","19","","","","","","IEEE","IEEE Conferences"
"Obtaining a fuzzy controller with high interpretability in mobile robots navigation","M. Mucientes; J. Casillas","Dept. of Electron. & Comput. Sci., Santiago de Compostela Univ., Spain; NA","2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)","","2004","3","","1637","1642 vol.3","The work presents the design of a fuzzy controller for the wall-following behavior in mobile robotics using the COR (cooperative rules) methodology with ant colony optimization. The system has been tested in several simulated environments using the Nomad 200 robot software, and compared with other controller based on genetic algorithms. The proposed approach obtains a highly interpretable knowledge base in a reduced time, and the designer only has to define the number of membership functions and the universe of discourse of each variable.","1098-7584","0-7803-8353","10.1109/FUZZY.2004.1375426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1375426","","Fuzzy control;Mobile robots;Navigation;Robot sensing systems;Control systems;Computer science;Uncertainty;Force control;Velocity control;Input variables","control system synthesis;fuzzy control;mobile robots;cooperative systems;multi-robot systems;spatial variables control;genetic algorithms","fuzzy controller;mobile robots navigation;wall-following behavior;cooperative rules methodology;ant colony optimization;Nomad 2000 robot software;genetic algorithm","","4","10","","","","","","IEEE","IEEE Conferences"
"Concurrent execution of communication protocols in high speed networks","M. Rupprecht; C. Engel","Dept. of Comput. Sci. IV, Aachen Univ. of Technol., Germany; Dept. of Comput. Sci. IV, Aachen Univ. of Technol., Germany","[Conference Record] SUPERCOMM/ICC '92 Discovering a New World of Communications","","1992","","","164","168 vol.1","A method based on a higher Petri net is introduced for implementing highspeed protocols. It provides a very efficient implementation on a multiple processor system as well as formal methods for analyzing and improving the software. A formal protocol specification technique has been modified and is presented. It leads to a simple extension of conventional sequential programming languages and two new methods of optimizing the granularity of the parallel program.<<ETX>>","","0-7803-0599","10.1109/ICC.1992.268269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=268269","","Intelligent networks;High-speed networks;Access protocols;Formal specifications;Process control;Petri nets;Transport protocols;Hardware;Media Access Protocol;Testing","computer networks;formal specification;multiprocessing systems;Petri nets;protocols","communication protocols;high speed networks;Petri net;multiple processor system;formal methods;software;formal protocol specification;sequential programming languages;parallel program","","1","16","","","","","","IEEE","IEEE Conferences"
"Reverse logistics and management of end-of-life electric products","Z. Istvan; E. Garamvolgyi","Inst. of Logistics & Production Eng., Bay Zoltan Found. for Appl. Res., Hungary; NA","Proceedings of the 2000 IEEE International Symposium on Electronics and the Environment (Cat. No.00CH37082)","","2000","","","15","19","Bay Zoltan Foundation for Applied Research Institute of Logistics and Production Engineering (BAYLOGI) in co-operation with the Hungarian subsidiary company of Electrolux Co. started a research work financed by the Hungarian National Committee for Industrial Development to build up the theoretic bases of the environmentally sound electric product recycling. Since used old type refrigerators have intensive influence on the environment considering the hazardous R11 content, this product was selected as test model for the investigation. Strategic methods were used to optimise the collection logistics, the warehouse management and financing. Special software tools developed by BAYLOGI were installed to help the management system at the collection sites and in the disassembly and recycling plant. A life cycle analysis method was used to determine the environmental effect of the equipment and disassembly strategies for the recycling and reuse possibilities of parts. Results of the project are presented in this paper.","1095-2020","0-7803-5962","10.1109/ISEE.2000.857619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857619","","Reverse logistics;Recycling;Refrigerators;Electronic equipment;Project management;Production engineering;Financial management;Environmental management;Waste management;Cooling","product development;recycling;refrigerators;management","end-of-life electric products management;reverse logistics;electric product recycling;refrigerators;software tools;disassembly;life cycle analysis method;Hungary;product reuse","","","4","","","","","","IEEE","IEEE Conferences"
"Microprocessor control of a hybrid energy system","W. B. Lawrance; C. V. Nayar; T. L. Pryor; S. J. Phillips","Curtin Univ. of Technol., Bentley, WA, Australia; Curtin Univ. of Technol., Bentley, WA, Australia; NA; NA","Proceedings of the 24th Intersociety Energy Conversion Engineering Conference","","1989","","","737","741 vol.2","A prototype hybrid energy system consisting of a diesel-alternator operating in parallel with a battery-inverter has been assembled and tested. The controller is based on a single-chip microprocessor and is designed to optimize the operation of the system. Management of the system is based on current system status as well as system constants. The theory behind the operation of the system and the development of the control software are described, and some of the results obtained with the prototype system are presented.<<ETX>>","","","10.1109/IECEC.1989.74550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=74550","","Microprocessors;Control systems;Pulse width modulation inverters;Voltage;Batteries;Digital control;Alternators;Equivalent circuits;Monitoring;Pulse inverters","alternators;computerised control;internal combustion engines;invertors;microcomputer applications;secondary cells","single-chip microprocessor control;hybrid energy system;diesel-alternator;battery-inverter;control software","","3","4","","","","","","IEEE","IEEE Conferences"
"A new adaptive multidimensioanal load shedding scheme using genetic algorithm","M. Sanaye-Pasand; M. Davarpanah","NA; NA","Canadian Conference on Electrical and Computer Engineering, 2005.","","2005","","","1974","1977","This paper proposes a fast and accurate load shedding scheme design. The algorithm uses the synchronized real time power flow information. It selects the suitable load shedding for generator tripping contingencies. Genetic algorithms are used to solve the steady state load shedding optimization problem. The constraints associated with the problem of load shedding are calculated from power flow study. A multidimensional objective is used to restore power system to normal condition after generator tripping. This method has been tested on the IEEE 30-bus system. Results have been illustrated with simulation by NEPLAN software","0840-7789","0-7803-8885","10.1109/CCECE.2005.1557370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1557370","","Genetic algorithms;Power system stability;Power system simulation;Power system relaying;Frequency;Power system restoration;Power system modeling;Steady-state;Power system interconnection;Power generation","electric generators;genetic algorithms;load flow;load shedding;power system faults;power system restoration","genetic algorithm;adaptive multidimensional load shedding scheme;synchronized real time power flow;generator tripping;power flow study;multidimensional objective;power system restoration;IEEE 30-bus system;NEPLAN software","","6","7","","","","","","IEEE","IEEE Conferences"
"Realization of the synchronization controller for multimedia applications","SangShin Yoo","Dept. of Comput. Eng., Jeonju Univ., South Korea","IEEE GLOBECOM 1998 (Cat. NO. 98CH36250)","","1998","2","","798","803 vol.2","We describe the design, implementation and evaluation for a multimedia conferencing system with intramedia and intermedia synchronization support between audio and video. The synchronization mechanism proposed here is capable of dynamically adapting to various network conditions thus providing an optimized QoS. A synchronization controller is designed and realized with a unique process in supporting intermedia synchronization. Each media agent handling its media stream is modified with the intramedia synchronization function. The communicative functions between media agents and the synchronization controller are added as well for the intermedia synchronization function. The realized system is configured and tested on the Ethernet and an ATM network where performance measurements were conducted and its effective synchronization support has been assured.","","0-7803-4984","10.1109/GLOCOM.1998.776844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=776844","","Streaming media;Synchronization;Multimedia systems;Timing;Application software;Videoconference;Measurement;Routing;Control systems;Design engineering","multimedia communication;teleconferencing;synchronisation;telecommunication control;local area networks;asynchronous transfer mode;performance evaluation;quality of service","synchronization controller;multimedia applications;multimedia conferencing system;intermedia synchronization;audio;video;network conditions;optimized QoS;media agents;media stream;intramedia synchronization;communicative functions;Ethernet;ATM network;performance measurements;LAN","","1","22","","","","","","IEEE","IEEE Conferences"
"An intelligent FFT-analyzer","G. Betta; C. Liguori; M. D'Apuzzo; A. Pietrosanto","Dept. of Ind. Eng., Cassino Univ., Italy; NA; NA; NA","IMTC/98 Conference Proceedings. IEEE Instrumentation and Measurement Technology Conference. Where Instrumentation is Going (Cat. No.98CH36222)","","1998","2","","1341","1346 vol.2","An intelligent FFT analyzer capable of adapting its operating parameters on the basis of the signal spectrum was set up and characterized. The realized instrument is based on a parameter optimization procedure which provides the instrument with auto-configuration capability. It was implemented on a multiple processor DSP architecture in order to achieve real-time behavior. The experimental tests carried out on a large number of signals highlight the instrument capability of correctly detecting tones with a good frequency resolution whatever the signal spectrum type.","1091-5281","0-7803-4797","10.1109/IMTC.1998.676970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=676970","","Signal analysis;Instruments;Signal processing algorithms;Spectral analysis;Frequency;Algorithm design and analysis;Signal processing;Application software;Industrial engineering;Tiles","fast Fourier transforms;spectral analysers;computerised instrumentation;real-time systems","intelligent FFT-analyzer;operating parameters;parameter optimization procedure;auto-configuration capability;multiple processor DSP architecture;real-time behavior;tone detection;frequency resolution;signal spectrum type","","2","23","","","","","","IEEE","IEEE Conferences"
"A case study of scheduling storage tanks using a hybrid genetic algorithm","K. P. Dahal; G. M. Burt; J. R. NcDonald; A. Moyes","Centre for Electr. Power Eng., Strathclyde Univ., Glasgow, UK; NA; NA; NA","IEEE Transactions on Evolutionary Computation","","2001","5","3","283","294","This paper proposes the application of a hybrid genetic algorithm (GA) for scheduling storage tanks. The proposed approach integrates GAs and heuristic rule-based techniques, decomposing the complex mixed-integer optimization problem into integer and real-number subproblems. The GA string considers the integer problem and the heuristic approach solves the real-number problems within the GA framework. The algorithm is demonstrated for three test scenarios of a water treatment facility at a port and has been found to be robust and to give a significantly better schedule than those generated using a random search and a heuristic-based approach.","1089-778X;1089-778X;1941-0026","","10.1109/4235.930316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=930316","","Computer aided software engineering;Genetic algorithms;Job shop scheduling;Electronic ballasts;Marine vehicles;Water pollution;Petroleum;Filling;Scheduling algorithm;Water resources","water treatment;process control;scheduling;genetic algorithms","storage tanks;scheduling;hybrid genetic algorithm;rule-based method;mixed-integer optimization;heuristics;water treatment","","9","23","","","","","","IEEE","IEEE Journals & Magazines"
"Landmark-based shape deformation with topology-preserving constraints","Song Wang; Jim Xiuquan Ji; Zhi-Pei Liang","Dept. of Comput. Sci. & Eng., South Carolina Univ., Columbia, SC, USA; NA; NA","Proceedings Ninth IEEE International Conference on Computer Vision","","2003","","","923","930 vol.2","This paper presents a novel approach for landmark-based shape deformation, in which fitting error and shape difference are formulated into a support vector machine (SVM) regression problem. To well describe nonrigid shape deformation, this paper measures the shape difference using a thin-plate spline model. The proposed approach is capable of preserving the topology of the template shape in the deformation. This property is achieved by inserting a set of additional points and imposing a set of linear equality and/or inequality constraints. The underlying optimization problem is solved using a quadratic programming algorithm. The proposed method has been tested using practical data in the context of shape-based image segmentation. Some relevant practical issues, such as missing detected landmarks and selection of the regularization parameter are also briefly discussed.","","0-7695-1950","10.1109/ICCV.2003.1238447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238447","","Topology;Image segmentation;Shape measurement;Deformable models;Biomedical imaging;Computer vision;Application software;Active contours;Support vector machines;Active shape model","computer vision;image segmentation;support vector machines;regression analysis;quadratic programming;feature extraction;splines (mathematics);topology","landmark-based shape deformation;topology-preserving constraints;fitting error;shape difference;support vector machine;SVM regression problem;nonrigid shape deformation;thin-plate spline model;template shape;linear equality;inequality constraints;optimization problem;quadratic programming algorithm;shape-based image segmentation;regularization parameter","","4","25","","","","","","IEEE","IEEE Conferences"
"Efficient range query retrieval for non-uniform data distributions","S. Mohammed; E. P. Harris; K. Ramamohanarao","Dept. of Comput. Technol., Monash Univ., Clayton, Vic., Australia; NA; NA","Proceedings 11th Australasian Database Conference. ADC 2000 (Cat. No.PR00528)","","2000","","","90","98","Answering range queries is a common database operation. Methods based on hashing techniques to minimise the cost of answering range queries by taking the query distribution into account have previously been proposed. These methods have all assumed a uniform distribution of data to disk pages to achieve good performance. This assumption makes them less useful in practice because most real data distributions are non-uniform. In this paper, we discuss a method to eliminate this restriction. Extensive experimentation using a multi-dimensional file structure, the BANG file, indicates that our method results in good performance for all data distributions. In one case an improvement of over 36 times was achieved without compromising the storage utilisation. Our method also results in a stable and efficient file organisation. If the query distribution does not change substantially, an optimised file organisation rarely requires reorganisation.","","0-7695-0528","10.1109/ADC.2000.819818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=819818","","Information retrieval;Cost function;Computer science;Software engineering;Australia;Electronic switching systems;Delay;Testing;Multidimensional systems;Interpolation","database management systems;query processing","query retrieval;range queries;query distribution;data distributions;multi-dimensional file structure;file organisation","","1","14","","","","","","IEEE","IEEE Conferences"
"XYZ: a motion-enabled, power aware sensor node platform for distributed sensor network applications","D. Lymberopoulos; A. Savvides","Embedded Networks & Applications Lab., Yale Univ., New Haven, CT, USA; Embedded Networks & Applications Lab., Yale Univ., New Haven, CT, USA","IPSN 2005. Fourth International Symposium on Information Processing in Sensor Networks, 2005.","","2005","","","449","454","This paper describes the XYZ, a new open-source sensing platform specifically designed to support our experimental research in mobile sensor networks. The XYZ node is designed around the OKI ML67Q500x ARM THUMB Microprocessor and the IEEE 802.15.4 compliant CC2420 radio from Chipcon. Its new features include support for two different CPU sleep modes and a long-term ultra low power sleep mode for the entire node. This allows the XYZ and its peripheral boards to transition into deep sleep for extended time intervals. To support mobility hardware control and computations, XYZ supports a wide dynamic range and power options. In low power configuration the node resembles existing small low power nodes. When needed, the node can scale up its resources to perform more powerful computations. Mobility is enabled with an additional accessory board that allows the node to move along a horizontal string. In this paper we provide an overview of the XYZ architecture and provide an insightful power characterization of the different operational modes to allow the users to optimize their platforms for power.","","0-7803-9201","10.1109/IPSN.2005.1440970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1440970","","Sensor phenomena and characterization;Hardware;Thumb;Costs;Microcontrollers;Testing;Pulse width modulation;Open source software;Microprocessors;Dynamic range","mobile radio;wireless sensor networks","motion-enabled sensor;power aware sensor node;distributed sensor network application;XYZ node;open-source sensing platform;mobile sensor network;OKI ML67Q500x ARM THUMB microprocessor;IEEE 802.15.4 compliant CC2420 radio;Chipcon;CPU sleep mode;ultra low power sleep mode;peripheral board;extended time interval;support mobility hardware control","","52","16","","","","","","IEEE","IEEE Conferences"
"Meta-reasoning for a distributed agent architecture","D. Chelberg; L. Welch; A. Lakshmikumar; M. Gillen; Qiang Zhou","Dept. of Electr. & Comput. Sci., Ohio Univ., Athens, OH, USA; NA; NA; NA; NA","Proceedings of the 33rd Southeastern Symposium on System Theory (Cat. No.01EX460)","","2001","","","377","381","Agent based computing offers the ability to decentralize computing solutions by incorporating autonomy and intelligence into cooperating, distributed applications. It provides an effective medium for expressing solutions to problems that involve interaction with real-world environments and allows modelling of the world state and its dynamics. This model can be then used to determine how candidate actions affect the world and how to choose the best from a set of actions. Most agent paradigms overlook real-time requirements and computing resource constraints. We discuss the application of agent based computing to RoboCup and examine methods to improve it. In particular, we discuss the incorporation of a meta-level reasoning mechanism that handles individual agent organization, plan generation, task allocation, integration and plan execution. We also propose an architecture where a meta-agent is further enhanced by combining it with system-level resource allocation and optimization. The approach adopted by us unifies agent based computing with adaptive resource management for dynamic real-time systems. The goal is to build and implement a distributed, intelligent, agent based system for dynamic real-time applications.","","0-7803-6661","10.1109/SSST.2001.918549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918549","","Quality of service;Real time systems;Resource management;Computer architecture;Distributed computing;Intelligent agent;Monitoring;Laboratories;Application software;Testing","inference mechanisms;planning (artificial intelligence);multi-agent systems;real-time systems;mobile robots;resource allocation","meta-reasoning;distributed agent architecture;real-world environments;world state;RoboCup;individual agent organization;plan generation;task allocation;plan execution;agent based computing;adaptive resource management","","","14","","","","","","IEEE","IEEE Conferences"
"Performance analysis of MPI collective operations","J. Pjesivac-Grbovic; T. Angskun; G. Bosilca; G. E. Fagg; E. Gabriel; J. J. Dongarra","Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA; Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA; Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA; Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA; Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA; Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA","19th IEEE International Parallel and Distributed Processing Symposium","","2005","","","8 pp.","","Previous studies of application usage show that the performance of collective communications are critical for high-performance computing and are often overlooked when compared to the point-to-point performance. In this paper, we analyze and attempt to improve intra-cluster collective communication in the context of the widely deployed MPI programming paradigm by extending accepted models of point-to-point communication, such as Hockney, LogP/LogGP, and PLogP. The predictions from the models were compared to the experimentally gathered data and our findings were used to optimize the implementation of collective operations in the FT-MPI library.","1530-2075","0-7695-2312","10.1109/IPDPS.2005.335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420226","","Performance analysis;High performance computing;Predictive models;Libraries;Topology;System testing;Lifting equipment;Laboratories;Computer science;Application software","message passing;parallel programming;parallel algorithms","high-performance computing;intra-cluster collective communication;MPI programming paradigm;point-to-point communication;FT-MPI library","","40","18","","","","","","IEEE","IEEE Conferences"
"Transactional execution: toward reliable, high-performance multithreading","R. Rajwar; J. Goodman","NA; NA","IEEE Micro","","2003","23","6","117","125","Although lock-based critical sections are the synchronization method of choice, they have significant performance limitations and lack certain properties, such as failure atomicity and stability. Addressing both these limitations requires considerable software overhead. Transactional lock removal can dynamically eliminate synchronization operations and achieve transparent transactional execution by treating lock-based critical sections as lock-free optimistic transactions.","0272-1732;1937-4143","","10.1109/MM.2003.1261395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261395","","Multithreading;Traffic control;Programming profession;System testing;Telecommunication traffic;Operating systems;Transaction databases;Web server;Stability","multi-threading;concurrency control;synchronisation;transaction processing;shared memory systems","transactional execution;reliable high-performance multithreading;failure atomicity;transactional lock removal;synchronization operations;transparent transactional execution;lock-based critical sections;lock-free optimistic transactions","","11","17","","","","","","IEEE","IEEE Journals & Magazines"
"Design issues and the system architecture of TICOM-IV, a highly parallel commercial computer","Yang-Woo Kim; Sei-Woong Oh; Jin-Won Park","Comput. Div. ETRI, Taejon, South Korea; Comput. Div. ETRI, Taejon, South Korea; Comput. Div. ETRI, Taejon, South Korea","Proceedings Euromicro Workshop on Parallel and Distributed Processing","","1995","","","219","226","We describe design issues and preliminary considerational factors for highly parallel commercial computer design. The design issues described in this paper include application area analysis, scalability, availability, network topology and memory hierarchy. We explain how these issues are related to each other and how they affect the system performance. We then describe the system architecture of TICOM-IV, a highly parallel commercial computer, and show how the design issues and the trade-offs are taken into consideration in the course of the system design. Then the architectural model is simulated in order to optimize the system performance. The simulation result shows that TICOM-IV has good performance scalability. This paper also includes some findings in the development trend for parallel computer systems from a survey of 11 commercialized parallel computers.<<ETX>>","","0-8186-7031","10.1109/EMPDP.1995.389139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=389139","","Computer architecture;Concurrent computing;Parallel processing;Application software;Computational modeling;Decision support systems;Scalability;System performance;Electronic equipment testing;Multiprocessing systems","parallel architectures;parallel machines;reconfigurable architectures;performance evaluation;virtual machines","system architecture;system design;TICOM-IV;highly parallel commercial computer;parallel commercial computer design;area analysis;scalability;availability;network topology;memory hierarchy;system performance;performance scalability","","2","14","","","","","","IEEE","IEEE Conferences"
"DES in four days using behavioural modeling &amp; synthesis","P. R. Wilson; A. D. Brown","Sch. of Electron. & Comput. Sci., Southampton Univ., UK; Sch. of Electron. & Comput. Sci., Southampton Univ., UK","BMAS 2005. Proceedings of the 2005 IEEE International Behavioral Modeling and Simulation Workshop, 2005.","","2005","","","82","87","The experience of implementing the data encryption standard (DES) using high level VHDL and behavioral synthesis is described. It is shown that it is possible to describe an algorithm which is notoriously low-level such that it is both readable and synthesizable using behavioral modelling appropriate for behavioural synthesis. The paper also discusses typical design issues that arise when working at the behavioral level and shows that human insight is still necessary to achieve the best possible results. However, this insight is brought to bear at a high level - which is what humans are good at - whilst the synthesis system provides ""decision support"" and optimisation - which is what software and computers are good at.","2160-3804;2160-3812","0-7803-9352","10.1109/BMAS.2005.1518192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1518192","","Cryptography;Mood;Testing;NIST;Humans;Hardware;Code standards;Books;High level languages;Delay","cryptography;hardware description languages;high level synthesis","data encryption standard;DES;VHDL;behavioral synthesis;behavioral modeling;decision support;optimization;cryptography;hardware description languages;high level synthesis","","","21","","","","","","IEEE","IEEE Conferences"
"Radial mode piezoelectric transformer design for fluorescent lamp ballast applications","E. M. Baker; W. Huang; D. Y. Chen; F. C. Lee","Bradley Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; Bradley Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; Bradley Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; Bradley Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA","2002 IEEE 33rd Annual IEEE Power Electronics Specialists Conference. Proceedings (Cat. No.02CH37289)","","2002","3","","1289","1294 vol.3","In a ballast circuit, the piezoelectric transformer (PT) is used to replace the conventional inductor-capacitor resonant tank saving valuable space and cost. During circuit operation, a very high voltage is required to initially ignite the lamp while during sustained operation the voltage requirements are significantly reduced. With ballast in mind, a design process has been developed to optimize a radial mode piezoelectric transformer or Transoner/sup /spl reg// to fit a specific application while simultaneously providing highly efficient performance and the capability to provide ZVS to the switches. The design procedure was verified by a custom-designed PT operating in a 32-watt ballast.","","0-7803-7262","10.1109/PSEC.2002.1022354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022354","","Fluorescent lamps;Electronic ballasts;Process design;Equivalent circuits;RLC circuits;Zero voltage switching;Magnetic resonance;Brain modeling;Application software;Electronic equipment testing","piezoelectric devices;transformers;fluorescent lamps;lamp accessories;switching circuits","radial mode piezoelectric transformer design;fluorescent lamp ballast applications;inductor-capacitor resonant tank;Transoner/spl reg/;ZVS;32 W","","18","6","","","","","","IEEE","IEEE Conferences"
"Preliminary Experience with the Automatic Generation of Production-Quality Code for the Ford/Intel 8061 Microprocessor","R. J. Srodawa; R. E. Gach; A. Glicker","The School of Engineering and Computer Science, Oakland University, Rochester, MI 48063.; The Engineering Computer Center, Ford Motor Company, Dearborn, MI 48121.; The Engineering Division, Computer Methods Corporation, Livonia, MI 48150.","IEEE Transactions on Industrial Electronics","","1985","IE-32","4","318","326","Production strategies for Ford/Intel 8061 applications are hand-coded in 8061 assembler language. A parallel and independent effort codes the strategy in Ford Automotive Control Terminology (FACT), a higher level application language. Output from the two strategies are compared as a part of the certification process. In 1982 a project was initiated to generate 8061 code directly from the FACT specification. That effort was successful and currently meets commercial standards for production systems. The compiler, its optimization and scaling algorithms, and its performance are described in this paper.","0278-0046;1557-9948","","10.1109/TIE.1985.350104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4158649","","Microprocessors;Assembly;Engines;Automotive engineering;Certification;Production;Terminology;Calibration;Automatic control;Software testing","","","","4","5","","","","","","IEEE","IEEE Journals & Magazines"
"COSIMIR transport: modeling, simulation and emulation of modular carrier based transport systems","R. Wischnewski; E. Freund","Institute of Robotics Research, University of Dortmund; NA","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","","2004","5","","5171","5176","More and more industrial enterprises tend to validate their planning for new factories using simulation. Production environments often contain automated transport systems to move items between different work stations. We present a solution for modeling transport systems with respect to their interaction with all peripheries like robots and treatment machines. Modeling follows the process of mechanical assembly by putting together constructional components in 3-D space. Models of devices are of sufficient physical and electrical accuracy to test unmodified controller programs in a virtual commissioning phase to save time at the real start-up. In emulation, real controllers connect to the model to test and optimize control programs. In contrast to conventional discrete event simulation software, developers can play with the model during simulation to test its reaction and perform event of fault analysis. Simulation results are suited to secure design decisions of planners and reduce the risk of incorrect planning. Easy to recognize 3-D geometry and simulation interactivity qualify models for presentation and personnel training. Finally, simulation reveals suboptimal behavior and makes optimizations easier.","1050-4729","0-7803-8232","10.1109/ROBOT.2004.1302538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1302538","","Emulation;Testing;Discrete event simulation;Solid modeling;Machinery production industries;Production facilities;Production systems;Service robots;Orbital robotics;Robotics and automation","","","","6","10","","","","","","IEEE","IEEE Conferences"
"GIANT-a computer code for general interactive analysis of trajectories","J. Jager; M. Lee; R. Servranckx; H. Shoaee","Stanford Linear Accelerator Center Stanford University, Stanford, California, 94305; Stanford Linear Accelerator Center Stanford University, Stanford, California, 94305; Stanford Linear Accelerator Center Stanford University, Stanford, California, 94305; Stanford Linear Accelerator Center Stanford University, Stanford, California, 94305","IEEE Transactions on Nuclear Science","","1985","32","5","1877","1879","Many model-driven diagnostic and correction procedures have been developed at SLAC for the on-line computer controlled operation of SPEAR, PEP, the LINAC, and the Electron Damping Ring. In order to facilitate future applications and enhancements, these procedures are being collected into a single program, GIANT. The program allows interactive diagnosis as well as performance optimization of any beam transport line or circular machine. The test systems for GIANT are those of the SLC project. The organization of this program and some of the recent applications of the procedures will be described in this paper.","0018-9499;1558-1578","","10.1109/TNS.1985.4333752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4333752","","Linear particle accelerator;Control systems;Storage rings;Databases;Lattices;Damping;Application software;Particle beams;Error correction;Optimal control","","","","1","2","","","","","","IEEE","IEEE Journals & Magazines"
"Charley: a genetic algorithm for the design of mesh networks","J. Hewitt; A. Soper; S. McKenzie","Charing Cross Hospital, UK; NA; NA","First International Conference on Genetic Algorithms in Engineering Systems: Innovations and Applications","","1995","","","118","122","This paper presents a genetic algorithm for the design of an optimal mesh network. The problem is of relevance in the design of communication networks where the backbone switching network takes the form of a highly connected mesh in order to provide reliability in the event of switch/link failure. The proposed algorithm addresses two important aspects of the problem-topology design and capacity allocation. The optimisation is done with respect to connection costs subject to performance (delay), connectivity and capacity constraints. Connection costs are assumed to depend on distance and link capacity. Though the algorithm was designed with mesh networks in mind, it can be applied to the simpler problem of the constrained minimum spanning tree. The algorithm has been tested on a tree network and two mesh networks. The results compare very favourably with those obtained from existing design techniques.","0537-9989","0-85296-650","10.1049/cp:19951035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=501658","","Communication systems;Genetic algorithms;Communication system software","telecommunication networks;genetic algorithms;telecommunication computing","mesh networks;genetic algorithm;communication networks;backbone switching network;reliability;topology design;capacity allocation;minimum spanning tree;capacity constraints","","","","","","","","","IET","IET Conferences"
"Feed chain design for the W3A Ka-band antenna","C. B. Ravipati; J. Uher; D. McLaren","EMS Technologies Canada, Ste-Anne-de-Bellevue, Quebec, Canada, H9X3R2; EMS Technologies Canada, Ste-Anne-de-Bellevue, Quebec, Canada, H9X3R2; EMS Technologies Canada, Ste-Anne-de-Bellevue, Quebec, Canada, H9X3R2","2002 9th International Symposium on Antenna Technology and Applied Electromagnetics","","2002","","","1","4","The paper presents the design concept and bread board test results of the feed chain for Eutelsat W3A Ka-band antenna. The design of the feed chain components was carried out by systematic synthesis, analysis, and optimization techniques using advanced full wave design tools. The predicted and measured results are in good agreement.","","978-0-9692563-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7864685","","Loss measurement;Antenna measurements;Antenna feeds;Transducers;Software;Insertion loss","","","","","1","","","","","","IEEE","IEEE Conferences"
"Mechanical properties, microstructure, and texture of electron beam butt welds in high purity niobium","H. Jiang; T. R. Bieler; C. Compton; T. L. Grimm","Chem. Eng. & Mater. Sci., Michigan State Univ., East Lansing, MI, USA; Chem. Eng. & Mater. Sci., Michigan State Univ., East Lansing, MI, USA; NA; NA","Proceedings of the 2003 Particle Accelerator Conference","","2003","2","","1359","1361 Vol.2","The effects of Electron Beam Welding on solidification microstructure, texture, microhardness and mechanical properties were investigated in high purity niobium weld specimens. The welds have an equiaxed microstructure with a 1 mm grain size in the fusion zone, 100 /spl mu/m in the heat affected zone (HAZ) and 50 /spl mu/m in the parent metal. The fusion zone had slightly higher microhardness values despite having a large grain size, while the unaffected material had the lowest microhardness. The texture in the weld consisted of a strong {111} fiber texture in the center and a mix of {111} - {100} components on the surface. Tensile tests of specimens gave /spl sigma//sub y/ = 60 MPa, but the UTS and elongation for weld specimens were lower than the parent material (137 vs. 165 MPa, 32% vs. 58%). The properties and microstructure of the weld are discussed in terms of optimizing the SRF cavity.","1063-3928","0-7803-7738","10.1109/PAC.2003.1289705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1289705","","Mechanical factors;Microstructure;Electron beams;Welding;Niobium;Grain size;Surface texture;Optical microscopy;Superconducting materials;Software measurement","electron beam welding;texture;microhardness;solidification;niobium;accelerator RF systems;accelerator cavities;superconducting cavity resonators;beam handling techniques;particle beam dynamics","mechanical properties;texture;electron beam butt welds;Electron Beam Welding;solidification microstructure;microhardness;high purity niobium weld specimens;fusion zone;SRF cavity;1 mm;100 mum;50 mum;60 MPa","","3","10","","","","","","IEEE","IEEE Conferences"
"DS-CDMA Code Acquisition in the Presence of Correlated FadingPart II: Application to Cellular Networks","","","IEEE Transactions on Communications","","2004","52","3","518","518","This paper applies the theoretical framework for the evaluation of code acquisition in the presence of fading developed in the companion Part I paper. Code synchronization for CDMA cellular networks, such as IS-95 and cdma2000, is considered. A multi-dwell testing procedure is adopted. A procedure for the optimization of the multi-dwell parameters (number of dwells, dwell lengths, thresholds) to achieve minimum acquisition time is introduced and enforced. The effects of fading, interchip interference, frequency offset, multiple-access interference, and noise are taken into account in the determination of false alarm and detection probabilities. In particular, anomalous interchip interference despreading and internal interference cancellation effects are described. Numerical and simulation results, in terms of time for correct acquisition, confirm the accuracy of the proposed approach and show how it can be effectively used in the design and evaluation of a code-acquisition subsystem.","0090-6778;1558-0857","","10.1109/TCOMM.2004.823557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1273704","","Multiaccess communication;Intelligent networks;Land mobile radio cellular systems;Performance analysis;Fading;Phase detection;Application software;Computer science;Physics;Multiple access interference","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Genetic algorithm for the design of multipoint connections in a local access network","J. Zhao; S. McKenzie; A. J. Soper","Greenwich Univ., UK; Greenwich Univ., UK; Greenwich Univ., UK","First International Conference on Genetic Algorithms in Engineering Systems: Innovations and Applications","","1995","","","30","33","Genetic Algorithms have been widely applied to optimisation problems for which existing approaches are computationally too expensive. This paper presents a genetic algorithm for finding a constrained minimum spanning tree. The problem is of relevance in the design of minimum cost communication networks, where there is a need to connect all the terminals at a user site to a terminal concentrator in a multipoint (tree) configuration, while ensuring that link capacity constraints are not violated. Test results show that the solutions from the proposed algorithm are broadly comparable with, and in several cases, better than, those obtained from traditional heuristic methods, such as the Prim and Esau-Williams algorithms.","0537-9989","0-85296-650","10.1049/cp:19951020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=501643","","Genetic algorithms;Communication systems;Trees (graphs);Communication system software","genetic algorithms;telecommunication networks;trees (mathematics);telecommunication computing","multipoint connections;local access network;Genetic Algorithms;constrained minimum spanning tree;link capacity constraints","","2","","","","","","","IET","IET Conferences"
"Automated error detection in multibeam bathymetry data","S. Shaw; J. Arnold","SRI Int., Menlo Park, CA, USA; SRI Int., Menlo Park, CA, USA","Proceedings of OCEANS '93","","1993","","","II/89","II/94 vol.2","For a variety of reasons, multibeam swath sounding systems produce errors that can seriously corrupt navigational charts. To address this problem, the authors have developed two algorithms for identifying subtle outlier errors in a variety of multibeam systems. The first algorithm treats the swath as a sequence of images. The algorithm is based on robust estimation of autoregressive (AR) model parameters. The second algorithm is based on energy minimization techniques. The data are represented by a weak-membrane or thin-plate model, and a global optimization procedure is used to find a stable surface shape. Both of these algorithms have undergone extensive testing at bathymetric processing centers to assess performance. The algorithms were found to have a probability of detection high enough to be useful and a false-alarm rate that does not significantly degrade the data quality. The resulting software is currently being used both at processing centers and at sea as an aid to bathymetric data processors.<<ETX>>","","0-7803-1385","10.1109/OCEANS.1993.326072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326072","","Surface reconstruction;Oceanographic techniques;Robustness;Sea surface;Iterative algorithms;Shape;Sonar detection;Surface treatment;Filters;Image reconstruction","sonar;acoustic imaging;bathymetry;oceanographic techniques;geophysical techniques;acoustic signal processing;geophysics computing","seafloor topography;ocean;geophysical method;measurement technique;sonar navigation;automated error detection;multibeam bathymetry;multibeam swath sounding system;navigation;algorithm;outlier error;autoregressive model;signal processing;weak-membrane;thin-plate model;global optimization","","9","9","","","","","","IEEE","IEEE Conferences"
"Uni-planar slot antenna for TM slab mode excitation","H. F. Hammad; Y. M. M. Antar; A. P. Freundorfer","Dept. of Electr. & Comput. Eng., Queen's Univ., Kingston, Ont., Canada; NA; NA","Electronics Letters","","2001","37","25","1500","1501","A new uni-planar coplanar waveguide slot antenna is presented. The antenna is used to efficiently excite the dominant transverse magnetic (TM) mode inside a grounded slab. The antenna design and optimisation using two commercially available softwares are described. The antenna was fabricated and tested. Good agreement between the numerical and experimental results is obtained.","0013-5194","","10.1049/el:20011009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972175","","","slot antennas;coplanar waveguide components","TM slab mode excitation;uni-planar coplanar waveguide slot antenna;design optimisation","","1","","","","","","","IET","IET Journals & Magazines"
"A new approach for the solution of the optimal set-point tracking problem for nonlinear systems","V. M. Becerra; P. D. Roberts","City Univ., London, UK; City Univ., London, UK","1994 International Conference on Control - Control '94.","","1994","1","","849","854 vol.1","In this paper, a discrete time dynamic integrated system optimisation and parameter estimation algorithm is applied to the solution of the nonlinear tracking optimal control problem. A version of the algorithm with a linear-quadratic model-based problem is developed and implemented in software. The algorithm implemented is tested with simulation examples.<<ETX>>","","0-85296-610","10.1049/cp:19940244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=327033","","Discrete time systems;Nonlinear systems;Optimal control;Parameter estimation","discrete time systems;nonlinear control systems;optimal control;parameter estimation","optimal set-point tracking problem;nonlinear systems;discrete-time dynamic integrated system optimisation;parameter estimation;nonlinear tracking optimal control;linear-quadratic model-based problem","","1","","","","","","","IET","IET Conferences"
"An IPv6 enabled packet engine design for home/SOHO routers","Mingshou Liu; Cheng-Hsien Hsu; Shi-Hong Kuo; Hsang-Chi Tsai","Dept. of Electr. Eng., National Chung Hsing Univ., Taiwan; NA; NA; NA","19th International Conference on Advanced Information Networking and Applications (AINA'05) Volume 1 (AINA papers)","","2005","2","","796","800 vol.2","Due to the diversity of Internet applications and services, traditional software-based networking devices may not be sufficient to afford the processing load imposed by the services. One example is the mixed-version IP environment in which routers must handle the IPv4/IPv6 translation while keeping the IP processing at the line speed. In this paper, we present our work for the design of a protocol optimized packet processing engine that provides common IP services and mixed version translation. This silicon is written in VHDL and is tested in a Xilinx Vertex II FPGA development board.","1550-445X;2332-5658","0-7695-2249","10.1109/AINA.2005.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1423792","","Network address translation;Protocols;Field programmable gate arrays;Search engines;Consumer electronics;Home appliances;Internetworking;Hardware;IP networks;Web and internet services","telecommunication network routing;Internet;transport protocols;packet switching;IP networks","IPv6 enabled packet engine;home/SOHO routers;Internet application;Internet service;mixed-version IP environment;IPv4/IPv6 translation;IP processing;protocol optimized packet processing engine;IP service;mixed version translation;VHDL;Xilinx Vertex II FPGA development board","","","8","","","","","","IEEE","IEEE Conferences"
"A novel method for in situ thermal expansion coefficient measurement of polysilicon thin films [MEMS materials applications]","Yuxing Zhang; Qing-An Huang; Weihua Li","Key Lab. of MEMS of Minist. of Educ., Southeast Univ., Nanjing, China; Key Lab. of MEMS of Minist. of Educ., Southeast Univ., Nanjing, China; Key Lab. of MEMS of Minist. of Educ., Southeast Univ., Nanjing, China","Proceedings. 7th International Conference on Solid-State and Integrated Circuits Technology, 2004.","","2004","3","","1856","1859 vol.3","This paper presents a novel structure for in situ determination of thermal expansion coefficient (TEC) of polysilicon thin films. Using this structure, all experiments are available in the air without any special requirements for vacuum or sealed chambers. Besides, a paramount novelty for this method is that all readouts are presented in electrical forms, compared with conventional ones obtained by optical observations. In the paper, a thermal-electro-mechanical compliant model is provided and verified with ANSYS software. Moreover, some optimized parameters are achieved through simulations, as well.","","0-7803-8511","10.1109/ICSICT.2004.1435197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1435197","","Thermal expansion;Transistors;Optical sensors;Temperature;Optical films;Testing;Micromechanical devices;Thin film circuits;Capacitive sensors;Capacitance measurement","micromechanical devices;silicon;elemental semiconductors;thermal expansion;thermal variables measurement;semiconductor thin films","in situ thermal expansion coefficient measurement;polysilicon thin films;MEMS materials;TEC;electrical form readout;thermal-electro-mechanical compliant model;parameter extraction;Si","","","9","","","","","","IEEE","IEEE Conferences"
"The peer tasking design method","N. R. Howes; J. D. Wood; A. Goforth","Inst. for Defense Anal., Alexandria, VA, USA; Inst. for Defense Anal., Alexandria, VA, USA; NA","Proceedings of Third Workshop on Parallel and Distributed Real-Time Systems","","1995","","","20","29","This paper is a preliminary report of an ARPA sponsored study. It focuses on designing real-time command and control or battle management systems for parallel and distributed architectures. Due to delays in other ARPA programs, the targeted architectures were not available during the time frame of the study. The results of the study were, however, tested on more conventional sequential and parallel platforms. The design method discussed here is fundamentally different from those assumed by current real-time scheduling theories, e.g., rate-monotonic, earliest-deadline-first, least-laxity or best-effort. These theories assume that the fundamental unit of prioritization is the task. In this new method, the fundamental unit of prioritization is called a work item. Work items are functions the system performs that have timing requirements (deadlines) associated with them in the requirements specification. Current scheduling theories are applied using artifact deadlines introduced by designers whereas this new method schedules work items to meet specification deadlines (sometimes called end-to-end deadlines) required by the user. With this method, tasks have no priorities. A collection of tasks with no priorities will be called a collection of peer tasks. The study showed that it is possible to schedule work items based on importance rather than urgency while still meeting as many work item deadlines as can be met by scheduling tasks with respect to urgency. Also, it showed that the minimum on-line deadline that can be guaranteed for a work item of highest importance, scheduled at run-time, is approximately the inverse of the throughput, measured in work items per second, for a work load consisting only of work items of that type. Further, it was shown that it provides optimal utilization of a single processor machine, and that its timing behavior is predictable (provable) for both single and multiprocessor machines. Finally, it was shown that throughput is not degraded during overload.<<ETX>>","","0-8186-7099","10.1109/WPDRTS.1995.470511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470511","","Design methodology;Timing;Throughput;Peer to peer computing;Real time systems;Command and control systems;Delay effects;Sequential analysis;Processor scheduling;Runtime","delays;processor scheduling;parallel architectures;real-time systems;command and control systems","peer tasking design method;real-time command and control;battle management systems;distributed architectures;parallel architectures;real-time scheduling;work item;timing requirements;end-to-end deadlines;multiprocessor","","2","14","","","","","","IEEE","IEEE Conferences"
"Real Time Analyzer of Antenna Near-Field Distribution","D. Picard; J. C. Bolomey; A. Ziyyat","Service d'Electromagntisme, SUPELEC/CNRS, F 91192 Gif sur Yvette Cedex. Phone: 33(1) 69418040, Fax: 33(1) 69413060; Service d'Electromagntisme, SUPELEC/CNRS, F 91192 Gif sur Yvette Cedex. Phone: 33(1) 69418040, Fax: 33(1) 69413060; Service d'Electromagntisme, SUPELEC/CNRS, F 91192 Gif sur Yvette Cedex. Phone: 33(1) 69418040, Fax: 33(1) 69413060","1992 22nd European Microwave Conference","","1992","1","","509","514","This paper describes a new facility for rapid near-field measurement in cylindrical coordinates. This facility is hardware and software optimized and allows quasi-real time measurements on large antennas in a frequency range extending from 2 to 10 GHz. By comparison to classical near-field (NF) techniques, the time gain factor is about 100 to 1000. Such a rapidity makes now NF approach still much more attractive. Three different examples have been selected to illustrate the capabilities of such a rapid NF facility.","","","10.1109/EUMA.1992.335756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4135500","","Noise measurement;Probes;Antenna measurements;Rotation measurement;Linear antenna arrays;Testing;Antenna arrays;Antenna accessories;Coordinate measuring machines;Hardware","","","","4","6","","","","","","IEEE","IEEE Conferences"
"Does Q = MC/sup 2/? (On the relationship between Quality in electronic design and the Model of Colloidal Computing)","R. Marculescu; D. Marculescu","Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA; Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA","Proceedings International Symposium on Quality Electronic Design","","2002","","","451","457","This paper introduces colloidal computing as an alternative to the classical view on computing systems in terms of design feasibility, application adaptability and better energy-performance trade-offs. In colloidal computing, simple per computational particles are dispersed into a communication medium which is inexpensive, (perhaps) unreliable, yet sufficiently fast. This type of clustering into computationally intensive kernels with loose inter-particle communication, but tight intra-particle communication is typical not only for the underlying hardware, but also for the actual application which runs on it. We believe that the colloidal model is appropriate to describe the next generation of embedded systems. For these systems, a significantly better design quality can be obtained via run-time trade-offs and application-driven adaptability, as opposed to classical systems where optimizations are sought in a rather static manner.","","0-7695-1561","10.1109/ISQED.2002.996787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996787","","Clocks;Application software;Kernel;Hardware;Computer architecture;Physics computing;Switches;Embedded computing;Design engineering;Embedded system","embedded systems;asynchronous circuits;computation theory;digital computers","electronic design quality;colloidal computing model;run-time trade-offs;application-driven adaptability;design feasibility;energy-performance trade-offs;embedded systems;computationally intensive kernels;loose inter-particle communication;tight intra-particle communication;asynchronous design","","4","14","","","","","","IEEE","IEEE Conferences"
"Market-based task allocation for dynamic processing environments","M. P. Wellman; S. -. Cheng","Michigan Univ., Ann Arbor, MI, USA; Michigan Univ., Ann Arbor, MI, USA","IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502)","","2003","","","109","114","Flexible and large-scale information processing across enterprises entails dynamic and decentralized control of workflow through adaptive allocation of knowledge and processing resources. Markets comprise a well-understood class of mechanisms for decentralized resource allocation, where agents interacting through a price system direct resources toward their most valued uses as indicated by these prices. The information-processing domain presents several challenges for market-based approaches, including (1) representing knowledge-intensive tasks and capabilities, (2) propagating price signals across multiple levels of information processing, (3) handling dynamic task arrival and changing priorities, and (4) accommodating the increasing-returns and public-good characteristics of information products. A market gaming environment provides a methodology for testing alternative market structures and agent strategies, and evaluating proposed solutions in a realistic decentralized manner.","","0-7803-7958","10.1109/KIMAS.2003.1245031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245031","","Information processing;Resource management;Distributed control;Centralized control;Communication system control;Aggregates;Optimal control;Programmable control;Adaptive control;Signal processing","task analysis;resource allocation;multi-agent systems;electronic commerce;software agents","market-based task allocation;dynamic processing environments;large-scale information processing;decentralized resource allocation;knowledge-intensive tasks;price signals propagation;information products;market gaming environment","","","23","","","","","","IEEE","IEEE Conferences"
"Parallel path planning with temporal parametrization","V. Moreno; E. Sanz; F. J. Blanco","Dept. Comput. & Autom., Salamanca Univ., Spain; NA; NA","Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'","","1997","","","102","107","The automatic generation of robot free-collision paths for accomplishing user-specified tasks is one of the key elements in robotics. Due to the high computational cost involved in the problem, it is very difficult to find solutions achieving online performance. In this paper, a general and new approach that provides this online motion planning capability to robot manipulators, is proposed. The new approach is based on a temporal parametrization of the state variables so that the problem can be seen as an optimal time one. The most advanced technique for path planning using the C-space representation has been applied and a parallel graph search algorithm has been developed with the aim of carrying out this task with a low computational cost. The proposed approach has been tested in real manipulators, it can be easily applied to solve industrial problems.","","0-8186-8138","10.1109/CIRA.1997.613845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=613845","","Path planning;Robotics and automation;Orbital robotics;Computational efficiency;Manipulators;State-space methods;Application software;Computer industry;Robots;Concurrent computing","path planning;manipulator dynamics;search problems;graph theory;parallel algorithms;real-time systems","dynamic path planning;temporal parametrization;robot;online motion planning;manipulators;C-space representation;parallel graph search algorithm;real time systems;optimisation","","","13","","","","","","IEEE","IEEE Conferences"
"Performance of Cooperative Loosely Coupled Microprocessor Architectures in an Interactive Data Base Task","J. J. Lenahan; F. K. Fung","NA; NA","IEEE Journal of Solid-State Circuits","","1980","15","1","97","116","Continuing technological advances in single-chip intelligence and storage cell density, and in bulk store performance provide increasing opportunities to construct multiple microprocessor systems. The objective of this experimental study was to explore the performance of selected system architectures in a manner sufficiently detailed, quantitative and realistic to 1) contribute to our understanding of the fundamental behavior of such systems and 2) permit practical designs to follow from the results. Four different classes of system architecture were studied: System I-fully connected; System II-bused with private primary store; System III-bused with both public and private primary store; and System IV-a uniprocessor system. All multiple-processor configurations comprised autonomous computing units which were loosely coupled with parallel links, and performed dedicated functions in a cooperative tasking environment. At the time the study began much of the anticipated technology was not yet available so detailed deterministic models of the systems were developed. All testing was done using a specially developed discrete simulation tool called the System-State Model (SSM). Both hardware and software details, derived from an operational system, were included in each model structure to represent the necessary concurrent, hierarchical, and ranked activity. A basic, interactive data base task was used as the test case. Experimental parameters included number of users, task load, console channel and bulk store channel configurations, and secondary store access time regimes. Over one hundred simulation runs were made which provided both internal and external performance measurements. Analysis of the results revealed a complex set of performance relationships which, in turn, led to several fundamental assessments. These include 1) every configuration can have its performance severely limited by the presence of any one of several key factors, 2) bulk store access time regimes and channel configuration have a critical effect on all system architectures, 3) performance distinctions among systems become more pronounced with increased number of users and with task load, and 4) each of Systems II, III, and IV manifests unique and useful performance characteristics and could be the configuration of choice under the right conditions. Overall, the results provide detailed confirmation of the utility of such systems and suggest a variety of specific tradeoffs among the architectures.","0018-9200;1558-173X","","10.1109/JSSC.1980.1051343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1051343","","Microprocessors;Buffer storage;Real time systems;Computer architecture;Concurrent computing;System testing;Hardware;Measurement;Performance analysis","","Computer architecture;Microcomputers","","","22","","","","","","IEEE","IEEE Journals & Magazines"
"On-Line, Digital Data Handling and Optimal Control Investigations at the Missouri University Research Reactor","C. L. Partain; R. E. Wilbur; C. R. Noyes; G. A. Schlapper","Nuclear Engineering Program University of Missouri-Columbia Columbia, Missouri 65201; Nuclear Engineering Program University of Missouri-Columbia Columbia, Missouri 65201; Nuclear Engineering Program University of Missouri-Columbia Columbia, Missouri 65201; Nuclear Engineering Program University of Missouri-Columbia Columbia, Missouri 65201","IEEE Transactions on Nuclear Science","","1971","18","1","395","402","An on-line, digital data handling and optimal control system based upon a PDP8s, general purpose, programmable digital computer, has been developed and tested at the Missouri University Research Reactor (MURR), a 5 megawatt, thermal, flux-trap type nuclear reactor. The instrumentation which has been successfully interfaced to the small digital computer includes a 100 channel, 250 Hz, randomly addressable reed scanner; a 10-bit, 100 kHz, analog-to-digital converter; a 60 Hz real time clock which provides a hardware interrupt every 16.7 milliseconds; and two relay contact outputs. Software which has been implemented includes a real-time executive which schedules eight different tasks in a specified priority and periodic scheme, a console utility package which allows direct, on-line, communication between computer core memory and the ASR-33 teletype, an analog signal input task, various control algorithms, an output task, and a systems log task. The automatic control algorithms are dependent upon the physical parameters of the reactor system. These parameters may be periodically identified by on-line dynamic testing and the control decision algorithm may be modified accordingly. Methods of z-transform analysis are applied in a linearized approach to the problem of representing each component of the closed-loop reactor, computer control system. The optimization theory based on the maximum principle of L. S. Pontryagin has been applied in an effort to improve the control calculations with respect to minimum time response. Testing methods used were hybrid computer simulation and on-line parameter input studies at the MURR.","0018-9499;1558-1578","","10.1109/TNS.1971.4325898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4325898","","Data handling;Optimal control;Inductors;Automatic control;Control systems;System testing;Reactor instrumentation;Computer interfaces;Analog-digital conversion;Clocks","","","","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Improving appointment scheduling for medical screening","R. D. Baker; P. L. Atherill","NA; NA","IMA Journal of Management Mathematics","","2002","13","4","225","243","We investigate the optimization of appointment scheduling for breast cancer screening, using the fact that a woman's attendance probability can be predicted. The methodology used applies to medical screening in general.; The results of the mathematical investigation presented in this paper include a new formula for the cost of a screening session, a probabilistic model of rebooking appointments, a model of attendance probability as a function of previous performance, and a heuristic cost optimization procedure.; Breast Test Wales have improved efficiency by introducing heavily overbooked sessions for patients who are unlikely to attend. We use simulation modelling and insights from probability theory to confirm the gain achieved by the Wales procedure and to assess the further gain achievable by optimization of appointment scheduling. It is found that a gain in throughput of at least 10% can be obtained by optimizing appointment scheduling for screening sessions, in particular by inviting patients in decreasing order of attendance probability, and by overbooking near the end of the session. This avoids the need to set up dedicated sessions for poor attenders. Another possibility is to book patients who change their appointment time, and who are therefore very likely to attend, into dedicated sessions.; The provision of appointment scheduling software with a builtin simulation and optimization module along the lines described in this paper could enable radiographers to tailor appointment scheduling for each area and so to schedule appointments very efficiently.","1471-678X;1471-6798","","10.1093/imaman/13.4.225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8132225","medical screening;scheduling;breast cancer;simulation","","","","","","","","","","","","OUP","OUP Journals & Magazines"
"Fuzzy logic-based power control system for multi field electrostatic precipitators","N. Grass","Siemens AG, Erlangen, Germany","Conference Record of the 2000 IEEE Industry Applications Conference. Thirty-Fifth IAS Annual Meeting and World Conference on Industrial Applications of Electrical Energy (Cat. No.00CH37129)","","2000","1","","563","568 vol.1","The power consumption of large precipitators can be in the range of one MW and above. Depending on the dust load properties, the electrical power may be reduced by up to 50% by applying fuzzy logic, without significantly increasing the dust emissions. The new approach using fuzzy logic is an optimisation of the existing precipitator. Software running on a standard personal computer platform under Windows NT facilitates the reduction in power usage. The controllers of the electrostatic precipitator power supplies are linked to the computer via an industrial network (e.g. PROFIBUS). The system determines online the differentials of emission versus electrical power of each field. This measurement is difficult because of overlaid events in the other zones, and process changes. The long response time of the resultant dust emission due to electrical power changes in the precipitator is an additional complication. Rules were defined for a coarse, but fast response power adaptation of all zones. Fine tuning the running system after the coarse optimization increased the accuracy and reliability. When installed on a 4 by 5 zone precipitator in a power station meaningful results can be obtained. The power savings over 3 months of operation were 40% to 60% depending on the load and fuel characteristics. Data was recorded over the test period of 3 months. The results are presented.","0197-2618","0-7803-6401","10.1109/IAS.2000.881166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881166","","Fuzzy control;Fuzzy logic;Fuzzy systems;Power control;Energy consumption;Software standards;Microcomputers;Electrostatic precipitators;Computer networks;Computer industry","electrostatic precipitators;fuzzy control;power control;power consumption;energy conservation;dust","fuzzy logic-based power control system;multi field electrostatic precipitators;power consumption;dust load properties;electrical power;personal computer platform;Windows NT;power usage reduction;industrial network;PROFIBUS;emission;long response time;dust emission;fast response power adaptation;fine tuning;reliability improvement;fuel characteristics;load characteristics;3 month","","1","12","","","","","","IEEE","IEEE Conferences"
"Finite state machine encoding for VHDL synthesis","K. Kuusilinna; V. Lahtinen; T. Hamalainen; J. Saarinen","Digital & Comput. Syst. Lab., Tampere Univ. of Technol., Finland; NA; NA; NA","IEE Proceedings - Computers and Digital Techniques","","2001","148","1","23","30","Finite state machine (FSM) optimisation has usually been studied through state assignment, state vector encoding, and combinational logic optimisation. Such details should not be consequential in behavioural descriptions. On the other hand, describing correct and efficient hardware structures in VHDL (VHSIC hardware description language), or generally in any high-level description language, is more a question of description style than correct language statements. Therefore, more or less conscious choices are made in the design description itself that guide the synthesis software toward a specific implementation. The best implementation is also dependent on the target technology and, therefore, there is no single best description style for all FSMs. The paper is a study of the kind of performance trade-offs that can be made by changing the description style. A program is shown to be able to generate these different descriptions from an intermediate format (kiss2) describing the FSM. Therefore, this process for finding a better description could be automated and performed by the synthesis software itself. Descriptions are tested on a set of 13 FSMs most from a benchmark suite LGSynth3. The results show at least two times better performance of speed or area in the best description compared with the worst. In performance critical applications this difference can be of a crucial importance.","1350-2387","","10.1049/ip-cdt:20010210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915281","","","hardware description languages;finite state machines;high level synthesis","VHDL synthesis;finite state machine;state assignment;state vector encoding;combinational logic optimisation;high-level description language;FSM","","13","","","","","","","IET","IET Journals & Magazines"
"Least-squares methods for the extraction of surface currents from CODAR crossed-loop data: Application at ARSLOE","B. Lipa; D. Barrick","Ocean Surface Res. Inc., Woodside, CA, USA; NA","IEEE Journal of Oceanic Engineering","","1983","8","4","226","253","Least-squares methods are demonstrated that extract surface current radial velocities from first-order Coastal Ocean Dynamics Applications Radar (CODAR) sea-echo Doppler spectra for the compact crossed-loop/monopole antenna system. Based on the known physics of first-order sea scatter at HF, these techniques, implemented as software, are objective and automatic in that they: a) determine from the sea-echo phase and amplitude correction factors for the antenna elements; b) separate the first-order spectrum from the surrounding continuum for arbitrarily varying current conditions; c) using statistical hypothesis testing, select and use either a single or dual-angle model for radial current patterns, whichever best fits the data; d) calculate angles associated with given radial velocities; e) combine the data into a polar-coordinate map of radial velocity versus position; and f) calculate radial velocity uncertainties at each point on the map. In addition, as interpretive aids, two methods are evaluated and compared that provide total current vectors from single-site CODAR data, along with their uncertainties: model fitting and the application of the equation of continuity. It is shown how these methods can be applied to the older, CODAR 4-element antenna system, however, the following advantages of the crossed-loop/monopole system are discussed: it is physically more compact; analysis procedures are more efficient; resulting current velocities are more accurate, because there are no side-lobe problems; and finally, it also gives the ocean wave-height directional spectrum. These methods are tested and optimized against data taken during the Atlantic Remote Sensing Land Ocean Experiment (ARSLOE) storm (October 23-27, 1980), when surface currents varied in speed between 0-50 cm/s and over nearly 300 in angle. Current velocities were measured to a range of 36 km from the radar. Standard deviations in angle are typically 1-3; these translate to 2-3 cm/s rms radial velocity uncertainties over most of the coverage area, with decreased accuracy in angular sectors nearest the coast. Total current velocity vectors in strips parallel to shore obtained from model fitting have typical speed and angle uncertainties of 4 cm/s and 12, respectively. Of the several formulations for the equation of continuity evaluated here, the best gave uncertainties of 5 cm/s, 12 at the closest range cells; these values increase rapidly with range to exceed 20 cm/s, 30 for distances greater than 20 km. The surface currents were observed to follow the wind throughout most of the storm at ARSLOE, but the current was almost always more closely parallel to the shore than the wind. An interesting exception occurred when the onshore storm wind that had prevailed for two days ceased; there was a rush of surface current directly offshore as the storm-surge sea level dropped. The surface current speed measured by CODAR in the upper meter of the ocean was, on the average, 2.1 percent of the windspeed.","0364-9059;1558-1691;2373-7786","","10.1109/JOE.1983.1145578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1145578","","Data mining;Sea surface;Oceans;Uncertainty;Sea measurements;Storms;Application software;Radar antennas;Equations;Current measurement","","HF radar;Least-squares approximation;Radar antennas;Sea surface electromagnetic scattering","","154","31","","","","","","IEEE","IEEE Journals & Magazines"
"Blackout prevention by optimal insertion of FACTS devices in power systems","D. Radu; Y. Besanger","Power Eng. Lab., UMR, St. Martin d'Heres, France; Power Eng. Lab., UMR, St. Martin d'Heres, France","2005 International Conference on Future Power Systems","","2005","","","9 pp.","6","Actual interconnected power systems are very large structures which suffer from a large gap between electricity demand and generation and from an inadequate transmission capacity of interconnection lines. In such systems, blackouts have become an all too common an occurrence. This paper proposes a method to reinforce the power systems by insertion of FACTS (flexible alternating current transmission system) devices, which are optimal placed in order to eliminate the dangerous contingencies of the systems and to prevent by this way the apparition of blackout if outages occurs. The FACTS devices are optimal placed in order to maximizing the power system security, the optimization is carried out using three parameters: the location of FACTS devices, their type and their sizes. For this purpose we developed a hybrid software based on GAs (genetic algorithms), which uses Matlabtrade and the EUROSTAGtrade software for load flow calculations. The proposed procedures are successfully tested on an IEEE 14-bus power system","","90-78205-02","10.1109/FPS.2005.204220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1600493","","Power systems;Power system interconnection;Hybrid power systems;Power generation;Flexible AC transmission systems;Power system security;Genetic algorithms;Computer languages;Load flow;System testing","flexible AC transmission systems;genetic algorithms;power engineering computing;power system interconnection;power transmission faults;power transmission protection","FACTS devices;optimal insertion;blackout prevention;interconnected power systems;electricity demand;inadequate transmission capacity;flexible alternating current transmission system devices;dangerous contingencies elimination;power system security;genetic algorithms;Matlab;EUROSTAG;load flow calculations;IEEE 14-bus power system","","9","12","","","","","","IEEE","IEEE Conferences"
"Graphics file generation for a computer-aided manual work station in the electronics manufacturing environment","A. R. Hidde; H. Rath","INSTA ELEKTRO GmbH, Luedenscheid, Germany; NA","Proceedings of 15th IEEE/CHMT International Electronic Manufacturing Technology Symposium","","1993","","","362","367","Based on a standardized, product and production independent concept for factory automation, some general thoughts concerning the acquisition of operation, machine and quality data for manufacturing are presented, and the application of a printed circuit board (PCB) production preparation system as a part of the manufacturing design system is discussed. The necessity of automation of production preparation is based on a continuous flow of information in the manufacturing environment to reduce the throughput/cycle time, to reduce the set-up time and to have better product quality. The decrease of time in the different hierarchical layers of the production reference model, especially in the area of production preparation, can be used to develop tested, error-free, simulated and optimized product, process and machine describing data files.<<ETX>>","","0-7803-1424","10.1109/IEMT.1993.398181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=398181","","Computer aided manufacturing;Computer graphics;Manufacturing automation;Production control;Production systems;Virtual manufacturing;Control systems;Manuals;Computer errors;Costs","computer integrated manufacturing;printed circuit manufacture;production control;production engineering computing;hierarchical systems;engineering workstations;workstations;engineering graphics;flexible manufacturing systems;graphical user interfaces;CAD/CAM;image recognition","PCB production preparation system;software configuration;graphics file generation;computer integrated manufacturing;reference model;hierarchical production control system;pixel/vector transformation;computer-aided manual work station;electronics manufacturing environment;factory automation;manufacturing design system;continuous flow of information;throughput/cycle time;set-up time;product quality;production reference model","","","9","","","","","","IEEE","IEEE Conferences"
"Sleipnir. An instruction-level simulator generator","T. E. Jeremiassen","Lucent Technol. Bell Labs., Murray Hill, NJ, USA","Proceedings 2000 International Conference on Computer Design","","2000","","","23","31","Instruction-level simulators occupy a central role in the software development for embedded processors. They provide a convenient virtual platform for testing, debugging and optimizing code. They can be made available long before any hardware is available, and are not as awkward to work with as test/evaluation boards. However, many available instruction-level simulators are lacking in desired functionality. Moreover, instruction-level simulators suitable to the task are tedious to write from scratch. This paper presents the Sleipnir simulator generator, a convenient tool for writing instruction-level simulators. Sleipnir allows simulators for simple architectures to be generated with a minimum of overhead, yet allows sufficient micro-architectural detail to be expressed to generate cycle accurate simulators for most embedded processors. Sleipnir has been used to successfully generate fast instruction-level simulators for six different architectures, including a RISC processor, two microcontrollers and three DSPs.","1063-6404","0-7695-0801","10.1109/ICCD.2000.878265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=878265","","Computational modeling;Testing;Hardware;Programming;Digital signal processing;Statistics;Instruments;Debugging;Writing;Computer architecture","computer architecture;embedded systems;virtual machines","Sleipnir;instruction-level simulator generator;instruction-level simulator;RISC processor;microcontrollers;micro-architecture","","7","23","","","","","","IEEE","IEEE Conferences"
"Temporal segmentation of video sequences for content-based coding","J. F. Camapum Wanderley; D. C. Danna","Departamento de Engenharia Eletrica, Brasilia Univ., Brazil; Departamento de Engenharia Eletrica, Brasilia Univ., Brazil","Proceedings. XV Brazilian Symposium on Computer Graphics and Image Processing","","2002","","","321","326","We present an overview of the state of the art of segmentation based video coding aimed at low-bit rate application. The most significant contribution is the algorithm for the segmentation of videoconference image sequences for content-based coding. The segmentation is based on motion estimation through the computation of the optical flow field and motion segmentation by applying a graph-theoretical clustering. The algorithm will be added to the video codecs of the OpenH323 project which is based on H.323 ITU-T Recommendation. This visual collaboration environment for Desktop, composed of terminals, gatekeepers and MCU (Multipoint Control Unit), has already been implemented and tested. It is particularly convenient for academic implementation, the software is freely distributed, and its code can be modified according to the needs of the user. However, the first tests using the H.261 codec showed a video delay of 3 seconds, confirming the need to optimize the video codec.","1530-1834","0-7695-1846","10.1109/SIBGRA.2002.1167161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167161","","Video sequences;Image segmentation;Clustering algorithms;Optical computing;Video codecs;Testing;Video coding;Videoconference;Image sequences;Image coding","video coding;image segmentation;image sequences;teleconferencing;motion estimation;graph theory;video codecs","video sequence segmentation;temporal segmentation;content-based coding;segmentation based video coding;low-bit rate application;videoconference image sequences;motion estimation;optical flow field;video coding;motion segmentation;graph-theoretical clustering;video codecs;OpenH323 project;H.323 ITU-T Recommendation;Multipoint Control Unit;H.261 codec;video delay","","","20","","","","","","IEEE","IEEE Conferences"
"A new task-based control architecture for personal robots","Jin-Oh Kim; Chang-Jun Im; Hyun-Jong Shin; Keon Young Yi; Ho Gil Lee","Dept. of Control & Instrum. Eng., Kwangwoon Univ., Seoul, South Korea; Dept. of Control & Instrum. Eng., Kwangwoon Univ., Seoul, South Korea; Dept. of Control & Instrum. Eng., Kwangwoon Univ., Seoul, South Korea; NA; NA","Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)","","2003","2","","1481","1486 vol.2","As the possible applications for personal robots increase and involve more complicated environments and tasks, they need more appropriate control architectures. Previous control architectures have fixed configurations that are optimized for specific applications or with limited flexibility. Consequently, they fail to provide the flexibility needed for various robot kinematical configurations as well as various tasks. To this end, we propose a new task-based control architecture called ""supervised hybrid architecture (SHA)"". SHA is based on supervised organization and distributed arbitration of hybrid controls of reconfigurable deliberative and reactive modules. It is composed of upper level hybrid control for high-level intelligence to interact with human and to plan tasks, as well as lower level hybrid control to allow low-level intelligence for prompt reaction in each robot configuration module. Through these double layers of the hybrid controller, we could provide the flexibility needed for so many different kinematical configurations and tasks. In addition, it is very easy to add or remove robot configuration modules. The proposed architecture are implemented and tested to show how it works successfully.","","0-7803-7860","10.1109/IROS.2003.1248853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1248853","","Robot sensing systems;Robot control;Intelligent robots;Robotics and automation;Distributed control;Instruments;Robotic assembly;Automatic control;Costs;Gas insulated transmission lines","robot programming;control system synthesis;control engineering computing;intelligent robots;reconfigurable architectures;robot kinematics;distributed control;mobile robots;software architecture","task-based control architecture;personal robots;supervised hybrid architecture;supervised organization;distributed arbitration;hybrid controls;reconfigurable deliberative modules;reactive modules;high-level intelligence control;human interaction;task planning;low-level intelligence control;flexibility;kinematical configurations","","","11","","","","","","IEEE","IEEE Conferences"
"Fuzzy constraint enforcement and control action curtailment in an optimal power flow","W. -. Edwin Liu; Xiaohong Guan","Pacific Gas & Electr. Co., San Francisco, CA, USA; NA","Proceedings of Power Industry Computer Applications Conference","","1995","","","73","78","Conventional power system optimal power flow (OPF) solutions utilize standard optimization techniques. These techniques limit the practical value and scope of OPF applications. Realistic OPF solutions require special attention to the constraint enforcement and control action curtailment. From a practical point of view, an OPF does not need to find a rigid minimum solution. Certain trade-offs among: (1) minimizing the objective function; (2) satisfying constraints; and (3) moving control variables would be desirable. In this paper, a fuzzy set method is applied to the OPF problem to model these considerations. The method has been implemented in a successive linear programming OPF software package. Test results based on the PG&E network are promising.","","0-7803-2663","10.1109/PICA.1995.515167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=515167","","Fuzzy control;Optimal control;Load flow;Fuzzy set theory;Power system modeling;Packaging;Energy management;Control systems;Power system control;Constraint optimization","power system control;power system analysis computing;load flow;load regulation;optimal control;control system analysis computing;digital simulation;fuzzy set theory;linear programming","power system;optimal power flow;fuzzy constraint enforcement;control action curtailment;objective function;constraints satisfaction;control variables;fuzzy set method;computer simulation;successive linear programming","","","16","","","","","","IEEE","IEEE Conferences"
"SIFT: snort intrusion filter for TCP","M. Attig; J. Lockwood","Dept. of Comput. Sci. & Eng., Washington Univ., St. Louis, MO, USA; Dept. of Comput. Sci. & Eng., Washington Univ., St. Louis, MO, USA","13th Symposium on High Performance Interconnects (HOTI'05)","","2005","","","121","127","Intrusion rule processing in reconfigurable hardware enables intrusion detection and prevention services to run at multiGigabit/second rates. High-level intrusion rules mapped directly into hardware separate malicious content from benign content in network traffic. Hardware parallelism allows intrusion systems to scale to support fast network links, such as OC-192 and 10 Gbps Ethernet. In this paper, a snort intrusion filter for TCP (SIFT) is presented that operates as a preprocessor to prevent benign traffic from being inspected by an intrusion monitor running Snort. Snort is a popular open-source rule-processing intrusion system. SIFT selectively forwards IP packets that contain questionable headers or defined signatures to a PC where complete rule processing is performed. SIFT alleviates the need for most network traffic from being inspected by software. Statistics, like how many packets match rules, are used to optimize rule processing systems. SIFT has been implemented and tested in FPGA hardware and used to process Internet traffic from a campus Internet backbone with live data.","1550-4794;2332-5569","0-7695-2449","10.1109/CONECT.2005.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544587","","Filters;Hardware;Telecommunication traffic;Open source software;Internet;Intrusion detection;Ethernet networks;TCPIP;Monitoring;Statistics","security of data;reconfigurable architectures;parallel architectures;information filters;Internet;transport protocols;field programmable gate arrays;IP networks;open systems","snort intrusion filter;TCP;open-source SIFT;intrusion rule processing;reconfigurable hardware parallelism;multiGigabit-second rate;benign network traffic;preprocessor;intrusion detection-prevention;Internet;FPGA;IP packet;questionable header;signature segmentation;10 Gbit/s","","15","23","","","","","","IEEE","IEEE Conferences"
"An efficient OS support for communication on Linux clusters","A. F. Diaz; J. Ortega; F. J. Fernandez; M. Anguita; A. Canas; A. Prieto","Dept. de Arquitectura y Tecnologia de Computadores, Granada Univ., Spain; NA; NA; NA; NA; NA","Proceedings International Conference on Parallel Processing Workshops","","2001","","","397","402","A communication layer is proposed that, besides improving communication performance on clusters of PCs, by reducing the latencies and increasing the bandwidth figures even for short messages, also meets other requirements such as multiprogramming, portability, protection against corrupted programs, reliable message delivery, direct access to the network for all applications, etc. Instead of removing the operating system kernel from the critical path and creating a user-level network interface, our aim was to optimize the operating system support to provide reliable and efficient network software, avoiding the TCP/IP protocol stack. The communication system was tested in a cluster of PCs with Linux OS and interconnected with Fast Ethernet. The performance figures obtained define the best situation that can be attained without modifying the device drivers or using a user-level network interface approach.","1530-2016","0-7695-1260","10.1109/ICPPW.2001.951978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=951978","","Linux;Personal communication networks;Telecommunication network reliability;Operating systems;Network interfaces;Delay;Bandwidth;Protection;Application software;Kernel","operating systems (computers);local area networks;performance evaluation;workstation clusters","OS support;Linux clusters;communication layer;Fast Ethernet;performance figures","","3","11","","","","","","IEEE","IEEE Conferences"
"Loop Scheduling for Multithreaded Processors","G. Dimitriou; C. Polychronopoulos","University of Thessaly, Volos, Greece; NA","Parallel Computing in Electrical Engineering, 2004. International Conference on","","2004","","","361","366","The presence of multiple active threads on the same processor can mask latency by rapid context switching, but it can adversely affect performance due to competition for shared datapath resources. In this paper we present Macro Software Pipelining (MSWP), a loop scheduling technique for multithreaded processors, which is based on the loop distribution transformation for loop pipelining. MSWP constructs loop schedules by partitioning the loop body into tasks and assigning each task to a thread that executes all iterations for that particular task. MSWP is applied top-down on a hierarchical program representation, and utilizes thread-level speculation for maximal exploitation of parallelism. We tested MSWP on a multithreaded architectural model, Coral 2000, using synthetic and SPEC benchmarks. We obtained speedups of up to 30% with respect to highly optimized superblock-based schedules on loops with unpredictable branches, and a speedup of up to 25% on perl, a highly sequential SPEC95 integer benchmark.","","0-7695-2080","10.1109/PCEE.2004.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376782","","Processor scheduling;Pipeline processing;Yarn;Parallel processing;Delay;Benchmark testing;Switches;Data engineering;Registers;Hardware","","","","1","21","","","","","","IEEE","IEEE Conferences"
"A Monte Carlo model of noise components in 3D PET","I. Castiglioni; O. Cremonesi; M. -. Gilardi; A. Savi; V. Bettinardi; G. Rizzo; E. Bellotti; F. Fazio","CNR, Milan, Italy; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Nuclear Science","","2002","49","5","2297","2303","This work presents a new model, developed by Monte Carlo methods, to estimate noise components (scatter and random coincidences) in three-dimensional (3-D) positron emission tomography (PET). The model allows the amount, spatial, and temporal distribution of true, scattered, and random coincidences to be estimated independently for any radioactive source (both phantoms and real patients), taking proper, account of system dead time. The model was applied to a 3-D NaI(Tl) current-generation PET scanner for which there are no currently available methods for estimating scatter and random components in whole-body studies. The quantitative accuracy of the developed noise model was tested by comparing simulated and measured PET data in terms of physical parameters, count-rate curves, and spatial distribution profiles. Scatter and random components were assessed for phantoms representing brain, abdomen, and whole-body studies. Evidence was found of high scatter and random contribution in 3-D PET clinical studies. The clinical response of the PET system, in terms of signal-to-noise ratio, was assessed and optimized, confirming the suitability of the default energy window, although suggesting a possible improvement by setting a lower energy threshold higher than the current default: The proposed noise model applies to any current generation 3-D PET scanner and has been included in the Monte Carlo software package PET-EGS, devoted to 3-D PET and freely available from the authors.","0018-9499;1558-1578","","10.1109/TNS.2002.803686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046907","","Monte Carlo methods;Positron emission tomography;Scattering;Whole-body PET;Imaging phantoms;Signal to noise ratio;Testing;Brain modeling;Noise measurement;Abdomen","Monte Carlo methods;positron emission tomography;random noise","Monte Carlo model;noise components;NaI(Tl) PET scanner;brain;abdomen;whole-body;signal-to-noise ratio;PET-EGS;count-rate curves;spatial distribution profiles;phantoms;3D PET;NaI:Tl","","8","29","","","","","","IEEE","IEEE Journals & Magazines"
"PuMa, the first fully digital pulsar machine","P. C. Van Haren; J. L. L. Voute; T. D. Beijaard; D. Driesens; M. L. A. Kouwenhoven; J. J. Langerak","Instrum. Group Phys., Utrecht Univ., Netherlands; NA; NA; NA; NA; NA","IEEE Transactions on Nuclear Science","","2000","47","2","91","98","Pulsars are neutron stars, rapidly rotating remains of supernova explosions, emitting bundles of broadband electromagnetic radiation. Researching these signals yields tests for fundamental physics theories and insight in the evolution of stars. To carry out pulsar observations, two hurdles have to be overcome. Typically, the signal-to-noise ratio is poor, requiring long observations and large bandwidths. Next there is dispersion, causing the pulsating signals to smear out and calls for narrow signal bands. Using many parallel narrow signal bands resolves this dilemma. Traditionally, pulsar machines use tens of parallel (analog) heterodyne receivers. Though impractical, it is desirable to have many more receivers. PuMa, the first Dutch pulsar machine, uses digital signal processing to split the incoming signal in up to thousands of narrow bands. The processor based design also increases flexibility as it allows different observational modes by loading the appropriate software into the signal processors. In total 192 SHARC processors (ADSP 21062) deliver the processing capacity. For PuMa a general purpose 6-processor SHARC board was developed, optimized for concurrent use of data busses. Other parts are commercially available components and all is joined in a VME environment. Mid 1998 PuMa was installed at the Westerbork Synthesis Radio Telescope in the Netherlands and its commissioning is completed.","0018-9499;1558-1578","","10.1109/23.846124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=846124","","Signal processing;Neutrons;Explosions;Electromagnetic radiation;Testing;Physics;Signal to noise ratio;Bandwidth;Signal resolution;Digital signal processing","pulsars;digital signal processing chips;data acquisition;radioastronomical techniques","PuMa;broadband electromagnetic radiation;pulsar observations;signal-to-noise ratio;heterodyne receivers;digital signal processing;SHARC processors;concurrent use;Westerbork Synthesis Radio Telescope","","3","19","","","","","","IEEE","IEEE Journals & Magazines"
"A performance methodology for commercial servers","S. R. Kunkel; R. J. Eickemeyer; M. H. Lipasti; T. J. Mullins; B. O'Krafka; H. Rosenberg; S. P. VanderWiel; P. L. Vitale; L. D. Whitley","IBM Server Group, 3605 Highway 52 N, Rochester, Minnesota 55901, USA; IBM Server Group, 3605 Highway 52 N, Rochester, Minnesota 55901, USA; University of Wisconsin at Madison, 4613 Engineering Hall, 1415 Engineering Drive, Wisconsin 53706, USA; IBM Server Group, 3605 Highway 52 N, Rochester, Minnesota 55901, USA; Sun Microsystems, MS AUS08, 5300 Riata Park Court, Austin, Texas 78727, USA; Sun Microsystems, One Network Drive, MS UBUR03-212, Burlington, Massachusetts 01803, USA; IBM Server Group, 3605 Highway 52 N, Rochester, Minnesota 55901, USA; IBM Server Group, 3605 Highway 52 N, Rochester, Minnesota 55901, USA; IBM Server Group, 3605 Highway 52 N, Rochester, Minnesota 55901, USA","IBM Journal of Research and Development","","2000","44","6","851","872","This paper discusses a methodology for analyzing and optimizing the performance of commercial servers. Commercial server workloads are shown to have unique characteristics which expand the elements that must be optimized to achieve good performance and require a unique performance methodology. The steps in the process of server performance optimization are described and include the following: 1. Selection of representative commercial workloads and identification of key characteristics to be evaluated. 2. Collection of performance data. Various instrumentation techniques are discussed in light of the requirements placed by commercial server workloads on the instrumentation. 3. Creation of input data for performance models on the basis of measured workload information. This step in the methodology must overcome the operating environment differences between the instance of the measured system under test and the target system design to be modeled . 4. Creation of performance models. Two general types are described: high-level models and detailed cycle-accurate simulators. These types are applied to model the processor, memory, and I/O system. 5. System performance optimization. The tuning of the operating system and application software is described. Optimization of performance among commercial applications is not simply an exercise in using traces to maximize the processor MIPS. Equally significant are items such as the use of probabilities to reflect future workload characteristics, software tuning, cache miss rate optimization, memory management, and I/O performance. The paper presents techniques for evaluating the performance of each of these key contributors so as to optimize the overall performance and cost/performance of commercial servers.","0018-8646;0018-8646","","10.1147/rd.446.0851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5389120","","","","","","18","","","","","","","IBM","IBM Journals & Magazines"
"Explicit model predictive control of gas-liquid separation plant","A. Grancharova; T. A. Johansen; J. Kocijan","Department of Engineering Cybernetics, Norwegian University of Science and Technology, 7491 Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology, 7491 Trondheim, Norway; Department of Systems and Control, Jozef Stefan Institute, Jamova 39, 1000 Ljubljana, Slovenia","2003 European Control Conference (ECC)","","2003","","","2475","2480","Exact or approximate solutions to constrained linear model predictive control problems can be pre-computed off-line in an explicit form as a piecewise linear state feedback defined on a polyhedral partition of the state space. This leads to efficient real-time computations and admits implementation at high sampling frequencies in real-time systems with high reliability and low software complexity. In this paper, an explicit model predictive controller for a gas-liquid separation plant is designed and experimentally tested.","","978-3-9524173-7","10.23919/ECC.2003.7085337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7085337","Constrained linear model predictive control;Multi-parametric quadratic programming;Gas-liquid separator","Real-time systems;Optimization;Approximation algorithms;Trajectory;Partitioning algorithms;Valves;Complexity theory","","","","","14","","","","","","IEEE","IEEE Conferences"
"Multielement CdTe stack detectors for gamma-ray spectroscopy","R. Redus; A. Huber; J. Pantazis; T. Pantazis; T. Takahashi; S. Woolf","Amptek Inc., Bedford, MA, USA; Amptek Inc., Bedford, MA, USA; Amptek Inc., Bedford, MA, USA; Amptek Inc., Bedford, MA, USA; NA; NA","IEEE Transactions on Nuclear Science","","2004","51","5","2386","2394","This paper describes the development of a field portable /spl gamma/-ray spectroscopy system based on multielement CdTe detectors. A stack of multiple planar detector elements is used, with each individual element thin enough for good charge transport and outputs summed so that the full volume is used. The detector elements are CdTe diodes mounted on a thermoelectric cooler, providing low noise and good charge transport with relatively simple electronics. Current devices have a volume of 5 mm /spl times/5 mm /spl times/2.25 mm, with an energy resolution of 1.5 (4.5) keV full-width at half-maximum (FWHM) at 122 (662) keV. These operate as simple planar devices, with no corrections or complex electronics. We anticipate achieving a resolution of <1% FWHM at 662 keV with a volume of at least 0.5 cm/sup 3/ and that the approach will be suitable for production. To optimize the detector configuration, simulation software was developed and test devices fabricated. This paper will present simulation results and experimental data obtained using multielement detectors.","0018-9499;1558-1578","","10.1109/TNS.2004.832295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1344342","","Gamma ray detection;Gamma ray detectors;Spectroscopy;Instruments;Energy resolution;Packaging;Thermoelectricity;Production;Photonic band gap;Leakage current","semiconductor counters;gamma-ray spectroscopy;gamma-ray detection;nuclear electronics;high energy physics instrumentation computing","multielement CdTe stack detectors;field portable gamma-ray spectroscopy system;multiple planar detector elements;charge transport;CdTe diodes;thermoelectric cooler;low noise;electronics;current devices;energy resolution;full-width at half-maximum;FWHM;planar devices;detector configuration;simulation software;5 mm;2.25 mm","","10","18","","","","","","IEEE","IEEE Journals & Magazines"
"Heterogeneous implementation of a rake receiver for DS-CDMA communication systems","D. C. de Souza; I. Krikidis; L. Naviner; J. -. Dangers; M. A. de Barros; B. G. A. Neto","Dept. COMELEC, GET/ENST - Telecom Paris, France; Dept. COMELEC, GET/ENST - Telecom Paris, France; Dept. COMELEC, GET/ENST - Telecom Paris, France; Dept. COMELEC, GET/ENST - Telecom Paris, France; Dept. de Sistemas de Computao, Brazil; Dept. de Engenharia Eltrica, Universidade Federal de Campina Grande, Brazil","2005 12th IEEE International Conference on Electronics, Circuits and Systems","","2005","","","1","4","This work describes the development of an optimized partitioning algorithm for HW/SW codesign, which employs more realistic cost measures than previous partitioning algorithms and takes into account FPGA reconfiguration time. This algorithm is then used to find the optimal implementation of a digital RAKE receiver for DS-CDMA on an heterogeneous (hardware/software) platform. We additionally describe a new architectural approach for this type of receiver: in contrast with conventional architectures, which suppose a constant number of demodulation forgers, our new architecture allows a dynamic number of fingers, one for each propagation environment. The introduced flexibility allows this RAKE receiver to have the required computational power for each possible environment. It constitutes the selected application to test our partitioning algorithm.","","978-9972-61-100","10.1109/ICECS.2005.4633551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4633551","","","code division multiple access;field programmable gate arrays;radio receivers","digital RAKE receiver;DS-CDMA communication systems;hardware-software codesign;partitioning algorithms;FPGA reconfiguration time;propagation environment;architectural approach;direct sequence code division multiple access","","","6","","","","","","IEEE","IEEE Conferences"
"High performance code generation for VLIW digital signal processors","Yin-Tsung Hwang; Ying-Chou Chuang","Inst. of Electron. & Inf. Eng., Nat. Yunlin Univ. of Sci. & Technol., Taiwan; NA","2000 IEEE Workshop on SiGNAL PROCESSING SYSTEMS. SiPS 2000. Design and Implementation (Cat. No.00TH8528)","","2000","","","683","692","VLIW (Very Long Instruction Word) architecture has been widely adopted in latest digital signal processor designs to meet the ever increasing need of computing power in, e.g. multimedia applications. In this paper, we present an efficient and retargetable code generation tool for VLIW based DSPs. To make the code generation tool retargetable, we first developed a versatile coding constraint model to faithfully characterize the target machine's limitations on hardware resource, pipeline execution and other specific instruction usage. Second, since loop executions account for most of the time consumption, the software pipelining technique was employed to overlap the execution of successive iterations. To generate efficient code for real time applications, a simulated evolution (SE) based code generation module was introduced to derive the steady state loop scheduling. Effective heuristics subject to various coding constraints were developed. In addition, to alleviate the scheduling overhead in checking coding/resource constraints repetitively, bit-parallel verification schemes was devised as well. Several test benches on TI TMS320C62/67X DSP have been conducted to verify the effectiveness of our tool and preliminary results show that it can achieve near hand optimized quality code.","1520-6130","0-7803-6488","10.1109/SIPS.2000.886766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886766","","Signal generators;VLIW;Digital signal processors;Digital signal processing;Pipeline processing;Computer architecture;Process design;Signal design;Computer aided instruction;Multimedia computing","program compilers;pipeline processing;digital signal processing chips;parallel architectures","code generation;VLIW digital signal processors;code generation tool;VLIW based DSPs;pipeline execution;software pipelining","","2","10","","","","","","IEEE","IEEE Conferences"
"Microprocessor design using silicon compiler","W. T. Webb; K. G. Nichols","Dept. of Electron. & Comput. Sci., Southampton Univ., UK; Dept. of Electron. & Comput. Sci., Southampton Univ., UK","IEE Proceedings E - Computers and Digital Techniques","","1991","138","4","232","240","The problems of designing microprocessors using a silicon compiler are discussed. The example used throughout is the Motorola MC6800. Details are given as to how the software description of the microprocessor was developed and optimised, how it was tested, and the necessary modifications required for the silicon compiler used. Consideration is given to faster, parallel architectures which give significant speed improvements at the cost of an increase in complexity.<<ETX>>","0143-7062","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=81901","","Design automation;Microprocessors","circuit layout CAD;microprocessor chips","microprocessors;silicon compiler;Motorola MC6800;software description;silicon compiler;parallel architectures","","","","","","","","","IET","IET Journals & Magazines"
"Visibility culling for time-varying volume rendering using temporal occlusion coherence","Jinzhu Gao; Han-Wei Shen; Jian Huang; J. A. Kohl","Oak Ridge State Lab., Ohio State Univ., Columbus, OH, USA; Oak Ridge State Lab., Ohio State Univ., Columbus, OH, USA; NA; NA","IEEE Visualization 2004","","2004","","","147","154","Typically there is a high coherence in data values between neighboring time steps in an iterative scientific software simulation; this characteristic similarly contributes to a corresponding coherence in the visibility of volume blocks when these consecutive time steps are rendered. Yet traditional visibility culling algorithms were mainly designed for static data, without consideration of such potential temporal coherency. We explore the use of temporal occlusion coherence (TOC) to accelerate visibility culling for time-varying volume rendering. In our algorithm, the opacity of volume blocks is encoded by means of plenoptic opacity functions (POFs). A coherence-based block fusion technique is employed to coalesce time-coherent data blocks over a span of time steps into a single, representative block. Then POFs need only be computed for these representative blocks. To quickly determine the subvolumes that do not require updates in their visibility status for each subsequent time step, a hierarchical ""TOC tree"" data structure is constructed to store the spans of coherent time steps. To achieve maximal culling potential, while remaining conservative, we have extended our previous POP into an optimized POP (OPOP) encoding scheme for this specific scenario. To test our general TOC and OPOF approach, we have designed a parallel time-varying volume rendering algorithm accelerated by visibility culling. Results from experimental runs on a 32-processor cluster confirm both the effectiveness and scalability of our approach.","","0-7803-8788","10.1109/VISUAL.2004.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1372191","visibility culling;time-varying data visualization;volume rendering;plenoptic opacity function;large data visualization","Data visualization;Computational modeling;Computer graphics;Algorithm design and analysis;Acceleration;Rendering (computer graphics);Clustering algorithms;Large-scale systems;Laboratories;Iterative algorithms","data visualisation;rendering (computer graphics);temporal databases;tree data structures","visibility culling;time-varying volume rendering;temporal occlusion coherence;iterative scientific software simulation;plenoptic opacity functions;tree data structure;parallel algorithm;data visualization","","1","32","","","","","","IEEE","IEEE Conferences"
"SLIC: scheduled linear image compositing for parallel volume rendering","A. Stompel; K. -. Ma; E. B. Lum; J. Ahrens; J. Patchett","California Univ., Davis, CA, USA; California Univ., Davis, CA, USA; California Univ., Davis, CA, USA; NA; NA","IEEE Symposium on Parallel and Large-Data Visualization and Graphics, 2003. PVG 2003.","","2003","","","33","40","Parallel volume rendering offers a feasible solution to the large data visualization problem by distributing both the data and rendering calculations among multiple computers connected by a network. In sort-last parallel volume rendering, each processor generates an image of its assigned subvolume, which is blended together with other images to derive the final image. Improving the efficiency of this compositing step, which requires interprocesssor communication, is the key to scalable, interactive rendering. The recent trend of using hardware-accelerated volume rendering demands further acceleration of the image compositing step. We present a new optimized parallel image compositing algorithm and its performance on a PC cluster. Our test results show that this new algorithm offers significant savings over previous algorithms in both communication and compositing costs. On a 64-node PC cluster with a 100BaseT network interconnect, we can achieve interactive rendering rates for images at resolutions up to 1024x1024 pixels at several frames per second.","","0-7803-8122","10.1109/PVGS.2003.1249040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249040","","Rendering (computer graphics);Clustering algorithms;Pixel;Concurrent computing;Computer graphics;Software algorithms;High performance computing;Algorithm design and analysis;Partitioning algorithms;Hardware","rendering (computer graphics);data visualisation;workstation clusters;computational complexity;image resolution;parallel processing","SLIC;scheduled linear image compositing;parallel volume rendering;data visualization;sort-last parallel volume rendering;interprocesssor communication;scalable interactive rendering;PC cluster;communication cost;100BaseT network interconnect;image resolution","","20","13","","","","","","IEEE","IEEE Conferences"
"Multiprocessor supercomputers for scientific/engineering applications","Kai Hwang","University of Southern California","Computer","","1985","18","6","57","73","Supercomputers, the fastest computers available, are playing a major role in advancing human civilization. Based on today's technology, a computer is considered a supermachine if it can perform hundreds of milllions of floating-point operations per second (100 M-Flops) with a word length of approximately 64 bits and a main memory capacity of millions of words. The value of supercomputers has been identified in three general areas: (1) supercomputers aid in knowledge acquisition by processing complex data within a reasonable time. (2) supercomputers provide computational tractability in instrumentation, predictive simulations, energy resource exploration, general circulation modeling, weapons effects, and atmospheric testing. (3) supercomputers promote productivity, as in system optimization, computeraided design and manufacturing, and VLSI circuit design. The author briefly reviews the evolution of supercomputers, then discusses supercomputing in science and engineering. He considers multiprocessing supercomputers and exploratory research multiprocessors and finally considers future directions.","0018-9162;1558-0814","","10.1109/MC.1985.1662923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662923","","Supercomputers;Application software;Military computing;Geophysics computing;Parallel processing;Information processing;Multiprocessing systems;Power engineering and energy","","","","16","43","","","","","","IEEE","IEEE Journals & Magazines"
"Low input current ripple converters for fuel cell power units","S. De Caro; A. Testa; D. Triolo; M. Cacciato; A. Consoli","DFMTFA, Messina Univ., Italy; DFMTFA, Messina Univ., Italy; DFMTFA, Messina Univ., Italy; NA; NA","2005 European Conference on Power Electronics and Applications","","2005","","","10 pp.","P.10","Proton exchange membrane fuel cell (PEMFC) generators are today considered as a viable solution to the development of auxiliary, or back up, power units for automotive, computer and telecommunications systems. Several of these applications require a power conditioner with high efficiency, high conversion ratio and very low input current ripple to interface the fuel cell generator to a high-voltage DC bus. In this paper, two step-up non insulated DC/DC converter topologies, suitable to equip low power fuel cell power units are developed and deeply examined. A key feature of the two proposed converters is that they are obtained by mixing some basic topologies, in order to optimize both duty cycle and the winding ratio. Moreover, multichannel interleaved power conversion structures are adopted in order to reduce the size of input and output filters. Finally, both the proposed topologies feature an input inductance, that plays a major role in lowering the fuel cell output current ripple, thus reducing fuel consumption. The two proposed converters are theoretically analyzed and experimentally tested","","90-75815-09","10.1109/EPE.2005.219647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665837","Fuel Cell system;DC power supply;Automotive application;Converter control;Multiphase drive","Fuel cells;Topology;Power generation;Protons;Biomembranes;Automotive engineering;Telecommunication computing;Application software;DC generators;Insulation","DC-DC power convertors;fuel cell power plants;network topology;power filters;proton exchange membrane fuel cells","current ripple converters;proton exchange membrane fuel cell;PEMFC generators;telecommunications systems;power conditioner;high-voltage DC bus;DC-DC converter topologies;winding ratio;multichannel interleaved power conversion structures;fuel consumption reduction","","8","10","","","","","","IEEE","IEEE Conferences"
"The Megacell concept: an approach to painless custom design","J. S. Brothers; J. W. Tomkins; J. S. Williams","Plessey Semiconductors Ltd., Swindon, UK; Plessey Research (Caswell) Ltd., Allen Clark Research Centre, Towcester, UK; Plessey Research (Caswell) Ltd., Allen Clark Research Centre, Towcester, UK","IEE Proceedings I - Solid-State and Electron Devices","","1985","132","2","91","98","Megacell is a complete design package for the creation of complex VLSI chips. It allows system engineers to develop their own custom integrated circuits. The challenge of Megacell was to create a design system that would exploit fully the low-power and high-speed performance of a UK developed 2 m CMOS technology, while producing the silicon utilisation efficiency typical of full custom design. This was accomplished without losing the now familiar semicustom design attributes of `first-time success, coupled with facilities for the user to complete his own design. The full features of the Megacell design system are described, and projections are made of how its capabilities will be extended as CMOS technology edges towards 1 m. The provision of a series of cell structures of increasing complexity within Megacell allows the user to optimise his layout, without inducing the uncharacterised variations that would be usual in a full custom design. The cell structure has within it three major types: microcells which are the low-level logic cells found in most array or cell design systems; paracells which are cells of high functional capability, compiled by user software specifically for the customer's application. The paracell is created from parameters, input by the user, as he designs his circuit; and supracells which have the highest complexity and are cells that replicate the function of today's LSI standard products. They include not only digital functions, such as microprocessors, but also analogue blocks such as A/D and D/A convenors. The heart of the Megacell design system is its CAD. Alongside the cell structures, a comprehensive set of integrated design tools has been developed to cover the complete spectrum of user requirements. Included in the system are schematic capture, simulation, test validation and generation, as well as full layout capabilities.","0143-7100","","10.1049/ip-i-1.1985.0020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4643878","","","circuit layout CAD;CMOS integrated circuits;integrated circuit technology;logic CAD;VLSI","layout optimisation;2 microns design rules;microcells;parallels;supracells;CAD;computer-aided design;custom design;VLSI chips;integrated circuits;CMOS technology;Megacell design system","","","","","","","","","IET","IET Journals & Magazines"
"A fast pipelined complex multiplier: the fault tolerance issues","L. Breveglieri; V. Piuri; D. Sciuto","Dipartimento di Elettronica, Politecnico di Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Italy","Proceedings 1992 IEEE International Workshop on Defect and Fault Tolerance in VLSI Systems","","1992","","","277","286","A comprehensive discussion of a dedicated device for serial complex multiplication is presented, covering architectural, reliability and fault tolerance properties. The pipelined architecture is briefly described. It is optimized w.r.t. several figure of merits: clock rate, external pipelining and pipeline filling degree. Testability features are analyzed under functional fault models by means of graph-theoretic methods, showing full testability of the device. Error detection is introduced by means of arithmetic codes and the tradeoff between error detection and cost is evaluated. Eventually on-line reconfiguration is introduced through the Diogenes approach and the tradeoff between fault tolerance and cost is also discussed. Discussion are based on analytic interpolation software simulation and the evaluation of prototypal layouts in CMOS technology.<<ETX>>","1550-5774","0-8186-2837","10.1109/DFTVS.1992.224347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224347","","Fault tolerance;Pipeline processing;Testing;Costs;CMOS technology;Clocks;Filling;Arithmetic;Interpolation;Analytical models","CMOS integrated circuits;multiplying circuits;parallel architectures;pipeline processing","pipelined complex multiplier;fault tolerance issues;serial complex multiplication;reliability;fault tolerance properties;pipelined architecture;clock rate;external pipelining;pipeline filling degree;functional fault models;graph-theoretic methods;testability;arithmetic codes;error detection;Diogenes approach;analytic interpolation;prototypal layouts;CMOS technology","","1","12","","","","","","IEEE","IEEE Conferences"
"Configuring a wafer-scale two-dimensional array of single-bit processors","A. Boubekeur; J. -. Patry; G. Saucier; J. Trilhe","Nat. Polytech. Inst. of Grenoble, France; Nat. Polytech. Inst. of Grenoble, France; Nat. Polytech. Inst. of Grenoble, France; NA","Computer","","1992","25","4","29","39","An overview of the ELSA (European large SIMD array) project, which uses a two-level strategy to achieve defect tolerance for wafer-scale architectures implemented in silicon, is presented. The target architecture is a 2-D array of processing elements for low-level image processing. An array is divided into subarrays called chips. At the chip level, defect tolerance is proved by an extra column of PEs (processing element) and bypassing techniques. At the wafer level, a double-rail connection network is used to construct a target array of defect-free chips that is as large and as fast as possible. Its main advantage is being independent of chip defects, as it is controlled from the I/O pads. An algorithm for constructing an optimized two-dimensional array on a wafer containing a given number of defect-free PEs and connections, a method to program the switches for the target architecture found by the algorithm, and software for programming the switches using laser cuts are discussed.<<ETX>>","0018-9162;1558-0814","","10.1109/2.129043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=129043","","Switches;Read-write memory;Wafer scale integration;Testing;Circuits;Transistors;Silicon;Image processing;Pixel;Filtering","computerised picture processing;fault tolerant computing;parallel architectures","ELSA;wafer-scale architectures;target architecture;low-level image processing;double-rail connection network;target array;defect-free chips","","13","14","","","","","","IEEE","IEEE Journals & Magazines"
"Space solar power workshop","D. Preble","Space Solar Power Inst., Jonesboro, Georgia","2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542)","","2001","7","","7","3427 vol.7","Strategic energy alternatives have assumed new importance in the light of growing energy demand, scarce supplies and environmental stress. The Space Solar Power Workshop (SSPW) http://classweb.gatech.edu/conf/sspw/ was created as a cooperative software and hardware development forum to better understand the SSP engineering and business model. It exists, to educate the interested public about this SSP opportunity. The SSPW intends to develop a straw business plan for transitioning to SSP power generation and delivery to client power grids. These goals include determining the costs resulting from various design assumptions. They transcend software and hardware development challenges to provide an open forum for the exchange of software and ideas which impact SSP design prototyping and optimization. Currently, we are developing design criteria and standardization paradigms that serve as interfaces among the developing component systems. Without more experience in developing SSP software, hardware, and procedures, designers are challenged to design and interface to other components some of which have not yet been designed, built, or GEO flight tested. This is a designer, practitioner, and strategic planner's workshop.","","0-7803-6599","10.1109/AERO.2001.931419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931419","","Solar energy","solar power;strategic planning;standardisation;solar power satellites","space solar power workshop;strategic energy alternatives;energy demand;development forum;business model;business plan;power generation;client power grids;SSPW;design prototyping;design criteria;standardization paradigms;GEO;strategic planning","","","","","","","","","IEEE","IEEE Conferences"
"DOLLOP (deposition of lithium by laser outside of plasma)-an overview [fusion materials]","G. Labik; J. Bartolick; J. Gorman; D. Johnson; G. LeMunyan; D. Long; D. Mansfield; M. Vocaturo","Plasma Phys. Lab., Princeton Univ., NJ, USA; NA; NA; NA; NA; NA; NA; NA","17th IEEE/NPSS Symposium Fusion Engineering (Cat. No.97CH36131)","","1997","2","","873","876 vol.2","This paper describes the spallation of lithium into TFTR by laser and operated during the last run period. Liquid droplets of lithium were delivered into the edge of TFTR plasmas at low velocities in order to coat the interior of the TFTR vacuum vessel, and study the effects on machine performance. A 1.6 Joule 1064 nm, 30 Hz pulsed YAG laser was used to form droplets from a heated cauldron containing liquid lithium. The cauldron was resistance heated and part of a new probe head attached to the existing Bay D Long Probe. The cauldron was moved into the bottom of the vacuum vessel and positioned in the shadow of a poloidal limiter. The laser, associated equipment, and beam expanding optics were located in a walled enclosure approximately 46 m from the cauldron. Three fixed mirrors directed the beam through the test cell wall and into a box containing the focusing optics, steering mirror and a remote viewing CID video camera system. The focused beam was aimed vertically downward through a vacuum window and into the lithium cauldron. The system was controlled using LabView/sup TM/ software. Safety issues were resolved with engineered hardware design, interlocked circuits and administrative control. Lithium was successfully introduced into TFTR. Some interesting physics results were obtained. There was insufficient time to optimize the operation and design but initial results point to interesting possibilities for future experiments. Laboratory tests indicated that the cauldron could feasibly be positioned horizontally or vertically.","","0-7803-4226","10.1109/FUSION.1997.687763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=687763","","Lithium;Plasmas;Laser beams;Pulsed laser deposition;Probes;Optical beams;Mirrors;Optical pulses;Resistance heating;Magnetic heads","lithium;fusion reactor materials;fusion reactor limiters;laser deposition;fusion reactor design;fusion reactor safety","DOLLOP;laser deposition;fusion materials;spallation;TFTR;liquid droplets;vacuum vessel;Bay D Long Probe;poloidal limiter;beam expanding optics;walled enclosure;fixed mirrors;safety issues;engineered hardware design;interlocked circuits;administrative control;1064 nm;30 Hz;1.6 J;Li","","1","2","","","","","","IEEE","IEEE Conferences"
"High-performance low-power bit-level systolic array signal processor with low-threshold dynamic logic circuits","W. S. Song; M. M. Vai; H. T. Nguyen","Lincoln Lab., MIT, Lexington, MA, USA; NA; NA","Conference Record of Thirty-Fifth Asilomar Conference on Signals, Systems and Computers (Cat.No.01CH37256)","","2001","1","","144","147 vol.1","MIT Lincoln Laboratory has developed a scalable full-custom cell library for implementing bit-level systolic array signal processors. The cell library achieves high performance and low power consumption by using dynamic logic circuits with low-threshold voltage CMOS devices. The cell library is designed to implement signal processing functions such as finite impulse response (FIR) filter, infinite impulse response (IIR) filter, polyphase filter bank, fast Fourier transform (FFT), inverse fast Fourier transform (IFFT) and matrix operations such as partial product computation and QR decomposition. The full custom cell library is highly optimized for fast clock speed, small area and low power consumption. The low-threshold-voltage dynamic logic devices allow operation at high clock speeds with significantly reduced power supply voltage. The dynamic logic also greatly reduces the device count. The cell library is designed to scale to smaller fabrication geometries. Design automation is also possible by using customized placement and routing software. A FIR filter test chip has been designed, fabricated and tested on a 0.25 /spl mu/m 2.5 V bulk CMOS process. The clock frequency exceeds 800 MHz running on only 1.3 V power supply; power efficiency up to 250 billion operations/sec/W has been demonstrated using power supply voltage down to 0.4 V.","1058-6393","0-7803-7147","10.1109/ACSSC.2001.986895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=986895","","Systolic arrays;Libraries;Finite impulse response filter;Filter bank;IIR filters;Clocks;Power supplies;Energy consumption;Dynamic voltage scaling;CMOS logic circuits","systolic arrays;signal processing;signal processing equipment;power consumption;CMOS logic circuits;FIR filters;IIR filters;channel bank filters;fast Fourier transforms;matrix decomposition;logic design;electronic design automation;array signal processing","systolic array signal processor;bit-level processor;low-power processor;dynamic logic circuits;MIT Lincoln Laboratory;cell library;power consumption;low-threshold voltage;CMOS devices;finite impulse response filter;FIR filter;infinite impulse response filter;IIR filter;polyphase filter bank;inverse fast Fourier transform;adaptive sensor arrays;array signal processing;partial product computation;matrix decomposition;design automation;800 MHz;0.25 micron;2.5 V;1.3 V;0.4 V","","1","12","","","","","","IEEE","IEEE Conferences"
"Neural information processing in real-world face-recognition applications","W. Konen","ZN GmbH, Germany","IEEE Expert","","1996","11","4","7","8","One of the challenges of neural information processing is to achieve, at least partially, a similar performance to humans on systems for automated visual face recognition. Commercial applications of new technology don't care about the underlying architecture or paradigms. The architecture simply needs to work all the time and be easy to use. That was the essence of ZN's product design plan when we began developing our ZN-Face access control system. ZN-Face relies on von der Malsburg's graph matching, which is robust enough to deal with the low quality pictures encountered outside the laboratory when developing automated image acquisition from real world scenes. (In this way, of course, the underlying neural system's robustness is essential, because otherwise we could not have fulfilled the works-all-the-time requirement.) At ZN, we developed the complete hardware and software setup for the biometric access control device. We optimized and adapted the algorithms to the specific verification task-that is, ""is the person in question identical to the cardholder?""and tested it in the hardware setup, leading to a 99.5% performance verification rate. As with most face recognizers, ZN-Face requires the cooperation of users, who must orient their heads toward the camera during picture acquisition (/spl plusmn/15/spl deg/). Today's algorithms can only partially solve the challenge of generalizing from, for example, a half profile view to a frontal view.","0885-9000;2374-9407","","10.1109/64.511769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=511769","","Information processing;Face recognition;Zinc;Access control;Robust control;Hardware;Humans;Product design;Laboratories;Layout","face recognition;feature extraction;biometrics (access control);neural nets","neural information processing;automated visual face recognition;product design plan;ZN-Face access control system;graph matching;low quality pictures;automated image acquisition;real world scenes;biometric access control device;verification task;performance verification rate;face recognizers;picture acquisition;real world face recognition applications","","2","3","","","","","","IEEE","IEEE Journals & Magazines"
"Concept and realization of an Airborne SAR/Interferometric Radar Altimeter System (ASIRAS)","H. Lentz; H. -. Braun; M. Younis; C. Fischer; W. Wiesbeck; C. Mavrocordatos","Radar Systemtechnik (RST) AG, Gallen, Switzerland; Radar Systemtechnik (RST) AG, Gallen, Switzerland; NA; NA; NA; NA","IEEE International Geoscience and Remote Sensing Symposium","","2002","6","","3099","3101 vol.6","Cryosat is an approved ESA mission. Its objective is to determine fluctuations in the Earth's land mass and marine ice fields. The mission utilizes a completely new SAR/interferometric altimeter concept, which combines a nadir-looking configuration with pulse-limited altimetry. The concept has been verified by simulations, but no dedicated experimental validation has been performed so far. For this reason, an Airborne SAR/Interferometric Radar Altimeter System (ASIRAS) is currently under development in the frame of an ESA contract. Due to the large bandwidth of the system, several components, i.e. the chirp generator, are critical and therefore require innovative solutions. A dedicated software performs SAR- and interferometric radar data processing and, in addition, supports internal instrument calibration. ASIRAS is intended to be tested, verified and optimized on a DO 228/100 aircraft carrier during a measurement campaign scheduled fall 2002 in the area of Fram Street north of Greenland. The paper presents mission objectives, the instrument concept of ASIRAS, and critical building blocks.","","0-7803-7536","10.1109/IGARSS.2002.1027097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1027097","","Synthetic aperture radar;Airborne radar;Instruments;Fluctuations;Earth;Ice;Altimetry;Contracts;Bandwidth;Chirp","geophysical techniques;geophysical equipment;terrain mapping;oceanographic techniques;hydrological techniques;sea ice;remote sensing by radar;airborne radar;radar equipment;synthetic aperture radar","ocean;sea ice;glaciology;land ice;geophysical measurement technique;InSAR;SAR interferometry;nadir-looking;ESA;airborne radar;radar remote sensing;Airborne SAR/Interferometric Radar Altimeter System;SAR;synthetic aperture radar;terrain mapping;land surface;ASIRAS","","5","1","","","","","","IEEE","IEEE Conferences"
"Development of a gamma radiation imaging detector based on a GSO crystal scintillator and a position sensitive PMT","S. Majewski; B. Kross; L. Majewski; M. Pohl; D. Steinbach; A. G. Weisenberger; R. Wojcik","Coll. of William & Mary, Williamsburg, VA, USA; NA; NA; NA; NA; NA; NA","1996 IEEE Nuclear Science Symposium. Conference Record","","1996","2","","1191","1195 vol.2","An imaging gamma detector was developed using a 5"" diameter Hamamatsu R3292 position sensitive PMT and GSO crystal scintillator plates from Hitachi. Plate sizes from 4/spl times/4 cm to 8/spl times/8 cm and thickness from 1.25 mm to 2.5 mm were used to detect 28-35 keV photons from /sup 125/I and 140 keV gamma's from /sup 99m/Tc. Several novel features of the detector were investigated including improvement of detector resolution by using anode wire sections and truncated center-of-gravity calculations. After the PSPMT's readout was optimized a resolution of about /spl sim/5 mm @ 140 keV was obtained using only fourteen (7x,7y) anode wire sections in lieu of the standard 28/spl times/28 individual crossed-wires. Due to good obtained overall energy resolution, efficient scatter rejection is expected by placing just one simple energy window on scintillation signals coming from the detector. Measurements were made with point and linear sources as well as thyroid and compressed breast phantoms with simulated lesions. The phantoms contained appropriate concentrations of /sup 99m/Tc emitting 140 keV gamma radiation. The detector combines good performance with small size and economical makeup. After additional phantom tests and software improvement we are planning to use it first as a small field-of-view small animal gamma imager.","1082-3654","0-7803-3534","10.1109/NSSMIC.1996.591620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=591620","","Gamma rays;Optical imaging;Gamma ray detection;Gamma ray detectors;Radiation detectors;Energy resolution;Imaging phantoms;Anodes;Wire;Solid scintillation detectors","position sensitive particle detectors;solid scintillation detectors;radioisotope imaging;gamma-ray apparatus;medical image processing","gamma radiation imaging detector;GSO crystal scintillator;position sensitive PMT;imaging gamma detector;Hamamatsu R3292 position sensitive PMT;GSO crystal scintillator plates;anode wire sections;truncated center-of-gravity calculations;energy resolution;efficient scatter rejection;thyroid;compressed breast phantoms;simulated lesions;small field-of-view small animal gamma imager;Gd/sub 2/(SiO/sub 4/)O:Ce","","2","8","","","","","","IEEE","IEEE Conferences"
"Thin Layers in Electrical Engineering. Example of Shell models in Analysing Eddy-Currents by Boundary and Finite Element Methods","L. Krahenbuhl; D. Muller","CEGELY, France; NA","Digest of the Fifth Biennial IEEE Conference on Electromagnetic Field Computation","","1992","","","MOC2","MOC2","Summary from only given, as follows. During last years, several numerical formulations have been developed by us to modelize physical problems like: conducting film effects over the surface of insulators (pollution) [1], high frequency eddy-currents [2], earth field effects on the hull of a ship [3]. The physical effects being at stake are completely different, but in each of these examples, they originate from a region thin in regard with the other geometrical dimensions. An efficient numerical approach consists on using a surfacic representation with special boundary conditions expressing the solution inside the thin region. We propose in that paper a didactical approach to thin regions in electromanetics and, as an example, the boundary conditions and surfacic equation for eddy currents flowing inside a thin ferromagnetic shell. The numerical tests are done using the BEM software PH13D, but the results could easily be transposed in a FEM context. The practical applications may concern the computation of losses (shield of electrical machines or transformers) or low frequency electromagnetic perturbations (screen effects, EMC) as well as special domains like optimization of the induction heating of pans (french art culinaire).","","","10.1109/CEFC.1992.720589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=720589","","Finite element methods","","","","2","13","","","","","","IEEE","IEEE Conferences"
"FPGA based fuzzy computation accelerator","S. Himavahi; B. Umamaheswari","Sch. of Electr. & Electron., Anna Univ., Madras, India; Sch. of Electr. & Electron., Anna Univ., Madras, India","2002 IEEE World Congress on Computational Intelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE'02. Proceedings (Cat. No.02CH37291)","","2002","1","","352","357 vol.1","This paper proposes an FPGA based fuzzy computation accelerator (FCA) for fixed point microcontroller implementation of fuzzy systems. It supports optimizable and hardware implementable fuzzy membership functions proposed previously. The algorithm for low-end hardware implementation has been tested using Intel 8/spl times/C196KC fixed-point microcontroller [2]. The proposed FCA improves the processing speed by performing complex time-consuming operations. The hardware and software features of the FCA are presented. The percentage improvement in speed for a typical fuzzy structure is computed and reported. The advantages, versatility and limitations of the proposed FCA are summarized The FCA supports fuzzy systems whose membership functions are negative powers of 2, irrespective of the structure of the system. This would greatly aid medium range applications that require high computational power at low cost.","","0-7803-7280","10.1109/FUZZ.2002.1005015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1005015","","Field programmable gate arrays;Acceleration;Hardware;Microcontrollers;Costs;Fuzzy systems;Fuzzy reasoning;High performance computing;Inference mechanisms;Consumer electronics","field programmable gate arrays;fuzzy systems;microcontrollers;fuzzy control","FPGA based fuzzy computation accelerator;fixed point microcontroller implementation;fuzzy systems;fuzzy membership functions;fuzzy structure","","","12","","","","","","IEEE","IEEE Conferences"
"Electrocardiosignals and motion signals telemonitoring and analysis system for sportsmen","S. Korsakas; A. Vainoras; V. Miskinis; R. Ruseckas; V. Jurkonis; R. Jurkoniene; K. Berskiene; L. Siupsinskas; V. Marozas","NA; NA; NA; NA; NA; NA; NA; NA; NA","Computers in Cardiology, 2005","","2005","","","363","366","The aim of this paper is to present a new electrocardiosignals (ECGs) and motion signals telemonitoring and analysis system for sportsmen paddling a canoe or kayak. The developed system is intended to facilitate the couch in optimizing and individualizing the training of elite athletes. The hardware system consists of rower and coach components. Rower components include 5 sensors for monitoring of mechanical and physiological parameters. The coach components include master unit and laptop computer. The coach software works in two modes: online version during training and off-line the detailed data analysis after training. The new method for systolic blood pressure evaluation was developed and checked for 313 persons. The accuracy of systolic blood pressure prediction could exceed range of 90%. The preliminary results of developed system testing and method for blood pressure evaluation are promising and show that telemetry system could be used for monitoring of ECG and motion parameters when sportsman is in action","0276-6574;2325-8853","0-7803-9337","10.1109/CIC.2005.1588112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1588112","","Motion analysis;Signal analysis;Biomedical monitoring;Blood pressure;Electrocardiography;Condition monitoring;Hardware;Mechanical sensors;Computerized monitoring;Portable computers","biomedical telemetry;blood pressure measurement;electrocardiography;medical computing;sport","electrocardiosignals;motion signal;telemonitoring;sportsmen;ECG;elite athletes;mechanical parameter;physiological parameter;rower components;coach components;laptop computer;systolic blood pressure evaluation;telemetry system","","7","6","","","","","","IEEE","IEEE Conferences"
"Microprocessor control system for PWM IGBT inverter feeding three-phase induction motor","A. Maamoun; M. M. Ahmed","Electron. Res. Inst., Nat. Res. Centre, Cairo, Egypt; Electron. Res. Inst., Nat. Res. Centre, Cairo, Egypt","Proceedings of IECON '95 - 21st Annual Conference on IEEE Industrial Electronics","","1995","2","","1354","1359 vol.2","In this work a three-phase PWM IGBT voltage source inverter has been introduced. The PWM control signals are produced using a microprocessor, so the complex control circuit hardware of the inverter can be replaced by microprocessor software. The suboptimal PWM switching strategy is proposed, which closely approximates and exhibits many of the desirable performance characteristics of optimized PWM strategies based on minimized total harmonic distortion. The drive system hardware includes the implementation of microprocessor system, transistorized inverter, interface circuits, isolating circuits and driving circuits. The IGBTs are used as efficient switching devices for the inverter power circuits, so simple and efficient transistor drive circuits can be used. Also, the overlap protection is taken into consideration. The PWM inverter is to feed an induction motor with adjustable voltage to frequency in a proper relation to maintain approximately constant rated flux in the machine over the operating frequency range (1-50 Hz). Hence, the drive system is used to control the speed of a three-phase induction motor from very low speed to the motor rated speed at a constant torque. The test results were obtained experimentally. The waveforms and the spectrum analysis of PWM control signals are given together with voltage and current of the motor at different frequencies.","","0-7803-3026","10.1109/IECON.1995.483993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=483993","","Pulse width modulation inverters;Microprocessors;Control systems;Insulated gate bipolar transistors;Circuits;Pulse width modulation;Induction motors;Frequency;Voltage;Hardware","induction motor drives;PWM invertors;machine control;harmonic distortion;switching circuits;microcomputer applications;power engineering computing;digital control;insulated gate bipolar transistors;power semiconductor switches;power system harmonics","PWM IGBT inverter;three-phase induction motor;microprocessor control system;PWM control signals;voltage source inverter;suboptimal PWM switching strategy;total harmonic distortion minimisation;transistorized inverter;interface circuits;isolating circuits;driving circuits;overlap protection;adjustable voltage;constant rated flux;drive system;spectrum analysis;1 to 50 Hz","","1","13","","","","","","IEEE","IEEE Conferences"
"A direct-conversion receiver integrated circuit for WCDMA mobile systems","S. K. Reynolds; B. A. Floyd; T. J. Beukema; T. Zwick; U. R. Pfeiffer; H. A. Ainspan","IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA; IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA; IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA; IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA; IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA; IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA","IBM Journal of Research and Development","","2003","47","2.3","337","353","A prototype of a 3-V SiGe direct-conversion receiver integrated circuit for use in third-generation (3G) WCDMA mobile cellular systems has been completed. The goal of its design was to minimize current draw while meeting WCDMA receiver rf specifications with margin. The design includes a bypassable low-noise amplifier, quadrature downconverter, and first-stage variable-gain baseband amplifiers integrated on chip. The design is optimized for use with a single-ended off-chip bandpass surface-acoustic-wave filter with no external matching components. The prototype design represents a first step toward a fully integrated monolithic WCDMA/UMTS receiver system-on-a-chip. A rigorous set of performance tests are used to characterize the noise and linearity performance of the packaged IC across its full frequency band of operation. A receiver test-bed system with a software baseband demodulator is used to determine the bit-error-rate performance of the receiver integrated circuit (IC) at sensitivity. Measured results are compared with estimated system performance requirements to determine compliance with key WCDMA rf specifications.","0018-8646;0018-8646","","10.1147/rd.472.0337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5388946","","","","","","12","","","","","","","IBM","IBM Journals & Magazines"
"Analysis of the diversity of population and convergence of genetic algorithms based on negentropy","Z. Lianying; W. Anmin","Management School, Tianjin University, Tianjin 300072, P. R. China; Management School, Tianjin University, Tianjin 300072, P. R. China","Journal of Systems Engineering and Electronics","","2005","16","1","215","219","With its wide use in different fields, the problem of the convergence of simple genetic algorithms (GAs) has been concerned. In the past, the research on the convergence of GAs was based on Holland' s model theorem. The diversity of the evolutionary population and the convergence of GAs are studied by using the concept of negentropy based on the discussion of the characteristic of GA. Some test functions are used to test the convergence of GAs, and good results have been obtained. It is shown that the global optimization may be obtained by selecting appropriate parameters of simple GAs if the evolution time is enough.","1004-4132","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071132","negentropy;genetic algorithms;diversity of evolutionary population;convergence","Entropy;Genetic algorithms;Convergence;Evolution (biology);Genetics;Thermodynamics;Convex functions","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Short and mid term hydro power plant reservoir inflow forecasting","T. Stokelj; D. Paravan; R. Golob","Soske Elektrarne (SENG), Nova Gorica, Slovenia; NA; NA","PowerCon 2000. 2000 International Conference on Power System Technology. Proceedings (Cat. No.00EX409)","","2000","2","","1107","1112 vol.2","In order to improve water management for hydro cascade systems, a new artificial neural network (ANN) approach to forecasting water inflow into the hydro power plant reservoir based on forecasted precipitation data is presented. Water inflow forecasting into the head hydro power plant (HPP) reservoir is one of most important inputs to the cascade hydro system (CHS) optimization process. Ability to properly forecast the increase of natural inflow can result in increased electric energy production due to enhanced flexibility in stored water management. Due to the Soca river torrential character two separate algorithms for short term and mid term water inflow forecasting are designed. Short term water inflow forecasting is based on precipitation data collected by the ombrometer stations in the river basin and is used for forecasts up to 6 hours ahead. The efficacy of the proposed method is tested for a practical case and some results are presented. Mid term water inflow forecasting is based on the forecasted precipitation data and is capable of predicting water inflows for the next two days. The precipitation forecasts data are obtained with the ALADIN (Aire Limitee Adaptation Dynamique development International) program, which was developed by the Meteo-France in cooperation with Slovenian hydrological institute and other Central European hydrological institutes. The data acquisition system to be implemented as a part of new software in the regional control center is briefly described. Finally, some practical results for both short and mid term water inflow forecasting for Soca river are presented.","","0-7803-6338","10.1109/ICPST.2000.897175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897175","","Power generation;Reservoirs;Water resources;Rivers;Energy management;Power system management;Artificial neural networks;Load forecasting;Production;Water storage","hydroelectric power stations;rain;rivers;lakes;geophysics computing;neural nets","mid term reservoir inflow forecasting;short term reservoir inflow forecasting;hydro power plant reservoir;water management;hydro cascade systems;artificial neural network;forecasted precipitation data;water inflow forecasting;head hydro power plant reservoir;electric energy production;stored water management;Soca river torrential character;ombrometer stations;river basin;ALADIN program;Aire Limitee Adaptation Dynamique development International;Meteo-France;Slovenian hydrological institute;Central European hydrological institutes;data acquisition system;regional control center","","5","6","","","","","","IEEE","IEEE Conferences"
"Using genetic algorithms to automate system implementation in a novel three-dimensional packaging technology","S. P. Larcombe; D. J. Prendergast; N. A. Thacker; P. A. Ivey","Electron. Syst. Group, Sheffield Univ., UK; NA; NA; NA","Proceedings International Conference on Computer Design. VLSI in Computers and Processors","","1996","","","274","279","Implementing electronic systems in a low-cost, three-dimensional multichip module (MCM) technology provides significantly improved system density over planar packaging technologies. However, the extra dimension necessitates a new approach to design automation. Through experience gained in manually partitioning electronic systems, generic design rules have been formulated and implemented in software. The approach adopted is based on a genetic algorithm which can produce multiple candidate solutions and has the potential to simultaneously optimize multiple design criteria. This paper presents the initial results for the automatic configuration of a variety of systems. The results obtained by the algorithm are compared to known optimal solutions for ""test"" cases and to the results obtained by manual partitioning for the first heterogeneous MCM-V system.","1063-6404","0-8186-7554","10.1109/ICCD.1996.563567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=563567","","Genetic algorithms;Packaging;Tellurium;Routing;Thermal resistance;Space technology;Surface resistance;Design automation;Epoxy resins;Power dissipation","genetic algorithms","genetic algorithms;system implementation;three-dimensional packaging technology;multichip module technology;design automation;generic design rules;multiple candidate solutions;automatic configuration;heterogeneous MCM-V system","","2","9","","","","","","IEEE","IEEE Conferences"
"The Megacell concept: an approach to painless custom design","J. S. Brothers; J. W. Tomkins; J. S. Williams","Plessey Semiconductors Ltd., Swindon, UK; Plessey Research (Caswell) Ltd., Allen Clark Research Centre, Towcester, UK; Plessey Research (Caswell) Ltd., Allen Clark Research Centre, Towcester, UK","IEE Proceedings E - Computers and Digital Techniques","","1985","132","2","91","98","Megacell is a complete design package for the creation of complex VLSI chips. It allows system engineers to develop their own custom integrated circuits. The challenge of Megacell was to create a design system that would exploit fully the low-power and high-speed performance of a UK developed 2 m CMOS technology, while producing the silicon utilisation efficiency typical of full custom design. This was accomplished without losing the now familiar semicustom design attributes of `first-time success', coupled with facilities for the user to complete his own design. The full features of the Megacell design system are described, and projections are made of how its capabilities will be extended as CMOS technology edges towards 1 m. The provision of a series of cell structures of increasing complexity within Megacell allows the user to optimise his layout, without inducing the uncharacterised variations that would be usual in a full custom design. The cell structure has within it three major types: microcells which are the low-level logic cells found in most array or cell design systems; paracells which are cells of high functional capability, compiled by user software specifically for the customer's application. The paracell is created from parameters, input by the user, as he designs his circuit; and supracells which have the highest complexity and are cells that replicate the function of today's LSI standard products. They include not only digital functions, such as microprocessors, but also analogue blocks such as A/D and D/A convertors. The heart of the Megacell design system is its CAD. Alongside the cell structures, a comprehensive set of integrated design tools has been developed to cover the complete spectrum of user requirements. Included in the system are schematic capture, simulation, test vali","0143-7062","","10.1049/ip-e.1985.0013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4646444","","","circuit CAD;CMOS integrated circuits;integrated circuit technology;VLSI","microcells;paracells;supracells;VLSI circuit CAD;complex VLSI chips;custom integrated circuits;2 m CMOS technology;Megacell design system;cell structures;integrated design tools","","","","","","","","","IET","IET Journals & Magazines"
"Hardware speech recognition for user interfaces in low cost, low power devices","S. Nedevschi; R. K. Patra; E. A. Brewer","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA","Proceedings. 42nd Design Automation Conference, 2005.","","2005","","","684","689","We propose a system architecture for real-time hardware speech recognition on low-cost, power-constrained devices. The system is intended to support real-time speech-based user interfaces as part of an effort to bring information and communication technologies (ICTs) to underdeveloped regions of the world. Our system architecture exploits a shared infrastructure model. The computationally intensive task of speech model training and retraining is performed offline by shared servers, while the actual recognition of speech is conducted on low-cost hand-held devices using custom hardware. The recognizer is extremely flexible and can support multiple languages or dialects with speaker-independent recognition. Dynamic loading of speech models is used for changing language grammar and retraining, while reprogramming is used to support evolution of recognition algorithms. The focus on small sets of words (at one time) reduces the complexity, cost and power consumption. We design the speech decoder, the central component of the recognizer, and we validate it via a prototype FPGA implementation. We then use ASIC synthesis to estimate power and size for the design. Our evaluations demonstrate an order of magnitude improvement in power compared with optimized recognition software running on a low-power embedded general-purpose processor of the same technology and of similar capabilities. The synthesis also estimates the area of the design to be about 2.5mm, showing potential for lower cost. In designing and testing our recognizer we use datasets in both English and Tamil languages.","0738-100X","1-59593-058","10.1145/1065579.1065760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510419","","Hardware;Speech recognition;User interfaces;Costs;Natural languages;Computer architecture;Real time systems;Power system modeling;Communications technology;Handheld computers","speech recognition;user interfaces;application specific integrated circuits;vocoders;field programmable gate arrays;natural languages","speech recognition;user interface;power-constrained device;information communication technology;speech model training;hand-held device;language grammar;power consumption;speech decoder;FPGA implementation;ASIC synthesis","","16","25","","","","","","IEEE","IEEE Conferences"
"Initial development of a genetic algorithm to automate system implementation in a novel electronic packaging technology","S. P. Larcombe; D. J. Prendergast; N. A. Thacker; P. A. Ivey","Electron. Syst. Group, Sheffield Univ., UK; NA; NA; NA","Second International Conference On Genetic Algorithms In Engineering Systems: Innovations And Applications","","1997","","","486","491","Implementing electronic systems in a low-cost, three-dimensional multichip module (MCM) technology provides significantly improved system density over planar packaging technologies. However, the extra dimension necessitates a new approach to design automation. Through experience gained in manually partitioning electronic systems, generic design rules have been formulated and implemented in software. The approach adopted is based on a genetic algorithm which can produce multiple candidate solutions and has the potential to simultaneously optimize multiple design criteria. This paper presents the initial results for the automatic configuration of a variety of systems. The results obtained by the algorithm are compared to known optimal solutions for ""test"" cases and to the results obtained by manual partitioning for the first heterogeneous MCM-V system.","0537-9989","0-85296-693","10.1049/cp:19971228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=681074","","","multichip modules","genetic algorithm;electronic packaging technology;low-cost three-dimensional multichip module technology;design automation;generic design rules;design criteria;automatic configuration;heterogeneous MCM-V system","","","","","","","","","IET","IET Conferences"
"Polar shapelets","R. Massey; A. Refregier","NA; NA","Monthly Notices of the Royal Astronomical Society","","2005","363","1","197","210","The shapelets method for image analysis is based upon the decomposition of localized objects into a series of orthogonal components with convenient mathematical properties. We extend the Cartesian shapelet formalism from earlier work, and construct polar shapelet basis functions that separate an image into components with explicit rotational symmetries. These frequently provide a more compact parametrization, and can be interpreted in an intuitive way. Image manipulation in shapelet space is simplified by the concise expressions for linear coordinate transformations, and shape measures (including object photometry, astrometry and galaxy morphology estimators) take a naturally elegant form. Particular attention is paid to the analysis of astronomical survey images, and we test shapelet techniques upon real data from the Hubble Space Telescope. We present a practical method to automatically optimize the quality of an arbitrary shapelet decomposition in the presence of observational noise, pixelization and a point spread function. A central component of this procedure is the adaptive choice of the scale size and the truncation order of the shapelet expansion. A complete software package to perform shapelet image analysis is made available on the World Wide Web.","0035-8711;1365-2966","","10.1111/j.1365-2966.2005.09453.x","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8173530","methods: analytical;methods: data analysis;techniques: image processing;galaxies: fundamental parameters","","","","","8","","","","","","","OUP","OUP Journals & Magazines"
"Proceedings International Test Conference 2000 (IEEE Cat. No.00CH37159)","","","Proceedings International Test Conference 2000 (IEEE Cat. No.00CH37159)","","2000","","","0_1","","","1089-3539","0-7803-6546","10.1109/TEST.2000.894146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894146","","","automatic testing;automatic test equipment;electronic equipment testing;logic testing;fault diagnosis;built-in self test","system test;ATE software generation;defect behaviour;BIST;fault diagnosis;design validation;ADC testing;delay fault testing;optimisation;memory testing;bridging faults;crosstalk test;embedded memories;PC board testing;1 GHz;SOC tests;low-power BIST;microprocessor tests;physical defects test;models;logic testing;FPGA;low-power optimisation;fault models;HF tests;error detection;delay testing;processor test cores;DFT;embedded tests","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings Design, Automation and Test in Europe","","","Proceedings Design, Automation and Test in Europe","","1998","","","i","","","","0-8186-8359","10.1109/DATE.1998.655828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655828","","","logic CAD;circuit CAD;high level synthesis;integrated circuit design;logic testing;integrated circuit testing;automatic testing;digital integrated circuits;analogue integrated circuits;hardware description languages;scheduling;circuit analysis computing;network routing;circuit layout CAD;circuit optimisation","design optimization;HW/SW partitioning;communication synthesis;VHDL-based design;FPGA testing;scheduling;BIST;image processing architectures;error detection;design validation;processor design;reconfigurable systems;routing;formal verification;high-level design;architectural synthesis;interconnect modelling;memory testing;sequential circuit testing;behavioural synthesis;analogue circuits;combinational logical synthesis;mixed-signal test;sequential logic synthesis;hardware/software codesign","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings Ninth International Symposium on Software Reliability Engineering (Cat. No.98TB100257)","","","Proceedings Ninth International Symposium on Software Reliability Engineering (Cat. No.98TB100257)","","1998","","","i","","","1071-9458","0-8186-8991","10.1109/ISSRE.1998.730759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730759","","","software reliability","software reliability engineering;reliability prediction;reliability estimation;software process improvement;software maintenance;software testability;reliability modelling;reliability validation;sofware test planning;automatic software testing;simulation;special test methods;fault diagnosis;reliability analysis;reliability optimization;reliability assessment;emerging techniques;evolutionary software;code defect classifications;safety-critical software;software fault injection","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings of the Fifteenth Annual International Computer Software and Applications Conference (Cat. No.91CH3023-9)","","","[1991] Proceedings The Fifteenth Annual International Computer Software & Applications Conference","","1991","","","0_1","","","","0-8186-2152","10.1109/CMPSAC.1991.170268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=170268","","","concurrency control;database management systems;document image processing;knowledge based systems;logic programming;object-oriented programming;operating systems (computers);parallel programming;program compilers;program testing;software metrics;software reliability;software reusability;software tools;user interfaces","knowledge models;deadlock resolution;software validation;query processing;software reuse;compiler optimization;parallel systems;data models;deadlock detection;mapping tasks;user interfaces;knowledge-based environments;formal methods;expert system building tools;visualization tools;specification supports;software productivity;quality measurements;parallel programming tools;logic programming;performance evaluation;database systems;object oriented databases;software testing;query language;software development;scheduling;OS;debugging;algorithms;concurrent object-oriented environments;software processes;software metrics;document processing;distributed software reliability","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings Sixth Asian Test Symposium (ATS'97)","","","Proceedings Sixth Asian Test Symposium (ATS'97)","","1997","","","i","","","1081-7735","0-8186-8209","10.1109/ATS.1997.643900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643900","","","logic testing","test generation;design for testability;fault tolerance;DFT techniques;Japanese industry;mixed-signal test;logic optimisation;decision diagrams;FPGA test;software test;circuit diagnosis;delay test;BIST;electric current testing;logic testing","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings. Design, Automation and Test in Europe","","","Design, Automation and Test in Europe","","2005","","","","","The following topics are dealt with: partitioning and optimisation for reconfigurable computing; SoC design-for-test; low power design; scheduling and synthesis for reconfigurable computing; analogue simulation, placement and statistical analysis; analogue and gigahertz test; ubiquitous computing; power aware design in DSM technology; analogue, mixed-signal and RF circuits; reliability at the very deep sub-micron region; IP-based design; HW/SW solutions for low power multimedia systems; embedded systems; logic synthesis; defect detection and characterisation; real-time scheduling; SoC power optimisation; system level languages, verification and simulation; reliable memory design; execution-time analysis; CMOS design; high-level verification; system modelling with UML; parallel and multithreaded processor architectures; very deep submicron simulation; SoC prototyping and simulation; memory optimisation and clocking for SoC; test power reduction; multiprocessor embedded systems; layout issues; pattern generation for fault detection and diagnosis; embedded software technology; advanced analogue performance modelling; SAT based verification; test pattern compression and delay test schemes; compiler/architecture codesign; network-on-chip design flows; biochips and quantum computing; CMOS-based biosensor arrays; network-on-chip architectures; concurrent error detection and correction; formal verification of processor architecture and DSP programs; interconnect optimisation; media and signal processing; secure and embedded security systems; MPSoC platforms; low-power wireless LANs; wireless communication and networking; automotive applications; IP-reuse; design verification; sensors.","1530-1591;1558-1101","0-7695-2288","10.1109/DATE.2005.314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395781","","","embedded systems;integrated circuit design;integrated circuit testing;system-on-chip;design for testability;low-power electronics;logic partitioning;reconfigurable architectures;ubiquitous computing;mixed analogue-digital integrated circuits;hardware-software codesign;multimedia systems;scheduling;logic design;fault diagnosis;formal verification;high level synthesis;parallel architectures;integrated circuit modelling;automatic test pattern generation;quantum computing;biosensors;electronic design automation","reconfigurable computing;SoC design-for-test;low power design;partitioning;scheduling;analogue simulation;statistical analysis;ubiquitous computing;power aware design;DSM technology;analogue circuits;mixed-signal circuits;RF circuits;IP-based design;HW/SW solutions;low power multimedia systems;embedded systems;logic synthesis;defect detection;real-time scheduling;SoC power optimisation;system level languages;reliable memory design;execution-time analysis;CMOS design;high-level verification;system modelling;UML;parallel architectures;multithreaded processor architectures;very deep submicron simulation;multiprocessor embedded systems;layout issues;pattern generation;fault diagnosis;embedded software technology;SAT based verification;test pattern compression;delay test schemes;compiler/architecture codesign;network-on-chip design flows;biochips;quantum computing;CMOS-based biosensor arrays;concurrent error detection/correction;formal verification;interconnect optimisation;signal processing;embedded security systems;MPSoC platforms;low-power wireless LAN;wireless communication;automotive applications;IP-reuse;design verification","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","","Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","","2000","","","i","","","","0-7695-0537","10.1109/DATE.2000.840006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840006","","","embedded systems;low-power electronics;integrated circuit design;industrial property;application specific integrated circuits;telecommunication equipment;built-in self test;mixed analogue-digital integrated circuits;decision diagrams;multiprocessing systems;logic CAD;logic testing;integrated circuit interconnections;integrated circuit modelling;development systems","embedded software generation;low-power design;IC design;IP;design reuse;SOC;telecom systems;BIST;mixed-signal ICs;decision diagrams;multiprocessor architectures;logic synthesis;logic testing;interconnect modelling;high-level power optimisation;defect-oriented test;emulation","","","","","","","","","IEEE","IEEE Conferences"
"IEEE/ACM International Conference on Computer Aided Design. IEEE/ACM Digest of Technical Papers (Cat. No.02CH37391)","","","IEEE/ACM International Conference on Computer Aided Design, 2002. ICCAD 2002.","","2002","","","","","","1092-3152","0-7803-7607","10.1109/ICCAD.2002.1167505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167505","","","integrated circuit design;high level synthesis;circuit CAD;circuit layout CAD;hardware-software codesign;timing;leakage currents;integrated circuit modelling;integrated circuit interconnections;circuit optimisation;design for testability;analogue integrated circuits;parameter estimation;low-power electronics;statistical analysis;VLSI;molecular electronics;integrated memory circuits;integrated circuit noise;Boolean functions;integrated circuit testing;automatic test pattern generation;reduced order systems;system-on-chip","IC CAD;substrate modeling;low-power design;routing;testing;high level synthesis;formal validation/synthesis techniques;subthreshold leakage modeling/reduction techniques;timing-driven placement;hardware/software codesign compilation;inductance modeling;analog/RF simulation;interconnect optimization;chip-level communication structures;DFT techniques;system-level analog design;inductance modelling;low power optimization;transistor level optimization;statistical power estimation techniques;statistical timing estimation;VLSI manufacturability;CAD computation;molecular electronics;satisfiability checking;device modeling;simulation technologies;circuit-level analog CAD;switch level verification;gate level verification;RT level verification;logic synthesis;noise effects;memory issues;low level aware behavioral synthesis;timing analysis;embedded system architecture customization;combinational synthesis;system-level performance modeling;system-level power modeling;optimization;dynamic voltage scheduling;model order reduction;ATPG Boolean engines;SAT Boolean engines;formal hardware verification;SoC technology","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings. 2005 IEEE International Conference on Web Service","","","IEEE International Conference on Web Services (ICWS'05)","","2005","","","","","The following topics are dealt with: Web services; Internet; grid computing; data security; middleware; quality of service; business communication; data privacy; business integration; open systems.","","0-7695-2409","10.1109/ICWS.2005.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530759","","","security of data;software standards;program verification;program testing;quality of service;grid computing;semantic Web;optimisation;mobile computing;software tools;message passing","service discovery;service selection;data and transactions;security language;security standard;verification;testing;quality of service;grid computing;mobility;optimization;SOAP messaging;Web provisioning;Web monitoring;automated service composition;semantic Web service;service management;SOA;service invocation","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings of EUROMICRO 96. 22nd Euromicro Conference. Beyond 2000: Hardware and Software Design Strategies","","","Proceedings of EUROMICRO 96. 22nd Euromicro Conference. Beyond 2000: Hardware and Software Design Strategies","","1996","","","i","","","1089-6503","0-8186-7487","10.1109/EURMIC.1996.546359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=546359","","","systems analysis","system level design;multiprocessor architectures;real-time systems;design methodologies;optimization;control systems;formal methods;hardware design;performance engineering;usability engineering;logic synthesis;fault tolerance;specification;validation;testing;dependable systems;high speed networks;memory;distributed systems;formal specification;multimedia;co-processors;image processing;parallel software engineering;operating system;network support;processor architectures;operational considerations;superscalar architectures","","","","","","","","","IEEE","IEEE Conferences"
"Annual Reliability and Maintainability Symposium. 2000 Proceedings. International Symposium on Product Quality and Integrity (Cat. No.00CH37055)","","","Annual Reliability and Maintainability Symposium. 2000 Proceedings. International Symposium on Product Quality and Integrity (Cat. No.00CH37055)","","2000","","","i","","","0149-144X","0-7803-5848","10.1109/RAMS.2000.816273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816273","","","reliability;reliability theory;maintenance engineering;fault trees;risk management;software reliability;life testing;aerospace","fault tree analysis;reliability programmes;maintenance programmes;modelling application;statistics application;commercial environments;industrial environments;software reliability;reliability analysis;maintainability analysis;reliability prediction;risk management;reliability optimisation;maintenance optimisation;accelerated life testing;space applications;reliability applications;design techniques;risk analysis","","","","","","","","","IEEE","IEEE Conferences"
"27th ACM/IEEE Design Automation Conference. Proceedings 1990 (Cat. No.90CH2894-4)","","","27th ACM/IEEE Design Automation Conference","","1990","","","0_1","","","0738-100X","0-89791-363","10.1109/DAC.1990.114953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114953","","","circuit analysis computing;circuit CAD;circuit layout CAD;configuration management;digital integrated circuits;electronic engineering computing;logic CAD;logic testing;MOS integrated circuits;optimisation;scheduling;specification languages;VLSI","VHDL;HDL validation;placement;binary decision diagrams;scheduling;mapping;allocation;timing-driven layout;data management;version control;data path optimization algorithms;floorplanning;formal methods;design verification;logic synthesis;testability;layout synthesis;MOS digital cells;software engineering;design automation;Boolean methods;timing;routing optimization;layout compactors;circuit simulation;logic simulation acceleration;data path synthesis;behavioral synthesis;performance-constrained routing;functional models;decomposition;partitioning;combinational test generation;channel-oriented multilayer routing","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings First ACM and IEEE International Conference on Formal Methods and Models for Co-Design. MEMOCODE'03","","","First ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2003. MEMOCODE '03. Proceedings.","","2003","","","","","The following topics are discussed: system level models and co-design; formal verification; field modifiability and verifiability; refinement; conformance; validation; synthesis; and optimization.","","0-7695-1923","10.1109/MEMCOD.2003.1210080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1210080","","","systems analysis;software reusability;program verification;refinement calculus;optimisation;supervisory programs;conformance testing","system level model;system design;formal verification;field modifiability;field verifiability;refinement;conformance;validation;synthesis;optimization","","","","","","","","","IEEE","IEEE Conferences"
"ICCAD-2005 International Conference on Computer Aided Design (IEEE Cat. No. 05CH37700)","","","ICCAD-2005. IEEE/ACM International Conference on Computer-Aided Design, 2005.","","2005","","","","","The following topics are dealt with: memory and arithmetic optimization; design manufacturing interaction; circuit layout; digital analog and RF test; design for manufacturing; logic synthesis; double-gated devices; network routing and application specific NoC architectures; memory driven codes; arithmetic constructs; buffers and voltage islands; sequential circuit optimization; nanoelectronics; dynamic voltage scaling; biochips and DNA-based nanofabrication; circuit simulation; analog circuit design; power aware system architecture; software optimization; cellular array architectures; variability aware clocking; oscillator analysis; power noise and thermal issues; nanocomputing; extraction and modeling for interconnect structures; system-level variability modeling; high-level synthesis; model order reduction; statistical timing analysis; formal verification; hardware and software design of sensors; formal equivalence checking and system on chip.","1092-3152;1558-2434","0-7803-9254","10.1109/ICCAD.2005.1560027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1560027","","Circuit optimization;Digital arithmetic;Routing;Design methodology;Logic design;Analog circuits;Cellular logic arrays;Oscillators;Design automation;Reduced order systems;Detectors","circuit optimisation;digital arithmetic;network routing;design for manufacture;logic design;network-on-chip;power grids;nanoelectronics;analogue circuits;cellular arrays;oscillators;formal verification;hardware-software codesign;circuit CAD;reduced order systems;system-on-chip;electric sensing devices","memory optimization;arithmetic optimization;design manufacturing interaction;circuit layout;digital testing;analog testing;RF testing;design for manufacturing;logic synthesis;double-gated devices;network routing;application specific NoC architectures;memory driven codes;arithmetic constructs;buffers and voltage islands;sequential circuit optimization;nanoelectronics;dynamic voltage scaling;biochips;DNA-based nanofabrication;circuit simulation;analog circuit design;power aware system architecture;software optimization;cellular array architectures;variability aware clocking;oscillator analysis;nanocomputing;interconnect structures;system-level variability modeling;high-level synthesis;model order reduction;statistical timing analysis;formal verification;sensor design;formal equivalence checking;system on chip","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings 2002 Design Automation Conference (IEEE Cat. No.02CH37324)","","","Proceedings 2002 Design Automation Conference (IEEE Cat. No.02CH37324)","","2002","","","","","The following topics are dealt with: Web and IP based design; embedded processor design; passive model order reduction; post-CMOS technology; formal verification; high level specification and design; timing abstraction; E-textiles; analog intellectual property; low-power system design; fabric-driven logic synthesis; memory management and address optimization in embedded systems; optics in EDA; nanometer design; DFT, BIST and diagnosis techniques; embedded system design; equivalence verification; embedded software automation; reconfigurable computing; test methods for non-classical faults; 10M gate ASIC design; power distribution; analog synthesis and design methodology; low-power physical design; unified tools for SoC embedded systems; multi-voltage, multi-threshold design; simulation techniques; design methodologies for network applications; analog modeling; routing and buffering; system on chip design; timing analysis and memory optimization; processors and accelerators; EDA drivers; cross-talk noise analysis and management; SoC test cost reduction; scheduling techniques for embedded systems; SoC design for yield improvement; Boolean satisfiability; inductance and substrate analysis; processors and communication networks; energy efficient mobile computing; floorplanning and placement; circuit effects in static timing; design space exploration for embedded systems; behavioral synthesis.","0738-100X","1-58113-461","10.1109/DAC.2002.1012583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012583","","","circuit CAD;circuit layout CAD;application specific integrated circuits;integrated circuit design;high level synthesis;industrial property;integrated optoelectronics;design for testability;built-in self test;embedded systems;integrated circuit testing;network routing;buffer circuits;integrated circuit yield;telecommunication equipment;analogue integrated circuits","design automation;IP based design;Web based design;embedded processor design;passive model order reduction;post-CMOS technology;formal verification;high level specification;high level design;timing abstraction;E-textiles;analog intellectual property;low-power system design;fabric-driven logic synthesis;memory management;address optimization;optics based EDA;nanometer design;DFT;BIST;embedded system design;embedded software automation;reconfigurable computing;nonclassical fault tests;ASIC design;power distribution;analog synthesis;physical design;SoC embedded systems;multi-voltage multi-threshold design;simulation;telecommunications network applications;routing;buffering;timing analysis;memory optimization;SoC test cost reduction;scheduling techniques;SoC design for yield;Boolean satisfiability;inductance;substrate analysis;energy efficient mobile computing;floorplanning;component placement;behavioral synthesis","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings of the ASP-DAC 2005. Asia and South Pacific Design Automation Conference 2005 (IEEE Cat. No.05EX950C)","","","Proceedings of the ASP-DAC 2005. Asia and South Pacific Design Automation Conference, 2005.","","2005","1","","0_1","","The following topics are dealt with: CAD for microarchitecture designs; design for manufacturability; tree construction and buffering; network-on-chip; integrated circuit testing; design for testability; clocks; power grid analysis; thermal analysis; network routing; integrated circuit interconnections; system level modeling; embedded software; TCAD; RF/analog circuits; logic synthesis; system level architecture design; formal verification; integrated circuit placement; security processor design; design optimization; high-performance digital circuits; floorplanning; SAT technology; interconnect modeling; high-level synthesis; low-power electronics; digital signal processing; FPGA; RF circuit design; embedded systems; real-time systems; crosstalk noise avoidance; power/ground network optimization; analog circuit design; SRAM; and system-on-chip.","2153-6961;2153-697X","0-7803-8736","10.1109/ASPDAC.2005.1466090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466090","","","integrated circuit design;electronic design automation;technology CAD (electronics);integrated circuit layout;integrated circuit modelling;integrated circuit interconnections;integrated circuit testing;system-on-chip;design for manufacture;design for testability;circuit CAD;clocks;radiofrequency integrated circuits;analogue circuits;microprocessor chips;digital circuits;high level synthesis;low-power electronics;circuit optimisation;SRAM chips;logic design;formal verification","CAD;design automation;microarchitecture designs;design for manufacturability;tree construction;buffering;network-on-chip;integrated circuit testing;design for testability;clocks;power grid analysis;thermal analysis;network routing;integrated circuit interconnections;system level modeling;embedded software;TCAD;logic synthesis;system level architecture design;formal verification;integrated circuit placement;security processor design;design optimization;high-performance digital circuits;floorplanning;satisfiability;interconnect modeling;high-level synthesis;low-power electronics;digital signal processing;FPGA;RF circuit design;embedded system;real-time system;crosstalk noise avoidance;power/ground network optimization;analog circuit design;SRAM","","","","","","","","","IEEE","IEEE Conferences"
"Risk analysis in software design","D. Verdon; G. McGraw","NA; NA","IEEE Security & Privacy","","2004","2","4","79","84","Risk analysis is, at best, a good general-purpose yardstick by which we can judge our security design's effectiveness. Because roughly 50 percent of security problems are the result of design flaws, performing a risk analysis at the design level is an important part of a solid software security program. Taking the trouble to apply risk-analysis methods at the design level for any application often yields valuable, business-relevant results. The risk analysis process is continuous and applies to many different levels, at once identifying system-level vulnerabilities, assigning probability arid impact, arid determining reasonable mitigation strategies. The paper looks at how, by considering the resulting ranked risks, business stakeholders can determine how to manage particular risks and what the most cost-effective controls might be.","1540-7993;1558-4046","","10.1109/MSP.2004.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1324606","software development;software design;misuse cases;abuse cases","Risk analysis;Software design;Costs;Computer security;Data security;Probability;Cryptography;Hardware;Acceleration;Life testing","risk analysis;software engineering;security","risk analysis;design-level analysis;software design;good judgement call;vulnerabilities;threats;impacts;probability","","36","6","","","","","","IEEE","IEEE Journals & Magazines"
"Proceedings of the ASP-DAC 2001. Asia and South Pacific Design Automation Conference 2001 (Cat. No.01EX455)","","","Proceedings of the ASP-DAC 2001. Asia and South Pacific Design Automation Conference 2001 (Cat. No.01EX455)","","2001","","","0_1","","","","0-7803-6633","10.1109/ASPDAC.2001.913257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=913257","","","integrated circuit design;monolithic integrated circuits;digital integrated circuits;analogue integrated circuits;circuit CAD;circuit layout CAD;high level synthesis;circuit simulation;circuit optimisation;low-power electronics;VLSI;design for manufacture;logic CAD;built-in self test;asynchronous circuits","codesigning;system level specification;system level simulation;sequential verification;interconnect design optimisation;equivalence checking;parasitic extraction;PLA-based logic synthesis;low power design methodology;asynchronous system design;analog design methodology;BIST;DSM design;signal integrity;compilation techniques;embedded software;multilevel logic optimization;high level DFT;performance driven floorplaning;performance driven placement;delay estimation;power estimation;timing optimisation;design space exploration;FPGA;processor synthesis;IC design automation","","","","","","","","","IEEE","IEEE Conferences"
"ICCAD 2004. International Conference on Computer Aided Design (IEEE Cat. No.04CH37606)","","","IEEE/ACM International Conference on Computer Aided Design, 2004. ICCAD-2004.","","2004","","","0_1","","The following topics are dealt with: statistical modeling and optimization methodologies: system-level energy management; equivalence verification; advances in interconnect analysis; soft error rate analysis; application specific memory and processor architecture design techniques; statistical static timer; crosstalk-aware timing and noise analysis; system software optimizations; formal verification; algorithms and modeling techniques for bio and nano technologies; developments in timing analysis and optimization; energy efficiency and interconnect design; floorplanning for advanced technologies; robust design tools; variability impact on design; architectural issues in system synthesis; integrated placement applications; logic synthesis; interconnect coding; statistical timing methods; power grid analysis; clocking; analog and digital diagnosis; design metrics; analog/RF macromodeling and simulation; design manufacturing interface; FPGA; high-level design; power analysis; routing; analog sizing; test generation; system level modeling and design; and mixed-signal modeling and design.","1092-3152","0-7803-8702","10.1109/ICCAD.2004.1382513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382513","","","integrated circuit design;integrated circuit interconnections;field programmable gate arrays;integrated circuit testing;statistical analysis;integrated circuit noise;integrated circuit modelling;high level synthesis;mixed analogue-digital integrated circuits;formal verification","statistical modeling;optimization methodologies;system-level energy management;equivalence verification;interconnect analysis;soft error rate analysis;application specific memory;processor architecture design techniques;statistical static timer;crosstalk-aware timing;noise analysis;system software optimizations;formal verification;bio technologies;nano technologies;timing analysis;energy efficiency;floorplanning;robust design tools;system synthesis;integrated placement applications;logic synthesis;interconnect coding;statistical timing methods;power grid analysis;analog diagnosis;digital diagnosis;design metrics;analog/RF macromodeling;simulation;design manufacturing interface;FPGA;high-level design;power analysis;analog sizing;test generation;system level modeling;mixed-signal modeling","","","","","","","","","IEEE","IEEE Conferences"
"Typographical, logic, or execution bug? [Book Review]","M. J. Lutz","Rochester Institute of Technology","Computer","","2001","34","7","93","93","","0018-9162;1558-0814","","10.1109/MC.2001.933512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=933512","","Book reviews;Logic;Robustness;Production;Application software;Software debugging;Software tools;Optimizing compilers;Automatic testing;System testing","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"The Ninth Intersociety Conference on Thermal and Thermomechanical Phenomena In Electronic Systems (IEEE Cat. No.04CH37543)","","","The Ninth Intersociety Conference on Thermal and Thermomechanical Phenomena In Electronic Systems (IEEE Cat. No.04CH37543)","","2004","2","","","","The following topics are dealt with: thermal management; thermal interface materials; experimental methods and procedures; natural convection and passive thermal management of electronics; thermal and packaging optimization in telecommunications; forced convection cooling; refrigerated systems; thermal design and sustainability; airborne and space system; heat sinks; micro and meso scale thermal management; fundamentals and applications of conduction; thermal simulation and modeling; system level thermal design of electronic equipment; data center thermal management; package characterization and modeling; novel cooling methods for microelectronics; software tools and techniques; thermal management of microelectronic systems; heat pipes and thermosyphons; high flux liquid cooling techniques; mechanics of materials; lead free solder materials and reliability; constitutive modeling and viscoplasticity; failure mechanics and damage modeling; package level reliability; modeling and simulation; nanoscale phenomena in microelectronics; green packaging; optoelectronics; MEMS modeling and packaging; nanoscale thermal effects in electronics; MEMS thermal management.","","0-7803-8357","10.1109/ITHERM.2004.1318202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318202","","","thermomechanical treatment;thermal management (packaging);natural convection;forced convection;cooling;refrigeration;heat sinks;heat conduction;integrated circuit packaging;integrated circuit reliability;integrated circuit modelling;thermoelectricity;thermoelectric devices;thermal conductivity;printed circuit design;heat pipes;soldering;electronics packaging;micromechanical devices;life testing","thermomechanical effects;thermal interface materials;natural convection;passive thermal management;electronics packaging optimization;telecommunication optimization;forced convection cooling;refrigerated systems;thermal design;airborne system;space system;heat sinks;mesoscale thermal management;microscale thermal management;heat conduction;thermal simulation;thermal modeling;system level thermal design;electronic equipment;data center thermal management;microelectronics;software tools;heat transfer;printed circuit design;life testing;heat pipes;thermosyphons;high flux liquid cooling techniques;lead free solder materials;reliability;constitutive modeling;viscoplasticity;failure mechanics;nanoscale phenomena;green packaging;optoelectronics;MEMS;nanoscale thermal effects","","","","","","","","","IEEE","IEEE Conferences"
"A D&T Roundtable: Design automation in Europe","A. Jerraya; J. Borel; A. Sauer; W. Rosenstiel; F. Ghenassia; E. Perea","NA; NA; NA; NA; NA; NA","IEEE Design & Test of Computers","","1999","16","4","90","95","Electronic system design automation is becoming the enabling technology in several key domains, including the mobile telecommunications, consumer, and automotive industry segments. European authorities realize that leading-edge companies in these industries must develop strong research and development in system design to maintain their leading positions. This is why the authorities ore heavily funding cooperation between industries and universities through a 2,000-million-euro (US$2,200 million) prolect called MEDEA (MicroEIectronics Development for European Applications). The goal is to master new electronic system design automation technologies, including hardware-software codesign and mixed analog/digital systems. In this roundtable, representatives of academia, industry, and European organizations evaluate the existing technologies and predict their future. The roundtable was held last March at the internotionol conference on Design and Test in Europe (DATE99) in Munich. IEEE Design & Test thanks roundtable participants Joseph Borel (STMicroelectronics), Anton Souer (MEDEA Office), Wolfgang Rosenstiel (University of Tuebingen), Frank Ghenossio (STMicroelectronics), and Ernest0 Perea (STMicroelectronics) D&Tgrotefully acknowledges the help of Ahmed Jerraya (TIMA), and Soman Adham (LogicVision) and Adam Osseiron (Fluence), who helped organize the roundtable and acted as recorders.","0740-7475;1558-1918","","10.1109/MDT.1999.808227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=808227","","Design automation;Europe;Productivity;Automotive engineering;Educational institutions;Optimizing compilers;System-on-a-chip;System-level design;Circuit testing;Silicon","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Proceedings. SCCC'99 XIX International Conference of the Chilean Computer Science Society","","","Proceedings. SCCC'99 XIX International Conference of the Chilean Computer Science Society","","1999","","","i","","","1522-4902","0-7695-0296","10.1109/SCCC.1999.810146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810146","","","computer science","computer science;computational reflection;optimistic distributed simulations;behaviour nets;intelligent agents;learning algorithms;neural net structure;replication techniques;CORBA;virtual guides;SAGRES virtual museum;decision tree-based paraconsistent learning;configuration mechanism;versioned documents;3-layer DSS;multi-objective value analysis;software product internationalization;probabilistic distributed batch scheduling;software testing quality assessment;interprocedural level;architectural styles;finite state machine validation;mutation testing;object-oriented parallel tabu search algorithm;task scheduling;automated performance prediction;bulk-synchronous parallel discrete-event simulation;adaptive square triangulations;multiresolution model;volume visualization;(d,s,N)-bus network problem;object-oriented multi-threaded languages;abstract data types;genetic algorithm construction;Bayesian network;face detection;legacy systems reengineering;software patterns;programming language;temporal deductive databases;concurrency;robots;scalable WWW cache server;multiresolution volume representation;wavelets;Firewire;industrial network;sorting;SGI Origin 2000;socio-cultural environment;software process modelling","","","","","","","","","IEEE","IEEE Conferences"
"Evolutionary Algorithms for VLSI CAD [Book Review]","R. Drechsler","Caelum Research Corp., CA","IEEE Transactions on Evolutionary Computation","","1999","3","3","251","253","","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.1999.788494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=788494","","Book reviews;Evolutionary computation;Very large scale integration;Design automation;Design optimization;Circuits;Algorithm design and analysis;Genetic algorithms;Application software;Logic testing","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Digest of Papers. Fault-Tolerant Computing: 20th International Symposium [Front Cover]","","","[1990] Digest of Papers. Fault-Tolerant Computing: 20th International Symposium","","1990","","","0_1","","Presents the front cover of the conference proceedings.","","0-8186-2051","10.1109/FTCS.1990.89344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89344","","","fault tolerant computing","test generation;fault location;check pointing;recover;algorithm-based fault-tolerance;yield optimization;modeling;regular geometric structures;fault injection;built-in self test;coding;distributed algorithms;system-level diagnosis;testable design;software fault tolerance;constant testing;evaluation methods;distributed-system architecture","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings of IEEE International Conference on Computer Aided Design (ICCAD) [Front Cover and Table of Contents]","","","1997 Proceedings of IEEE International Conference on Computer Aided Design (ICCAD)","","1997","","","iii","","Presents the table of contents/splash page of the proceedings record.","1092-3152","0-8186-8200","10.1109/ICCAD.1997.643250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643250","","","circuit CAD;circuit layout CAD;logic CAD","decision diagrams;power estimation;optimization;hardware/software codesign;reactive systems;placement;power bounds;block Krylov methods;interconnects;fault simulation;fault diagnosis;logic synthesis;low power;real-time systems;covering problem;code generation;test generation techniques;processor design;high-level power prediction;power reduction;system specification;product engineering;noise analysis;routing;test theory;high-level validation;timing analysis;microelectromechanical systems;high-performance digital circuits;sequential circuits;scheduling techniques;clock design;circuit simulation;encoding;partitioning;analogue modelling;circuit verification;built-in self-test;parasitics extraction;electronic design automation;networks;integrated circuits","","","","","","","","","IEEE","IEEE Conferences"
"Proceedings of Twentieth Euromicro Conference. System Architecture and Integration","","","Proceedings of Twentieth Euromicro Conference. System Architecture and Integration","","1994","","","0_1","","","","0-8186-6430","10.1109/EURMIC.1994.390424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=390424","","","computer architecture;object-oriented methods;knowledge based systems;fault tolerant computing;distributed processing;neural nets;database management systems","expert systems;design;optimisation;database retrieval techniques;parallel systems;VLSI high-level synthesis;hardware structures;software reuse;performance;object-oriented techniques;real-time system;VLSI testing;testability;databases;protocols;dedicated devices;architecture evaluation;knowledge based systems;parallel architectures;distributed systems;neural nets;FSM synthesis;fault tolerance","","","","","","","","","IEEE","IEEE Conferences"
